{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d7b6f7",
   "metadata": {},
   "source": [
    "# Verification of Shrunken Database\n",
    "\n",
    "**Purpose:** To verify the data integrity of the shrunken database (`chess_games_shrunk.db`) by comparing it against the original database (`chess_games.db`).\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Aggregate Count Comparison:** Check if the total number of records in the `player`, `opening`, and partitioned `player_opening_stats` tables are identical between the two databases.\n",
    "2.  **ID Mapping Reconstruction:** Recreate the logic used to map old, non-sequential IDs to new, sequential IDs.\n",
    "3.  **Random Spot Checks:**\n",
    "    *   Select 200 random players and openings from the original database.\n",
    "    *   Find their corresponding records in the new database using the reconstructed ID mappings.\n",
    "    *   Verify that their core data (name, title, eco) is identical.\n",
    "4.  **Stats Record Spot Check:**\n",
    "    *   Select 200 random records from one of the `player_opening_stats` partitions in the original database.\n",
    "    *   Find the corresponding records in the new database by mapping both `player_id` and `opening_id`.\n",
    "    *   Verify that the game statistics (`num_wins`, `num_draws`, `num_losses`) and `color` are identical.\n",
    "\n",
    "This process will provide strong evidence that the shrinking process was successful and lossless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc140339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DB path: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "Shrunken DB path: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games_shrunk.db\n",
      "Partitions to check: ['A', 'B', 'C', 'D', 'E', 'other']\n",
      "Sample size for spot checks: 200\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils.database.db_utils import get_db_connection\n",
    "\n",
    "# Define paths to the original and new database files\n",
    "project_root = Path.cwd().parent if \"notebooks\" in str(Path.cwd()) else Path.cwd()\n",
    "db_path_original = project_root / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "db_path_shrunk = project_root / \"data\" / \"processed\" / \"chess_games_shrunk.db\"\n",
    "\n",
    "# Define partitions for player_opening_stats\n",
    "partitions = list(\"ABCDE\") + [\"other\"]\n",
    "SAMPLE_SIZE = 200\n",
    "\n",
    "# Print configuration details\n",
    "print(f\"Original DB path: {db_path_original}\")\n",
    "print(f\"Shrunken DB path: {db_path_shrunk}\")\n",
    "print(f\"Partitions to check: {partitions}\")\n",
    "print(f\"Sample size for spot checks: {SAMPLE_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd47c17",
   "metadata": {},
   "source": [
    "### 1. Aggregate Count Comparison\n",
    "First, let's compare the total row counts for the main tables in both databases. They should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5243b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             player_count  opening_count  stats_count counts_match\n",
      "Original DB         44459           3132     12867612          NaN\n",
      "Shrunken DB         44459           3132     12867612          NaN\n"
     ]
    }
   ],
   "source": [
    "def get_db_summary(db_path, db_name):\n",
    "    summary = {}\n",
    "    with get_db_connection(db_path) as con:\n",
    "        summary['player_count'] = con.execute(\"SELECT COUNT(*) FROM player\").fetchone()[0]\n",
    "        summary['opening_count'] = con.execute(\"SELECT COUNT(*) FROM opening\").fetchone()[0]\n",
    "        \n",
    "        total_stats_count = 0\n",
    "        for letter in partitions:\n",
    "            total_stats_count += con.execute(f\"SELECT COUNT(*) FROM player_opening_stats_{letter}\").fetchone()[0]\n",
    "        summary['stats_count'] = total_stats_count\n",
    "        \n",
    "    return pd.Series(summary, name=db_name)\n",
    "\n",
    "summary_original = get_db_summary(db_path_original, \"Original DB\")\n",
    "summary_shrunk = get_db_summary(db_path_shrunk, \"Shrunken DB\")\n",
    "\n",
    "comparison_df = pd.DataFrame([summary_original, summary_shrunk])\n",
    "comparison_df['counts_match'] = comparison_df.iloc[0] == comparison_df.iloc[1]\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2f55e",
   "metadata": {},
   "source": [
    "### 2. Reconstruct ID Mappings\n",
    "To compare individual records, we need to know how the old IDs map to the new ones. The new IDs were generated sequentially based on an alphabetical sort. We'll load these mappings into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2faa633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player mapping created with 44,459 records.\n",
      "   old_id        name title  new_id\n",
      "0   60571   1001Moves  None       1\n",
      "1    6462     2700172  None       2\n",
      "2    4095       A-2-A  None       3\n",
      "3   57770        A-HF  None       4\n",
      "4   20850  A-Haimoura  None       5\n",
      "\n",
      "Opening mapping created with 3,132 records.\n",
      "   old_id  eco                        name  new_id\n",
      "0  246513  A00                 Amar Gambit       1\n",
      "1    1624  A00                Amar Opening       2\n",
      "2   38952  A00  Amar Opening: Paris Gambit       3\n",
      "3   53918  A00            Amsterdam Attack       4\n",
      "4     228  A00         Anderssen's Opening       5\n"
     ]
    }
   ],
   "source": [
    "with get_db_connection(db_path_original) as con_orig:\n",
    "    # Player ID mapping\n",
    "    player_mapping_df = con_orig.execute(\"\"\"\n",
    "        SELECT \n",
    "            id as old_id, \n",
    "            name,\n",
    "            title,\n",
    "            ROW_NUMBER() OVER (ORDER BY name) as new_id\n",
    "        FROM player\n",
    "    \"\"\").df()\n",
    "\n",
    "    # Opening ID mapping\n",
    "    opening_mapping_df = con_orig.execute(\"\"\"\n",
    "        SELECT \n",
    "            id as old_id,\n",
    "            eco,\n",
    "            name,\n",
    "            ROW_NUMBER() OVER (ORDER BY eco, name) as new_id\n",
    "        FROM opening\n",
    "    \"\"\").df()\n",
    "\n",
    "print(f\"Player mapping created with {len(player_mapping_df):,} records.\")\n",
    "print(player_mapping_df.head())\n",
    "print(f\"\\nOpening mapping created with {len(opening_mapping_df):,} records.\")\n",
    "print(opening_mapping_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e27e51",
   "metadata": {},
   "source": [
    "### 3. Spot Check `player` and `opening` Tables\n",
    "Now, we'll take a random sample from the original `player` and `opening` tables and ensure their data exists and is correct in the new database under the new ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_spot_checks(sample_df, shrunk_con, table_name, id_col='new_id'):\n",
    "    \"\"\"\n",
    "    Verifies a sample DataFrame against a table in the shrunken database.\n",
    "    Returns a DataFrame with original and shrunken data for comparison.\n",
    "    \"\"\"\n",
    "    shrunk_data = []\n",
    "    for _, row in sample_df.iterrows():\n",
    "        lookup_id = row[id_col]\n",
    "        \n",
    "        # Fetch the corresponding record from the shrunken DB\n",
    "        shrunk_record = shrunk_con.execute(f\"SELECT * FROM {table_name} WHERE id = ?\", [lookup_id]).fetchone()\n",
    "        \n",
    "        if shrunk_record:\n",
    "            shrunk_data.append({desc[0]: val for desc, val in zip(shrunk_con.description, shrunk_record)})\n",
    "        else:\n",
    "            # Append a placeholder if not found\n",
    "            shrunk_data.append(None)\n",
    "\n",
    "    # Prepare dataframes for comparison\n",
    "    original_df = sample_df.reset_index(drop=True)\n",
    "    shrunk_df = pd.DataFrame(shrunk_data).reset_index(drop=True)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    original_df.columns = [f\"orig_{c}\" for c in original_df.columns]\n",
    "    shrunk_df.columns = [f\"shrunk_{c}\" for c in shrunk_df.columns]\n",
    "    \n",
    "    # Combine and display\n",
    "    comparison_df = pd.concat([original_df, shrunk_df], axis=1)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "with get_db_connection(db_path_shrunk) as con_shrunk:\n",
    "    print(\"--- Verifying Player Table ---\")\n",
    "    player_sample = player_mapping_df.sample(n=SAMPLE_SIZE)\n",
    "    player_comparison = verify_spot_checks(player_sample, con_shrunk, 'player')\n",
    "    \n",
    "    print(f\"Displaying {SAMPLE_SIZE} random player records for comparison:\")\n",
    "    display(player_comparison)\n",
    "\n",
    "    print(\"\\n--- Verifying Opening Table ---\")\n",
    "    opening_sample = opening_mapping_df.sample(n=SAMPLE_SIZE)\n",
    "    opening_comparison = verify_spot_checks(opening_sample, con_shrunk, 'opening')\n",
    "\n",
    "    print(f\"Displaying {SAMPLE_SIZE} random opening records for comparison:\")\n",
    "    display(opening_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d6926",
   "metadata": {},
   "source": [
    "### 4. Spot Check `player_opening_stats` Table\n",
    "This is the most critical check. We'll sample from a stats partition in the original DB, map both the `player_id` and `opening_id` to their new values, and verify that the full record (including win/draw/loss counts) is identical in the new DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we'll check the 'A' partition\n",
    "partition_to_check = 'A'\n",
    "stats_table = f\"player_opening_stats_{partition_to_check}\"\n",
    "\n",
    "with get_db_connection(db_path_original) as con_orig, get_db_connection(db_path_shrunk) as con_shrunk:\n",
    "    print(f\"--- Verifying {stats_table} ---\")\n",
    "    \n",
    "    # Get a random sample from the original stats table\n",
    "    original_stats_sample_df = con_orig.execute(f\"SELECT * FROM {stats_table} ORDER BY RANDOM() LIMIT {SAMPLE_SIZE}\").df()\n",
    "    \n",
    "    # Add new IDs to the sample dataframe for easy lookup\n",
    "    stats_sample_merged = original_stats_sample_df.merge(\n",
    "        player_mapping_df[['old_id', 'new_id']], left_on='player_id', right_on='old_id', suffixes=('', '_player')\n",
    "    ).merge(\n",
    "        opening_mapping_df[['old_id', 'new_id']], left_on='opening_id', right_on='old_id', suffixes=('', '_opening')\n",
    "    )\n",
    "    stats_sample_merged.rename(columns={'new_id': 'new_player_id', 'new_id_opening': 'new_opening_id'}, inplace=True)\n",
    "\n",
    "    shrunk_data = []\n",
    "    for _, row in stats_sample_merged.iterrows():\n",
    "        # Query the new DB with the new composite primary key\n",
    "        shrunk_record = con_shrunk.execute(\n",
    "            f\"SELECT * FROM {stats_table} WHERE player_id = ? AND opening_id = ? AND color = ?\",\n",
    "            [row['new_player_id'], row['new_opening_id'], row['color']]\n",
    "        ).fetchone()\n",
    "\n",
    "        if shrunk_record:\n",
    "            shrunk_data.append({desc[0]: val for desc, val in zip(con_shrunk.description, shrunk_record)})\n",
    "        else:\n",
    "            shrunk_data.append(None)\n",
    "\n",
    "    # Prepare dataframes for comparison\n",
    "    original_df = stats_sample_merged.reset_index(drop=True)\n",
    "    shrunk_df = pd.DataFrame(shrunk_data).reset_index(drop=True)\n",
    "    \n",
    "    original_df.columns = [f\"orig_{c}\" for c in original_df.columns]\n",
    "    shrunk_df.columns = [f\"shrunk_{c}\" for c in shrunk_df.columns]\n",
    "    \n",
    "    comparison_df = pd.concat([original_df, shrunk_df], axis=1)\n",
    "\n",
    "    print(f\"Displaying {SAMPLE_SIZE} random stats records from '{stats_table}' for comparison:\")\n",
    "    display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c49b4",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "If all checks above passed, we can be highly confident that the database shrinking process was successful and did not result in data loss. The size reduction is primarily due to:\n",
    "1.  **Rebuilding the file:** Exporting to Parquet and re-importing into a new file eliminates all historical data bloat and fragmentation from past transactions (the biggest factor).\n",
    "2.  **Sequential Primary Keys:** Using sequential integers for IDs is slightly more space-efficient than using non-sequential ones, though this is a minor factor compared to rebuilding.\n",
    "3.  **Data Type Optimization:** Changing `VARCHAR` to a 1-byte `ENUM` for the `color` column and `INTEGER` to `SMALLINT` for `num_draws` saves several bytes per record across millions of rows, adding up to a significant saving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
