{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dab4527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database path: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "Partitions: ['A', 'B', 'C', 'D', 'E', 'other']\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "from pathlib import Path\n",
    "from utils.database.db_utils import get_db_connection\n",
    " \n",
    "# Define the path to the DuckDB database file\n",
    "project_root = Path.cwd().parent if \"notebooks\" in str(Path.cwd()) else Path.cwd()\n",
    "db_path = project_root / \"data\" / \"processed\" / \"chess_games.db\"\n",
    " \n",
    "# Define partitions for player_opening_stats\n",
    "partitions = list(\"ABCDE\") + [\"other\"]\n",
    " \n",
    "# Print configuration details\n",
    "print(f\"Database path: {db_path}\")\n",
    "print(f\"Partitions: {partitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014e89b",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "To decrease the size of our opening stats DB, making queries faster and more efficient.\n",
    "\n",
    "# Methods:\n",
    "\n",
    "    1. Change INTs to SMALLINT\n",
    "        - player_id and opening_id\n",
    "        - num_draws\n",
    "            - Not doing this on num_wins and num_losses because those may exceed the limit of this data type (~32,000)\n",
    "        - Not doing this with player_opening_stats ID because that's a composite string, not an int \n",
    "        - Saves 2 bytes per entry\n",
    "    \n",
    "    2. Change `color` VARCHAR to w/b enum\n",
    "        - 1b\n",
    "        - Compared to the 12-16bytes the previous varchar took up\n",
    "        \n",
    "# New Strategy: Export and Re-import\n",
    "\n",
    "The previous in-place compaction strategy caused the database file to bloat due to how DuckDB handles large transactions and table modifications. The new approach is to export the optimized data to temporary Parquet files and then create a completely new, clean database from them. This ensures the final file is as small as possible, with no leftover transactional history or fragmentation.\n",
    "\n",
    "# Considerations\n",
    "    - Saving a copy of the DB in case I bork this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1574311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current database file size: 1385.51 MB\n"
     ]
    }
   ],
   "source": [
    "# Print current database size\n",
    "import os\n",
    "try:\n",
    "    db_size_bytes = os.path.getsize(db_path)\n",
    "    db_size_mb = db_size_bytes / (1024 * 1024)\n",
    "    print(f\"Current database file size: {db_size_mb:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Database file not found.\")\n",
    "    db_size_mb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9870cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CURRENT TABLE SCHEMAS ===\n",
      "\n",
      "PLAYER table schema:\n",
      "  id: INTEGER\n",
      "  name: VARCHAR\n",
      "  title: VARCHAR\n",
      "\n",
      "OPENING table schema:\n",
      "  id: INTEGER\n",
      "  eco: VARCHAR\n",
      "  name: VARCHAR\n",
      "\n",
      "PLAYER_OPENING_STATS_A table schema:\n",
      "  player_id: INTEGER\n",
      "  opening_id: INTEGER\n",
      "  color: VARCHAR\n",
      "  num_wins: INTEGER\n",
      "  num_draws: INTEGER\n",
      "  num_losses: INTEGER\n",
      "\n",
      "=== DATA RANGES ===\n",
      "Max player_id: 32,964,861\n",
      "Max opening_id: 5,222,927\n",
      "player_opening_stats_A: 3,137,793 rows\n",
      "player_opening_stats_B: 3,339,867 rows\n",
      "player_opening_stats_C: 4,175,489 rows\n",
      "player_opening_stats_D: 1,771,597 rows\n",
      "player_opening_stats_E: 442,866 rows\n",
      "player_opening_stats_other: 0 rows\n",
      "\n",
      "Total stats rows: 12,867,612\n",
      "\n",
      "Color distribution in stats_A: [('b', 2029230), ('w', 1108563)]\n",
      "\n",
      "num_draws in stats_A - Min: 0, Max: 480, Avg: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic - check actual table schemas and data counts\n",
    "with get_db_connection(db_path) as con:\n",
    "    print(\"=== CURRENT TABLE SCHEMAS ===\")\n",
    "    \n",
    "    # Check player table\n",
    "    player_schema = con.execute(\"DESCRIBE player\").fetchall()\n",
    "    print(f\"\\nPLAYER table schema:\")\n",
    "    for col in player_schema:\n",
    "        print(f\"  {col[0]}: {col[1]}\")\n",
    "    \n",
    "    # Check opening table  \n",
    "    opening_schema = con.execute(\"DESCRIBE opening\").fetchall()\n",
    "    print(f\"\\nOPENING table schema:\")\n",
    "    for col in opening_schema:\n",
    "        print(f\"  {col[0]}: {col[1]}\")\n",
    "    \n",
    "    # Check one stats table\n",
    "    stats_schema = con.execute(\"DESCRIBE player_opening_stats_A\").fetchall()\n",
    "    print(f\"\\nPLAYER_OPENING_STATS_A table schema:\")\n",
    "    for col in stats_schema:\n",
    "        print(f\"  {col[0]}: {col[1]}\")\n",
    "    \n",
    "    print(\"\\n=== DATA RANGES ===\")\n",
    "    \n",
    "    # Check max values\n",
    "    max_player_id = con.execute(\"SELECT MAX(id) FROM player\").fetchone()[0]\n",
    "    max_opening_id = con.execute(\"SELECT MAX(id) FROM opening\").fetchone()[0]\n",
    "    print(f\"Max player_id: {max_player_id:,}\")\n",
    "    print(f\"Max opening_id: {max_opening_id:,}\")\n",
    "    \n",
    "    # Check total row counts\n",
    "    total_stats_rows = 0\n",
    "    for letter in ['A', 'B', 'C', 'D', 'E', 'other']:\n",
    "        count = con.execute(f\"SELECT COUNT(*) FROM player_opening_stats_{letter}\").fetchone()[0]\n",
    "        print(f\"player_opening_stats_{letter}: {count:,} rows\")\n",
    "        total_stats_rows += count\n",
    "    \n",
    "    print(f\"\\nTotal stats rows: {total_stats_rows:,}\")\n",
    "    \n",
    "    # Check color values distribution\n",
    "    color_dist = con.execute(\"SELECT color, COUNT(*) FROM player_opening_stats_A GROUP BY color\").fetchall()\n",
    "    print(f\"\\nColor distribution in stats_A: {color_dist}\")\n",
    "    \n",
    "    # Check num_draws distribution\n",
    "    draws_stats = con.execute(\"SELECT MIN(num_draws), MAX(num_draws), AVG(num_draws) FROM player_opening_stats_A\").fetchone()\n",
    "    print(f\"\\nnum_draws in stats_A - Min: {draws_stats[0]}, Max: {draws_stats[1]}, Avg: {draws_stats[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f7058f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New database will be created at: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games_shrunk.db\n",
      "Temporary export directory: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/temp_export\n",
      "\n",
      "=== ANALYZING SOURCE DATABASE ===\n",
      "Players: 44,459 records\n",
      "Openings: 3,132 records\n",
      "\n",
      "Player IDs can use SMALLINT: False -> Using INTEGER\n",
      "Opening IDs can use SMALLINT: True -> Using SMALLINT\n",
      "\n",
      "=== EXPORTING AND TRANSFORMING DATA ===\n",
      "Exporting compacted player table...\n",
      "Exporting compacted opening table...\n",
      "Exporting and transforming stats tables...\n",
      "  - Exporting player_opening_stats_A to stats_A.parquet...\n",
      "\n",
      "=== ANALYZING SOURCE DATABASE ===\n",
      "Players: 44,459 records\n",
      "Openings: 3,132 records\n",
      "\n",
      "Player IDs can use SMALLINT: False -> Using INTEGER\n",
      "Opening IDs can use SMALLINT: True -> Using SMALLINT\n",
      "\n",
      "=== EXPORTING AND TRANSFORMING DATA ===\n",
      "Exporting compacted player table...\n",
      "Exporting compacted opening table...\n",
      "Exporting and transforming stats tables...\n",
      "  - Exporting player_opening_stats_A to stats_A.parquet...\n",
      "  - Exporting player_opening_stats_B to stats_B.parquet...\n",
      "  - Exporting player_opening_stats_C to stats_C.parquet...\n",
      "  - Exporting player_opening_stats_B to stats_B.parquet...\n",
      "  - Exporting player_opening_stats_C to stats_C.parquet...\n",
      "  - Exporting player_opening_stats_D to stats_D.parquet...\n",
      "  - Exporting player_opening_stats_E to stats_E.parquet...\n",
      "  - Exporting player_opening_stats_other to stats_other.parquet...\n",
      "  - Exporting player_opening_stats_D to stats_D.parquet...\n",
      "  - Exporting player_opening_stats_E to stats_E.parquet...\n",
      "  - Exporting player_opening_stats_other to stats_other.parquet...\n",
      "\n",
      "=== EXPORT COMPLETE ===\n",
      "\n",
      "=== CREATING NEW DATABASE AND IMPORTING DATA ===\n",
      "Creating new tables with optimized schema...\n",
      "Importing player and opening data...\n",
      "Creating and importing stats tables...\n",
      "  - Creating and populating player_opening_stats_A...\n",
      "\n",
      "=== EXPORT COMPLETE ===\n",
      "\n",
      "=== CREATING NEW DATABASE AND IMPORTING DATA ===\n",
      "Creating new tables with optimized schema...\n",
      "Importing player and opening data...\n",
      "Creating and importing stats tables...\n",
      "  - Creating and populating player_opening_stats_A...\n",
      "  - Creating and populating player_opening_stats_B...\n",
      "  - Creating and populating player_opening_stats_B...\n",
      "  - Creating and populating player_opening_stats_C...\n",
      "  - Creating and populating player_opening_stats_C...\n",
      "  - Creating and populating player_opening_stats_D...\n",
      "  - Creating and populating player_opening_stats_D...\n",
      "  - Creating and populating player_opening_stats_E...\n",
      "  - Creating and populating player_opening_stats_E...\n",
      "  - Creating and populating player_opening_stats_other...\n",
      "Recreating the consolidated view...\n",
      "\n",
      "=== COMPACTION AND RE-IMPORT COMPLETE ===\n",
      "Final player IDs: 1 to 44,459 (44,459 records)\n",
      "Final opening IDs: 1 to 3,132 (3,132 records)\n",
      "\n",
      "Total optimized records: 12,867,612\n",
      "\n",
      "Temporary export directory cleaned up: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/temp_export\n",
      "  - Creating and populating player_opening_stats_other...\n",
      "Recreating the consolidated view...\n",
      "\n",
      "=== COMPACTION AND RE-IMPORT COMPLETE ===\n",
      "Final player IDs: 1 to 44,459 (44,459 records)\n",
      "Final opening IDs: 1 to 3,132 (3,132 records)\n",
      "\n",
      "Total optimized records: 12,867,612\n",
      "\n",
      "Temporary export directory cleaned up: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/temp_export\n"
     ]
    }
   ],
   "source": [
    "# Execute PROPER database shrinking - export and re-import to a new DB file\n",
    "if db_size_mb > 0:\n",
    "    # Define paths for the new database and temporary export directory\n",
    "    db_path_new = db_path.with_name(f\"{db_path.stem}_shrunk.db\")\n",
    "    export_dir = project_root / \"data\" / \"processed\" / \"temp_export\"\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"New database will be created at: {db_path_new}\")\n",
    "    print(f\"Temporary export directory: {export_dir}\")\n",
    "\n",
    "    with get_db_connection(db_path) as con:\n",
    "        print(\"\\n=== ANALYZING SOURCE DATABASE ===\")\n",
    "        player_count = con.execute('SELECT COUNT(*) FROM player').fetchone()[0]\n",
    "        opening_count = con.execute('SELECT COUNT(*) FROM opening').fetchone()[0]\n",
    "        \n",
    "        print(f'Players: {player_count:,} records')\n",
    "        print(f'Openings: {opening_count:,} records')\n",
    "\n",
    "        # Determine optimal integer types\n",
    "        player_id_type = \"SMALLINT\" if player_count <= 32767 else \"INTEGER\"\n",
    "        opening_id_type = \"SMALLINT\" if opening_count <= 32767 else \"INTEGER\"\n",
    "        print(f'\\nPlayer IDs can use SMALLINT: {player_id_type == \"SMALLINT\"} -> Using {player_id_type}')\n",
    "        print(f'Opening IDs can use SMALLINT: {opening_id_type == \"SMALLINT\"} -> Using {opening_id_type}')\n",
    "\n",
    "        # --- EXPORT PHASE ---\n",
    "        print(\"\\n=== EXPORTING AND TRANSFORMING DATA ===\")\n",
    "\n",
    "        # Export compacted player table\n",
    "        print(\"Exporting compacted player table...\")\n",
    "        con.execute(f\"\"\"\n",
    "            COPY (\n",
    "                SELECT \n",
    "                    ROW_NUMBER() OVER (ORDER BY name) as new_id,\n",
    "                    name,\n",
    "                    title,\n",
    "                    id as old_id\n",
    "                FROM player\n",
    "            ) TO '{export_dir / 'player_mapping.parquet'}' (FORMAT PARQUET);\n",
    "        \"\"\")\n",
    "\n",
    "        # Export compacted opening table\n",
    "        print(\"Exporting compacted opening table...\")\n",
    "        con.execute(f\"\"\"\n",
    "            COPY (\n",
    "                SELECT \n",
    "                    ROW_NUMBER() OVER (ORDER BY eco, name) as new_id,\n",
    "                    eco,\n",
    "                    name,\n",
    "                    id as old_id\n",
    "                FROM opening\n",
    "            ) TO '{export_dir / 'opening_mapping.parquet'}' (FORMAT PARQUET);\n",
    "        \"\"\")\n",
    "\n",
    "        # Export and transform stats tables\n",
    "        print(\"Exporting and transforming stats tables...\")\n",
    "        for letter in partitions:\n",
    "            old_table = f\"player_opening_stats_{letter}\"\n",
    "            export_path = export_dir / f'stats_{letter}.parquet'\n",
    "            print(f\"  - Exporting {old_table} to {export_path.name}...\")\n",
    "            \n",
    "            con.execute(f\"\"\"\n",
    "                COPY (\n",
    "                    SELECT \n",
    "                        s.player_id as old_player_id,\n",
    "                        s.opening_id as old_opening_id,\n",
    "                        s.color,\n",
    "                        s.num_wins,\n",
    "                        s.num_draws,\n",
    "                        s.num_losses\n",
    "                    FROM {old_table} s\n",
    "                    WHERE s.num_wins > 0 OR s.num_draws > 0 OR s.num_losses > 0\n",
    "                ) TO '{export_path}' (FORMAT PARQUET);\n",
    "            \"\"\")\n",
    "\n",
    "    print(\"\\n=== EXPORT COMPLETE ===\")\n",
    "\n",
    "    # --- IMPORT PHASE ---\n",
    "    # Delete the new DB file if it exists to ensure a fresh start\n",
    "    if db_path_new.exists():\n",
    "        db_path_new.unlink()\n",
    "        print(f\"\\nDeleted existing new database file: {db_path_new}\")\n",
    "\n",
    "    with get_db_connection(db_path_new) as con_new:\n",
    "        print(\"\\n=== CREATING NEW DATABASE AND IMPORTING DATA ===\")\n",
    "        \n",
    "        # Create new tables\n",
    "        print(\"Creating new tables with optimized schema...\")\n",
    "        con_new.execute(f\"\"\"\n",
    "            CREATE TABLE player (\n",
    "                id      {player_id_type} PRIMARY KEY,\n",
    "                name    VARCHAR NOT NULL,\n",
    "                title   VARCHAR\n",
    "            );\n",
    "        \"\"\")\n",
    "        con_new.execute(f\"\"\"\n",
    "            CREATE TABLE opening (\n",
    "                id      {opening_id_type} PRIMARY KEY,\n",
    "                eco     VARCHAR(3) NOT NULL,\n",
    "                name    VARCHAR NOT NULL\n",
    "            );\n",
    "        \"\"\")\n",
    "        con_new.execute(\"CREATE TYPE color_enum AS ENUM ('w', 'b');\")\n",
    "\n",
    "        # Import player and opening data\n",
    "        print(\"Importing player and opening data...\")\n",
    "        con_new.execute(f\"INSERT INTO player (id, name, title) SELECT new_id, name, title FROM read_parquet('{export_dir / 'player_mapping.parquet'}');\")\n",
    "        con_new.execute(f\"INSERT INTO opening (id, eco, name) SELECT new_id, eco, name FROM read_parquet('{export_dir / 'opening_mapping.parquet'}');\")\n",
    "\n",
    "        # Create and import stats tables\n",
    "        print(\"Creating and importing stats tables...\")\n",
    "        for letter in partitions:\n",
    "            new_table = f\"player_opening_stats_{letter}\"\n",
    "            print(f\"  - Creating and populating {new_table}...\")\n",
    "            \n",
    "            con_new.execute(f\"\"\"\n",
    "                CREATE TABLE {new_table} (\n",
    "                    player_id   {player_id_type} NOT NULL,\n",
    "                    opening_id  {opening_id_type} NOT NULL,\n",
    "                    color       color_enum NOT NULL,\n",
    "                    num_wins    INTEGER DEFAULT 0,\n",
    "                    num_draws   SMALLINT DEFAULT 0,\n",
    "                    num_losses  INTEGER DEFAULT 0,\n",
    "                    PRIMARY KEY (player_id, opening_id, color)\n",
    "                );\n",
    "            \"\"\")\n",
    "\n",
    "            con_new.execute(f\"\"\"\n",
    "                INSERT INTO {new_table}\n",
    "                WITH player_mapping AS (\n",
    "                    SELECT old_id, new_id FROM read_parquet('{export_dir / 'player_mapping.parquet'}')\n",
    "                ),\n",
    "                opening_mapping AS (\n",
    "                    SELECT old_id, new_id FROM read_parquet('{export_dir / 'opening_mapping.parquet'}')\n",
    "                )\n",
    "                SELECT \n",
    "                    pm.new_id AS player_id,\n",
    "                    om.new_id AS opening_id,\n",
    "                    s.color::color_enum AS color,\n",
    "                    s.num_wins,\n",
    "                    s.num_draws::SMALLINT,\n",
    "                    s.num_losses\n",
    "                FROM read_parquet('{export_dir / f'stats_{letter}.parquet'}') s\n",
    "                JOIN player_mapping pm ON s.old_player_id = pm.old_id\n",
    "                JOIN opening_mapping om ON s.old_opening_id = om.old_id;\n",
    "            \"\"\")\n",
    "\n",
    "        # Recreate the view\n",
    "        print(\"Recreating the consolidated view...\")\n",
    "        union_selects = \"\\nUNION ALL\\n\".join([\n",
    "            f\"SELECT * FROM player_opening_stats_{letter}\"\n",
    "            for letter in partitions\n",
    "        ])\n",
    "        con_new.execute(f\"CREATE OR REPLACE VIEW player_opening_stats AS {union_selects};\")\n",
    "        \n",
    "        print(\"\\n=== COMPACTION AND RE-IMPORT COMPLETE ===\")\n",
    "        final_player_stats = con_new.execute('SELECT MIN(id), MAX(id), COUNT(*) FROM player').fetchone()\n",
    "        final_opening_stats = con_new.execute('SELECT MIN(id), MAX(id), COUNT(*) FROM opening').fetchone()\n",
    "        \n",
    "        print(f'Final player IDs: {final_player_stats[0]:,} to {final_player_stats[1]:,} ({final_player_stats[2]:,} records)')\n",
    "        print(f'Final opening IDs: {final_opening_stats[0]:,} to {final_opening_stats[1]:,} ({final_opening_stats[2]:,} records)')\n",
    "        \n",
    "        total_final_records = 0\n",
    "        for letter in partitions:\n",
    "            count = con_new.execute(f'SELECT COUNT(*) FROM player_opening_stats_{letter}').fetchone()[0]\n",
    "            total_final_records += count\n",
    "        print(f'\\nTotal optimized records: {total_final_records:,}')\n",
    "\n",
    "# Clean up temporary files\n",
    "import shutil\n",
    "shutil.rmtree(export_dir)\n",
    "print(f\"\\nTemporary export directory cleaned up: {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02881c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shrunken database file size: 655.51 MB\n",
      "Vacuuming the new database to optimize storage...\n",
      "Vacuum complete.\n",
      "Post-vacuum shrunken database file size: 655.51 MB\n"
     ]
    }
   ],
   "source": [
    "# Print new database size and vacuum\n",
    "try:\n",
    "    db_path_new = db_path.with_name(f\"{db_path.stem}_shrunk.db\")\n",
    "    db_size_bytes = os.path.getsize(db_path_new)\n",
    "    db_size_mb = db_size_bytes / (1024 * 1024)\n",
    "    print(f\"New shrunken database file size: {db_size_mb:.2f} MB\")\n",
    " \n",
    "    with get_db_connection(db_path_new) as con:\n",
    "        print(\"Vacuuming the new database to optimize storage...\")\n",
    "        con.execute(\"VACUUM;\")\n",
    "        print(\"Vacuum complete.\")\n",
    " \n",
    "    db_size_bytes = os.path.getsize(db_path_new)\n",
    "    db_size_mb = db_size_bytes / (1024 * 1024)\n",
    "    print(f\"Post-vacuum shrunken database file size: {db_size_mb:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"New database file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff22530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shrunken database file size: 655.51 MB\n"
     ]
    }
   ],
   "source": [
    "# Final check on the new database\n",
    "try:\n",
    "    db_path_new = db_path.with_name(f\"{db_path.stem}_shrunk.db\")\n",
    "    db_size_bytes = os.path.getsize(db_path_new)\n",
    "    db_size_mb = db_size_bytes / (1024 * 1024)\n",
    "    print(f\"Final shrunken database file size: {db_size_mb:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"New database file not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
