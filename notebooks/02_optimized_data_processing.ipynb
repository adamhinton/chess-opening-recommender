{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f045a790",
   "metadata": {},
   "source": [
    "# Optimized Chess Data Processing\n",
    "\n",
    "This notebook implements efficient strategies for processing a large PGN database, focusing on:\n",
    "\n",
    "1. **Early filtering** - Filter games during reading to avoid loading unnecessary data\n",
    "2. **Incremental processing** - Process in chunks and save progress \n",
    "3. **Parallelization** - Use multiple CPU cores where beneficial\n",
    "4. **Type safety** - Strong typing for better IDE support and code quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b9ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from typing import TypedDict, Optional, Dict, List, Set, Iterator, Tuple, Any, Literal, Union\n",
    "import chess.pgn\n",
    "import zstandard as zstd\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05150e32",
   "metadata": {},
   "source": [
    "## Type Definitions\n",
    "\n",
    "First, let's define strong types for our data structures to ensure type safety and better IDE support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30e4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define types for game results\n",
    "GameResult = Literal[\"1-0\", \"0-1\", \"1/2-1/2\", \"*\"]\n",
    "\n",
    "# Time control categories\n",
    "TimeControlCategory = Literal[\"bullet\", \"blitz\", \"rapid\", \"classical\", \"correspondence\", \"unknown\"]\n",
    "\n",
    "class GameHeaders(TypedDict, total=False):\n",
    "    \"\"\"Type for game headers extracted from PGN.\"\"\"\n",
    "    Event: str\n",
    "    Site: str\n",
    "    Date: str\n",
    "    White: str\n",
    "    Black: str\n",
    "    Result: GameResult\n",
    "    WhiteElo: str\n",
    "    BlackElo: str\n",
    "    ECO: str\n",
    "    Opening: str\n",
    "    TimeControl: str\n",
    "    Termination: str\n",
    "    WhiteRatingDiff: str\n",
    "    BlackRatingDiff: str\n",
    "\n",
    "class OpeningResults(TypedDict):\n",
    "    \"\"\"Statistics for a player's results with a particular opening.\"\"\"\n",
    "    opening_name: str\n",
    "    results: Dict[str, Union[int, float]]\n",
    "\n",
    "# Player statistics structure\n",
    "class PlayerStats(TypedDict):\n",
    "    \"\"\"Statistics for an individual player.\"\"\"\n",
    "    rating: int\n",
    "    white_games: Dict[str, OpeningResults]  # ECO code -> results\n",
    "    black_games: Dict[str, OpeningResults]  # ECO code -> results\n",
    "    num_games_total: int\n",
    "\n",
    "# Configuration parameters with defaults\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for the game processing pipeline.\"\"\"\n",
    "    # Filtering parameters\n",
    "    min_elo: int = 1500  # Minimum player Elo to include\n",
    "    exclude_time_controls: Set[str] = field(default_factory=lambda: {\"bullet\", \"hyperbullet\", \"ultrabullet\"})\n",
    "    min_moves: int = 10  # Minimum number of moves in game\n",
    "    \n",
    "    # Processing parameters\n",
    "    chunk_size: int = 100_000  # Number of games to process in each chunk\n",
    "    max_chunks: Optional[int] = None  # Maximum number of chunks to process (None for all)\n",
    "    save_interval: int = 1  # Save after processing this many chunks\n",
    "    \n",
    "    # File paths\n",
    "    save_dir: str = \"../data/processed\"\n",
    "    player_data_file: str = \"player_stats.json\"\n",
    "    progress_file: str = \"processing_progress.json\"\n",
    "    \n",
    "    # Parallelization\n",
    "    use_parallel: bool = True\n",
    "    num_processes: int = multiprocessing.cpu_count() - 1  # Use all but one CPU core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e97568",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define helper functions for time control categorization and game filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fda4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_time_control(time_control: str) -> TimeControlCategory:\n",
    "    \"\"\"\n",
    "    Categorize the time control string into standard categories.\n",
    "    \n",
    "    Args:\n",
    "        time_control: The time control string from the game headers\n",
    "        \n",
    "    Returns:\n",
    "        The category of time control\n",
    "    \"\"\"\n",
    "    # Handle missing or malformed time control\n",
    "    if not time_control or time_control == \"?\" or time_control == \"Unknown\":\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # Split time control into initial time and increment\n",
    "    # Format is typically \"initial+increment\" like \"180+2\" (3 minutes + 2 second increment)\n",
    "    parts = time_control.split(\"+\")\n",
    "    \n",
    "    try:\n",
    "        # Initial time in seconds\n",
    "        initial_time = int(parts[0])\n",
    "        \n",
    "        # Categorize based on initial time\n",
    "        if initial_time < 180:  # Less than 3 minutes\n",
    "            return \"bullet\"\n",
    "        elif initial_time < 600:  # 3-10 minutes\n",
    "            return \"blitz\"\n",
    "        elif initial_time < 1800:  # 10-30 minutes\n",
    "            return \"rapid\"\n",
    "        elif initial_time <= 6000:  # 30-100 minutes\n",
    "            return \"classical\"\n",
    "        else:  # More than 100 minutes\n",
    "            return \"correspondence\"\n",
    "    except (ValueError, IndexError):\n",
    "        # Handle correspondence format like \"1 day\"\n",
    "        if \"day\" in time_control.lower():\n",
    "            return \"correspondence\"\n",
    "        return \"unknown\"\n",
    "\n",
    "def should_include_game(headers: GameHeaders, config: ProcessingConfig) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a game should be included based on filtering criteria.\n",
    "    \n",
    "    Args:\n",
    "        headers: The game headers\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        True if the game should be included, False otherwise\n",
    "    \"\"\"\n",
    "    # Skip games with missing essential information\n",
    "    if not all(key in headers for key in ['White', 'Black', 'Result', 'TimeControl']):\n",
    "        return False\n",
    "    \n",
    "    # Skip games with too low Elo\n",
    "    try:\n",
    "        white_elo = int(headers.get('WhiteElo', '0'))\n",
    "        black_elo = int(headers.get('BlackElo', '0'))\n",
    "        if white_elo < config.min_elo or black_elo < config.min_elo:\n",
    "            return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    # Skip games with excluded time controls\n",
    "    time_control = headers.get('TimeControl', '')\n",
    "    tc_category = categorize_time_control(time_control)\n",
    "    \n",
    "    if tc_category in config.exclude_time_controls:\n",
    "        return False\n",
    "        \n",
    "    # Skip abandoned or incomplete games\n",
    "    if headers.get('Result', '') == '*':\n",
    "        return False\n",
    "    \n",
    "    # Game passes all filters\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d99bd",
   "metadata": {},
   "source": [
    "## Efficient PGN Reading\n",
    "\n",
    "Now let's implement an efficient reader that filters games early and processes in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7709b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_reader(file_path: str, config: ProcessingConfig) -> Iterator[chess.pgn.Game]:\n",
    "    \"\"\"\n",
    "    Generator that reads and yields games from a compressed PGN file,\n",
    "    applying early filtering to avoid unnecessary processing.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the compressed PGN file\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Yields:\n",
    "        Chess games that pass the filtering criteria\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        stream_reader = dctx.stream_reader(f)\n",
    "        text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "        \n",
    "        games_processed = 0\n",
    "        games_included = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            # Read the next game\n",
    "            game = chess.pgn.read_game(text_stream)\n",
    "            \n",
    "            # Check if we've reached the end of the file\n",
    "            if game is None:\n",
    "                break\n",
    "                \n",
    "            games_processed += 1\n",
    "            \n",
    "            # Extract headers for filtering\n",
    "            headers = dict(game.headers)\n",
    "            \n",
    "            # Check if game meets inclusion criteria\n",
    "            if should_include_game(headers, config):\n",
    "                games_included += 1\n",
    "                yield game\n",
    "            \n",
    "            # Print progress periodically\n",
    "            if games_processed % 5_000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                inclusion_rate = (games_included / games_processed) * 100 if games_processed > 0 else 0\n",
    "                print(f\"Processed: {games_processed} games, Included: {games_included} \" +\n",
    "                      f\"({inclusion_rate:.1f}%) in {elapsed:.2f} seconds\")\n",
    "\n",
    "def process_game_chunk(chunk: List[chess.pgn.Game]) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a chunk of games and return player statistics.\n",
    "    \n",
    "    Args:\n",
    "        chunk: A list of chess games to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping player usernames to their statistics\n",
    "    \"\"\"\n",
    "    players_data: Dict[str, PlayerStats] = {}\n",
    "    \n",
    "    for game in chunk:\n",
    "        headers = dict(game.headers)\n",
    "        \n",
    "        white_player = headers['White']\n",
    "        black_player = headers['Black']\n",
    "        white_elo = int(headers.get('WhiteElo', 0))\n",
    "        black_elo = int(headers.get('BlackElo', 0))\n",
    "        result = headers['Result']\n",
    "        eco_code = headers.get('ECO', 'Unknown')\n",
    "        opening_name = headers.get('Opening', 'Unknown Opening')\n",
    "        \n",
    "        # Process white player's game\n",
    "        if white_player not in players_data:\n",
    "            players_data[white_player] = {\n",
    "                \"rating\": white_elo,\n",
    "                \"white_games\": {},\n",
    "                \"black_games\": {},\n",
    "                \"num_games_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Update white player's data\n",
    "        if eco_code not in players_data[white_player][\"white_games\"]:\n",
    "            players_data[white_player][\"white_games\"][eco_code] = {\n",
    "                \"opening_name\": opening_name,\n",
    "                \"results\": {\n",
    "                    \"num_games\": 0,\n",
    "                    \"num_wins\": 0,\n",
    "                    \"num_losses\": 0,\n",
    "                    \"num_draws\": 0,\n",
    "                    \"score_percentage_with_opening\": 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Update game counts\n",
    "        players_data[white_player][\"num_games_total\"] += 1\n",
    "        players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_games\"] += 1\n",
    "        \n",
    "        # Update result counts\n",
    "        if result == \"1-0\":  # White win\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_wins\"] += 1\n",
    "        elif result == \"0-1\":  # Black win (white loss)\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_losses\"] += 1\n",
    "        elif result == \"1/2-1/2\":  # Draw\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_draws\"] += 1\n",
    "            \n",
    "        # Update score percentage\n",
    "        wins = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_wins\"]\n",
    "        draws = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_draws\"]\n",
    "        total = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_games\"]\n",
    "        score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "        players_data[white_player][\"white_games\"][eco_code][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "        \n",
    "        # Similarly process black player's game\n",
    "        if black_player not in players_data:\n",
    "            players_data[black_player] = {\n",
    "                \"rating\": black_elo,\n",
    "                \"white_games\": {},\n",
    "                \"black_games\": {},\n",
    "                \"num_games_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Update black player's data\n",
    "        if eco_code not in players_data[black_player][\"black_games\"]:\n",
    "            players_data[black_player][\"black_games\"][eco_code] = {\n",
    "                \"opening_name\": opening_name,\n",
    "                \"results\": {\n",
    "                    \"num_games\": 0,\n",
    "                    \"num_wins\": 0,\n",
    "                    \"num_losses\": 0,\n",
    "                    \"num_draws\": 0,\n",
    "                    \"score_percentage_with_opening\": 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Update game counts\n",
    "        players_data[black_player][\"num_games_total\"] += 1\n",
    "        players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_games\"] += 1\n",
    "        \n",
    "        # Update result counts\n",
    "        if result == \"0-1\":  # Black win\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_wins\"] += 1\n",
    "        elif result == \"1-0\":  # White win (black loss)\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_losses\"] += 1\n",
    "        elif result == \"1/2-1/2\":  # Draw\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_draws\"] += 1\n",
    "            \n",
    "        # Update score percentage\n",
    "        wins = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_wins\"]\n",
    "        draws = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_draws\"]\n",
    "        total = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_games\"]\n",
    "        score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "        players_data[black_player][\"black_games\"][eco_code][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "    \n",
    "    return players_data\n",
    "\n",
    "def merge_player_stats(data1: Dict[str, PlayerStats], data2: Dict[str, PlayerStats]) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Merge two player statistics dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        data1: First player statistics dictionary\n",
    "        data2: Second player statistics dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Merged player statistics\n",
    "    \"\"\"\n",
    "    merged_data: Dict[str, PlayerStats] = data1.copy()\n",
    "    \n",
    "    for player, stats in data2.items():\n",
    "        if player not in merged_data:\n",
    "            merged_data[player] = stats\n",
    "        else:\n",
    "            # Update total game count\n",
    "            merged_data[player][\"num_games_total\"] += stats[\"num_games_total\"]\n",
    "            \n",
    "            # Update white games\n",
    "            for eco, opening_data in stats[\"white_games\"].items():\n",
    "                if eco not in merged_data[player][\"white_games\"]:\n",
    "                    merged_data[player][\"white_games\"][eco] = opening_data\n",
    "                else:\n",
    "                    # Update results for this opening\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_games\"] += opening_data[\"results\"][\"num_games\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_wins\"] += opening_data[\"results\"][\"num_wins\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_losses\"] += opening_data[\"results\"][\"num_losses\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_draws\"] += opening_data[\"results\"][\"num_draws\"]\n",
    "                    \n",
    "                    # Recalculate score percentage\n",
    "                    wins = merged_data[player][\"white_games\"][eco][\"results\"][\"num_wins\"]\n",
    "                    draws = merged_data[player][\"white_games\"][eco][\"results\"][\"num_draws\"]\n",
    "                    total = merged_data[player][\"white_games\"][eco][\"results\"][\"num_games\"]\n",
    "                    score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "            \n",
    "            # Update black games\n",
    "            for eco, opening_data in stats[\"black_games\"].items():\n",
    "                if eco not in merged_data[player][\"black_games\"]:\n",
    "                    merged_data[player][\"black_games\"][eco] = opening_data\n",
    "                else:\n",
    "                    # Update results for this opening\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_games\"] += opening_data[\"results\"][\"num_games\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_wins\"] += opening_data[\"results\"][\"num_wins\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_losses\"] += opening_data[\"results\"][\"num_losses\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_draws\"] += opening_data[\"results\"][\"num_draws\"]\n",
    "                    \n",
    "                    # Recalculate score percentage\n",
    "                    wins = merged_data[player][\"black_games\"][eco][\"results\"][\"num_wins\"]\n",
    "                    draws = merged_data[player][\"black_games\"][eco][\"results\"][\"num_draws\"]\n",
    "                    total = merged_data[player][\"black_games\"][eco][\"results\"][\"num_games\"]\n",
    "                    score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e3eee",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline\n",
    "\n",
    "Now let's implement the main processing pipeline that ties everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0325347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(players_data: Dict[str, PlayerStats], \n",
    "                  chunk_num: int, \n",
    "                  config: ProcessingConfig) -> None:\n",
    "    \"\"\"\n",
    "    Save current progress to disk.\n",
    "    \n",
    "    Args:\n",
    "        players_data: Current player statistics\n",
    "        chunk_num: Current chunk number\n",
    "        config: Processing configuration\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    save_dir = Path(config.save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save player data\n",
    "    player_data_path = save_dir / config.player_data_file\n",
    "    \n",
    "    # For large datasets, pickle can be more efficient than JSON\n",
    "    with open(player_data_path, 'wb') as f:\n",
    "        pickle.dump(players_data, f)\n",
    "        \n",
    "    # Save progress information\n",
    "    progress_path = save_dir / config.progress_file\n",
    "    progress_info = {\n",
    "        \"last_chunk_processed\": chunk_num,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"num_players\": len(players_data),\n",
    "        \"config\": {k: v for k, v in config.__dict__.items()}\n",
    "    }\n",
    "    \n",
    "    with open(progress_path, 'w') as f:\n",
    "        json.dump(progress_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved progress after chunk {chunk_num}. \" +\n",
    "          f\"Current data includes {len(players_data)} players.\")\n",
    "\n",
    "def load_progress(config: ProcessingConfig) -> Tuple[Dict[str, PlayerStats], int]:\n",
    "    \"\"\"\n",
    "    Load previous progress from disk.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (player_data, last_chunk_processed)\n",
    "    \"\"\"\n",
    "    player_data_path = Path(config.save_dir) / config.player_data_file\n",
    "    progress_path = Path(config.save_dir) / config.progress_file\n",
    "    \n",
    "    # Default values if no saved progress\n",
    "    players_data: Dict[str, PlayerStats] = {}\n",
    "    last_chunk = 0\n",
    "    \n",
    "    # Load player data if it exists\n",
    "    if player_data_path.exists():\n",
    "        try:\n",
    "            with open(player_data_path, 'rb') as f:\n",
    "                players_data = pickle.load(f)\n",
    "            print(f\"Loaded player data with {len(players_data)} players.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading player data: {e}\")\n",
    "            players_data = {}\n",
    "    \n",
    "    # Load progress info if it exists\n",
    "    if progress_path.exists():\n",
    "        try:\n",
    "            with open(progress_path, 'r') as f:\n",
    "                progress_info = json.load(f)\n",
    "                last_chunk = progress_info.get(\"last_chunk_processed\", 0)\n",
    "            print(f\"Resuming from chunk {last_chunk}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading progress info: {e}\")\n",
    "            last_chunk = 0\n",
    "            \n",
    "    return players_data, last_chunk\n",
    "\n",
    "def process_pgn_file(file_path: str, config: ProcessingConfig) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a PGN file in chunks, with support for resuming from previous progress.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PGN file\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        Player statistics dictionary\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"PGN file not found: {file_path}\")\n",
    "    \n",
    "    # Load previous progress if any\n",
    "    players_data, start_chunk = load_progress(config)\n",
    "    \n",
    "    # Create game reader\n",
    "    game_gen = game_reader(file_path, config)\n",
    "    \n",
    "    # Skip chunks that were already processed\n",
    "    for _ in range(start_chunk):\n",
    "        print(f\"Skipping chunk {_ + 1}...\")\n",
    "        for _ in range(config.chunk_size):\n",
    "            try:\n",
    "                next(game_gen)\n",
    "            except StopIteration:\n",
    "                print(\"Reached end of file while skipping chunks.\")\n",
    "                return players_data\n",
    "    \n",
    "    # Process chunks\n",
    "    chunk_num = start_chunk\n",
    "    while True:\n",
    "        if config.max_chunks and chunk_num >= start_chunk + config.max_chunks:\n",
    "            print(f\"Reached maximum number of chunks ({config.max_chunks}). Stopping.\")\n",
    "            break\n",
    "            \n",
    "        # Collect a chunk of games\n",
    "        chunk = []\n",
    "        for _ in range(config.chunk_size):\n",
    "            try:\n",
    "                game = next(game_gen)\n",
    "                chunk.append(game)\n",
    "            except StopIteration:\n",
    "                print(\"Reached end of file.\")\n",
    "                break\n",
    "                \n",
    "        if not chunk:\n",
    "            print(\"No more games to process.\")\n",
    "            break\n",
    "            \n",
    "        print(f\"Processing chunk {chunk_num + 1} with {len(chunk)} games...\")\n",
    "        chunk_start_time = time.time()\n",
    "        \n",
    "        # Process chunk\n",
    "        if config.use_parallel and len(chunk) >= 1000:  # Only parallelize for larger chunks\n",
    "            # Split chunk into smaller pieces for parallel processing\n",
    "            num_processes = min(config.num_processes, len(chunk) // 100)  # Ensure each process has enough work\n",
    "            chunk_size = len(chunk) // num_processes\n",
    "            chunks = [chunk[i:i + chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            \n",
    "            # Process chunks in parallel\n",
    "            with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "                results = pool.map(process_game_chunk, chunks)\n",
    "                \n",
    "            # Merge results\n",
    "            chunk_data = {}\n",
    "            for result in results:\n",
    "                chunk_data = merge_player_stats(chunk_data, result)\n",
    "        else:\n",
    "            # Process chunk sequentially\n",
    "            chunk_data = process_game_chunk(chunk)\n",
    "            \n",
    "        # Merge with existing data\n",
    "        players_data = merge_player_stats(players_data, chunk_data)\n",
    "        \n",
    "        chunk_end_time = time.time()\n",
    "        print(f\"Processed chunk in {chunk_end_time - chunk_start_time:.2f} seconds.\")\n",
    "        \n",
    "        # Save progress periodically\n",
    "        chunk_num += 1\n",
    "        if chunk_num % config.save_interval == 0:\n",
    "            save_progress(players_data, chunk_num, config)\n",
    "    \n",
    "    # Save final progress\n",
    "    save_progress(players_data, chunk_num, config)\n",
    "    \n",
    "    return players_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e8242",
   "metadata": {},
   "source": [
    "## Usage Example\n",
    "\n",
    "Let's set up the configuration and run the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4da90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 5000 games, Included: 2540 (50.8%) in 13.85 seconds\n",
      "Processed: 10000 games, Included: 5073 (50.7%) in 27.43 seconds\n",
      "Processed: 15000 games, Included: 7550 (50.3%) in 40.27 seconds\n",
      "Processing chunk 1 with 10000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    }
   ],
   "source": [
    "# Path to the compressed PGN file\n",
    "pgn_path = \"/Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/lichess_db_standard_rated_2025-07.pgn.zst\"\n",
    "\n",
    "# Create a configuration with customized parameters\n",
    "config = ProcessingConfig(\n",
    "    # Filtering parameters\n",
    "    min_elo=1200,  # Only include games with players above this Elo\n",
    "    exclude_time_controls={\"bullet\", \"hyperbullet\", \"ultrabullet\"},\n",
    "    min_moves=15,  # Exclude very short games\n",
    "    \n",
    "    # Processing parameters\n",
    "    chunk_size=10000,  # Process this many games at once\n",
    "    max_chunks=5,  # Process a maximum of 5 chunks (50K games) - change as needed\n",
    "    save_interval=1,  # Save after each chunk\n",
    "    \n",
    "    # File paths\n",
    "    save_dir=\"../data/processed\",\n",
    "    \n",
    "    # Parallelization\n",
    "    use_parallel=True,\n",
    "    num_processes= multiprocessing.cpu_count() - 1\n",
    ")\n",
    "\n",
    "# Run the processing pipeline\n",
    "players_data = process_pgn_file(pgn_path, config)\n",
    "\n",
    "# Show some statistics\n",
    "print(f\"Total number of players: {len(players_data)}\")\n",
    "\n",
    "# Show a sample player\n",
    "import random\n",
    "if players_data:\n",
    "    sample_player = random.choice(list(players_data.keys()))\n",
    "    print(f\"\\nSample stats for player: {sample_player}\")\n",
    "    print(f\"Rating: {players_data[sample_player]['rating']}\")\n",
    "    print(f\"Total games: {players_data[sample_player]['num_games_total']}\")\n",
    "    print(\"\\nWhite openings:\")\n",
    "    for eco, data in players_data[sample_player]['white_games'].items():\n",
    "        print(f\"  {eco} - {data['opening_name']}: {data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "    print(\"\\nBlack openings:\")\n",
    "    for eco, data in players_data[sample_player]['black_games'].items():\n",
    "        print(f\"  {eco} - {data['opening_name']}: {data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf99592",
   "metadata": {},
   "source": [
    "## Analysis of Efficiency Improvements\n",
    "\n",
    "Our optimized approach provides several major efficiency improvements:\n",
    "\n",
    "1. **Early filtering**: We filter games as we read them, avoiding the need to load all games into memory.\n",
    "\n",
    "2. **Chunked processing**: We process games in chunks, allowing us to save progress and resume later.\n",
    "\n",
    "3. **Parallel processing**: We use multiple CPU cores to process chunks faster.\n",
    "\n",
    "4. **Incremental saving**: We save progress after each chunk, so we don't lose work if the process crashes.\n",
    "\n",
    "5. **Strong typing**: We use type hints throughout, providing better IDE support and code quality.\n",
    "\n",
    "With these optimizations, we can process large datasets much more efficiently, potentially reducing the processing time from days to hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64768c",
   "metadata": {},
   "source": [
    "## Estimating Time Savings\n",
    "\n",
    "Let's analyze the potential time savings from these optimizations:\n",
    "\n",
    "1. **Early filtering**: If 70% of games can be filtered out early (e.g., bullet games, low-rated players), we only need to fully process 30% of the data.\n",
    "\n",
    "2. **Parallel processing**: Using multiple CPU cores (e.g., 7 on an 8-core machine) can provide a ~5-6x speedup for the processing phase.\n",
    "\n",
    "3. **Incremental saving**: Even if processing takes several hours, you can stop and resume at any time without losing progress.\n",
    "\n",
    "Instead of processing 34 million games at 5,000 games per 13 seconds (1.4 million/hour), we might be able to process at 5-10 million effective games per hour with these optimizations.\n",
    "\n",
    "For example:\n",
    "- Original approach: 34 million games ÷ 1.4 million/hour ≈ 24 hours\n",
    "- Optimized approach: 30% of 34 million games ÷ (1.4 million × 5 speedup)/hour ≈ 1.5 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44562071",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run the optimized processing pipeline with a small `max_chunks` value to test it.\n",
    "2. Adjust the `min_elo`, `exclude_time_controls`, and other parameters to match your needs.\n",
    "3. When satisfied with the results, set `max_chunks` to `None` to process the entire file.\n",
    "4. Use the saved player data for your chess opening recommendation system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
