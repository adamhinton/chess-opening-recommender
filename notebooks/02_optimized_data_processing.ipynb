{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f045a790",
   "metadata": {},
   "source": [
    "# Optimized Chess Data Processing\n",
    "\n",
    "This notebook implements efficient strategies for processing a large PGN database, focusing on:\n",
    "\n",
    "1. **Early filtering** - Filter games during reading to avoid loading unnecessary data\n",
    "2. **Incremental processing** - Process in chunks and save progress \n",
    "3. **Parallelization** - Use multiple CPU cores where beneficial\n",
    "4. **Type safety** - Strong typing for better IDE support and code quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9ade0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.3.2-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.3.2-cp312-cp312-macosx_14_0_x86_64.whl (6.6 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from typing import TypedDict, Optional, Dict, List, Set, Iterator, Tuple, Any, Literal, Union\n",
    "import chess.pgn\n",
    "import zstandard as zstd\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from dataclasses import dataclass, field\n",
    "import platform\n",
    "import psutil\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05150e32",
   "metadata": {},
   "source": [
    "## Type Definitions\n",
    "\n",
    "First, let's define strong types for our data structures to ensure type safety and better IDE support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define types for game results\n",
    "GameResult = Literal[\"1-0\", \"0-1\", \"1/2-1/2\", \"*\"]\n",
    "\n",
    "# Time control categories\n",
    "TimeControlCategory = Literal[\"bullet\", \"blitz\", \"rapid\", \"classical\", \"correspondence\", \"unknown\"]\n",
    "\n",
    "class GameHeaders(TypedDict, total=False):\n",
    "    \"\"\"Type for game headers extracted from PGN.\"\"\"\n",
    "    Event: str\n",
    "    Site: str\n",
    "    Date: str\n",
    "    White: str\n",
    "    Black: str\n",
    "    Result: GameResult\n",
    "    WhiteElo: str\n",
    "    BlackElo: str\n",
    "    ECO: str\n",
    "    Opening: str\n",
    "    TimeControl: str\n",
    "    Termination: str\n",
    "    WhiteRatingDiff: str\n",
    "    BlackRatingDiff: str\n",
    "\n",
    "class OpeningResults(TypedDict):\n",
    "    \"\"\"Statistics for a player's results with a particular opening.\"\"\"\n",
    "    opening_name: str\n",
    "    results: Dict[str, Union[int, float]]\n",
    "\n",
    "# Player statistics structure\n",
    "class PlayerStats(TypedDict):\n",
    "    \"\"\"Statistics for an individual player.\"\"\"\n",
    "    rating: int\n",
    "    white_games: Dict[str, OpeningResults]  # ECO code -> results\n",
    "    black_games: Dict[str, OpeningResults]  # ECO code -> results\n",
    "    num_games_total: int\n",
    "\n",
    "# Configuration parameters with defaults\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for the game processing pipeline.\"\"\"\n",
    "    # Filtering parameters\n",
    "    min_elo: int = 1500  # Minimum player Elo to include\n",
    "    exclude_time_controls: Set[str] = field(default_factory=lambda: {\"bullet\", \"hyperbullet\", \"ultrabullet\"})\n",
    "    min_moves: int = 10  # Minimum number of moves in game\n",
    "    \n",
    "    # Processing parameters\n",
    "    chunk_size: int = 100_000  # Number of games to process in each chunk\n",
    "    max_chunks: Optional[int] = None  # Maximum number of chunks to process (None for all)\n",
    "    save_interval: int = 1  # Save after processing this many chunks\n",
    "    \n",
    "    # File paths\n",
    "    save_dir: str = \"../data/processed\"\n",
    "    player_data_file: str = \"player_stats.json\"\n",
    "    progress_file: str = \"processing_progress.json\"\n",
    "    \n",
    "    # Parallelization\n",
    "    use_parallel: bool = True\n",
    "    num_processes: int = multiprocessing.cpu_count() - 1  # Use all but one CPU core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e97568",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define helper functions for time control categorization and game filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_time_control(time_control: str) -> TimeControlCategory:\n",
    "    \"\"\"\n",
    "    Categorize the time control string into standard categories.\n",
    "    \n",
    "    Args:\n",
    "        time_control: The time control string from the game headers\n",
    "        \n",
    "    Returns:\n",
    "        The category of time control\n",
    "    \"\"\"\n",
    "    # Handle missing or malformed time control\n",
    "    if not time_control or time_control == \"?\" or time_control == \"Unknown\":\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # Split time control into initial time and increment\n",
    "    # Format is typically \"initial+increment\" like \"180+2\" (3 minutes + 2 second increment)\n",
    "    parts = time_control.split(\"+\")\n",
    "    \n",
    "    try:\n",
    "        # Initial time in seconds\n",
    "        initial_time = int(parts[0])\n",
    "        \n",
    "        # Categorize based on initial time\n",
    "        if initial_time < 180:  # Less than 3 minutes\n",
    "            return \"bullet\"\n",
    "        elif initial_time < 600:  # 3-10 minutes\n",
    "            return \"blitz\"\n",
    "        elif initial_time < 1800:  # 10-30 minutes\n",
    "            return \"rapid\"\n",
    "        elif initial_time <= 6000:  # 30-100 minutes\n",
    "            return \"classical\"\n",
    "        else:  # More than 100 minutes\n",
    "            return \"correspondence\"\n",
    "    except (ValueError, IndexError):\n",
    "        # Handle correspondence format like \"1 day\"\n",
    "        if \"day\" in time_control.lower():\n",
    "            return \"correspondence\"\n",
    "        return \"unknown\"\n",
    "\n",
    "def should_include_game(headers: GameHeaders, config: ProcessingConfig) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a game should be included based on filtering criteria.\n",
    "    \n",
    "    Args:\n",
    "        headers: The game headers\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        True if the game should be included, False otherwise\n",
    "    \"\"\"\n",
    "    # Skip games with missing essential information\n",
    "    if not all(key in headers for key in ['White', 'Black', 'Result', 'TimeControl']):\n",
    "        return False\n",
    "    \n",
    "    # Skip games with too low Elo\n",
    "    try:\n",
    "        white_elo = int(headers.get('WhiteElo', '0'))\n",
    "        black_elo = int(headers.get('BlackElo', '0'))\n",
    "        if white_elo < config.min_elo or black_elo < config.min_elo:\n",
    "            return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    # Skip games with excluded time controls\n",
    "    time_control = headers.get('TimeControl', '')\n",
    "    tc_category = categorize_time_control(time_control)\n",
    "    \n",
    "    if tc_category in config.exclude_time_controls:\n",
    "        return False\n",
    "        \n",
    "    # Skip abandoned or incomplete games\n",
    "    if headers.get('Result', '') == '*':\n",
    "        return False\n",
    "    \n",
    "    # Game passes all filters\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d99bd",
   "metadata": {},
   "source": [
    "## Efficient PGN Reading\n",
    "\n",
    "Now let's implement an efficient reader that filters games early and processes in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7709b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_reader(file_path: str, config: ProcessingConfig) -> Iterator[chess.pgn.Game]:\n",
    "    \"\"\"\n",
    "    Generator that reads and yields games from a compressed PGN file,\n",
    "    applying early filtering to avoid unnecessary processing.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the compressed PGN file\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Yields:\n",
    "        Chess games that pass the filtering criteria\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        stream_reader = dctx.stream_reader(f)\n",
    "        text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "        \n",
    "        games_processed = 0\n",
    "        games_included = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            # Read the next game\n",
    "            game = chess.pgn.read_game(text_stream)\n",
    "            \n",
    "            # Check if we've reached the end of the file\n",
    "            if game is None:\n",
    "                break\n",
    "                \n",
    "            games_processed += 1\n",
    "            \n",
    "            # Extract headers for filtering\n",
    "            headers = dict(game.headers)\n",
    "            \n",
    "            # Check if game meets inclusion criteria\n",
    "            if should_include_game(headers, config):\n",
    "                games_included += 1\n",
    "                yield game\n",
    "            \n",
    "            # Print progress periodically\n",
    "            if games_processed % 5_000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                inclusion_rate = (games_included / games_processed) * 100 if games_processed > 0 else 0\n",
    "                print(f\"Processed: {games_processed} games, Included: {games_included} \" +\n",
    "                      f\"({inclusion_rate:.1f}%) in {elapsed:.2f} seconds\")\n",
    "\n",
    "def process_game_chunk(chunk: List[chess.pgn.Game]) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a chunk of games and return player statistics.\n",
    "    \n",
    "    Args:\n",
    "        chunk: A list of chess games to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping player usernames to their statistics\n",
    "    \"\"\"\n",
    "    players_data: Dict[str, PlayerStats] = {}\n",
    "    \n",
    "    for game in chunk:\n",
    "        headers = dict(game.headers)\n",
    "        \n",
    "        white_player = headers['White']\n",
    "        black_player = headers['Black']\n",
    "        white_elo = int(headers.get('WhiteElo', 0))\n",
    "        black_elo = int(headers.get('BlackElo', 0))\n",
    "        result = headers['Result']\n",
    "        eco_code = headers.get('ECO', 'Unknown')\n",
    "        opening_name = headers.get('Opening', 'Unknown Opening')\n",
    "        \n",
    "        # Process white player's game\n",
    "        if white_player not in players_data:\n",
    "            players_data[white_player] = {\n",
    "                \"rating\": white_elo,\n",
    "                \"white_games\": {},\n",
    "                \"black_games\": {},\n",
    "                \"num_games_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Update white player's data\n",
    "        if eco_code not in players_data[white_player][\"white_games\"]:\n",
    "            players_data[white_player][\"white_games\"][eco_code] = {\n",
    "                \"opening_name\": opening_name,\n",
    "                \"results\": {\n",
    "                    \"num_games\": 0,\n",
    "                    \"num_wins\": 0,\n",
    "                    \"num_losses\": 0,\n",
    "                    \"num_draws\": 0,\n",
    "                    \"score_percentage_with_opening\": 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Update game counts\n",
    "        players_data[white_player][\"num_games_total\"] += 1\n",
    "        players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_games\"] += 1\n",
    "        \n",
    "        # Update result counts\n",
    "        if result == \"1-0\":  # White win\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_wins\"] += 1\n",
    "        elif result == \"0-1\":  # Black win (white loss)\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_losses\"] += 1\n",
    "        elif result == \"1/2-1/2\":  # Draw\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_draws\"] += 1\n",
    "            \n",
    "        # Update score percentage\n",
    "        wins = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_wins\"]\n",
    "        draws = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_draws\"]\n",
    "        total = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_games\"]\n",
    "        score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "        players_data[white_player][\"white_games\"][eco_code][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "        \n",
    "        # Similarly process black player's game\n",
    "        if black_player not in players_data:\n",
    "            players_data[black_player] = {\n",
    "                \"rating\": black_elo,\n",
    "                \"white_games\": {},\n",
    "                \"black_games\": {},\n",
    "                \"num_games_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Update black player's data\n",
    "        if eco_code not in players_data[black_player][\"black_games\"]:\n",
    "            players_data[black_player][\"black_games\"][eco_code] = {\n",
    "                \"opening_name\": opening_name,\n",
    "                \"results\": {\n",
    "                    \"num_games\": 0,\n",
    "                    \"num_wins\": 0,\n",
    "                    \"num_losses\": 0,\n",
    "                    \"num_draws\": 0,\n",
    "                    \"score_percentage_with_opening\": 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Update game counts\n",
    "        players_data[black_player][\"num_games_total\"] += 1\n",
    "        players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_games\"] += 1\n",
    "        \n",
    "        # Update result counts\n",
    "        if result == \"0-1\":  # Black win\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_wins\"] += 1\n",
    "        elif result == \"1-0\":  # White win (black loss)\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_losses\"] += 1\n",
    "        elif result == \"1/2-1/2\":  # Draw\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_draws\"] += 1\n",
    "            \n",
    "        # Update score percentage\n",
    "        wins = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_wins\"]\n",
    "        draws = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_draws\"]\n",
    "        total = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_games\"]\n",
    "        score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "        players_data[black_player][\"black_games\"][eco_code][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "    \n",
    "    return players_data\n",
    "\n",
    "def merge_player_stats(data1: Dict[str, PlayerStats], data2: Dict[str, PlayerStats]) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Merge two player statistics dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        data1: First player statistics dictionary\n",
    "        data2: Second player statistics dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Merged player statistics\n",
    "    \"\"\"\n",
    "    merged_data: Dict[str, PlayerStats] = data1.copy()\n",
    "    \n",
    "    for player, stats in data2.items():\n",
    "        if player not in merged_data:\n",
    "            merged_data[player] = stats\n",
    "        else:\n",
    "            # Update total game count\n",
    "            merged_data[player][\"num_games_total\"] += stats[\"num_games_total\"]\n",
    "            \n",
    "            # Update white games\n",
    "            for eco, opening_data in stats[\"white_games\"].items():\n",
    "                if eco not in merged_data[player][\"white_games\"]:\n",
    "                    merged_data[player][\"white_games\"][eco] = opening_data\n",
    "                else:\n",
    "                    # Update results for this opening\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_games\"] += opening_data[\"results\"][\"num_games\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_wins\"] += opening_data[\"results\"][\"num_wins\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_losses\"] += opening_data[\"results\"][\"num_losses\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_draws\"] += opening_data[\"results\"][\"num_draws\"]\n",
    "                    \n",
    "                    # Recalculate score percentage\n",
    "                    wins = merged_data[player][\"white_games\"][eco][\"results\"][\"num_wins\"]\n",
    "                    draws = merged_data[player][\"white_games\"][eco][\"results\"][\"num_draws\"]\n",
    "                    total = merged_data[player][\"white_games\"][eco][\"results\"][\"num_games\"]\n",
    "                    score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "            \n",
    "            # Update black games\n",
    "            for eco, opening_data in stats[\"black_games\"].items():\n",
    "                if eco not in merged_data[player][\"black_games\"]:\n",
    "                    merged_data[player][\"black_games\"][eco] = opening_data\n",
    "                else:\n",
    "                    # Update results for this opening\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_games\"] += opening_data[\"results\"][\"num_games\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_wins\"] += opening_data[\"results\"][\"num_wins\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_losses\"] += opening_data[\"results\"][\"num_losses\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_draws\"] += opening_data[\"results\"][\"num_draws\"]\n",
    "                    \n",
    "                    # Recalculate score percentage\n",
    "                    wins = merged_data[player][\"black_games\"][eco][\"results\"][\"num_wins\"]\n",
    "                    draws = merged_data[player][\"black_games\"][eco][\"results\"][\"num_draws\"]\n",
    "                    total = merged_data[player][\"black_games\"][eco][\"results\"][\"num_games\"]\n",
    "                    score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e3eee",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline\n",
    "\n",
    "Now let's implement the main processing pipeline that ties everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(players_data: Dict[str, PlayerStats], \n",
    "                  chunk_num: int, \n",
    "                  config: ProcessingConfig) -> None:\n",
    "    \"\"\"\n",
    "    Save current progress to disk.\n",
    "    \n",
    "    Args:\n",
    "        players_data: Current player statistics\n",
    "        chunk_num: Current chunk number\n",
    "        config: Processing configuration\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    save_dir = Path(config.save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save player data\n",
    "    player_data_path = save_dir / config.player_data_file\n",
    "    \n",
    "    # For large datasets, pickle can be more efficient than JSON\n",
    "    with open(player_data_path, 'wb') as f:\n",
    "        pickle.dump(players_data, f)\n",
    "        \n",
    "    # Save progress information\n",
    "    progress_path = save_dir / config.progress_file\n",
    "    progress_info = {\n",
    "        \"last_chunk_processed\": chunk_num,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"num_players\": len(players_data),\n",
    "        \"config\": {k: v for k, v in config.__dict__.items()}\n",
    "    }\n",
    "    \n",
    "    with open(progress_path, 'w') as f:\n",
    "        json.dump(progress_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved progress after chunk {chunk_num}. \" +\n",
    "          f\"Current data includes {len(players_data)} players.\")\n",
    "\n",
    "def load_progress(config: ProcessingConfig) -> Tuple[Dict[str, PlayerStats], int]:\n",
    "    \"\"\"\n",
    "    Load previous progress from disk.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (player_data, last_chunk_processed)\n",
    "    \"\"\"\n",
    "    player_data_path = Path(config.save_dir) / config.player_data_file\n",
    "    progress_path = Path(config.save_dir) / config.progress_file\n",
    "    \n",
    "    # Default values if no saved progress\n",
    "    players_data: Dict[str, PlayerStats] = {}\n",
    "    last_chunk = 0\n",
    "    \n",
    "    # Load player data if it exists\n",
    "    if player_data_path.exists():\n",
    "        try:\n",
    "            with open(player_data_path, 'rb') as f:\n",
    "                players_data = pickle.load(f)\n",
    "            print(f\"Loaded player data with {len(players_data)} players.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading player data: {e}\")\n",
    "            players_data = {}\n",
    "    \n",
    "    # Load progress info if it exists\n",
    "    if progress_path.exists():\n",
    "        try:\n",
    "            with open(progress_path, 'r') as f:\n",
    "                progress_info = json.load(f)\n",
    "                last_chunk = progress_info.get(\"last_chunk_processed\", 0)\n",
    "            print(f\"Resuming from chunk {last_chunk}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading progress info: {e}\")\n",
    "            last_chunk = 0\n",
    "            \n",
    "    return players_data, last_chunk\n",
    "\n",
    "def process_pgn_file(file_path: str, config: ProcessingConfig) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a PGN file in chunks, with support for resuming from previous progress.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PGN file\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        Player statistics dictionary\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"PGN file not found: {file_path}\")\n",
    "    \n",
    "    # Load previous progress if any\n",
    "    players_data, start_chunk = load_progress(config)\n",
    "    \n",
    "    # Create game reader\n",
    "    game_gen = game_reader(file_path, config)\n",
    "    \n",
    "    # Skip chunks that were already processed\n",
    "    for _ in range(start_chunk):\n",
    "        print(f\"Skipping chunk {_ + 1}...\")\n",
    "        for _ in range(config.chunk_size):\n",
    "            try:\n",
    "                next(game_gen)\n",
    "            except StopIteration:\n",
    "                print(\"Reached end of file while skipping chunks.\")\n",
    "                return players_data\n",
    "    \n",
    "    # Process chunks\n",
    "    chunk_num = start_chunk\n",
    "    while True:\n",
    "        if config.max_chunks and chunk_num >= start_chunk + config.max_chunks:\n",
    "            print(f\"Reached maximum number of chunks ({config.max_chunks}). Stopping.\")\n",
    "            break\n",
    "            \n",
    "        # Collect a chunk of games\n",
    "        chunk = []\n",
    "        for _ in range(config.chunk_size):\n",
    "            try:\n",
    "                game = next(game_gen)\n",
    "                chunk.append(game)\n",
    "            except StopIteration:\n",
    "                print(\"Reached end of file.\")\n",
    "                break\n",
    "                \n",
    "        if not chunk:\n",
    "            print(\"No more games to process.\")\n",
    "            break\n",
    "            \n",
    "        print(f\"Processing chunk {chunk_num + 1} with {len(chunk)} games...\")\n",
    "        chunk_start_time = time.time()\n",
    "        \n",
    "        # Process chunk\n",
    "        if config.use_parallel and len(chunk) >= 1000:  # Only parallelize for larger chunks\n",
    "            # Split chunk into smaller pieces for parallel processing\n",
    "            num_processes = min(config.num_processes, len(chunk) // 100)  # Ensure each process has enough work\n",
    "            chunk_size = len(chunk) // num_processes\n",
    "            chunks = [chunk[i:i + chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            \n",
    "            # Process chunks in parallel\n",
    "            with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "                results = pool.map(process_game_chunk, chunks)\n",
    "                \n",
    "            # Merge results\n",
    "            chunk_data = {}\n",
    "            for result in results:\n",
    "                chunk_data = merge_player_stats(chunk_data, result)\n",
    "        else:\n",
    "            # Process chunk sequentially\n",
    "            chunk_data = process_game_chunk(chunk)\n",
    "            \n",
    "        # Merge with existing data\n",
    "        players_data = merge_player_stats(players_data, chunk_data)\n",
    "        \n",
    "        chunk_end_time = time.time()\n",
    "        print(f\"Processed chunk in {chunk_end_time - chunk_start_time:.2f} seconds.\")\n",
    "        \n",
    "        # Save progress periodically\n",
    "        chunk_num += 1\n",
    "        if chunk_num % config.save_interval == 0:\n",
    "            save_progress(players_data, chunk_num, config)\n",
    "    \n",
    "    # Save final progress\n",
    "    save_progress(players_data, chunk_num, config)\n",
    "    \n",
    "    return players_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e8242",
   "metadata": {},
   "source": [
    "## CPU and Memory Optimization\n",
    "\n",
    "Let's add advanced optimizations to fully leverage your Mac's CPU and memory resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4da90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 5000 games, Included: 2540 (50.8%) in 13.85 seconds\n",
      "Processed: 10000 games, Included: 5073 (50.7%) in 27.43 seconds\n",
      "Processed: 15000 games, Included: 7550 (50.3%) in 40.27 seconds\n",
      "Processing chunk 1 with 10000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_game_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-21:\n",
      "Process SpawnPoolWorker-20:\n",
      "Process SpawnPoolWorker-17:\n",
      "Process SpawnPoolWorker-23:\n",
      "Process SpawnPoolWorker-22:\n",
      "Process SpawnPoolWorker-16:\n",
      "Process SpawnPoolWorker-19:\n",
      "Process SpawnPoolWorker-18:\n",
      "Process SpawnPoolWorker-14:\n",
      "Process SpawnPoolWorker-15:\n",
      "Process SpawnPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m      5\u001b[39m config = ProcessingConfig(\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Filtering parameters\u001b[39;00m\n\u001b[32m      7\u001b[39m     min_elo=\u001b[32m1200\u001b[39m,  \u001b[38;5;66;03m# Only include games with players above this Elo\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     num_processes= multiprocessing.cpu_count() - \u001b[32m1\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Run the processing pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m players_data = \u001b[43mprocess_pgn_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpgn_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Show some statistics\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal number of players: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(players_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mprocess_pgn_file\u001b[39m\u001b[34m(file_path, config)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Process chunks in parallel\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing.Pool(processes=num_processes) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     results = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_game_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# Merge results\u001b[39;00m\n\u001b[32m    145\u001b[39m chunk_data = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/multiprocessing/pool.py:367\u001b[39m, in \u001b[36mPool.map\u001b[39m\u001b[34m(self, func, iterable, chunksize)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    363\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03m    in a list that is returned.\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/multiprocessing/pool.py:768\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    767\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ready():\n\u001b[32m    770\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/multiprocessing/pool.py:765\u001b[39m, in \u001b[36mApplyResult.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import psutil\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def get_system_info() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get detailed information about the system's hardware resources.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with system information\n",
    "    \"\"\"\n",
    "    info = {\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"processor\": platform.processor(),\n",
    "        \"physical_cores\": psutil.cpu_count(logical=False),\n",
    "        \"logical_cores\": psutil.cpu_count(logical=True),\n",
    "        \"total_memory_gb\": round(psutil.virtual_memory().total / (1024**3), 2)\n",
    "    }\n",
    "    \n",
    "    # Get current CPU usage\n",
    "    info[\"cpu_usage_percent\"] = psutil.cpu_percent(interval=1)\n",
    "    \n",
    "    # Get available memory\n",
    "    mem = psutil.virtual_memory()\n",
    "    info[\"available_memory_gb\"] = round(mem.available / (1024**3), 2)\n",
    "    info[\"memory_usage_percent\"] = mem.percent\n",
    "    \n",
    "    return info\n",
    "\n",
    "def optimize_chunk_size(available_memory_gb: float) -> int:\n",
    "    \"\"\"\n",
    "    Calculate an optimal chunk size based on available system memory.\n",
    "    \n",
    "    A single game object is approximately 5-10KB in memory.\n",
    "    We aim to use about 25% of available memory for a chunk.\n",
    "    \n",
    "    Args:\n",
    "        available_memory_gb: Available memory in GB\n",
    "        \n",
    "    Returns:\n",
    "        Recommended chunk size\n",
    "    \"\"\"\n",
    "    # Conservative estimate of memory per game (in bytes)\n",
    "    memory_per_game = 10 * 1024  # 10KB\n",
    "    \n",
    "    # Use 25% of available memory for chunk\n",
    "    memory_for_chunk = available_memory_gb * 0.25 * 1024**3\n",
    "    \n",
    "    # Calculate chunk size\n",
    "    chunk_size = int(memory_for_chunk / memory_per_game)\n",
    "    \n",
    "    # Round to nearest 10,000\n",
    "    chunk_size = max(1000, round(chunk_size / 10000) * 10000)\n",
    "    \n",
    "    return chunk_size\n",
    "\n",
    "def optimize_parallelism() -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Calculate optimal parallelism settings based on the system.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with recommended settings\n",
    "    \"\"\"\n",
    "    physical_cores = psutil.cpu_count(logical=False) or 1\n",
    "    logical_cores = psutil.cpu_count(logical=True) or 1\n",
    "    \n",
    "    # On Mac M-series chips, all cores are efficient\n",
    "    # On Intel Macs, we might want to leave 1 core free for system\n",
    "    is_apple_silicon = platform.processor() == 'arm'\n",
    "    \n",
    "    recommendations = {\n",
    "        # Use all cores for Apple Silicon, leave 1 free for Intel\n",
    "        \"num_processes\": logical_cores if is_apple_silicon else max(1, logical_cores - 1),\n",
    "        \n",
    "        # For work distribution\n",
    "        \"optimal_batch_size\": max(1, (logical_cores * 4) // physical_cores),\n",
    "        \n",
    "        # For memory considerations\n",
    "        \"max_parallel_chunks\": max(1, physical_cores // 2)\n",
    "    }\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Get system info and print\n",
    "system_info = get_system_info()\n",
    "print(\"System Information:\")\n",
    "for key, value in system_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Get optimization recommendations\n",
    "parallel_settings = optimize_parallelism()\n",
    "print(\"\\nRecommended Parallelism Settings:\")\n",
    "for key, value in parallel_settings.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Recommend chunk size based on memory\n",
    "recommended_chunk_size = optimize_chunk_size(system_info[\"available_memory_gb\"])\n",
    "print(f\"\\nRecommended chunk size based on available memory: {recommended_chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a118f97",
   "metadata": {},
   "source": [
    "## Enhanced Processing Pipeline\n",
    "\n",
    "Now let's enhance our processing pipeline to maximize CPU and memory utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OptimizedProcessingConfig(ProcessingConfig):\n",
    "    \"\"\"Enhanced configuration with advanced performance options.\"\"\"\n",
    "    # Advanced performance tuning\n",
    "    buffer_size_mb: int = 64  # Size of zstd decompression buffer in MB\n",
    "    batch_size: int = 250     # Number of games to process in each task\n",
    "    process_start_method: str = 'fork'  # Process start method: 'fork', 'spawn', or 'forkserver'\n",
    "    use_process_pool: bool = True       # Use ProcessPoolExecutor instead of Pool for better control\n",
    "    max_memory_percent: float = 80.0    # Maximum memory usage percentage before throttling\n",
    "    \n",
    "    # I/O optimization\n",
    "    io_mode: str = \"buffered\"  # \"buffered\" or \"direct\" (direct uses more memory but is faster)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize and adjust config based on system capabilities.\"\"\"\n",
    "        # Set defaults if not provided\n",
    "        if self.num_processes <= 0:\n",
    "            self.num_processes = psutil.cpu_count(logical=True) or 2\n",
    "            \n",
    "        # On Apple Silicon, we can use all cores efficiently\n",
    "        is_apple_silicon = platform.processor() == 'arm'\n",
    "        if is_apple_silicon and self.num_processes < psutil.cpu_count(logical=True):\n",
    "            print(\"Apple Silicon detected - increasing core utilization\")\n",
    "            self.num_processes = psutil.cpu_count(logical=True)\n",
    "            \n",
    "        # Optimize batch size based on core count\n",
    "        if self.batch_size == 250:  # Only if using default\n",
    "            cores = psutil.cpu_count(physical=True) or 1\n",
    "            self.batch_size = max(100, 1000 // cores)\n",
    "\n",
    "def optimized_game_reader(file_path: str, config: OptimizedProcessingConfig) -> Iterator[chess.pgn.Game]:\n",
    "    \"\"\"\n",
    "    Enhanced generator that reads games more efficiently using optimized buffer sizes.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the compressed PGN file\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Yields:\n",
    "        Chess games that pass the filtering criteria\n",
    "    \"\"\"\n",
    "    # Calculate buffer size in bytes\n",
    "    buffer_size = config.buffer_size_mb * 1024 * 1024\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Configure the decompressor with a large window size for better performance\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        \n",
    "        # Use a larger read_size for better throughput\n",
    "        stream_reader = dctx.stream_reader(f, read_size=buffer_size)\n",
    "        \n",
    "        # Configure text stream with a large buffer\n",
    "        text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8', \n",
    "                                      buffer_size=buffer_size)\n",
    "        \n",
    "        games_processed = 0\n",
    "        games_included = 0\n",
    "        start_time = time.time()\n",
    "        last_report_time = start_time\n",
    "        \n",
    "        while True:\n",
    "            # Check if we need to throttle due to memory pressure\n",
    "            if games_processed % 1000 == 0:\n",
    "                mem_usage = psutil.virtual_memory().percent\n",
    "                if mem_usage > config.max_memory_percent:\n",
    "                    print(f\"Memory usage high ({mem_usage}%). Pausing briefly to let GC catch up.\")\n",
    "                    time.sleep(2)  # Give system time to free memory\n",
    "            \n",
    "            # Read the next game with optimized parser\n",
    "            game = chess.pgn.read_game(text_stream)\n",
    "            \n",
    "            # Check if we've reached the end of the file\n",
    "            if game is None:\n",
    "                break\n",
    "                \n",
    "            games_processed += 1\n",
    "            \n",
    "            # Extract headers for filtering\n",
    "            headers = dict(game.headers)\n",
    "            \n",
    "            # Check if game meets inclusion criteria\n",
    "            if should_include_game(headers, config):\n",
    "                games_included += 1\n",
    "                yield game\n",
    "            \n",
    "            # Print progress periodically (every 5000 games or 30 seconds)\n",
    "            current_time = time.time()\n",
    "            if games_processed % 5_000 == 0 or (current_time - last_report_time) >= 30:\n",
    "                elapsed = current_time - start_time\n",
    "                rate = games_processed / elapsed if elapsed > 0 else 0\n",
    "                inclusion_rate = (games_included / games_processed) * 100 if games_processed > 0 else 0\n",
    "                \n",
    "                print(f\"Processed: {games_processed:,} games, Included: {games_included:,} \" +\n",
    "                      f\"({inclusion_rate:.1f}%) at {rate:.1f} games/sec\")\n",
    "                \n",
    "                # Update last report time if we're reporting based on time\n",
    "                if (current_time - last_report_time) >= 30:\n",
    "                    last_report_time = current_time\n",
    "\n",
    "def process_games_with_executor(chunk: List[chess.pgn.Game], \n",
    "                               config: OptimizedProcessingConfig) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process games using a process pool executor for better resource control.\n",
    "    \n",
    "    Args:\n",
    "        chunk: A list of chess games to process\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping player usernames to their statistics\n",
    "    \"\"\"\n",
    "    # If the chunk is small, process it directly\n",
    "    if len(chunk) < 1000 or not config.use_process_pool:\n",
    "        return process_game_chunk(chunk)\n",
    "    \n",
    "    # Divide chunk into batches\n",
    "    batch_size = config.batch_size\n",
    "    batches = [chunk[i:i + batch_size] for i in range(0, len(chunk), batch_size)]\n",
    "    \n",
    "    # Use ProcessPoolExecutor for better control\n",
    "    results = []\n",
    "    \n",
    "    # Set the start method for multiprocessing\n",
    "    # 'fork' is fastest on macOS/Linux but less stable, 'spawn' is slower but more stable\n",
    "    ctx = multiprocessing.get_context(config.process_start_method)\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=config.num_processes, \n",
    "                            mp_context=ctx) as executor:\n",
    "        # Submit all batches and collect futures\n",
    "        futures = [executor.submit(process_game_chunk, batch) for batch in batches]\n",
    "        \n",
    "        # Process results as they complete (this helps with memory usage)\n",
    "        for i, future in enumerate(futures):\n",
    "            batch_result = future.result()\n",
    "            results.append(batch_result)\n",
    "            \n",
    "            # Periodically report progress\n",
    "            if (i + 1) % 10 == 0 or i == len(futures) - 1:\n",
    "                print(f\"Completed {i + 1}/{len(futures)} batches\")\n",
    "    \n",
    "    # Merge all results\n",
    "    merged_result = {}\n",
    "    for result in results:\n",
    "        merged_result = merge_player_stats(merged_result, result)\n",
    "    \n",
    "    return merged_result\n",
    "\n",
    "def optimized_process_pgn_file(file_path: str, config: OptimizedProcessingConfig) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Enhanced version of the PGN processing function with better resource utilization.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PGN file\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        Player statistics dictionary\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not Path(file_path).exists():\n",
    "        raise FileNotFoundError(f\"PGN file not found: {file_path}\")\n",
    "    \n",
    "    # Load previous progress if any\n",
    "    players_data, start_chunk = load_progress(config)\n",
    "    \n",
    "    # Create game reader with optimized settings\n",
    "    game_gen = optimized_game_reader(file_path, config)\n",
    "    \n",
    "    # Skip chunks that were already processed\n",
    "    if start_chunk > 0:\n",
    "        print(f\"Resuming from chunk {start_chunk}. Skipping {start_chunk * config.chunk_size:,} games...\")\n",
    "        games_skipped = 0\n",
    "        skip_start_time = time.time()\n",
    "        \n",
    "        for _ in range(start_chunk * config.chunk_size):\n",
    "            try:\n",
    "                next(game_gen)\n",
    "                games_skipped += 1\n",
    "                \n",
    "                # Show progress while skipping\n",
    "                if games_skipped % 50_000 == 0:\n",
    "                    elapsed = time.time() - skip_start_time\n",
    "                    rate = games_skipped / elapsed if elapsed > 0 else 0\n",
    "                    print(f\"Skipped {games_skipped:,} games ({games_skipped/(start_chunk * config.chunk_size)*100:.1f}%) \" +\n",
    "                          f\"at {rate:.1f} games/sec\")\n",
    "                    \n",
    "            except StopIteration:\n",
    "                print(\"Reached end of file while skipping chunks.\")\n",
    "                return players_data\n",
    "    \n",
    "    # Process chunks with improved tracking\n",
    "    chunk_num = start_chunk\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        if config.max_chunks and chunk_num >= start_chunk + config.max_chunks:\n",
    "            print(f\"Reached maximum number of chunks ({config.max_chunks}). Stopping.\")\n",
    "            break\n",
    "            \n",
    "        # Collect a chunk of games\n",
    "        chunk = []\n",
    "        chunk_collection_start = time.time()\n",
    "        \n",
    "        for _ in range(config.chunk_size):\n",
    "            try:\n",
    "                game = next(game_gen)\n",
    "                chunk.append(game)\n",
    "                \n",
    "                # Show progress for large chunks\n",
    "                if len(chunk) % 50_000 == 0:\n",
    "                    elapsed = time.time() - chunk_collection_start\n",
    "                    print(f\"Collected {len(chunk):,}/{config.chunk_size:,} games for chunk {chunk_num + 1} \" +\n",
    "                          f\"in {elapsed:.1f} sec\")\n",
    "                    \n",
    "            except StopIteration:\n",
    "                print(\"Reached end of file.\")\n",
    "                break\n",
    "                \n",
    "        if not chunk:\n",
    "            print(\"No more games to process.\")\n",
    "            break\n",
    "            \n",
    "        chunk_size = len(chunk)\n",
    "        print(f\"Processing chunk {chunk_num + 1} with {chunk_size:,} games...\")\n",
    "        chunk_start_time = time.time()\n",
    "        \n",
    "        # Process chunk with advanced parallelism\n",
    "        chunk_data = process_games_with_executor(chunk, config)\n",
    "        \n",
    "        # Merge with existing data\n",
    "        players_data = merge_player_stats(players_data, chunk_data)\n",
    "        \n",
    "        # Calculate and display detailed stats\n",
    "        chunk_end_time = time.time()\n",
    "        chunk_time = chunk_end_time - chunk_start_time\n",
    "        games_per_second = chunk_size / chunk_time if chunk_time > 0 else 0\n",
    "        \n",
    "        print(f\"Processed chunk {chunk_num + 1} in {chunk_time:.2f} seconds \" +\n",
    "              f\"({games_per_second:.1f} games/sec)\")\n",
    "        \n",
    "        # Overall progress\n",
    "        total_elapsed = chunk_end_time - total_start_time\n",
    "        total_chunks_done = chunk_num - start_chunk + 1\n",
    "        avg_chunk_time = total_elapsed / total_chunks_done\n",
    "        \n",
    "        print(f\"Overall progress: {total_chunks_done} chunks in {total_elapsed/60:.1f} minutes \" +\n",
    "              f\"(avg: {avg_chunk_time:.1f} sec/chunk)\")\n",
    "        \n",
    "        # Save progress periodically\n",
    "        chunk_num += 1\n",
    "        if chunk_num % config.save_interval == 0:\n",
    "            save_progress(players_data, chunk_num, config)\n",
    "            \n",
    "            # Report memory usage after saving\n",
    "            mem = psutil.virtual_memory()\n",
    "            print(f\"Memory usage: {mem.percent}% (Used: {mem.used/1024**3:.1f}GB, \" +\n",
    "                  f\"Available: {mem.available/1024**3:.1f}GB)\")\n",
    "    \n",
    "    # Save final progress\n",
    "    save_progress(players_data, chunk_num, config)\n",
    "    \n",
    "    # Final statistics\n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\nProcessing complete in {total_time/60:.1f} minutes\")\n",
    "    print(f\"Total players: {len(players_data):,}\")\n",
    "    \n",
    "    return players_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install psutil if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"Installing psutil package...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "    import psutil\n",
    "    print(\"psutil installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf99592",
   "metadata": {},
   "source": [
    "## Analysis of Efficiency Improvements\n",
    "\n",
    "Our optimized approach provides several major efficiency improvements:\n",
    "\n",
    "1. **Early filtering**: We filter games as we read them, avoiding the need to load all games into memory.\n",
    "\n",
    "2. **Chunked processing**: We process games in chunks, allowing us to save progress and resume later.\n",
    "\n",
    "3. **Parallel processing**: We use multiple CPU cores to process chunks faster.\n",
    "\n",
    "4. **Incremental saving**: We save progress after each chunk, so we don't lose work if the process crashes.\n",
    "\n",
    "5. **Strong typing**: We use type hints throughout, providing better IDE support and code quality.\n",
    "\n",
    "With these optimizations, we can process large datasets much more efficiently, potentially reducing the processing time from days to hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64768c",
   "metadata": {},
   "source": [
    "## Estimating Time Savings\n",
    "\n",
    "Let's analyze the potential time savings from these optimizations:\n",
    "\n",
    "1. **Early filtering**: If 70% of games can be filtered out early (e.g., bullet games, low-rated players), we only need to fully process 30% of the data.\n",
    "\n",
    "2. **Parallel processing**: Using multiple CPU cores (e.g., 7 on an 8-core machine) can provide a ~5-6x speedup for the processing phase.\n",
    "\n",
    "3. **Incremental saving**: Even if processing takes several hours, you can stop and resume at any time without losing progress.\n",
    "\n",
    "Instead of processing 34 million games at 5,000 games per 13 seconds (1.4 million/hour), we might be able to process at 5-10 million effective games per hour with these optimizations.\n",
    "\n",
    "For example:\n",
    "- Original approach: 34 million games ÷ 1.4 million/hour ≈ 24 hours\n",
    "- Optimized approach: 30% of 34 million games ÷ (1.4 million × 5 speedup)/hour ≈ 1.5 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44562071",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run the optimized processing pipeline with a small `max_chunks` value to test it.\n",
    "2. Adjust the `min_elo`, `exclude_time_controls`, and other parameters to match your needs.\n",
    "3. When satisfied with the results, set `max_chunks` to `None` to process the entire file.\n",
    "4. Use the saved player data for your chess opening recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e419f",
   "metadata": {},
   "source": [
    "## Running with Maximum CPU Utilization\n",
    "\n",
    "Let's use our new optimized processing pipeline with settings specifically tuned for your Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the compressed PGN file\n",
    "pgn_path = \"/Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/lichess_db_standard_rated_2025-07.pgn.zst\"\n",
    "\n",
    "# Create a fully optimized configuration\n",
    "system_info = get_system_info()\n",
    "parallel_settings = optimize_parallelism()\n",
    "recommended_chunk_size = optimize_chunk_size(system_info[\"available_memory_gb\"])\n",
    "\n",
    "# Check if we're on Apple Silicon\n",
    "is_apple_silicon = platform.processor() == 'arm'\n",
    "\n",
    "# Create optimized configuration\n",
    "optimized_config = OptimizedProcessingConfig(\n",
    "    # Filtering parameters\n",
    "    min_elo=1200,  # Only include games with players above this Elo\n",
    "    exclude_time_controls={\"bullet\", \"hyperbullet\", \"ultrabullet\"},\n",
    "    min_moves=15,  # Exclude very short games\n",
    "    \n",
    "    # Processing parameters - use memory-optimized chunk size\n",
    "    chunk_size=recommended_chunk_size,\n",
    "    max_chunks=5,  # Process a maximum of 5 chunks - change as needed\n",
    "    save_interval=1,  # Save after each chunk\n",
    "    \n",
    "    # File paths\n",
    "    save_dir=\"../data/processed\",\n",
    "    \n",
    "    # Advanced parallelization settings\n",
    "    use_parallel=True,\n",
    "    num_processes=parallel_settings[\"num_processes\"],  # Use optimal number of processes for your Mac\n",
    "    \n",
    "    # Mac-specific optimizations\n",
    "    process_start_method='fork',  # Fastest on Mac, use 'spawn' if you get errors\n",
    "    buffer_size_mb=128,  # Larger buffer for faster decompression\n",
    "    batch_size=parallel_settings[\"optimal_batch_size\"],  # Optimal batch size for your CPU\n",
    "    use_process_pool=True  # Better control over resources\n",
    ")\n",
    "\n",
    "print(f\"Optimized for your Mac with {system_info['physical_cores']} physical cores, \" +\n",
    "      f\"{system_info['logical_cores']} logical cores\")\n",
    "print(f\"Using {optimized_config.num_processes} processes for parallel execution\")\n",
    "print(f\"Memory-optimized chunk size: {optimized_config.chunk_size:,} games\")\n",
    "\n",
    "# Run the optimized processing pipeline\n",
    "players_data = optimized_process_pgn_file(pgn_path, optimized_config)\n",
    "\n",
    "# Show some statistics\n",
    "print(f\"Total number of players: {len(players_data)}\")\n",
    "\n",
    "# Show a sample player\n",
    "import random\n",
    "if players_data:\n",
    "    sample_player = random.choice(list(players_data.keys()))\n",
    "    print(f\"\\nSample stats for player: {sample_player}\")\n",
    "    print(f\"Rating: {players_data[sample_player]['rating']}\")\n",
    "    print(f\"Total games: {players_data[sample_player]['num_games_total']}\")\n",
    "    print(\"\\nWhite openings:\")\n",
    "    for eco, data in players_data[sample_player]['white_games'].items():\n",
    "        print(f\"  {eco} - {data['opening_name']}: {data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "    print(\"\\nBlack openings:\")\n",
    "    for eco, data in players_data[sample_player]['black_games'].items():\n",
    "        print(f\"  {eco} - {data['opening_name']}: {data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
