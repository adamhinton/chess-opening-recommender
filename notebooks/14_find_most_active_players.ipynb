{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5938f7b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c657718",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook will find the 50k most active Lichess players (rapid blitz classical), measured by number of games.\n",
    "\n",
    "It does this by downloading and reading parquet files of raw game data.\n",
    "\n",
    "It's not practical to download the hundreds of 1GB parquet files; so we will download a certain number and extrapolate from there.\n",
    "\n",
    "We don't need to know exactly who is the *most* active, just need to find *very* active players.\n",
    "\n",
    "Then, we will process their games for data to feed in to our chess opening recommender AI model.\n",
    "\n",
    "# Reason\n",
    "\n",
    "Originally, we were just collecting data on millions of players without caring who was active or not.\n",
    "\n",
    "This grew impractical quickly, with a massive local DB. Games processing slowed down exponentially because SQL queries to the local duckdb got so slow due to the db's size.\n",
    "\n",
    "So I've decided to focus on the 50k most active players instead.\n",
    "\n",
    "# Method\n",
    "\n",
    "1. Get names of parquet files\n",
    "    - We will want to look at games data over the course of a year or so\n",
    "    - So, we'll download some 1GB raw game data parquet files (number TBD) from each month (there are about 60-70 total per month)\n",
    "    - To do this, we need the names of those files to get from the HuggingFace API\n",
    "    - Luckily we already have functionality elsewhere to do this; I'll import it here and update slightly for our needs\n",
    "\n",
    "2. Download parquet files\n",
    "    - Once we have the names of the needed files, we'll download them\n",
    "    - I already have functionality to do this elsewhere, just need to import it here\n",
    "\n",
    "3. Save game counts\n",
    "    - Once we have our parquet files downloaded, we'll count the number of games each username has\n",
    "    - There will be some filters; only Rapid/Blitz/Classical games, only players over 1200 rating\n",
    "    - Need to decide on a data saving method; probably in a CSV with username/num_games\n",
    "    - Wherever it's saved it needs to be persistent, as this processing will happen over multiple sessions.\n",
    "\n",
    "4. Get most active players\n",
    "    - Finally, we'll analyze our CSV to retrieve the usernames of the 50,000 most active Lichess players\n",
    "    - Profit\n",
    "\n",
    "\n",
    "- Note that all data for this will be saved in data/processed/find_most_active_players directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bac497",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Below, we'll define some important variables that we want to use in this notebook.\n",
    "\n",
    "I like to keep them in one place at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.downloading_raw_parquet_data.get_parquet_file_names import (\n",
    "    get_parquet_file_names,\n",
    ")\n",
    "\n",
    "# The year and month we want to download data for\n",
    "\n",
    "month_for_downloading = 6\n",
    "year_for_downloading = 2025\n",
    "\n",
    "max_files_to_download_per_month = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070cdbc",
   "metadata": {},
   "source": [
    "## 1. Get file names\n",
    "\n",
    "Now, we'll get the file names for the month and year we want to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_for_month = get_parquet_file_names(year_for_downloading, month_for_downloading)\n",
    "# print with line separations\n",
    "print(\"\\n\".join(file_names_for_month[:max_files_to_download_per_month]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fcbcc1",
   "metadata": {},
   "source": [
    "## Database Setup\n",
    "\n",
    "Now, we'll initialize our database.\n",
    "\n",
    "This is a small database that tracks the number of games played by a Lichess username, as well as which files have been checked so there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623816ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from utils.database.db_utils import get_db_connection\n",
    "from utils.database.player_game_counts_db_utils import setup_player_game_counts_table\n",
    "\n",
    "# Define the project root\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# Initialize the database\n",
    "DB_PATH = project_root / \"data\" / \"processed\" / \"find_most_active_players\" / \"player_game_counts.duckdb\"\n",
    "DB_DIR = DB_PATH.parent\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not DB_DIR.exists():\n",
    "    DB_DIR.mkdir(parents=True)\n",
    "    print(f\"Created directory: {DB_DIR}\")\n",
    "\n",
    "# Ensure the database file exists\n",
    "if not DB_PATH.exists():\n",
    "    print(f\"Database file {DB_PATH} does not exist. Initializing...\")\n",
    "    con = get_db_connection(str(DB_PATH))\n",
    "    setup_player_game_counts_table(con)\n",
    "    con.close()\n",
    "    print(\"Database created and initialized.\")\n",
    "else:\n",
    "    print(f\"Database file {DB_PATH} already exists. Skipping initialization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae561e81",
   "metadata": {},
   "source": [
    "## Main Pipeline\n",
    "Now that we have the names of the files we need, we will do the following for each file name (until we've downloaded our predefined max number of files for the given month)\n",
    "\n",
    "1. Check our local db to make sure that we haven't already downloaded the file in question\n",
    "    - If we have, cycle through that month's list of files until we find one that hasn't been downloaded\n",
    "2. Download the file in question\n",
    "    - Note that HuggingFace's API is smart enough to not re-download files we already have on our local machine, which saves me a lot of headache when doing this repeatedly for testing.\n",
    "3. Mark the file as having been processed in our local db to avoid duplicates\n",
    "4. Process the file, recording each player's num_games in the local db\n",
    "5. Delete the file\n",
    "6. Move on to next file unless we've reached our maximum number of files\n",
    "\n",
    "Goals:\n",
    "\n",
    "- Thorough logging, especially for how long each step takes, including games per second while processing files\n",
    "- Each file is about 1.4 million rows so that helps when providing a file ETA\n",
    "- Also log which file number we're on out of the max total files to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_processing.types_and_classes import ProcessingConfig\n",
    "import pandas as pd\n",
    "from typing import Set\n",
    "import re\n",
    "\n",
    "# --- Processing Configuration ---\n",
    "# Base config for processing. This will be used for each file.\n",
    "base_config = ProcessingConfig(\n",
    "    parquet_path=\"\",  # This will be set per-file\n",
    "    db_path=DB_PATH,\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference_between_players=100,\n",
    "    allowed_time_controls={\"Blitz\", \"Rapid\", \"Classical\"},\n",
    ")\n",
    "\n",
    "LOG_FREQUENCY = 100_000\n",
    "TOTAL_ROWS_IN_FILE = 1_400_000\n",
    "\n",
    "def is_valid_game(row: pd.Series, config: ProcessingConfig) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a game is valid based on the provided processing configuration.\n",
    "    \"\"\"\n",
    "    # Check for BOTs\n",
    "    if (pd.notna(row[\"WhiteTitle\"]) and \"BOT\" in row[\"WhiteTitle\"]) or \\\n",
    "       (pd.notna(row[\"BlackTitle\"]) and \"BOT\" in row[\"BlackTitle\"]):\n",
    "        return False\n",
    "\n",
    "    # Check ratings\n",
    "    if row[\"WhiteElo\"] < config.min_player_rating or row[\"BlackElo\"] < config.min_player_rating:\n",
    "        return False\n",
    "\n",
    "    # Check ELO difference\n",
    "    if abs(row[\"WhiteElo\"] - row[\"BlackElo\"]) > config.max_elo_difference_between_players:\n",
    "        return False\n",
    "\n",
    "    # Check time control\n",
    "    event_lower = str(row[\"Event\"]).lower()\n",
    "    if not any(re.search(r'\\b' + re.escape(tc.lower()) + r'\\b', event_lower) for tc in config.allowed_time_controls):\n",
    "        return False\n",
    "        \n",
    "    # Check result\n",
    "    if row[\"Result\"] not in {\"1-0\", \"0-1\", \"1/2-1/2\"}:\n",
    "        return False\n",
    "        \n",
    "    # Check for player names\n",
    "    if not row['White'] or not row['Black']:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527ca91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61730001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils.database.player_game_counts_db_utils import is_file_already_downloaded\n",
    "from utils.downloading_raw_parquet_data.raw_parquet_data_file_downloader import (\n",
    "    download_single_parquet_file,\n",
    ")\n",
    "\n",
    "processed_file_count = 0\n",
    "\n",
    "for file_name in file_names_for_month[:1]:\n",
    "    if processed_file_count >= max_files_to_download_per_month:\n",
    "        print(\"Reached the maximum number of files to process. Stopping.\")\n",
    "        break\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Check in the local db whether the file has already been processed - if so, move on to next file\n",
    "    con = get_db_connection(str(DB_PATH))\n",
    "    is_already_downloaded = is_file_already_downloaded(con, file_name, year_for_downloading, month_for_downloading)\n",
    "    con.close()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Checked file {file_name} in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    if is_already_downloaded:\n",
    "        print(f\"File {file_name} has already been processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"File {file_name} has not been processed yet. Processing now.\")\n",
    "\n",
    "    # 2. If it hasn't been processed, download the file\n",
    "    start_time = time.time()\n",
    "    downloaded_file_path = download_single_parquet_file(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        file_to_download=file_name,\n",
    "        local_dir=Path(\"data/raw\"),\n",
    "        year=year_for_downloading,\n",
    "        month=month_for_downloading,\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if downloaded_file_path:\n",
    "        print(f\"Downloaded file {file_name} in {elapsed_time:.2f} seconds.\")\n",
    "        processed_file_count += 1\n",
    "    else:\n",
    "        print(f\"Failed to download file {file_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 3. Mark the file as processed in the local db. Doing this part first because downloading is cheap and quick, we can just move on to the next file if something goes wrong\n",
    "    # TODO this would be inconvenient during testing; reinstate once we know the rest of the code works\n",
    "    # start_time = time.time()\n",
    "    # con = get_db_connection(str(DB_PATH))\n",
    "    # from utils.database.player_game_counts_db_utils import mark_file_as_downloaded\n",
    "    # mark_file_as_downloaded(con, file_name, year_for_downloading, month_for_downloading)\n",
    "    # con.close()\n",
    "    # elapsed_time = time.time() - start_time\n",
    "    # print(f\"Marked file {file_name} as downloaded in {elapsed_time:.2f\n",
    "\n",
    "    # 4. Process the file, updating valid player game counts in the local db\n",
    "    start_processing_time = time.time()\n",
    "    games_processed = 0\n",
    "    \n",
    "    import pyarrow.parquet as pq\n",
    "    from collections import Counter\n",
    "\n",
    "    # Create a specific config for this file\n",
    "    file_config = base_config.replace(parquet_path=str(downloaded_file_path))\n",
    "\n",
    "    parquet_file = pq.ParquetFile(file_config.parquet_path)\n",
    "    player_game_counts = Counter()\n",
    "\n",
    "    print(f\"Processing {downloaded_file_path.name}...\")\n",
    "\n",
    "    for batch in parquet_file.iter_batches(batch_size=LOG_FREQUENCY):\n",
    "        df = batch.to_pandas()\n",
    "        for index, row in df.iterrows():\n",
    "            games_processed += 1\n",
    "            if is_valid_game(row, file_config):\n",
    "                player_game_counts[row['White']] += 1\n",
    "                player_game_counts[row['Black']] += 1\n",
    "\n",
    "            if games_processed % LOG_FREQUENCY == 0:\n",
    "                elapsed_since_start = time.time() - start_processing_time\n",
    "                if elapsed_since_start > 0:\n",
    "                    games_per_second = games_processed / elapsed_since_start\n",
    "                    eta_seconds = (TOTAL_ROWS_IN_FILE - games_processed) / games_per_second if games_per_second > 0 else float('inf')\n",
    "                    print(f\"  - Processed {games_processed}/{TOTAL_ROWS_IN_FILE} games. \"\n",
    "                          f\"({games_per_second:.0f} games/sec, ETA: {eta_seconds:.2f}s)\")\n",
    "\n",
    "    # Update the database with the collected game counts\n",
    "    con = get_db_connection(str(DB_PATH))\n",
    "    from utils.database.player_game_counts_db_utils import update_player_game_count\n",
    "    for username, num_games in player_game_counts.items():\n",
    "        update_player_game_count(con, username, num_games)\n",
    "    con.close()\n",
    "    \n",
    "    processing_elapsed_time = time.time() - start_processing_time\n",
    "    print(f\"Finished processing file. Total time: {processing_elapsed_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "    # 5. Delete the file\n",
    "\n",
    "    # 6. Move on to the next file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
