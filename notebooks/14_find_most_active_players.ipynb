{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5938f7b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c657718",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook will find the 50k most active Lichess players (rapid blitz classical), measured by number of games.\n",
    "\n",
    "It does this by downloading and reading parquet files of raw game data.\n",
    "\n",
    "It's not practical to download the hundreds of 1GB parquet files; so we will download a certain number and extrapolate from there.\n",
    "\n",
    "We don't need to know exactly who is the *most* active, just need to find *very* active players.\n",
    "\n",
    "Then, we will process their games for data to feed in to our chess opening recommender AI model.\n",
    "\n",
    "# Reason\n",
    "\n",
    "Originally, we were just collecting data on millions of players without caring who was active or not.\n",
    "\n",
    "This grew impractical quickly, with a massive local DB. Games processing slowed down exponentially because SQL queries to the local duckdb got so slow due to the db's size.\n",
    "\n",
    "So I've decided to focus on the 50k most active players instead.\n",
    "\n",
    "# Method\n",
    "\n",
    "1. Get names of parquet files\n",
    "    - We will want to look at games data over the course of a year or so\n",
    "    - So, we'll download some 1GB raw game data parquet files (number TBD) from each month (there are about 60-70 total per month)\n",
    "    - To do this, we need the names of those files to get from the HuggingFace API\n",
    "    - Luckily we already have functionality elsewhere to do this; I'll import it here and update slightly for our needs\n",
    "\n",
    "2. Download parquet files\n",
    "    - Once we have the names of the needed files, we'll download them\n",
    "    - I already have functionality to do this elsewhere, just need to import it here\n",
    "\n",
    "3. Save game counts\n",
    "    - Once we have our parquet files downloaded, we'll count the number of games each username has\n",
    "    - There will be some filters; only Rapid/Blitz/Classical games, only players over 1200 rating\n",
    "    - Need to decide on a data saving method; probably in a CSV with username/num_games\n",
    "    - Wherever it's saved it needs to be persistent, as this processing will happen over multiple sessions.\n",
    "\n",
    "4. Get most active players\n",
    "    - Finally, we'll analyze our CSV to retrieve the usernames of the 50,000 most active Lichess players\n",
    "    - Profit\n",
    "\n",
    "\n",
    "- Note that all data for this will be saved in data/processed/find_most_active_players directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bac497",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Below, we'll define some important variables that we want to use in this notebook.\n",
    "\n",
    "I like to keep them in one place at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e15df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.downloading_raw_parquet_data.get_parquet_file_names import (\n",
    "    get_parquet_file_names,\n",
    ")\n",
    "\n",
    "# The year and month we want to download data for\n",
    "\n",
    "month_for_downloading = 1\n",
    "year_for_downloading = 2023\n",
    "\n",
    "max_files_to_download_per_month = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070cdbc",
   "metadata": {},
   "source": [
    "## 1. Get file names\n",
    "\n",
    "Now, we'll get the file names for the month and year we want to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca8fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/year=2023/month=01/train-00000-of-00433.parquet\n",
      "data/year=2023/month=01/train-00001-of-00433.parquet\n"
     ]
    }
   ],
   "source": [
    "file_names_for_month = get_parquet_file_names(year_for_downloading, month_for_downloading)\n",
    "# print with line separations\n",
    "print(\"\\n\".join(file_names_for_month[:max_files_to_download_per_month]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fcbcc1",
   "metadata": {},
   "source": [
    "## Database Setup\n",
    "\n",
    "Now, we'll initialize our database.\n",
    "\n",
    "This is a small database that tracks the number of games played by a Lichess username, as well as which files have been checked so there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623816ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/find_most_active_players\n",
      "Database file /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/find_most_active_players/player_game_counts.duckdb does not exist. Initializing...\n",
      "Initializing player game counts and downloaded files tables...\n",
      "Tables initialized successfully.\n",
      "Database created and initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from utils.database.db_utils import get_db_connection\n",
    "from utils.database.player_game_counts_db_utils import setup_player_game_counts_table\n",
    "\n",
    "# Define the project root\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# Initialize the database\n",
    "DB_PATH = project_root / \"data\" / \"processed\" / \"find_most_active_players\" / \"player_game_counts.duckdb\"\n",
    "DB_DIR = DB_PATH.parent\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not DB_DIR.exists():\n",
    "    DB_DIR.mkdir(parents=True)\n",
    "    print(f\"Created directory: {DB_DIR}\")\n",
    "\n",
    "# Ensure the database file exists\n",
    "if not DB_PATH.exists():\n",
    "    print(f\"Database file {DB_PATH} does not exist. Initializing...\")\n",
    "    con = get_db_connection(str(DB_PATH))\n",
    "    setup_player_game_counts_table(con)\n",
    "    con.close()\n",
    "    print(\"Database created and initialized.\")\n",
    "else:\n",
    "    print(f\"Database file {DB_PATH} already exists. Skipping initialization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae561e81",
   "metadata": {},
   "source": [
    "## Main Pipeline\n",
    "Now that we have the names of the files we need, we will do the following for each file name (until we've downloaded our predefined max number of files for the given month)\n",
    "\n",
    "1. Check our local db to make sure that we haven't already downloaded the file in question\n",
    "    - If we have, cycle through that month's list of files until we find one that hasn't been downloaded\n",
    "2. Download the file in question\n",
    "    - Note that HuggingFace's API is smart enough to not re-download files we already have on our local machine, which saves me a lot of headache when doing this repeatedly for testing.\n",
    "3. Mark the file as having been processed in our local db to avoid duplicates\n",
    "4. Process the file, recording each player's num_games in the local db\n",
    "5. Delete the file\n",
    "6. Move on to next file unless we've reached our maximum number of files\n",
    "\n",
    "Goals:\n",
    "\n",
    "- Thorough logging, especially for how long each step takes, including games per second while processing files\n",
    "- Each file is about 1.4 million rows so that helps when providing a file ETA\n",
    "- Also log which file number we're on out of the max total files to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafd7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_processing.types_and_classes import ProcessingConfig\n",
    "import pandas as pd\n",
    "from typing import Set\n",
    "import re\n",
    "\n",
    "# --- Processing Configuration ---\n",
    "# Base config for processing. This will be used for each file.\n",
    "base_config = ProcessingConfig(\n",
    "    parquet_path=\"\",  # This will be set per-file\n",
    "    db_path=DB_PATH,\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference_between_players=100,\n",
    "    allowed_time_controls={\"Blitz\", \"Rapid\", \"Classical\"},\n",
    ")\n",
    "\n",
    "LOG_FREQUENCY = 100_000\n",
    "TOTAL_ROWS_IN_FILE = 1_400_000\n",
    "\n",
    "def is_valid_game(row: pd.Series, config: ProcessingConfig) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a game is valid based on the provided processing configuration.\n",
    "    \"\"\"\n",
    "    # Check for BOTs\n",
    "    if (pd.notna(row[\"WhiteTitle\"]) and \"BOT\" in row[\"WhiteTitle\"]) or \\\n",
    "       (pd.notna(row[\"BlackTitle\"]) and \"BOT\" in row[\"BlackTitle\"]):\n",
    "        return False\n",
    "\n",
    "    # Check ratings\n",
    "    if row[\"WhiteElo\"] < config.min_player_rating or row[\"BlackElo\"] < config.min_player_rating:\n",
    "        return False\n",
    "\n",
    "    # Check ELO difference\n",
    "    if abs(row[\"WhiteElo\"] - row[\"BlackElo\"]) > config.max_elo_difference_between_players:\n",
    "        return False\n",
    "\n",
    "    # Check time control\n",
    "    event_lower = str(row[\"Event\"]).lower()\n",
    "    if not any(re.search(r'\\b' + re.escape(tc.lower()) + r'\\b', event_lower) for tc in config.allowed_time_controls):\n",
    "        return False\n",
    "        \n",
    "    # Check result\n",
    "    if row[\"Result\"] not in {\"1-0\", \"0-1\", \"1/2-1/2\"}:\n",
    "        return False\n",
    "        \n",
    "    # Check for player names\n",
    "    if not row['White'] or not row['Black']:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527ca91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61730001",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (111900624.py, line 51)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mTODO this would be inconvenient during testing; reinstate once we know the rest of the code works\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import importlib\n",
    "from utils.database import player_game_counts_db_utils\n",
    "from utils.database.player_game_counts_db_utils import is_file_already_downloaded\n",
    "from utils.downloading_raw_parquet_data.raw_parquet_data_file_downloader import (\n",
    "    download_single_parquet_file,\n",
    ")\n",
    "\n",
    "processed_file_count = 0\n",
    "\n",
    "for file_name in file_names_for_month:\n",
    "    if processed_file_count >= max_files_to_download_per_month:\n",
    "        print(\"Reached the maximum number of files to process. Stopping.\")\n",
    "        break\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Check in the local db whether the file has already been processed - if so, move on to next file\n",
    "    con = get_db_connection(str(DB_PATH))\n",
    "    is_already_downloaded = is_file_already_downloaded(con, file_name, year_for_downloading, month_for_downloading)\n",
    "    con.close()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Checked file {file_name} in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    if is_already_downloaded:\n",
    "        print(f\"File {file_name} has already been processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"File {file_name} has not been processed yet. Processing now.\")\n",
    "\n",
    "    # 2. If it hasn't been processed, download the file\n",
    "    start_time = time.time()\n",
    "    downloaded_file_path = download_single_parquet_file(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        file_to_download=file_name,\n",
    "        local_dir=Path(\"data/raw\"),\n",
    "        year=year_for_downloading,\n",
    "        month=month_for_downloading,\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if downloaded_file_path:\n",
    "        print(f\"Downloaded file {file_name} in {elapsed_time:.2f} seconds.\")\n",
    "        processed_file_count += 1\n",
    "    else:\n",
    "        print(f\"Failed to download file {file_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 3. Mark the file as processed in the local db. Doing this part first because downloading is cheap and quick, we can just move on to the next file if something goes wrong\n",
    "    # TODO this would be inconvenient during testing; reinstate once we know the rest of the code works\n",
    "    start_time = time.time()\n",
    "    con = get_db_connection(str(DB_PATH))\n",
    "    from utils.database.player_game_counts_db_utils import mark_file_as_downloaded\n",
    "    mark_file_as_downloaded(con, file_name, year_for_downloading, month_for_downloading)\n",
    "    con.close()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Marked file {file_name} as downloaded in {elapsed_time:.2f } seconds.\")\n",
    "\n",
    "    # 4. Process the file, updating valid player game counts in the local db\n",
    "    start_processing_time = time.time()\n",
    "    \n",
    "    import duckdb\n",
    "    from collections import Counter\n",
    "\n",
    "    # Create a specific config for this file\n",
    "    file_config = base_config.replace(parquet_path=str(downloaded_file_path))\n",
    "\n",
    "    print(f\"Processing {downloaded_file_path.name} with SQL...\")\n",
    "\n",
    "    # SQL statement to get valid games\n",
    "    time_control_pattern = '|'.join([tc.lower() for tc in file_config.allowed_time_controls])\n",
    "    \n",
    "    with duckdb.connect() as con:\n",
    "        # Get total number of games in the file to calculate percentage\n",
    "        total_games_query = f\"SELECT COUNT(*) FROM '{file_config.parquet_path}' WHERE ECO IS NOT NULL;\"\n",
    "        total_games = con.execute(total_games_query).fetchone()[0]\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT White, Black\n",
    "            FROM '{file_config.parquet_path}'\n",
    "            WHERE\n",
    "                (WhiteTitle IS NULL OR WhiteTitle NOT LIKE '%%BOT%%')\n",
    "                AND (BlackTitle IS NULL OR BlackTitle NOT LIKE '%%BOT%%')\n",
    "                AND WhiteElo >= {file_config.min_player_rating}\n",
    "                AND BlackElo >= {file_config.min_player_rating}\n",
    "                AND abs(WhiteElo - BlackElo) <= {file_config.max_elo_difference_between_players}\n",
    "                AND regexp_matches(lower(Event), '{time_control_pattern}')\n",
    "                AND Result IN ('1-0','0-1','1/2-1/2')\n",
    "                AND White IS NOT NULL AND Black IS NOT NULL AND ECO IS NOT NULL;\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Executing SQL query to get valid games...\")\n",
    "        sql_start_time = time.time()\n",
    "        valid_games_df = con.execute(query).df()\n",
    "        sql_elapsed_time = time.time() - sql_start_time\n",
    "        \n",
    "        num_valid_games = len(valid_games_df)\n",
    "        games_per_second = num_valid_games / sql_elapsed_time if sql_elapsed_time > 0 else 0\n",
    "        \n",
    "        print(f\"SQL query finished in {sql_elapsed_time:.2f} seconds.\")\n",
    "        print(f\"Found {num_valid_games} valid games out of {total_games} total games.\")\n",
    "        if total_games > 0:\n",
    "            percentage_valid = (num_valid_games / total_games) * 100\n",
    "            print(f\"Percentage of valid games: {percentage_valid:.2f}%\")\n",
    "        print(f\"Processing rate: {games_per_second:.0f} games/sec.\")\n",
    "\n",
    "        # Count games per player\n",
    "        print(\"\\nCounting games per player...\")\n",
    "        count_start_time = time.time()\n",
    "\n",
    "        # We can query the pandas DataFrame directly with DuckDB\n",
    "        count_query = \"\"\"\n",
    "            WITH all_players AS (\n",
    "                SELECT White AS player FROM valid_games_df\n",
    "                UNION ALL\n",
    "                SELECT Black AS player FROM valid_games_df\n",
    "            )\n",
    "            SELECT player, COUNT(*) as num_games\n",
    "            FROM all_players\n",
    "            GROUP BY player\n",
    "            ORDER BY num_games DESC;\n",
    "        \"\"\"\n",
    "        \n",
    "        player_counts_df = con.execute(count_query).df()\n",
    "        \n",
    "        count_elapsed_time = time.time() - count_start_time\n",
    "        \n",
    "        num_players = len(player_counts_df)\n",
    "        players_per_second = num_players / count_elapsed_time if count_elapsed_time > 0 else 0\n",
    "        \n",
    "        print(f\"Counted games for {num_players} unique players in {count_elapsed_time:.2f} seconds.\")\n",
    "        print(f\"Processing rate: {players_per_second:.0f} players/sec.\")\n",
    "        \n",
    "        print(\"\\nTop 100 most active players in this file:\")\n",
    "        print(player_counts_df.head(100).to_string())\n",
    "\n",
    "        # Update the database with the new counts\n",
    "        print(\"\\nUpdating player game counts in the database...\")\n",
    "        update_start_time = time.time()\n",
    "        \n",
    "        db_con = get_db_connection(str(DB_PATH))\n",
    "        importlib.reload(player_game_counts_db_utils)\n",
    "        from utils.database.player_game_counts_db_utils import update_all_player_game_counts\n",
    "        \n",
    "        try:\n",
    "            update_all_player_game_counts(db_con, player_counts_df)\n",
    "        finally:\n",
    "            db_con.close()\n",
    "            \n",
    "        update_elapsed_time = time.time() - update_start_time\n",
    "        print(f\"Database update finished in {update_elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "\n",
    "    # 5. Delete the file - not implementing this yet, not really necessary unless we decide to do a lot of files\n",
    "\n",
    "    # 6. Move on to the next file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a5b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85816b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO DELETE THIS\n",
    "# Here I'm getting the above player game counts from the DB just to make sure it worked right.\n",
    "\n",
    "# Can delete this later.\n",
    "\n",
    "# Verify the update by querying the database\n",
    "db_con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:\n",
    "    print(\"--- Verification Step ---\")\n",
    "\n",
    "    # Get the 100 players with the most games\n",
    "    print(\"\\nTop 100 players with the most games:\")\n",
    "    top_100_df = db_con.execute(\n",
    "        \"SELECT username, num_games FROM player_game_counts ORDER BY num_games DESC LIMIT 100\"\n",
    "    ).df()\n",
    "    print(top_100_df.to_string())\n",
    "\n",
    "    # Get the 100 players with the least games\n",
    "    print(\"\\nTop 100 players with the least games:\")\n",
    "    bottom_100_df = db_con.execute(\n",
    "        \"SELECT username, num_games FROM player_game_counts ORDER BY num_games ASC LIMIT 100\"\n",
    "    ).df()\n",
    "    print(bottom_100_df.to_string())\n",
    "\n",
    "finally:\n",
    "    db_con.close()\n",
    "    print(\"\\nVerification complete. Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
