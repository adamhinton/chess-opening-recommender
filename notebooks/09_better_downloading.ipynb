{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecceed8",
   "metadata": {},
   "source": [
    "# Downloading Experiments\n",
    "\n",
    "This notebook is a proof of concept\n",
    "\n",
    "Right now, we have some flaws in our raw data download system. We want to make sure we get this right because we will be downloading and processing hundreds or thousands of GBs of raw data parquet files.\n",
    "\n",
    "So, we'll be messing around here with some implementations, and if they work, we'll be replacing parts of our original code with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5bdd03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd6cc3",
   "metadata": {},
   "source": [
    "## Getting file names\n",
    "\n",
    "We want to use file names and other meta data to do dupe checks of what we've already processed, and see what we still need to download from a certain month. Let's run this code that gets all file names from the repo, to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82d1ccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 files found for 2025-03\n",
      "data/year=2025/month=03/train-00000-of-00069.parquet\n",
      "data/year=2025/month=03/train-00001-of-00069.parquet\n",
      "data/year=2025/month=03/train-00002-of-00069.parquet\n",
      "data/year=2025/month=03/train-00003-of-00069.parquet\n",
      "data/year=2025/month=03/train-00004-of-00069.parquet\n",
      "data/year=2025/month=03/train-00005-of-00069.parquet\n",
      "data/year=2025/month=03/train-00006-of-00069.parquet\n",
      "data/year=2025/month=03/train-00007-of-00069.parquet\n",
      "data/year=2025/month=03/train-00008-of-00069.parquet\n",
      "data/year=2025/month=03/train-00009-of-00069.parquet\n",
      "data/year=2025/month=03/train-00010-of-00069.parquet\n",
      "data/year=2025/month=03/train-00011-of-00069.parquet\n",
      "data/year=2025/month=03/train-00012-of-00069.parquet\n",
      "data/year=2025/month=03/train-00013-of-00069.parquet\n",
      "data/year=2025/month=03/train-00014-of-00069.parquet\n",
      "data/year=2025/month=03/train-00015-of-00069.parquet\n",
      "data/year=2025/month=03/train-00016-of-00069.parquet\n",
      "data/year=2025/month=03/train-00017-of-00069.parquet\n",
      "data/year=2025/month=03/train-00018-of-00069.parquet\n",
      "data/year=2025/month=03/train-00019-of-00069.parquet\n"
     ]
    }
   ],
   "source": [
    "# Here we get a list of raw data files for the given month/year\n",
    "\n",
    "# File names in the remote repo are structured like:\n",
    "# data/year=2025/month=03/train-00001-of-00065.parquet\n",
    "# Obviously, there will be different amounts of them so it won't always be -00065.parquet\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from pathlib import Path\n",
    "\n",
    "year = \"2025\"\n",
    "month = \"03\"  # <-- type manually\n",
    "\n",
    "api = HfApi()\n",
    "files = api.list_repo_files(\n",
    "    repo_id=\"Lichess/standard-chess-games\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Filter for that year/month\n",
    "target_prefix = f\"data/year={year}/month={month}/\"\n",
    "all_file_names_in_month = [f for f in files if f.startswith(target_prefix)]\n",
    "\n",
    "print(len(all_file_names_in_month), f\"files found for {year}-{month}\")\n",
    "for f in all_file_names_in_month[:20]:  # preview first 20\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3554f",
   "metadata": {},
   "source": [
    "## Dupe checks\n",
    "\n",
    "Now that we have the list of raw data file names for the given month and year, we'll perform our dupe checks. This parses through the list of file names to make sure we haven't already processed any of these files.\n",
    "\n",
    "Each file is a 1GB download, so it's obviously in our best interest not to download a file we've already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d358efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 new files to download\n"
     ]
    }
   ],
   "source": [
    "# Parse the list of raw data file names to make sure we haven't already processed any of these files, and skip downloading any dupes\n",
    "\n",
    "from huggingface_hub import get_hf_file_metadata, hf_hub_url\n",
    "\n",
    "import sys\n",
    "\n",
    "# Current working directory (should be project root)\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.file_processing.raw_data_file_dupe_checks import FileRegistry  # noqa: E402\n",
    "\n",
    "# Init registry\n",
    "registry = FileRegistry()\n",
    "\n",
    "# Remove files already processed\n",
    "non_dupe_files = []\n",
    "for f in all_file_names_in_month:\n",
    "    url = hf_hub_url(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        filename=f,\n",
    "    )\n",
    "    meta = get_hf_file_metadata(url=url)\n",
    "    size = meta.size\n",
    "    etag = meta.etag\n",
    "    if not registry.is_file_processed(f\"{year}-{month}\", Path(f).name, size, etag):\n",
    "        non_dupe_files.append(f)\n",
    "\n",
    "print(len(non_dupe_files), \"new files to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bbf91",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Now to configure how we want our downloading and processing pipeline to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a040c3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial data with 0 players.\n"
     ]
    }
   ],
   "source": [
    "# Config stuff\n",
    "\n",
    "from utils.downloading_raw_parquet_data.raw_parquet_data_file_downloader import (\n",
    "    download_single_parquet_file,\n",
    ")  # noqa: F401\n",
    "from utils.file_processing.process_parquet_file import (\n",
    "    process_parquet_file,\n",
    ")  # noqa: F401\n",
    "from utils.file_processing.types_and_classes import ProcessingConfig\n",
    "from utils.file_processing.save_and_load_progress import load_progress\n",
    "\n",
    "# --- Configuration ---\n",
    "year, month = 2025, 3\n",
    "max_files_to_download = 5  # For testing; set to None to process all new files\n",
    "local_dir = Path(\"../data/raw/better_downloading_experiments\")\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Base config for processing. This will be used for each file.\n",
    "base_config = ProcessingConfig(\n",
    "    parquet_path=\"\",  # This will be set per-file\n",
    "    batch_size=400_000,\n",
    "    save_interval=5,  # Save progress every 5 batches\n",
    "    save_dir=\"../data/processed\",\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference_between_players=100,\n",
    "    allowed_time_controls={\"Blitz\", \"Rapid\", \"Classical\"},\n",
    ")\n",
    "\n",
    "# --- Initialization ---\n",
    "# Load all existing player data once at the beginning.\n",
    "# This dictionary will be passed to and updated by each file processing job.\n",
    "all_players_data, _ = load_progress(base_config)\n",
    "print(f\"Loaded initial data with {len(all_players_data):,} players.\")\n",
    "\n",
    "# Use the list of non-duplicate files from the previous cell\n",
    "# Note that we still need to actually download these files; that comes next\n",
    "files_to_download = non_dupe_files\n",
    "if max_files_to_download is not None:\n",
    "    files_to_download = non_dupe_files[:max_files_to_download]\n",
    "\n",
    "# for file_to_download in file_names_to_download:\n",
    "#     downloaded_file_path = download_single_parquet_file(\n",
    "#         repo_id=\"Lichess/standard-chess-games\",\n",
    "#         repo_type=\"dataset\",\n",
    "#         file_to_download=file_to_download,\n",
    "#         local_dir=local_dir,\n",
    "#         year=year,\n",
    "#         month=month,\n",
    "#     )\n",
    "#     if downloaded_file_path:\n",
    "#         # Here you would call your processing function, e.g.:\n",
    "#         # process_and_delete_file(downloaded_file_path)\n",
    "#         print(f\"Successfully downloaded {downloaded_file_path.name}\")\n",
    "\n",
    "#     # Now to process the downloaded file, then delete it to save space\n",
    "#     process_parquet_file(\n",
    "\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2fbd3",
   "metadata": {},
   "source": [
    "## Downloading, processing, deleting\n",
    "\n",
    "Now, we'lll downloadm process and delete our raw data files one by one.\n",
    "\n",
    "The workflow is:\n",
    "\n",
    "1. Download a file\n",
    "2. Process that file, extracting the game data we want\n",
    "3. Delete that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd4c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Downloading [1/5] ---\n",
      "File saved to ../data/raw/better_downloading_experiments/2025-03-train-00000-of-00069.parquet\n",
      "Successfully downloaded: 2025-03-train-00000-of-00069.parquet\n",
      "Will process 1,413,223 rows in 4 batches of size 400,000\n",
      "\n",
      "Processing batch 1/4 (offset 0)\n",
      "Progress: 5,000/400,000 (1.2%) - Acceptance rate: 44.6% - Rate: 9900.2 games/sec\n",
      "Progress: 10,000/400,000 (2.5%) - Acceptance rate: 43.3% - Rate: 11322.1 games/sec\n",
      "Progress: 15,000/400,000 (3.8%) - Acceptance rate: 42.7% - Rate: 12047.1 games/sec\n",
      "Progress: 20,000/400,000 (5.0%) - Acceptance rate: 42.6% - Rate: 12735.5 games/sec\n",
      "Progress: 25,000/400,000 (6.2%) - Acceptance rate: 42.6% - Rate: 13191.9 games/sec\n",
      "Progress: 30,000/400,000 (7.5%) - Acceptance rate: 42.5% - Rate: 13424.4 games/sec\n",
      "Progress: 35,000/400,000 (8.8%) - Acceptance rate: 42.3% - Rate: 13216.9 games/sec\n",
      "Progress: 40,000/400,000 (10.0%) - Acceptance rate: 42.2% - Rate: 13352.1 games/sec\n",
      "Progress: 45,000/400,000 (11.2%) - Acceptance rate: 42.2% - Rate: 13540.1 games/sec\n",
      "Progress: 50,000/400,000 (12.5%) - Acceptance rate: 41.8% - Rate: 13737.6 games/sec\n",
      "Progress: 55,000/400,000 (13.8%) - Acceptance rate: 41.8% - Rate: 13868.1 games/sec\n",
      "Progress: 60,000/400,000 (15.0%) - Acceptance rate: 41.4% - Rate: 14000.9 games/sec\n",
      "Progress: 65,000/400,000 (16.2%) - Acceptance rate: 41.6% - Rate: 14093.6 games/sec\n",
      "Progress: 70,000/400,000 (17.5%) - Acceptance rate: 41.6% - Rate: 14177.4 games/sec\n",
      "Progress: 75,000/400,000 (18.8%) - Acceptance rate: 41.6% - Rate: 14257.5 games/sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     34\u001b[39m file_config = base_config.replace(parquet_path=\u001b[38;5;28mstr\u001b[39m(downloaded_file_path))\n\u001b[32m     36\u001b[39m file_context = {\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcurrent_file_num\u001b[39m\u001b[33m\"\u001b[39m: i + \u001b[32m1\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtotal_files\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(files_to_download),\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     * \u001b[32m1_400_000\u001b[39m,\n\u001b[32m     43\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m is_processing_successful = \u001b[43mprocess_parquet_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplayers_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_players_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33m\"\u001b[39m\u001b[33mis_processing_successful:\u001b[39m\u001b[33m\"\u001b[39m, is_processing_successful)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_processing_successful:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/notebooks/utils/file_processing/process_parquet_file.py:127\u001b[39m, in \u001b[36mprocess_parquet_file\u001b[39m\u001b[34m(config, players_data, log_frequency, file_context)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_df.empty:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplayers_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mperf_tracker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m batch_time = perf_tracker.end_batch(\u001b[38;5;28mlen\u001b[39m(batch_df))\n\u001b[32m    137\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    138\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Processed batch in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms. Players: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(players_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/notebooks/utils/file_processing/process_game_batch.py:98\u001b[39m, in \u001b[36mprocess_batch\u001b[39m\u001b[34m(batch_df, players_data, config, log_frequency, perf_tracker, file_context)\u001b[39m\n\u001b[32m     95\u001b[39m batch_filtered = \u001b[32m0\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Process each game in the batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Log progress periodically within the batch\u001b[39;49;00m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_frequency\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43melapsed\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/frame.py:1559\u001b[39m, in \u001b[36mDataFrame.iterrows\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1557\u001b[39m using_cow = using_copy_on_write()\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, \u001b[38;5;28mself\u001b[39m.values):\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m     s = \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1560\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mgr.is_single_block:\n\u001b[32m   1561\u001b[39m         s._mgr.add_references(\u001b[38;5;28mself\u001b[39m._mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/series.py:588\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    586\u001b[39m manager = _get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.data_manager\u001b[39m\u001b[33m\"\u001b[39m, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m     data = \u001b[43mSingleBlockManager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    590\u001b[39m     data = SingleArrayManager.from_array(data, index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:1872\u001b[39m, in \u001b[36mSingleBlockManager.from_array\u001b[39m\u001b[34m(cls, array, index, refs)\u001b[39m\n\u001b[32m   1870\u001b[39m array = maybe_coerce_values(array)\n\u001b[32m   1871\u001b[39m bp = BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(index)))\n\u001b[32m-> \u001b[39m\u001b[32m1872\u001b[39m block = \u001b[43mnew_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(block, index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:2801\u001b[39m, in \u001b[36mnew_block\u001b[39m\u001b[34m(values, placement, ndim, refs)\u001b[39m\n\u001b[32m   2789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_block\u001b[39m(\n\u001b[32m   2790\u001b[39m     values,\n\u001b[32m   2791\u001b[39m     placement: BlockPlacement,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2798\u001b[39m     \u001b[38;5;66;03m# - check_ndim/ensure_block_shape already checked\u001b[39;00m\n\u001b[32m   2799\u001b[39m     \u001b[38;5;66;03m# - maybe_coerce_values already called/unnecessary\u001b[39;00m\n\u001b[32m   2800\u001b[39m     klass = get_block_type(values.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2801\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplacement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "# --- Main Loop ---\n",
    "total_start_time = time.time()\n",
    "\n",
    "for i, file_to_download in enumerate(files_to_download):\n",
    "    print(f\"\\n--- Downloading [{i+1}/{len(files_to_download)}] ---\")\n",
    "\n",
    "    # 1. Download the file\n",
    "    downloaded_file_path = download_single_parquet_file(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        file_to_download=file_to_download,\n",
    "        local_dir=local_dir,\n",
    "        year=year,\n",
    "        month=month,\n",
    "    )\n",
    "\n",
    "    if not downloaded_file_path:\n",
    "        print(f\"DOWNLOAD FAILED for {file_to_download}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Successfully downloaded: {downloaded_file_path.name}\")\n",
    "\n",
    "    # Get metadata for the downloaded file\n",
    "    url = hf_hub_url(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        filename=file_to_download,\n",
    "    )\n",
    "    meta = get_hf_file_metadata(url=url)\n",
    "\n",
    "    # 3. Process the file\n",
    "    file_config = base_config.replace(parquet_path=str(downloaded_file_path))\n",
    "\n",
    "    file_context = {\n",
    "        \"current_file_num\": i + 1,\n",
    "        \"total_files\": len(files_to_download),\n",
    "        \"total_start_time\": total_start_time,\n",
    "        \"avg_rows_per_file\": 1_400_000,  # Rough average for ETA purposes\n",
    "        \"total_rows_estimate\": len(files_to_download)\n",
    "        * 1_400_000,\n",
    "    }\n",
    "\n",
    "    is_processing_successful = process_parquet_file(\n",
    "        config=file_config,\n",
    "        players_data=all_players_data,\n",
    "        file_context=file_context,\n",
    "        log_frequency=50_000,  # Log progress every 50k rows\n",
    "    )\n",
    "\n",
    "    print (\"is_processing_successful:\", is_processing_successful)\n",
    "\n",
    "    if is_processing_successful:\n",
    "        print(f\"PROCESSING SUCCESSFUL for {downloaded_file_path.name}\")\n",
    "        # Register the file as processed\n",
    "        # So we don't re-download and re-process it in the future\n",
    "        registry.mark_file_processed(\n",
    "            month=f\"{year}-{month}\",\n",
    "            filename=downloaded_file_path.name,\n",
    "            size=meta.size,\n",
    "            etag=meta.etag,\n",
    "        )\n",
    "        print(\"Registered file as processed.\")\n",
    "        # Delete the file to save space\n",
    "        os.remove(downloaded_file_path)\n",
    "        print(f\"Deleted local file: {downloaded_file_path.name}\")\n",
    "    else:\n",
    "        print(f\"PROCESSING FAILED for {downloaded_file_path.name}\")\n",
    "        # Still mark it as processed in file registry; don't bother re-downloading it; we have eighty bajillion other files to work with\n",
    "        registry.mark_file_processed(\n",
    "            month=f\"{year}-{month}\",\n",
    "            filename=downloaded_file_path.name,\n",
    "            size=meta.size,\n",
    "            etag=meta.etag,\n",
    "        )\n",
    "        os.remove(downloaded_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8a219",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
