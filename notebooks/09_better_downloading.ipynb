{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecceed8",
   "metadata": {},
   "source": [
    "# Downloading Experiments\n",
    "\n",
    "This notebook is a proof of concept\n",
    "\n",
    "Right now, we have some flaws in our raw data download system. We want to make sure we get this right because we will be downloading and processing hundreds or thousands of GBs of raw data parquet files.\n",
    "\n",
    "So, we'll be messing around here with some implementations, and if they work, we'll be replacing parts of our original code with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5bdd03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd6cc3",
   "metadata": {},
   "source": [
    "## Getting file names\n",
    "\n",
    "We want to use file names and other meta data to do dupe checks of what we've already processed, and see what we still need to download from a certain month. Let's run this code that gets all file names from the repo, to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d1ccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 files found for 2025-03\n",
      "data/year=2025/month=03/train-00000-of-00069.parquet\n",
      "data/year=2025/month=03/train-00001-of-00069.parquet\n",
      "data/year=2025/month=03/train-00002-of-00069.parquet\n",
      "data/year=2025/month=03/train-00003-of-00069.parquet\n",
      "data/year=2025/month=03/train-00004-of-00069.parquet\n",
      "data/year=2025/month=03/train-00005-of-00069.parquet\n",
      "data/year=2025/month=03/train-00006-of-00069.parquet\n",
      "data/year=2025/month=03/train-00007-of-00069.parquet\n",
      "data/year=2025/month=03/train-00008-of-00069.parquet\n",
      "data/year=2025/month=03/train-00009-of-00069.parquet\n",
      "data/year=2025/month=03/train-00010-of-00069.parquet\n",
      "data/year=2025/month=03/train-00011-of-00069.parquet\n",
      "data/year=2025/month=03/train-00012-of-00069.parquet\n",
      "data/year=2025/month=03/train-00013-of-00069.parquet\n",
      "data/year=2025/month=03/train-00014-of-00069.parquet\n",
      "data/year=2025/month=03/train-00015-of-00069.parquet\n",
      "data/year=2025/month=03/train-00016-of-00069.parquet\n",
      "data/year=2025/month=03/train-00017-of-00069.parquet\n",
      "data/year=2025/month=03/train-00018-of-00069.parquet\n",
      "data/year=2025/month=03/train-00019-of-00069.parquet\n"
     ]
    }
   ],
   "source": [
    "# Here we get a list of raw data files for the given month/year\n",
    "\n",
    "# File names in the remote repo are structured like:\n",
    "# data/year=2025/month=03/train-00001-of-00065.parquet\n",
    "# Obviously, there will be different amounts of them so it won't always be -00065.parquet\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from pathlib import Path\n",
    "\n",
    "year = \"2025\"\n",
    "month = \"03\"  # <-- type manually\n",
    "\n",
    "api = HfApi()\n",
    "files = api.list_repo_files(\n",
    "    repo_id=\"Lichess/standard-chess-games\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Filter for that year/month\n",
    "target_prefix = f\"data/year={year}/month={month}/\"\n",
    "all_file_names_in_month = [f for f in files if f.startswith(target_prefix)]\n",
    "\n",
    "print(len(all_file_names_in_month), f\"files found for {year}-{month}\")\n",
    "for f in all_file_names_in_month[:20]:  # preview first 20\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3554f",
   "metadata": {},
   "source": [
    "## Dupe checks\n",
    "\n",
    "Now that we have the list of raw data file names for the given month and year, we'll perform our dupe checks. This parses through the list of file names to make sure we haven't already processed any of these files.\n",
    "\n",
    "Each file is a 1GB download, so it's obviously in our best interest not to download a file we've already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d358efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 new files to download\n"
     ]
    }
   ],
   "source": [
    "# Parse the list of raw data file names to make sure we haven't already processed any of these files, and skip downloading any dupes\n",
    "\n",
    "from huggingface_hub import get_hf_file_metadata, hf_hub_url\n",
    "\n",
    "import sys\n",
    "\n",
    "# Current working directory (should be project root)\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.file_processing.raw_data_file_dupe_checks import FileRegistry  # noqa: E402\n",
    "\n",
    "# Init registry\n",
    "registry = FileRegistry()\n",
    "\n",
    "# Remove files already processed\n",
    "non_dupe_files = []\n",
    "for f in all_file_names_in_month:\n",
    "    url = hf_hub_url(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        filename=f,\n",
    "    )\n",
    "    meta = get_hf_file_metadata(url=url)\n",
    "    size = meta.size\n",
    "    etag = meta.etag\n",
    "    if not registry.is_file_processed(f\"{year}-{month}\", Path(f).name, size, etag):\n",
    "        non_dupe_files.append(f)\n",
    "\n",
    "print(len(non_dupe_files), \"new files to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bbf91",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Now to configure how we want our downloading and processing pipeline to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a040c3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing database schema...\n",
      "Database tables are ready.\n",
      "Prepared to download and process 5 files.\n"
     ]
    }
   ],
   "source": [
    "# Config stuff\n",
    "import importlib\n",
    "from utils.database import db_utils\n",
    "importlib.reload(db_utils)\n",
    "\n",
    "from utils.downloading_raw_parquet_data.raw_parquet_data_file_downloader import (\n",
    "    download_single_parquet_file,\n",
    ")\n",
    "from utils.file_processing.process_parquet_file import (\n",
    "    process_parquet_file,\n",
    ")\n",
    "from utils.file_processing.types_and_classes import ProcessingConfig\n",
    "from utils.database.db_utils import get_db_connection, setup_database\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "year, month = 2025, 3\n",
    "max_files_to_download = 5  # For testing; set to None to process all new files\n",
    "local_dir = Path(\"../data/raw/better_downloading_experiments\")\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the path for the DuckDB database file.\n",
    "db_path = Path(\"../data/processed/chess_games.db\")\n",
    "db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Base config for processing. This will be used for each file.\n",
    "base_config = ProcessingConfig(\n",
    "    parquet_path=\"\",  # This will be set per-file\n",
    "    db_path=db_path,\n",
    "    batch_size=400_000,\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference_between_players=100,\n",
    "    allowed_time_controls={\"Blitz\", \"Rapid\", \"Classical\"},\n",
    ")\n",
    "\n",
    "# --- Database Initialization ---\n",
    "# Set up the database schema before starting any processing.\n",
    "# This is idempotent; it will only create tables if they don't exist.\n",
    "with get_db_connection(db_path) as con:\n",
    "    setup_database(con)\n",
    "\n",
    "\n",
    "# --- File Selection ---\n",
    "# Use the list of non-duplicate files from the previous cell\n",
    "files_to_download = non_dupe_files\n",
    "if max_files_to_download is not None:\n",
    "    files_to_download = non_dupe_files[:max_files_to_download]\n",
    "\n",
    "print(f\"Prepared to download and process {len(files_to_download)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2fbd3",
   "metadata": {},
   "source": [
    "## Downloading, processing, deleting\n",
    "\n",
    "Now, we'lll download, process and delete our raw data files one by one.\n",
    "\n",
    "The workflow is:\n",
    "\n",
    "1. Download a file\n",
    "2. Process that file, extracting the game data we want\n",
    "3. Delete that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dcd4c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Downloading [1/5] ---\n",
      "File saved to ../data/raw/better_downloading_experiments/2025-03-train-00000-of-00069.parquet\n",
      "Successfully downloaded: 2025-03-train-00000-of-00069.parquet\n",
      "Processing 1,413,223 rows in 4 batches of size 400,000\n",
      "\n",
      "--- Starting Batch 1/4 ---\n",
      "    _get_or_create_ids for player with 97386 names.\n",
      "    All 97386 player names already exist in DB.\n",
      "    _get_or_create_ids for opening with 457 names.\n",
      "    Found 448 new opening names to insert: ['E80', 'B51', 'C39', 'A10', 'C46', 'D71', 'C11', 'D48', 'A53', 'E00']...\n",
      "An error occurred during file processing: Constraint Error: NOT NULL constraint failed: opening.eco\n",
      "Database connection closed.\n",
      "PROCESSING FAILED for 2025-03-train-00000-of-00069.parquet\n",
      "Registered file as processed to avoid re-downloading.\n",
      "Deleted local file: 2025-03-train-00000-of-00069.parquet\n",
      "\n",
      "--- Downloading [2/5] ---\n",
      "File saved to ../data/raw/better_downloading_experiments/2025-03-train-00001-of-00069.parquet\n",
      "Successfully downloaded: 2025-03-train-00001-of-00069.parquet\n",
      "Processing 1,413,223 rows in 4 batches of size 400,000\n",
      "\n",
      "--- Starting Batch 1/4 ---\n",
      "    _get_or_create_ids for player with 118548 names.\n",
      "    Found 93505 new player names to insert: ['SmykNikita', 'AlexZam1', 'Raskolnichess', 'Santatra14', 'zunguzungu', 'Artem231790', 'ayushhhh_7', 'manjtmor', 'tantakelly7', 'DerHund']...\n",
      "Database connection closed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     38\u001b[39m file_context = {\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcurrent_file_num\u001b[39m\u001b[33m\"\u001b[39m: i + \u001b[32m1\u001b[39m,\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtotal_files\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(files_to_download),\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtotal_start_time\u001b[39m\u001b[33m\"\u001b[39m: total_start_time,\n\u001b[32m     42\u001b[39m }\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# The main processing call. All data is now written to the DuckDB file.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m is_processing_successful = \u001b[43mprocess_parquet_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# 3. Register and Delete on Success\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_processing_successful:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/notebooks/utils/file_processing/process_parquet_file.py:96\u001b[39m, in \u001b[36mprocess_parquet_file\u001b[39m\u001b[34m(config, file_context)\u001b[39m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Hand off the batch to the core processing function.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_con\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mperf_tracker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperf_tracker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m batch_time = perf_tracker.end_batch(\u001b[38;5;28mlen\u001b[39m(batch_df))\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Batch processed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/notebooks/utils/file_processing/process_game_batch.py:215\u001b[39m, in \u001b[36mprocess_batch\u001b[39m\u001b[34m(batch_df, con, config, perf_tracker)\u001b[39m\n\u001b[32m    213\u001b[39m player_id_cache: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m] = {}\n\u001b[32m    214\u001b[39m opening_id_cache: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m] = {}\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[43m_get_or_create_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplayer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer_id_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer_titles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m _get_or_create_ids(con, \u001b[33m\"\u001b[39m\u001b[33mopening\u001b[39m\u001b[33m\"\u001b[39m, openings_eco, opening_id_cache, opening_names)\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# 4. Aggregate game results in memory\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/notebooks/utils/file_processing/process_game_batch.py:139\u001b[39m, in \u001b[36m_get_or_create_ids\u001b[39m\u001b[34m(con, entity_type, names, cache, extra_data)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m entity_type == \u001b[33m\"\u001b[39m\u001b[33mplayer\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    138\u001b[39m     new_entries = [(name, extra_data.get(name)) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m new_names]\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mINSERT INTO \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mentity_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m (player_name, title) VALUES (?, ?)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_entries\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m entity_type == \u001b[33m\"\u001b[39m\u001b[33mopening\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    143\u001b[39m     new_entries = [\n\u001b[32m    144\u001b[39m         (eco, extra_data.get(eco, \u001b[33m\"\u001b[39m\u001b[33mUnknown Opening\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m eco \u001b[38;5;129;01min\u001b[39;00m new_names\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen abc>:117\u001b[39m, in \u001b[36m__instancecheck__\u001b[39m\u001b[34m(cls, instance)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "# --- Main Loop ---\n",
    "total_start_time = time.time()\n",
    "\n",
    "for i, file_to_download in enumerate(files_to_download):\n",
    "    print(f\"\\n--- Downloading [{i+1}/{len(files_to_download)}] ---\")\n",
    "\n",
    "    # 1. Download the file\n",
    "    downloaded_file_path = download_single_parquet_file(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        file_to_download=file_to_download,\n",
    "        local_dir=local_dir,\n",
    "        year=year,\n",
    "        month=month,\n",
    "    )\n",
    "\n",
    "    if not downloaded_file_path:\n",
    "        print(f\"DOWNLOAD FAILED for {file_to_download}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Successfully downloaded: {downloaded_file_path.name}\")\n",
    "\n",
    "    # Get metadata for the downloaded file\n",
    "    url = hf_hub_url(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        filename=file_to_download,\n",
    "    )\n",
    "    meta = get_hf_file_metadata(url=url)\n",
    "\n",
    "    # 2. Process the file\n",
    "    # Create a specific config for this file by updating the path.\n",
    "    file_config = base_config.replace(parquet_path=str(downloaded_file_path))\n",
    "\n",
    "    # The file_context dictionary provides logging/ETA info for the processing function.\n",
    "    file_context = {\n",
    "        \"current_file_num\": i + 1,\n",
    "        \"total_files\": len(files_to_download),\n",
    "        \"total_start_time\": total_start_time,\n",
    "    }\n",
    "\n",
    "    # The main processing call. All data is now written to the DuckDB file.\n",
    "    is_processing_successful = process_parquet_file(\n",
    "        config=file_config,\n",
    "        file_context=file_context,\n",
    "    )\n",
    "\n",
    "    # 3. Register and Delete on Success\n",
    "    if is_processing_successful:\n",
    "        print(f\"PROCESSING SUCCESSFUL for {downloaded_file_path.name}\")\n",
    "        # Register the file as processed so we don't re-download it.\n",
    "        registry.mark_file_processed(\n",
    "            month=f\"{year}-{month}\",\n",
    "            filename=downloaded_file_path.name,\n",
    "            size=meta.size,\n",
    "            etag=meta.etag,\n",
    "        )\n",
    "        print(\"Registered file as processed.\")\n",
    "        # Delete the local raw file to save space.\n",
    "        os.remove(downloaded_file_path)\n",
    "        print(f\"Deleted local file: {downloaded_file_path.name}\")\n",
    "    else:\n",
    "        print(f\"PROCESSING FAILED for {downloaded_file_path.name}\")\n",
    "        # As per the plan, we mark the file as processed even on failure\n",
    "        # to avoid getting stuck on a problematic file.\n",
    "        registry.mark_file_processed(\n",
    "            month=f\"{year}-{month}\",\n",
    "            filename=downloaded_file_path.name,\n",
    "            size=meta.size,\n",
    "            etag=meta.etag,\n",
    "        )\n",
    "        print(\"Registered file as processed to avoid re-downloading.\")\n",
    "        os.remove(downloaded_file_path)\n",
    "        print(f\"Deleted local file: {downloaded_file_path.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8a219",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
