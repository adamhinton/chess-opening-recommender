{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecceed8",
   "metadata": {},
   "source": [
    "# Downloading Experiments\n",
    "\n",
    "This notebook is a proof of concept\n",
    "\n",
    "Right now, we have some flaws in our raw data download system. We want to make sure we get this right because we will be downloading and processing hundreds or thousands of GBs of raw data parquet files.\n",
    "\n",
    "So, we'll be messing around here with some implementations, and if they work, we'll be replacing parts of our original code with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5bdd03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Allocating 10 threads based on 85% CPU usage.\n",
      "OMP_NUM_THREADS: 10\n",
      "OPENBLAS_NUM_THREADS: 10\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub --quiet\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Percent of CPU to allocate (approximation, affects DuckDB threads)\n",
    "cpu_allocation_percent = 85  # Adjust this value as needed\n",
    "allocated_threads = max(1, int(os.cpu_count() * (cpu_allocation_percent / 100)))\n",
    "print(\n",
    "    f\"Allocating {allocated_threads} threads based on {cpu_allocation_percent}% CPU usage.\"\n",
    ")\n",
    "\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(allocated_threads)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(allocated_threads)\n",
    "\n",
    "print(\"OMP_NUM_THREADS:\", os.environ.get(\"OMP_NUM_THREADS\"))\n",
    "print(\"OPENBLAS_NUM_THREADS:\", os.environ.get(\"OPENBLAS_NUM_THREADS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd6cc3",
   "metadata": {},
   "source": [
    "## Getting file names\n",
    "\n",
    "We want to use file names and other meta data to do dupe checks of what we've already processed, and see what we still need to download from a certain month. Let's run this code that gets all file names from the repo, to see what the data looks like.\n",
    "\n",
    "TODO I wrote get_parquet_file_names for this; if we use this notebook again, convert this functionality to use that util."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d1ccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 files found for 2024-11\n",
      "data/year=2024/month=11/train-00000-of-00385.parquet\n",
      "data/year=2024/month=11/train-00001-of-00385.parquet\n",
      "data/year=2024/month=11/train-00002-of-00385.parquet\n",
      "data/year=2024/month=11/train-00003-of-00385.parquet\n",
      "data/year=2024/month=11/train-00004-of-00385.parquet\n",
      "data/year=2024/month=11/train-00005-of-00385.parquet\n",
      "data/year=2024/month=11/train-00006-of-00385.parquet\n",
      "data/year=2024/month=11/train-00007-of-00385.parquet\n",
      "data/year=2024/month=11/train-00008-of-00385.parquet\n",
      "data/year=2024/month=11/train-00009-of-00385.parquet\n",
      "data/year=2024/month=11/train-00010-of-00385.parquet\n",
      "data/year=2024/month=11/train-00011-of-00385.parquet\n",
      "data/year=2024/month=11/train-00012-of-00385.parquet\n",
      "data/year=2024/month=11/train-00013-of-00385.parquet\n",
      "data/year=2024/month=11/train-00014-of-00385.parquet\n",
      "data/year=2024/month=11/train-00015-of-00385.parquet\n",
      "data/year=2024/month=11/train-00016-of-00385.parquet\n",
      "data/year=2024/month=11/train-00017-of-00385.parquet\n",
      "data/year=2024/month=11/train-00018-of-00385.parquet\n",
      "data/year=2024/month=11/train-00019-of-00385.parquet\n"
     ]
    }
   ],
   "source": [
    "# Here we get a list of raw data files for the given month/year\n",
    "\n",
    "# TODO I wrote get_parquet_file_names for this; if we use this notebook again, convert this functionality to use that util.\n",
    "\n",
    "# File names in the remote repo are structured like:\n",
    "# data/year=2025/month=03/train-00001-of-00065.parquet\n",
    "# Obviously, there will be different amounts of them so it won't always be -00065.parquet\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "year, month = 2024, 11\n",
    "max_files_to_download = 250  # For testing; set to None to process all new files\n",
    "\n",
    "api = HfApi()\n",
    "files = api.list_repo_files(\n",
    "    repo_id=\"Lichess/standard-chess-games\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Filter for that year/month\n",
    "target_prefix = f\"data/year={year}/month={month:02d}/\"\n",
    "all_file_names_in_month = [f for f in files if f.startswith(target_prefix)]\n",
    "\n",
    "print(len(all_file_names_in_month), f\"files found for {year}-{month}\")\n",
    "for f in all_file_names_in_month[:20]:  # preview first 20\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3554f",
   "metadata": {},
   "source": [
    "## Dupe checks\n",
    "\n",
    "Now that we have the list of raw data file names for the given month and year, we'll perform our dupe checks. This parses through the list of file names to make sure we haven't already processed any of these files.\n",
    "\n",
    "Each file is a 1GB download, so it's obviously in our best interest not to download a file we've already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the list of raw data file names to make sure we haven't already processed any of these files, and skip downloading any dupes\n",
    "\n",
    "from huggingface_hub import get_hf_file_metadata, hf_hub_url\n",
    "\n",
    "import sys\n",
    "\n",
    "# Current working directory (should be project root)\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.file_processing.raw_data_file_dupe_checks import FileRegistry  # noqa: E402\n",
    "\n",
    "# Init registry\n",
    "registry = FileRegistry()\n",
    "\n",
    "# Remove files already processed\n",
    "non_dupe_files = []\n",
    "month_str = f\"{year}-{month}\"\n",
    "for f in all_file_names_in_month:\n",
    "    url = hf_hub_url(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        filename=f,\n",
    "    )\n",
    "    meta = get_hf_file_metadata(url=url)\n",
    "    size = meta.size\n",
    "    etag = meta.etag\n",
    "\n",
    "    # This is the filename format that will be saved in the registry\n",
    "    expected_filename_in_registry = f\"{year}-{month:02d}-{Path(f).name}\"\n",
    "\n",
    "    if not registry.is_file_processed(\n",
    "        month_str, expected_filename_in_registry, size, etag\n",
    "    ):\n",
    "        non_dupe_files.append(f)\n",
    "\n",
    "print(len(non_dupe_files), \"new files to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405bbf91",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Now to configure how we want our downloading and processing pipeline to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config stuff\n",
    "import importlib\n",
    "from utils.database import db_utils, player_game_counts_db_utils\n",
    "from utils.file_processing import process_game_batch\n",
    "importlib.reload(db_utils)\n",
    "importlib.reload(process_game_batch)\n",
    "importlib.reload(player_game_counts_db_utils)\n",
    "\n",
    "\n",
    "from utils.downloading_raw_parquet_data.raw_parquet_data_file_downloader import (\n",
    "    download_single_parquet_file,\n",
    ")\n",
    "from utils.file_processing.process_parquet_file import (\n",
    "    process_parquet_file,\n",
    ")\n",
    "from utils.file_processing.types_and_classes import ProcessingConfig\n",
    "from utils.database.db_utils import get_db_connection, setup_database\n",
    "from utils.database.player_game_counts_db_utils import get_eligible_player_usernames\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "local_dir = Path(\"../data/raw/better_downloading_experiments\")\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the path for the DuckDB database file.\n",
    "db_path = Path(\"../data/processed/chess_games.db\")\n",
    "db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Path to the database containing our list of eligible players.\n",
    "player_counts_db_path = Path(\n",
    "    \"../data/processed/find_most_active_players/player_game_counts.duckdb\"\n",
    ")\n",
    "\n",
    "\n",
    "# Base config for processing. This will be used for each file.\n",
    "base_config = ProcessingConfig(\n",
    "    parquet_path=\"\",  # This will be set per-file\n",
    "    db_path=db_path,\n",
    "    batch_size=1_500_000,\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference_between_players=100,\n",
    "    allowed_time_controls={\"Blitz\", \"Rapid\", \"Classical\"},\n",
    ")\n",
    "\n",
    "# --- Database Initialization ---\n",
    "# Set up the database schema before starting any processing.\n",
    "# Only run setup_database if the DB is new or empty. Otherwise, skip to avoid schema/constraint errors.\n",
    "with get_db_connection(db_path) as con:\n",
    "    tables = con.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema='main';\").fetchall()\n",
    "    table_names = {t[0] for t in tables}\n",
    "    # Only run setup_database if no stats tables exist\n",
    "    if not any(t.startswith(\"player_opening_stats_\") for t in table_names):\n",
    "        setup_database(con)\n",
    "    con.execute(f\"PRAGMA threads={allocated_threads}\")\n",
    "\n",
    "# --- Load Eligible Players ---\n",
    "# Load the set of usernames for players we want to include in our analysis.\n",
    "# Games will be filtered to only include those where at least one player is in this set.\n",
    "with get_db_connection(player_counts_db_path) as con:\n",
    "    eligible_players = get_eligible_player_usernames(con)\n",
    "\n",
    "\n",
    "# --- File Selection ---\n",
    "# Use the list of non-duplicate files from the previous cell\n",
    "files_to_download = non_dupe_files\n",
    "if max_files_to_download is not None:\n",
    "    files_to_download = non_dupe_files[:max_files_to_download]\n",
    "\n",
    "print(f\"Prepared to download and process {len(files_to_download)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2fbd3",
   "metadata": {},
   "source": [
    "## Downloading, processing, deleting\n",
    "\n",
    "Now, we'lll download, process and delete our raw data files one by one.\n",
    "\n",
    "The workflow is:\n",
    "\n",
    "1. Download a file\n",
    "2. Process that file, extracting the game data we want\n",
    "3. Delete that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Timing Stats ---\n",
    "timing_stats = defaultdict(list)\n",
    "file_metrics = []  # To store metrics for each file\n",
    "total_start_time = time.time()\n",
    "\n",
    "for i, file_to_download in enumerate(files_to_download):\n",
    "    # Buffer for consolidated logging\n",
    "    log_buffer = []\n",
    "\n",
    "    log_buffer.append(f\"\\n--- Downloading [{i+1}/{len(files_to_download)}] ---\")\n",
    "    step_start_time = time.time()\n",
    "\n",
    "    # 1. Download the file\n",
    "    downloaded_file_path = download_single_parquet_file(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        file_to_download=file_to_download,\n",
    "        local_dir=local_dir,\n",
    "        year=year,\n",
    "        month=month,\n",
    "    )\n",
    "\n",
    "    timing_stats['download'].append(time.time() - step_start_time)\n",
    "    if not downloaded_file_path:\n",
    "        log_buffer.append(f\"DOWNLOAD FAILED for {file_to_download}. Skipping.\")\n",
    "        print(\"\\n\".join(log_buffer))  # Print all logs for this file at once\n",
    "        continue\n",
    "\n",
    "    log_buffer.append(f\"Successfully downloaded: {downloaded_file_path.name}\")\n",
    "\n",
    "    # Get metadata for the downloaded file\n",
    "    url = hf_hub_url(\n",
    "        repo_id=\"Lichess/standard-chess-games\",\n",
    "        repo_type=\"dataset\",\n",
    "        filename=file_to_download,\n",
    "    )\n",
    "    meta = get_hf_file_metadata(url=url)\n",
    "\n",
    "    # 2. Process the file\n",
    "    step_start_time = time.time()\n",
    "    file_config = base_config.replace(parquet_path=str(downloaded_file_path))\n",
    "    file_context = {\n",
    "        \"current_file_num\": i + 1,\n",
    "        \"total_files\": len(files_to_download),\n",
    "        \"total_start_time\": total_start_time,\n",
    "    }\n",
    "    is_processing_successful = process_parquet_file(\n",
    "        config=file_config,\n",
    "        eligible_players=eligible_players,\n",
    "        file_context=file_context,\n",
    "    )\n",
    "    process_time = time.time() - step_start_time\n",
    "    timing_stats['process'].append(process_time)\n",
    "\n",
    "    # Calculate games per second (if applicable)\n",
    "    games_processed = meta.size // 1_000_000  # Approximation based on file size\n",
    "    games_per_second = games_processed / process_time if process_time > 0 else 0\n",
    "\n",
    "    # 3. Register and Delete on Success\n",
    "    step_start_time = time.time()\n",
    "    if is_processing_successful:\n",
    "        log_buffer.append(f\"PROCESSING SUCCESSFUL for {downloaded_file_path.name}\")\n",
    "        registry.mark_file_processed(\n",
    "            month=f\"{year}-{month}\",\n",
    "            filename=downloaded_file_path.name,\n",
    "            size=meta.size,\n",
    "            etag=meta.etag,\n",
    "        )\n",
    "        log_buffer.append(\"Registered file as processed.\")\n",
    "        os.remove(downloaded_file_path)\n",
    "        log_buffer.append(f\"Deleted local file: {downloaded_file_path.name}\")\n",
    "    else:\n",
    "        log_buffer.append(f\"PROCESSING FAILED for {downloaded_file_path.name}\")\n",
    "        # registry.mark_file_processed(\n",
    "        #     month=f\"{year}-{month}\",\n",
    "        #     filename=downloaded_file_path.name,\n",
    "        #     size=meta.size,\n",
    "        #     etag=meta.etag,\n",
    "        # )\n",
    "        log_buffer.append(\"Registered file as processed to avoid re-downloading.\")\n",
    "        os.remove(downloaded_file_path)\n",
    "        log_buffer.append(f\"Deleted local file: {downloaded_file_path.name}\")\n",
    "    timing_stats['register_delete'].append(time.time() - step_start_time)\n",
    "\n",
    "    # Add file metrics to summary\n",
    "    file_metrics.append({\n",
    "        \"file_name\": downloaded_file_path.name,\n",
    "        \"total_time\": process_time,\n",
    "        \"games_per_second\": games_per_second,\n",
    "    })\n",
    "\n",
    "    # Log file-level summary\n",
    "    log_buffer.append(f\"\\n--- File Summary ---\")\n",
    "    log_buffer.append(f\"File: {downloaded_file_path.name}, Total Time: {process_time:.2f}s, Games Per Second: {games_per_second:.2f}\")\n",
    "\n",
    "    print(\"\\n\".join(log_buffer))  # Print all logs for this file at once\n",
    "\n",
    "total_elapsed_time = time.time() - total_start_time\n",
    "print(f\"\\nTotal elapsed time for all files: {total_elapsed_time:.2f}s\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
