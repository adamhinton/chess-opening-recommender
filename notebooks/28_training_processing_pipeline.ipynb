{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e163a7",
   "metadata": {},
   "source": [
    "# Notebook 28 ‚Äî Opening Recommender Model: Training Pipeline\n",
    "\n",
    "### 0. Overview and Goals\n",
    "\n",
    "This notebook defines the full pipeline for training the chess opening recommender model.\n",
    "The objective is to predict **player‚Äìopening performance scores** ((wins + (0.5 * draws) / num games)) for openings a player hasn‚Äôt yet played, based on their results in the openings they *have* played.\n",
    "\n",
    "The model will use **matrix factorization** with **stochastic gradient descent (SGD)** to learn latent factors representing player and opening characteristics.\n",
    "All computations will be implemented in **PyTorch**, with data loaded from my local **DuckDB** database.\n",
    "\n",
    "**High-level specs:**\n",
    "- Use only *White* openings initially (we‚Äôll extend to Black later).\n",
    "- Data source: processed player‚Äìopening stats from local DuckDB.\n",
    "- Predict: normalized ‚Äúscore‚Äù = win rate ((wins + 0.5 x draws) / total games).\n",
    "- Filter: only include entries with ‚â• `MIN_GAMES_THRESHOLD` (default = 10).\n",
    "- Ignore: rating differences, time controls, and other metadata for the base model.\n",
    "- Model parameters (to be defined in appropriate places for easy editing):\n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_PLAYERS_TO_PROCESS`\n",
    "- Logging and checkpoints throughout for reproducibility.\n",
    "- All random operations seeded for deterministic runs.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Extraction\n",
    "- Connect to local DuckDB and pull all processed player‚Äìopening statistics.\n",
    "- Verify schema consistency and include row-count sanity checks.\n",
    "- Filter for players with ratings above a minimum threshold (e.g., 1200).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Sanitization & Normalization\n",
    "- Apply confidence weighting using hierarchical Bayesian shrinkage to adjust scores for low-game-count entries.\n",
    "- Normalize player ratings (z-score) for use as side information.\n",
    "- Resequence player and opening IDs to be contiguous integers for embedding layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Splits\n",
    "- Split data into train, validation, and test sets (e.g., 75/15/10).\n",
    "- Ensure splits are handled correctly to avoid data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Enumerate Categorical Variables\n",
    "- Process ECO codes into categorical features (e.g., `eco_letter`, `eco_number`).\n",
    "- Store these as opening-level side information.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Data Structure\n",
    "- Convert the final, processed DataFrames into PyTorch Tensors.\n",
    "- Create custom `Dataset` and `DataLoader` classes for efficient batching.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Cross-Validation & Hyperparameter Tuning\n",
    "- Define ranges for hyperparameters (`NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`).\n",
    "- Perform k-fold cross-validation on a subset of the training data to find the best hyperparameter combination before the final training run.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Final Model Training Setup\n",
    "- Define constants and hyperparameters for the final model (using results from CV).\n",
    "- Instantiate the PyTorch model, optimizer (SGD), and learning rate scheduler.\n",
    "- Implement helper functions for training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Final Model Training Loop\n",
    "- Initialize player and opening embeddings.\n",
    "- Iterate through epochs with mini-batch SGD.\n",
    "- Compute and log training and validation metrics (e.g., RMSE) per epoch.\n",
    "- Save model checkpoints locally.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Evaluation\n",
    "- Evaluate the final trained model on the held-out test set.\n",
    "- Report final metrics (MSE, RMSE) and create visualizations (e.g., predicted vs. actual scores).\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Next Steps\n",
    "- Extend model to include Black openings.\n",
    "- Experiment with more complex architectures or hybrid inputs.\n",
    "- Integrate the trained model into an API for serving recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "- Every random seed and parameter definition will be explicit.\n",
    "- Every major step includes row-count, schema, and type validation.\n",
    "- Model artifacts and logs will be saved locally for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d823",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "Connect to DuckDB and extract all player-opening statistics.\n",
    "Verify schema and perform sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcea569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA EXTRACTION\n",
      "üìÅ Database exists: True\n",
      "üé® Color filter: White\n",
      "üîí Minimum holdout players: 1,000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd() / 'utils'))\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = Path.cwd().parent / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "COLOR_FILTER = 'w'  # 'w' for white, 'b' for black\n",
    "MIN_HOLDOUT_PLAYERS = 1000  # Minimum number of players to reserve for fold-in verification. These will not be used at all in this notebook for training, test/val or anything else.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: DATA EXTRACTION\")\n",
    "print(f\"üìÅ Database exists: {DB_PATH.exists()}\")\n",
    "print(f\"üé® Color filter: {'White' if COLOR_FILTER == 'w' else 'Black'}\")\n",
    "print(f\"üîí Minimum holdout players: {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9435033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£  Extracting player-opening stats (color: 'w')...\n",
      "   ‚Ä¢ Minimum rating filter: 1200\n",
      "\n",
      "2Ô∏è‚É£  Selecting holdout players for fold-in verification...\n",
      "   ‚Ä¢ Holdout size: 1,000 players minimum\n",
      "   ‚Ä¢ Total eligible players: 49,551\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 48,551\n",
      "   ‚Ä¢ Holdout percentage: 2.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fd2ccabc8e473ca45c305de3169c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Extracted 11,563,060 rows\n",
      "   ‚úì All required columns present: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "   ‚Ä¢ player_id: int32\n",
      "   ‚Ä¢ opening_id: int32\n",
      "   ‚Ä¢ num_games: int32\n",
      "   ‚Ä¢ score: float64\n",
      "   ‚Ä¢ eco: object\n",
      "\n",
      "6Ô∏è‚É£  Data statistics...\n",
      "   ‚Ä¢ Total rows: 11,563,060\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "   ‚Ä¢ Total games (sum): 228,740,365\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 50000\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 13462\n",
      "   ‚Ä¢ Mean: 19.8\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5007\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0          1          39          6  0.666667  A00\n",
      "1          1          49          2  0.500000  A00\n",
      "2          1          53          1  0.000000  A00\n",
      "3          1         182          1  0.000000  A04\n",
      "4          1         187          1  1.000000  A04\n",
      "5          1         671          3  1.000000  B00\n",
      "6          1         675          3  0.000000  B00\n",
      "7          1         677          6  0.333333  B00\n",
      "8          1         688         25  0.620000  B00\n",
      "9          1         717          1  1.000000  B00\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (11563060, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚úì Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# Get our training database\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "MAX_PLAYERS = 50_000\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player-opening stats (color: '{COLOR_FILTER}')...\")\n",
    "\n",
    "    # Extract stats with calculated score and num_games\n",
    "    # Filter by color, minimum rating, and calculate score in the database\n",
    "    MIN_RATING = 1200\n",
    "    print(f\"   ‚Ä¢ Minimum rating filter: {MIN_RATING}\")\n",
    "\n",
    "    print(f\"\\n2Ô∏è‚É£  Selecting holdout players for fold-in verification...\")\n",
    "    print(f\"   ‚Ä¢ Holdout size: {MIN_HOLDOUT_PLAYERS:,} players minimum\")\n",
    "\n",
    "    player_query = f\"\"\"\n",
    "        SELECT DISTINCT p.id as player_id\n",
    "        FROM player p\n",
    "        JOIN player_opening_stats pos ON p.id = pos.player_id\n",
    "        WHERE p.rating >= {MIN_RATING}\n",
    "        AND pos.color = '{COLOR_FILTER}'\n",
    "        LIMIT {MAX_PLAYERS}\n",
    "    \"\"\"\n",
    "\n",
    "    all_eligible_players = pd.DataFrame(con.execute(player_query).df())\n",
    "    total_eligible = len(all_eligible_players)\n",
    "    print(f\"   ‚Ä¢ Total eligible players: {total_eligible:,}\")\n",
    "\n",
    "    if total_eligible < MIN_HOLDOUT_PLAYERS:\n",
    "        raise ValueError(f\"Not enough eligible players ({total_eligible:,}) to create holdout set of {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "    # Randomly sample holdout players (deterministic with seed)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    holdout_player_ids = np.random.choice(\n",
    "        all_eligible_players['player_id'].values,\n",
    "        size=MIN_HOLDOUT_PLAYERS,\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    training_player_ids = set(all_eligible_players['player_id'].values) - set(holdout_player_ids)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Holdout players selected: {len(holdout_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Training players available: {len(training_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Holdout percentage: {100 * len(holdout_player_ids) / total_eligible:.1f}%\")\n",
    "\n",
    "    # SQL-friendly string\n",
    "    training_player_ids_str = ','.join(map(str, training_player_ids))\n",
    "\n",
    "    # Extract data ONLY for training players\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            pos.player_id,\n",
    "            pos.opening_id,\n",
    "            pos.num_wins + pos.num_draws + pos.num_losses as num_games,\n",
    "            (pos.num_wins + (pos.num_draws * 0.5)) / \n",
    "                NULLIF(pos.num_wins + pos.num_draws + pos.num_losses, 0) as score,\n",
    "            o.eco\n",
    "        FROM player_opening_stats pos\n",
    "        JOIN opening o ON pos.opening_id = o.id\n",
    "        JOIN player p ON pos.player_id = p.id\n",
    "        WHERE pos.color = '{COLOR_FILTER}'\n",
    "        AND p.rating >= {MIN_RATING}\n",
    "        AND pos.player_id IN ({training_player_ids_str})\n",
    "        ORDER BY pos.player_id, pos.opening_id\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data = pd.DataFrame(con.execute(query).df())\n",
    "\n",
    "    print(f\"   ‚úì Extracted {len(raw_data):,} rows\")\n",
    "\n",
    "    # Also save holdout player IDs for later use\n",
    "    holdout_players_df = pd.DataFrame({'player_id': holdout_player_ids})\n",
    "\n",
    "    # Schema verification\n",
    "    required_columns = ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
    "\n",
    "    for col in required_columns:\n",
    "        if col not in raw_data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    print(f\"   ‚úì All required columns present: {required_columns}\")\n",
    "\n",
    "    # Data types verification\n",
    "    print(f\"   ‚Ä¢ player_id: {raw_data['player_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ opening_id: {raw_data['opening_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ num_games: {raw_data['num_games'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ score: {raw_data['score'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ eco: {raw_data['eco'].dtype}\")\n",
    "\n",
    "    print(\"\\n6Ô∏è‚É£  Data statistics...\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {len(raw_data):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Total games (sum): {raw_data['num_games'].sum():,}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Player ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['player_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['player_id'].max()}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Opening ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['opening_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['opening_id'].max()}\")\n",
    "\n",
    "    print(f\"\\n   Games per entry:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['num_games'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['num_games'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['num_games'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['num_games'].median():.0f}\")\n",
    "\n",
    "    print(f\"\\n   Score distribution:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['score'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['score'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['score'].mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['score'].median():.4f}\")\n",
    "\n",
    "    print(\"\\n7Ô∏è‚É£  Checking for null values...\")\n",
    "    null_counts = raw_data.isnull().sum()\n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"   ‚úì No null values found\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "\n",
    "    print(\"\\n8Ô∏è‚É£  Sample of extracted data (first 10 rows):\")\n",
    "    print(raw_data.head(10).to_string())\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nData shape: {raw_data.shape}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "\n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"\\n‚úì Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad110b85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "519d8f2f",
   "metadata": {},
   "source": [
    "## Step 2: Data Sanitization & Normalization\n",
    "\n",
    "Filter low-quality data, handle duplicates, and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74f0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: DATA SANITIZATION & NORMALIZATION\n",
      "\n",
      "üìä Starting data shape: (11563060, 5)\n",
      "   ‚Ä¢ Rows: 11,563,060\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Before: 11,563,060 rows\n",
      "   ‚Ä¢ After: 2,896,054 rows\n",
      "   ‚Ä¢ Filtered out: 8,667,006 rows (75.0%)\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Data statistics:\n",
      "   ‚Ä¢ Total rows: 2,896,054\n",
      "   ‚Ä¢ Unique players: 48,468\n",
      "   ‚Ä¢ Unique openings: 2,713\n",
      "   ‚Ä¢ Total games: 206,154,116\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1067.5\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5111\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "2732041      46400        3235         55  0.436364  D00\n",
      "456145        7654         165         69  0.528986  A03\n",
      "1003387      16902         773        100  0.400000  B01\n",
      "870851       14639        2148         38  0.565789  C55\n",
      "2874547      49404        2053        202  0.621287  C50\n",
      "1641984      27736        3178         18  0.388889  C50\n",
      "2598412      43971        1437         11  0.681818  C11\n",
      "2257967      38153        2560         46  0.543478  D20\n",
      "813388       13672        2052         29  0.689655  C50\n",
      "2395988      40519        2133         12  0.458333  C54\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2896054, 5)\n",
      "Data reduction: 75.0%\n"
     ]
    }
   ],
   "source": [
    "# 2a. Filter low-quality data, handle duplicates, and prepare for training.\n",
    "\n",
    "# Configuration\n",
    "MIN_GAMES_THRESHOLD = 10\n",
    "\n",
    "print(\"STEP 2: DATA SANITIZATION & NORMALIZATION\")\n",
    "\n",
    "# Start with raw_data from Step 1\n",
    "print(f\"\\nüìä Starting data shape: {raw_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Rows: {len(raw_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "\n",
    "# 1. Filter by minimum games threshold\n",
    "print(f\"\\n1Ô∏è‚É£  Filtering entries with < {MIN_GAMES_THRESHOLD} games...\")\n",
    "before_filter = len(raw_data)\n",
    "clean_data = raw_data.query(f'num_games >= {MIN_GAMES_THRESHOLD}').copy()\n",
    "num_rows_after_filter = len(clean_data)\n",
    "num_rows_filtered_out = before_filter - num_rows_after_filter\n",
    "\n",
    "print(f\"   ‚Ä¢ Before: {before_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ After: {num_rows_after_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ Filtered out: {num_rows_filtered_out:,} rows ({100*num_rows_filtered_out/before_filter:.1f}%)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "num_duplicates = clean_data.duplicated(subset=['player_id', 'opening_id']).sum()\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {num_duplicates} duplicate player-opening entries\")\n",
    "    dup_mask = clean_data.duplicated(subset=['player_id', 'opening_id'], keep=False)\n",
    "    print(\"\\n   Sample of duplicates:\")\n",
    "    print(clean_data[dup_mask].head(10).to_string())\n",
    "    \n",
    "    # Keep only first occurrence of any duplicate player-opening pair\n",
    "    print(\"\\n   Removing duplicates (keeping first occurrence)...\")\n",
    "    clean_data = pd.DataFrame.drop_duplicates(clean_data, subset=['player_id', 'opening_id'], keep='first')\n",
    "    print(f\"   ‚úì After deduplication: {len(clean_data):,} rows\")\n",
    "\n",
    "# 3. Remove players with no qualifying openings\n",
    "\n",
    "# Note that a few players only play stuff like the Van't Kruijs which we've excluded, so a small numer of players will be excluded here\n",
    "players_before = clean_data['player_id'].nunique()\n",
    "\n",
    "# Count openings per player\n",
    "num_openings_per_player = pd.DataFrame(clean_data.groupby('player_id').size(), columns=['count'])\n",
    "players_with_data = num_openings_per_player[num_openings_per_player['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter\n",
    "clean_data = clean_data[clean_data['player_id'].isin(players_with_data)]\n",
    "players_after = clean_data['player_id'].nunique()\n",
    "\n",
    "# 4. Remove openings with no qualifying players\n",
    "num_openings_before = clean_data['opening_id'].nunique()\n",
    "\n",
    "num_players_per_opening = pd.DataFrame(clean_data.groupby('opening_id').size(), columns=['count'])\n",
    "openings_with_data = num_players_per_opening[num_players_per_opening['count'] > 0].index.tolist()\n",
    "\n",
    "clean_data = clean_data[clean_data['opening_id'].isin(openings_with_data)]\n",
    "openings_after = clean_data['opening_id'].nunique()\n",
    "\n",
    "# 5. Verify no null values using pd.isna()\n",
    "null_counts = pd.DataFrame.isna(clean_data).sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"   ‚úì No null values found\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "    for col, count in null_counts[null_counts > 0].items():\n",
    "        print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    # Drop rows with nulls using pd.DataFrame.dropna()\n",
    "    clean_data = pd.DataFrame.dropna(clean_data)\n",
    "    print(f\"   ‚úì Dropped null rows. New shape: {clean_data.shape}\")\n",
    "\n",
    "# Reset index using pd.DataFrame.reset_index()\n",
    "clean_data = pd.DataFrame.reset_index(clean_data, drop=True)\n",
    "\n",
    "# Final statistics using pd functions\n",
    "print(f\"\\n6Ô∏è‚É£  Data statistics:\")\n",
    "print(f\"   ‚Ä¢ Total rows: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {pd.Series.nunique(clean_data['player_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {pd.Series.nunique(clean_data['opening_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Total games: {pd.Series.sum(clean_data['num_games']):,}\")\n",
    "print(f\"   ‚Ä¢ Avg games per entry: {pd.Series.mean(clean_data['num_games']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg openings per player: {len(clean_data) / pd.Series.nunique(clean_data['player_id']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg players per opening: {len(clean_data) / pd.Series.nunique(clean_data['opening_id']):.1f}\")\n",
    "\n",
    "# Score distribution using pd functions\n",
    "print(f\"\\n   Score statistics:\")\n",
    "print(f\"   ‚Ä¢ Min: {pd.Series.min(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {pd.Series.quantile(clean_data['score'], 0.25):.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {pd.Series.median(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {pd.Series.quantile(clean_data['score'], 0.75):.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {pd.Series.max(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {pd.Series.mean(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {pd.Series.std(clean_data['score']):.4f}\")\n",
    "\n",
    "# Sample of cleaned data using pd.DataFrame.sample()\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of cleaned data (10 random rows):\")\n",
    "print(pd.DataFrame.sample(clean_data, min(10, len(clean_data)), random_state=42).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SANITIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCleaned data shape: {clean_data.shape}\")\n",
    "print(f\"Data reduction: {100 * (1 - len(clean_data)/len(raw_data)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e671b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\n",
      "============================================================\n",
      "\n",
      "üìä Global statistics:\n",
      "   ‚Ä¢ Global mean score: 0.5111\n",
      "   ‚Ä¢ Total entries: 2,896,054\n",
      "   ‚Ä¢ Unique openings: 2,713\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1667\n",
      "   ‚Ä¢ 25th percentile: 0.4962\n",
      "   ‚Ä¢ Median: 0.5160\n",
      "   ‚Ä¢ 75th percentile: 0.5364\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0506\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4860\n",
      "   ‚Ä¢ Players per opening (median): 156\n",
      "   ‚Ä¢ Total games range: [10, 5491055]\n",
      "   ‚Ä¢ Players range: [1, 42901]\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,896,054 entries\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Range: [0.1667, 0.9963]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000959\n",
      "   ‚Ä¢ Std adjustment: 0.076781\n",
      "   ‚Ä¢ Max adjustment: 0.457339\n",
      "   ‚Ä¢ Min adjustment: -0.457385\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003638\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001800\n",
      "   ‚Ä¢ 75th percentile (n=65 games): avg adjustment = -0.000413\n",
      "   ‚Ä¢ >75th percentile (n>65 games): avg adjustment = -0.001355\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ Median: 0.5117\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 351\u001b[39m\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m    349\u001b[39m K_PLAYER = \u001b[32m50\u001b[39m  \u001b[38;5;66;03m# Shrinkage constant for player-opening scores\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m clean_data = \u001b[43mapply_hierarchical_bayesian_shrinkage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_player\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK_PLAYER\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mapply_hierarchical_bayesian_shrinkage\u001b[39m\u001b[34m(data, k_player)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    159\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m120\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    161\u001b[39m low_game_sample = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_games\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m&\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_games\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_games\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m&\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_games\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m low_game_sample.iterrows():\n\u001b[32m    175\u001b[39m     adjustment = row[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m] - row[\u001b[33m\"\u001b[39m\u001b[33mscore_original\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/generic.py:6127\u001b[39m, in \u001b[36mNDFrame.sample\u001b[39m\u001b[34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[39m\n\u001b[32m   6124\u001b[39m obj_len = \u001b[38;5;28mself\u001b[39m.shape[axis]\n\u001b[32m   6126\u001b[39m \u001b[38;5;66;03m# Process random_state argument\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6127\u001b[39m rs = \u001b[43mcommon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6129\u001b[39m size = sample.process_sampling_size(n, frac, replace)\n\u001b[32m   6130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/common.py:454\u001b[39m, in \u001b[36mrandom_state\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03mHelper function for processing random_state arguments.\u001b[39;00m\n\u001b[32m    436\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    451\u001b[39m \n\u001b[32m    452\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(state) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, (np.ndarray, np.random.BitGenerator)):\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRandomState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, np.random.RandomState):\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2b. Apply hierarchical Bayesian shrinkage to adjust scores based on sample size confidence\n",
    "\n",
    "# Check if confidence already exists - if so, skip this processing\n",
    "if 'confidence' in clean_data.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'confidence' column already exists in data\")\n",
    "else:\n",
    "    # Define the processing function\n",
    "    # This is a long function, I recommend you fold it down in your editor\n",
    "    def apply_hierarchical_bayesian_shrinkage(data, k_player=50):\n",
    "        \"\"\"\n",
    "        Apply two-level hierarchical Bayesian shrinkage to adjust scores.\n",
    "        \n",
    "        A lot of our player-opening entries have a small number of games played, because openings are so specific.\n",
    "        This introduces sample size issues.\n",
    "        \n",
    "        We use TWO-LEVEL shrinkage:\n",
    "        Level 1: Calculate opening-specific means (these are our \"ground truth\" for each opening)\n",
    "        Level 2: Shrink individual player-opening scores toward their opening's mean\n",
    "        This is better than shrinking toward global mean because different openings have different baseline win rates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Clean data with columns: player_id, opening_id, score, num_games, eco\n",
    "        k_player : int\n",
    "            Shrinkage constant for player-opening scores (default: 50)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Data with adjusted scores and new 'confidence' column\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Calculate global mean score for comparison\n",
    "        global_mean_score = data[\"score\"].mean()\n",
    "        print(f\"\\nüìä Global statistics:\")\n",
    "        print(f\"   ‚Ä¢ Global mean score: {global_mean_score:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Total entries: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique openings: {data['opening_id'].nunique():,}\")\n",
    "        \n",
    "        # Store original scores for comparison\n",
    "        data = data.copy()\n",
    "        data[\"score_original\"] = data[\"score\"].copy()\n",
    "        \n",
    "        # LEVEL 1: Calculate opening-specific means and statistics        \n",
    "        opening_stats = (\n",
    "            data.groupby(\"opening_id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"score\": \"mean\",\n",
    "                    \"num_games\": \"sum\",\n",
    "                    \"player_id\": \"count\",  # Number of players who played this opening\n",
    "                }\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"score\": \"opening_mean\",\n",
    "                    \"num_games\": \"opening_total_games\",\n",
    "                    \"player_id\": \"opening_num_players\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        # Opening mean statistics\n",
    "        print(f\"\\n   Opening mean score distribution:\")\n",
    "        print(f\"   ‚Ä¢ Min: {opening_stats['opening_mean'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {opening_stats['opening_mean'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {opening_stats['opening_mean'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {opening_stats['opening_mean'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {opening_stats['opening_mean'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {opening_stats['opening_mean'].std():.4f}\")\n",
    "        \n",
    "        # Show distribution of opening sizes\n",
    "        print(f\"\\n   Opening sample size distribution:\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games per opening (median): {opening_stats['opening_total_games'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players per opening (median): {opening_stats['opening_num_players'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games range: [{opening_stats['opening_total_games'].min():.0f}, {opening_stats['opening_total_games'].max():.0f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players range: [{opening_stats['opening_num_players'].min():.0f}, {opening_stats['opening_num_players'].max():.0f}]\"\n",
    "        )\n",
    "        \n",
    "        # Merge opening means back into main dataframe\n",
    "        data = data.merge(\n",
    "            opening_stats[[\"opening_mean\"]], left_on=\"opening_id\", right_index=True, how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # LEVEL 2: Shrink player-opening scores toward opening-specific means\n",
    "        print(\n",
    "            f\"   Formula: adjusted_score = (num_games √ó player_score + {k_player} √ó opening_mean) / (num_games + {k_player})\"\n",
    "        )\n",
    "        \n",
    "        numerator = (data[\"num_games\"] * data[\"score_original\"]) + (\n",
    "            k_player * data[\"opening_mean\"]\n",
    "        )\n",
    "        denominator = data[\"num_games\"] + k_player\n",
    "        data[\"score\"] = numerator / denominator\n",
    "        \n",
    "        print(f\"   ‚úì Scores adjusted for {len(data):,} entries\")\n",
    "        \n",
    "        # Calculate confidence weights (will be used in loss function later)\n",
    "        data[\"confidence\"] = data[\"num_games\"] / (\n",
    "            data[\"num_games\"] + k_player\n",
    "        )\n",
    "        print(f\"   ‚úì Confidence weights calculated\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Range: [{data['confidence'].min():.4f}, {data['confidence'].max():.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # Statistics on the adjustment\n",
    "        score_diff = data[\"score\"] - data[\"score_original\"]\n",
    "        print(f\"\\n4Ô∏è‚É£  Adjustment statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean adjustment: {score_diff.mean():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Std adjustment: {score_diff.std():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Max adjustment: {score_diff.max():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Min adjustment: {score_diff.min():.6f}\")\n",
    "        \n",
    "        # Show distribution of adjustments\n",
    "        print(f\"\\n   Adjustment by num_games quartiles:\")\n",
    "        quartiles = data[\"num_games\"].quantile([0.25, 0.5, 0.75])\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 25th percentile (n={quartiles[0.25]:.0f} games): avg adjustment = {score_diff[data['num_games'] <= quartiles[0.25]].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 50th percentile (n={quartiles[0.5]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.25]) & (data['num_games'] <= quartiles[0.5])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 75th percentile (n={quartiles[0.75]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.5]) & (data['num_games'] <= quartiles[0.75])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ >75th percentile (n>{quartiles[0.75]:.0f} games): avg adjustment = {score_diff[data['num_games'] > quartiles[0.75]].mean():.6f}\"\n",
    "        )\n",
    "        \n",
    "        # New score distribution after adjustment\n",
    "        print(f\"\\n5Ô∏è‚É£  Adjusted score statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {data['score'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {data['score'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {data['score'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {data['score'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {data['score'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {data['score'].mean():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {data['score'].std():.4f}\")\n",
    "        \n",
    "        # Detailed sample showing the effect across different game counts\n",
    "        print(f\"\\n6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\")\n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        low_game_sample = data[\n",
    "            (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in low_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Medium-game entries (50-100 games) - MODERATE shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        med_game_sample = data[\n",
    "            (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in med_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   High-game entries (200+ games) - LOW shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        high_game_sample = data[data[\"num_games\"] >= 200].sample(\n",
    "            min(10, len(data[data[\"num_games\"] >= 200])), random_state=42\n",
    "        )\n",
    "        for idx, row in high_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        # Show extreme cases - comparing to both opening mean AND global mean\n",
    "        print(f\"\\n7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\")\n",
    "        \n",
    "        # Find entries where opening mean differs significantly from global mean\n",
    "        data[\"opening_deviation_from_global\"] = (\n",
    "            data[\"opening_mean\"] - global_mean_score\n",
    "        ).abs()\n",
    "        \n",
    "        print(f\"\\n   Openings with HIGHEST win rates (strong for White):\")\n",
    "        strong_openings = data.nlargest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in strong_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"(+{deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   Openings with LOWEST win rates (weak for White):\")\n",
    "        weak_openings = data.nsmallest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in weak_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"({deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        # Show specific examples where hierarchical shrinkage made a difference\n",
    "        print(f\"\\n8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\")\n",
    "        \n",
    "        # Find entries with strong openings where player did well\n",
    "        strong_opening_ids = data.nlargest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        strong_examples = data[\n",
    "            (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] > 0.6)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] > 0.6)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"\\n   Strong opening + good player performance (shrunk toward HIGH opening mean):\"\n",
    "        )\n",
    "        for idx, row in strong_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would lose {difference:+.4f} of deserved credit)\"\n",
    "            )\n",
    "        \n",
    "        # Find entries with weak openings where player did poorly\n",
    "        weak_opening_ids = data.nsmallest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        weak_examples = data[\n",
    "            (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] < 0.45)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] < 0.45)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Weak opening + poor player performance (shrunk toward LOW opening mean):\")\n",
    "        for idx, row in weak_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would unfairly boost by {-difference:+.4f})\"\n",
    "            )\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        data = data.drop(\n",
    "            columns=[\"score_original\", \"opening_mean\", \"opening_deviation_from_global\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nFinal data shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        print(f\"\\nNew columns added:\")\n",
    "        print(f\"   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\")\n",
    "        print(f\"   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\")\n",
    "        print(f\"\\nKey improvement over simple shrinkage:\")\n",
    "        print(f\"   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\")\n",
    "        print(f\"   ‚Ä¢ Preserves opening difficulty differences\")\n",
    "        print(f\"   ‚Ä¢ More accurate for both strong and weak openings\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    K_PLAYER = 50  # Shrinkage constant for player-opening scores\n",
    "    \n",
    "    clean_data = apply_hierarchical_bayesian_shrinkage(clean_data, k_player=K_PLAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a80e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. Gather player rating statistics\n",
    "\n",
    "print(\"STEP 2C: PLAYER RATING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to database and extract player ratings\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:    \n",
    "    unique_player_ids = clean_data['player_id'].unique()\n",
    "    player_ids_str = ','.join(map(str, unique_player_ids))\n",
    "    \n",
    "    # Query to get player ratings\n",
    "    rating_query = f\"\"\"\n",
    "        SELECT \n",
    "            id as player_id,\n",
    "            name,\n",
    "            title,\n",
    "            rating\n",
    "        FROM player\n",
    "        WHERE id IN ({player_ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    player_ratings = pd.DataFrame(con.execute(rating_query).df())\n",
    "    print(f\"   ‚úì Retrieved ratings for {len(player_ratings):,} players\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Check for missing ratings\n",
    "missing_ratings = player_ratings['rating'].isna().sum()\n",
    "if missing_ratings > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: {missing_ratings:,} players have missing ratings\")\n",
    "else:\n",
    "    print(f\"   ‚úì All players have ratings\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Basic rating statistics:\")\n",
    "print(f\"   ‚Ä¢ Count: {player_ratings['rating'].notna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Missing: {player_ratings['rating'].isna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Min: {player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Max: {player_ratings['rating'].max():.0f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {player_ratings['rating'].mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"   ‚Ä¢ Std Dev: {player_ratings['rating'].std():.2f}\")\n",
    "\n",
    "\n",
    "print(f\"\\n5Ô∏è‚É£  Ratings Percentiles\")\n",
    "percentiles = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,\n",
    "               0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "print(f\"\\n   {'Percentile':<12} {'Rating':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*12} {'-'*10} {'-'*40}\")\n",
    "\n",
    "for p in percentiles:\n",
    "    rating_value = player_ratings['rating'].quantile(p)\n",
    "    # Create a simple bar visualization\n",
    "    bar_length = int((rating_value - player_ratings['rating'].min()) / \n",
    "                     (player_ratings['rating'].max() - player_ratings['rating'].min()) * 40)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {p*100:>5.0f}%       {rating_value:>7.0f}    {bar}\")\n",
    "\n",
    "# Rating ranges and counts\n",
    "print(f\"\\n6Ô∏è‚É£  Rating distribution by range:\")\n",
    "rating_ranges = [\n",
    "    (0, 1000), (1000, 1200), (1200, 1400), (1400, 1600), \n",
    "    (1600, 1800), (1800, 2000), (2000, 2200), (2200, 2400), \n",
    "    (2400, 2600), (2600, 3000)\n",
    "]\n",
    "\n",
    "print(f\"\\n   {'Range':<15} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*15} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "\n",
    "for low, high in rating_ranges:\n",
    "    count = len(player_ratings[(player_ratings['rating'] >= low) & (player_ratings['rating'] < high)])\n",
    "    pct = 100 * count / len(player_ratings)\n",
    "    bar_length = int(pct * 0.4)  # Scale for visualization\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {low:>4}-{high:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Interquartile range\n",
    "iqr = player_ratings['rating'].quantile(0.75) - player_ratings['rating'].quantile(0.25)\n",
    "print(f\"\\n7Ô∏è‚É£  Spread statistics:\")\n",
    "print(f\"   ‚Ä¢ Range: {player_ratings['rating'].max() - player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Interquartile Range (IQR): {iqr:.0f}\")\n",
    "print(f\"   ‚Ä¢ 10th-90th percentile range: {player_ratings['rating'].quantile(0.90) - player_ratings['rating'].quantile(0.10):.0f}\")\n",
    "\n",
    "# Sample of players at different rating levels\n",
    "print(f\"\\n9Ô∏è‚É£  Sample players at different rating levels:\")\n",
    "sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "for p in sample_percentiles:\n",
    "    rating_threshold = player_ratings['rating'].quantile(p)\n",
    "    # Get a player near this rating\n",
    "    sample_player = player_ratings.iloc[(player_ratings['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "    print(f\"\\n   ~{p*100:.0f}th percentile (rating ‚âà {rating_threshold:.0f}):\")\n",
    "    for idx, row in sample_player.iterrows():\n",
    "        # print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f} {f'({row['title']})' if pd.notna(row['title']) else ''}\")\n",
    "        title_str = f\" ({row['title']})\" if pd.notna(row['title']) else \"\"\n",
    "        print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f}{title_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ RATING STATISTICS COMPLETE\")\n",
    "print(f\"   ‚Ä¢ Total players: {len(player_ratings):,}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings['rating'].min():.0f}, {player_ratings['rating'].max():.0f}]\")\n",
    "print(f\"   ‚Ä¢ Mean ¬± std: {player_ratings['rating'].mean():.0f} ¬± {player_ratings['rating'].std():.0f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d. Normalize player ratings using z-score normalization (for use as side information in MF model)\n",
    "\n",
    "# Check if we've already normalized ratings\n",
    "if 'player_side_info' in globals() and 'rating_z' in player_side_info.columns:\n",
    "    print(\"   SKIPPING STEP 2D: RATING NORMALIZATION\")\n",
    "    print(\"Ratings have already been normalized\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing normalized rating statistics:\")\n",
    "    print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "    print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "\n",
    "else:\n",
    "    def normalize_player_ratings(player_ratings_df):\n",
    "        \"\"\"\n",
    "        Apply z-score normalization to player ratings for use as side information.\n",
    "        \n",
    "        This creates a SEPARATE table of player-level features, NOT merged into clean_data.\n",
    "        Rating is side information - it describes the player, not the player-opening interaction.\n",
    "        \n",
    "        During training, the model will LOOK UP each player's rating_z from this table.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ratings_df : pd.DataFrame\n",
    "            Player ratings with columns: player_id, name, title, rating\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (player_side_info DataFrame, RATING_MEAN, RATING_STD)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calculate normalization parameters\n",
    "        RATING_MEAN = player_ratings_df['rating'].mean()\n",
    "        RATING_STD = player_ratings_df['rating'].std()\n",
    "        \n",
    "        print(f\"\\n1Ô∏è‚É£  Normalization parameters (calculated from {len(player_ratings_df):,} players):\")\n",
    "        print(f\"   ‚Ä¢ Mean: {RATING_MEAN:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Std Dev: {RATING_STD:.2f}\")\n",
    "        \n",
    "        # Create side information table - only keep player_id and rating for now\n",
    "        player_side_info = player_ratings_df[['player_id', 'rating']].copy()\n",
    "        player_side_info['rating_z'] = (player_side_info['rating'] - RATING_MEAN) / RATING_STD\n",
    "        \n",
    "        print(f\"\\n2Ô∏è‚É£  Normalized rating statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "        print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "        print(f\"   ‚Ä¢ Range: [{player_side_info['rating_z'].min():.2f}, {player_side_info['rating_z'].max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\n3Ô∏è‚É£  Sample normalized ratings across skill levels:\")\n",
    "        sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "        for p in sample_percentiles:\n",
    "            rating_threshold = player_side_info['rating'].quantile(p)\n",
    "            sample_player = player_side_info.iloc[(player_side_info['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "            for idx, row in sample_player.iterrows():\n",
    "                print(f\"   ~{p*100:.0f}th percentile: Player {idx} | \"\n",
    "                      f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n4Ô∏è‚É£  Interpretation guide:\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(1200 - RATING_MEAN)/RATING_STD:.1f}: 1200 player (minimum)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.25) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.25):.0f} player (25th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà  0.0: {RATING_MEAN:.0f} player (mean)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.75) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.75):.0f} player (75th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].max() - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].max():.0f} player (maximum)\")\n",
    "        \n",
    "        print(f\"\\n5Ô∏è‚É£  Player side information table structure:\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        # Set player_id as index for fast lookups\n",
    "        player_side_info = player_side_info.set_index('player_id')\n",
    "        \n",
    "        print(f\"\\n6Ô∏è‚É£  Sample entries from side information table:\")\n",
    "        sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "        for idx, row in sample_data.iterrows():\n",
    "            print(f\"   Player {idx:>5} | Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        # Drop rating column - we only need rating_z for the model\n",
    "        player_side_info = player_side_info.drop(columns=['rating'])\n",
    "        \n",
    "        # This is important - make sure every player in clean_data has a rating\n",
    "        missing_players = set(clean_data['player_id'].unique()) - set(player_side_info.index)\n",
    "        if len(missing_players) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in clean_data are missing from side_info!\")\n",
    "            print(f\"   Missing player IDs: {sorted(list(missing_players))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(player_side_info):,} players in clean_data have side information\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ RATING NORMALIZATION COMPLETE\")\n",
    "        print(f\"\\nCreated: player_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: player_id\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ clean_data: {clean_data.shape[0]:,} rows (player-opening interactions)\")\n",
    "        print(f\"   ‚Ä¢ player_side_info: {len(player_side_info):,} rows (one per player)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\")\n",
    "        print(f\"   RATING_MEAN = {RATING_MEAN:.2f}\")\n",
    "        print(f\"   RATING_STD = {RATING_STD:.2f}\")\n",
    "        print(f\"\\n   You'll need them to normalize ratings for new users at inference time.\")\n",
    "        \n",
    "        return player_side_info, RATING_MEAN, RATING_STD\n",
    "    \n",
    "    player_side_info, RATING_MEAN, RATING_STD = normalize_player_ratings(player_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eba9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_data.sample(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac119ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(player_side_info.sample(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30280db",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test/Val splits\n",
    "\n",
    "Here, I split my data and drop columns that are no longer needed. We're very close to being able to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train/Validation/Test Split (75/15/10)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 3: TRAIN/VALIDATION/TEST SPLIT\")\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Preparing data for split...\")\n",
    "\n",
    "# We don't need num_games for modeling because we have the `confidence` based on num_games\n",
    "# Keep: player_id, opening_id, score, eco, confidence\n",
    "X = clean_data[[\"player_id\", \"opening_id\", \"eco\", \"confidence\"]].copy()\n",
    "y = clean_data[\"score\"].copy()\n",
    "\n",
    "print(f\"   ‚Ä¢ Features (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Target (y): {y.shape}\")\n",
    "print(f\"   ‚Ä¢ Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Clean up player_side_info - only keep rating_z\n",
    "# May regret this if we add more side info later and forget we took this step\n",
    "# God help me that would be a nasty bug\n",
    "player_side_info_clean = player_side_info[[\"rating_z\"]].copy()\n",
    "\n",
    "#  Use index-based splitting to avoid DataFrame copies\n",
    "idx = np.arange(len(X))\n",
    "\n",
    "# First split: separate out test set (10%)\n",
    "idx_temp, idx_test = train_test_split(idx, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: split remaining into train (75%) and val (15%)\n",
    "# 15% of original = 15/90 ‚âà 0.1667 of temp\n",
    "idx_train, idx_val = train_test_split(idx_temp, test_size=15/90, random_state=42, shuffle=True)\n",
    "\n",
    "# Create splits using iloc (view, not copy)\n",
    "X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
    "X_test, y_test = X.iloc[idx_test], y.iloc[idx_test]\n",
    "\n",
    "print(f\"   ‚Ä¢ Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Val: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"   ‚Ä¢ Total samples: {total:,} (should equal {len(X):,})\")\n",
    "print(f\"   ‚Ä¢ Train %: {len(X_train)/total*100:.2f}% (target: 75%)\")\n",
    "print(f\"   ‚Ä¢ Val %: {len(X_val)/total*100:.2f}% (target: 15%)\")\n",
    "print(f\"   ‚Ä¢ Test %: {len(X_test)/total*100:.2f}% (target: 10%)\")\n",
    "\n",
    "# Pre-compute unique arrays once\n",
    "players_train = X_train[\"player_id\"].unique()\n",
    "players_val = X_val[\"player_id\"].unique()\n",
    "players_test = X_test[\"player_id\"].unique()\n",
    "\n",
    "openings_train = X_train[\"opening_id\"].unique()\n",
    "openings_val = X_val[\"opening_id\"].unique()\n",
    "openings_test = X_test[\"opening_id\"].unique()\n",
    "\n",
    "print(f\"\\n   Players:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(players_train):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Val: {len(players_val):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Test: {len(players_test):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['player_id'].nunique():,} players\")\n",
    "\n",
    "print(f\"\\n   Openings:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(openings_train):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Val: {len(openings_val):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Test: {len(openings_test):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['opening_id'].nunique():,} openings\")\n",
    "\n",
    "# Use NumPy setdiff1d for cold-start analysis (C-speed)\n",
    "\n",
    "val_cold_players = np.setdiff1d(players_val, players_train, assume_unique=True)\n",
    "val_cold_openings = np.setdiff1d(openings_val, openings_train, assume_unique=True)\n",
    "\n",
    "test_cold_players = np.setdiff1d(players_test, players_train, assume_unique=True)\n",
    "test_cold_openings = np.setdiff1d(openings_test, openings_train, assume_unique=True)\n",
    "\n",
    "print(f\"\\n   Validation set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(val_cold_players):,} ({len(val_cold_players)/len(players_val)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(val_cold_openings):,} ({len(val_cold_openings)/len(openings_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(test_cold_players):,} ({len(test_cold_players)/len(players_test)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(test_cold_openings):,} ({len(test_cold_openings)/len(openings_test)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n7Ô∏è‚É£  Score distribution across splits:\")\n",
    "\n",
    "y_train_stats = y_train.describe()\n",
    "y_val_stats = y_val.describe()\n",
    "y_test_stats = y_test.describe()\n",
    "\n",
    "print(f\"\\n   Train y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_train_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_train_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_train_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_val_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_val_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_val_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_test_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_test_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_test_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n8Ô∏è‚É£  Confidence distribution across splits:\")\n",
    "\n",
    "conf_train_stats = X_train['confidence'].describe()\n",
    "conf_val_stats = X_val['confidence'].describe()\n",
    "conf_test_stats = X_test['confidence'].describe()\n",
    "\n",
    "print(f\"\\n   Train confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_train_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_val_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_test_stats['50%']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SPLIT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   ‚Ä¢ Training data: {len(X_train):,} samples (75%)\")\n",
    "print(f\"   ‚Ä¢ Validation data: {len(X_val):,} samples (15%)\")\n",
    "print(f\"   ‚Ä¢ Test data: {len(X_test):,} samples (10%)\")\n",
    "print(f\"   ‚Ä¢ Player side info: {len(player_side_info_clean):,} players\")\n",
    "print(f\"   ‚Ä¢ Side info columns: {list(player_side_info_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781a382",
   "metadata": {},
   "source": [
    "## Step 3b: Remap Player and Opening IDs to Sequential Integers\n",
    "\n",
    "**Why remap IDs?**\n",
    "- Database IDs may have gaps (e.g., [1, 5, 10, 15, ...]) from deleted entries\n",
    "- Embedding layers need 0-based contiguous indices for efficiency\n",
    "- Remapping saves memory (no unused embedding slots)\n",
    "\n",
    "**Process:**\n",
    "1. Check if IDs are already sequential (0 or 1-based with no gaps)\n",
    "2. If not, create mappings: old_id ‚Üí new_sequential_id\n",
    "3. Remap all DataFrames and side info tables\n",
    "4. Verify mappings with spot checks\n",
    "\n",
    "This ensures embeddings use minimal memory and indices align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b: Remap player and opening IDs to 0-based sequential integers\n",
    "\n",
    "print(\"STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\")\n",
    "\n",
    "def check_and_remap_ids(df_list, id_column, entity_name):\n",
    "    \"\"\"\n",
    "    Check if IDs are sequential starting from 0, and remap if not.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of DataFrames\n",
    "        List of DataFrames containing the ID column to check/remap\n",
    "    id_column : str\n",
    "        Name of the ID column ('player_id' or 'opening_id')\n",
    "    entity_name : str\n",
    "        Name for logging ('player' or 'opening')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (df_list with remapped IDs, id_to_idx mapping dict, needs_remapping bool)\n",
    "    \"\"\"\n",
    "    # Get all unique IDs across all dataframes\n",
    "    all_ids = pd.concat([df[id_column] for df in df_list]).unique()\n",
    "    all_ids_sorted = sorted(all_ids)\n",
    "    \n",
    "    print(f\"Checking {entity_name} IDs...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ‚Ä¢ Total unique {entity_name}s: {len(all_ids_sorted)}\")\n",
    "    print(f\"   ‚Ä¢ ID range: [{all_ids_sorted[0]}, {all_ids_sorted[-1]}]\")\n",
    "    \n",
    "    # Check if IDs are already 0-based sequential (0, 1, 2, ...)\n",
    "    expected_sequential = list(range(len(all_ids_sorted)))\n",
    "    is_sequential = (all_ids_sorted == expected_sequential)\n",
    "    \n",
    "    if is_sequential:\n",
    "        print(f\"   ‚úì {entity_name} IDs are already 0-based sequential - no remapping needed!\")\n",
    "        return df_list, None, False\n",
    "    \n",
    "    # Check if IDs are 1-based sequential (1, 2, 3, ...)\n",
    "    expected_sequential_1based = list(range(1, len(all_ids_sorted) + 1))\n",
    "    is_sequential_1based = (all_ids_sorted == expected_sequential_1based)\n",
    "    \n",
    "    if is_sequential_1based:\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs are 1-based sequential - will remap to 0-based\")\n",
    "    else:\n",
    "        # Calculate gaps\n",
    "        num_gaps = (all_ids_sorted[-1] - all_ids_sorted[0] + 1) - len(all_ids_sorted)\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs have gaps - will remap to 0-based sequential\")\n",
    "        print(f\"   ‚Ä¢ Number of gaps: {num_gaps}\")\n",
    "    \n",
    "    # Create mapping: old_id -> new_idx (0-based)\n",
    "    id_to_idx = {old_id: new_idx for new_idx, old_id in enumerate(all_ids_sorted)}\n",
    "    idx_to_id = {new_idx: old_id for old_id, new_idx in id_to_idx.items()}\n",
    "    \n",
    "    print(f\"\\n   Creating mapping...\")\n",
    "    print(f\"   ‚Ä¢ Example mappings:\")\n",
    "    sample_ids = all_ids_sorted[:5] + all_ids_sorted[-5:]\n",
    "    for old_id in sample_ids[:10]:  # Show first 5 and last 5\n",
    "        print(f\"      {entity_name}_id {old_id} ‚Üí {id_to_idx[old_id]}\")\n",
    "    \n",
    "    # Remap all DataFrames\n",
    "    print(f\"\\n   Remapping {len(df_list)} DataFrames...\")\n",
    "    remapped_dfs = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[id_column] = df_copy[id_column].map(id_to_idx)\n",
    "        remapped_dfs.append(df_copy)\n",
    "        print(f\"   ‚úì Remapped DataFrame {i+1}/{len(df_list)}\")\n",
    "    \n",
    "    return remapped_dfs, (id_to_idx, idx_to_id), True\n",
    "\n",
    "# 1. Remap player IDs\n",
    "print(f\"\\n1Ô∏è‚É£  Processing player IDs...\")\n",
    "player_dfs = [X_train, X_val, X_test, clean_data, player_side_info.reset_index()]\n",
    "remapped_player_dfs, player_mappings, player_remapped = check_and_remap_ids(\n",
    "    player_dfs, 'player_id', 'player'\n",
    ")\n",
    "\n",
    "if player_remapped:\n",
    "    X_train, X_val, X_test, clean_data, player_side_info_remapped = remapped_player_dfs\n",
    "    player_id_to_idx, player_idx_to_id = player_mappings\n",
    "    player_side_info = player_side_info_remapped.set_index('player_id')\n",
    "    print(f\"\\n   ‚úÖ Player ID remapping complete!\")\n",
    "else:\n",
    "    player_id_to_idx, player_idx_to_id = None, None\n",
    "\n",
    "# 2. Remap opening IDs\n",
    "print(f\"\\n2Ô∏è‚É£  Processing opening IDs...\")\n",
    "opening_dfs = [X_train, X_val, X_test, clean_data]\n",
    "remapped_opening_dfs, opening_mappings, opening_remapped = check_and_remap_ids(\n",
    "    opening_dfs, 'opening_id', 'opening'\n",
    ")\n",
    "\n",
    "if opening_remapped:\n",
    "    X_train, X_val, X_test, clean_data = remapped_opening_dfs\n",
    "    opening_id_to_idx, opening_idx_to_id = opening_mappings\n",
    "    print(f\"\\n   ‚úÖ Opening ID remapping complete!\")\n",
    "else:\n",
    "    opening_id_to_idx, opening_idx_to_id = None, None\n",
    "\n",
    "# 3. Spot checks to verify mappings\n",
    "print(f\"\\n3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\")\n",
    "print(f\"   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\")\n",
    "\n",
    "# Sample 10 entries: first, last, and 8 in between\n",
    "print(f\"\\n   Sampling 10 entries from X_train (before remapping was applied)...\")\n",
    "total_rows = len(X_train)\n",
    "# Get indices: first, last, and 8 evenly spaced in between\n",
    "sample_indices = [0]  # First row\n",
    "step = (total_rows - 1) // 9  # Divide remaining rows into 9 parts\n",
    "sample_indices.extend([min(i * step, total_rows - 1) for i in range(1, 9)])\n",
    "sample_indices.append(total_rows - 1)  # Last row\n",
    "\n",
    "print(f\"   ‚Ä¢ Sample indices: {sample_indices}\")\n",
    "\n",
    "# Store samples with their NEW (remapped) IDs\n",
    "samples = []\n",
    "for idx in sample_indices:\n",
    "    row = X_train.iloc[idx]\n",
    "    samples.append({\n",
    "        'index': idx,\n",
    "        'new_player_id': row['player_id'],\n",
    "        'new_opening_id': row['opening_id'],\n",
    "        'confidence': row['confidence']\n",
    "    })\n",
    "\n",
    "print(f\"\\n   Verification checks:\")\n",
    "print(f\"   {'#':<4} {'Row Idx':<10} {'New Player':<12} {'New Opening':<12} {'Confidence':<12} {'Status':<15}\")\n",
    "print(f\"   {'-'*4} {'-'*10} {'-'*12} {'-'*12} {'-'*12} {'-'*15}\")\n",
    "\n",
    "all_checks_passed = True\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    new_player_id = sample['new_player_id']\n",
    "    new_opening_id = sample['new_opening_id']\n",
    "    confidence = sample['confidence']\n",
    "    idx = sample['index']\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: New player ID is valid (0-based sequential)\n",
    "    if player_remapped:\n",
    "        player_valid = 0 <= new_player_id < len(player_id_to_idx)\n",
    "        checks.append((\"player_id\", player_valid))\n",
    "    else:\n",
    "        checks.append((\"player_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 2: New opening ID is valid (0-based sequential)\n",
    "    if opening_remapped:\n",
    "        opening_valid = 0 <= new_opening_id < len(opening_id_to_idx)\n",
    "        checks.append((\"opening_id\", opening_valid))\n",
    "    else:\n",
    "        checks.append((\"opening_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 3: Player exists in player_side_info\n",
    "    player_exists = new_player_id in player_side_info.index\n",
    "    checks.append((\"in_side_info\", player_exists))\n",
    "    \n",
    "    # Check 4: Opening exists in clean_data\n",
    "    opening_exists = new_opening_id in clean_data['opening_id'].values\n",
    "    checks.append((\"in_clean\", opening_exists))\n",
    "    \n",
    "    # All checks must pass\n",
    "    all_pass = all(check[1] for check in checks)\n",
    "    status = \"‚úì PASS\" if all_pass else f\"‚úó FAIL ({','.join([c[0] for c in checks if not c[1]])})\"\n",
    "    \n",
    "    if not all_pass:\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(f\"   {i:<4} {idx:<10} {new_player_id:<12} {new_opening_id:<12} {confidence:<12.4f} {status:<15}\")\n",
    "\n",
    "# Additional verification: check if we can reverse map to original IDs\n",
    "if player_remapped or opening_remapped:\n",
    "    print(f\"\\n   Reverse mapping verification (sample of 3 entries):\")\n",
    "    print(f\"   {'#':<4} {'New‚ÜíOld Player':<30} {'New‚ÜíOld Opening':<30}\")\n",
    "    print(f\"   {'-'*4} {'-'*30} {'-'*30}\")\n",
    "    \n",
    "    for i in [0, len(samples)//2, len(samples)-1]:  # First, middle, last\n",
    "        sample = samples[i]\n",
    "        \n",
    "        # Reverse map player\n",
    "        if player_remapped:\n",
    "            old_player = player_idx_to_id.get(sample['new_player_id'], 'NOT_FOUND')\n",
    "            player_str = f\"{sample['new_player_id']} ‚Üí {old_player}\"\n",
    "        else:\n",
    "            player_str = f\"{sample['new_player_id']} (unchanged)\"\n",
    "        \n",
    "        # Reverse map opening\n",
    "        if opening_remapped:\n",
    "            old_opening = opening_idx_to_id.get(sample['new_opening_id'], 'NOT_FOUND')\n",
    "            opening_str = f\"{sample['new_opening_id']} ‚Üí {old_opening}\"\n",
    "        else:\n",
    "            opening_str = f\"{sample['new_opening_id']} (unchanged)\"\n",
    "        \n",
    "        print(f\"   {i+1:<4} {player_str:<30} {opening_str:<30}\")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(f\"\\n   ‚úÖ All spot checks passed! ID mappings are correct.\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Some spot checks failed - investigate immediately!\")\n",
    "    raise ValueError(\"ID remapping verification failed!\")\n",
    "\n",
    "# 4. Summary\n",
    "print(f\"\\n4Ô∏è‚É£  Summary:\")\n",
    "print(f\"\\n   Player IDs:\")\n",
    "if player_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{player_idx_to_id[0]}, {player_idx_to_id[len(player_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(player_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "    \n",
    "print(f\"\\n   Opening IDs:\")\n",
    "if opening_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{opening_idx_to_id[0]}, {opening_idx_to_id[len(opening_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(opening_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "\n",
    "print(f\"\\n   Updated DataFrames:\")\n",
    "print(f\"   ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ X_val: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ clean_data: {clean_data.shape}\")\n",
    "print(f\"   ‚Ä¢ player_side_info: {player_side_info.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ID REMAPPING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Important:\")\n",
    "print(f\"   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\")\n",
    "print(f\"   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\")\n",
    "print(f\"   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\")\n",
    "print(f\"   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a580c0",
   "metadata": {},
   "source": [
    "## 4. Enumerate Categorical Variables\n",
    "\n",
    "I believe the only variable we need to enumerate here is `eco`. That's the broad categorization of a specific opening.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- One ECO code will have many openings\n",
    "- They're sorted by letter, then further by number. For instance, C21 and C44 are in the `C` family.\n",
    "- Maybe we make this side information?\n",
    "\n",
    "First, let's get some data on ECO codes to help us better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9504a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: ECO Code Statistics (no mutations, just exploration)\n",
    "\n",
    "print(\"STEP 4: ECO CODE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Missing ECO values: {clean_data['eco'].isna().sum()}\")\n",
    "\n",
    "# ECO value counts\n",
    "eco_counts = clean_data['eco'].value_counts().sort_index()\n",
    "print(f\"\\n2Ô∏è‚É£  Distribution of entries per ECO code:\")\n",
    "print(f\"   ‚Ä¢ Mean entries per ECO: {eco_counts.mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Median entries per ECO: {eco_counts.median():.1f}\")\n",
    "print(f\"   ‚Ä¢ Min entries: {eco_counts.min()}\")\n",
    "print(f\"   ‚Ä¢ Max entries: {eco_counts.max()}\")\n",
    "print(f\"   ‚Ä¢ Std: {eco_counts.std():.1f}\")\n",
    "\n",
    "# ECO by first letter (family)\n",
    "print(f\"\\n3Ô∏è‚É£  ECO families (by first letter):\")\n",
    "eco_families = clean_data['eco'].str[0].value_counts().sort_index()\n",
    "print(f\"\\n   {'Family':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "for family, count in eco_families.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.4)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {family:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Top 20 most common ECO codes\n",
    "print(f\"\\n4Ô∏è‚É£  Top 20 most common ECO codes:\")\n",
    "top_eco = clean_data['eco'].value_counts().head(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*12} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(top_eco.items(), 1):\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.3)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Bottom 20 least common ECO codes\n",
    "print(f\"\\n5Ô∏è‚É£  Bottom 20 least common ECO codes:\")\n",
    "bottom_eco = clean_data['eco'].value_counts().tail(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(bottom_eco.items(), 1):\n",
    "    bar_length = min(count, 30)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {bar}\")\n",
    "\n",
    "# Check for any unusual ECO codes\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of ECO codes:\")\n",
    "sample_eco = clean_data['eco'].drop_duplicates().sample(min(20, clean_data['eco'].nunique()), random_state=42).sort_values()\n",
    "print(f\"   {', '.join(sample_eco.values)}\")\n",
    "\n",
    "# ECO statistics by split\n",
    "print(f\"\\n8Ô∏è‚É£  ECO distribution across splits:\")\n",
    "print(f\"\\n   Train split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_train['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\n   Validation split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_val['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_val):,}\")\n",
    "val_new_eco = set(X_val['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(val_new_eco)}\")\n",
    "\n",
    "print(f\"\\n   Test split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_test['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_test):,}\")\n",
    "test_new_eco = set(X_test['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(test_new_eco)}\")\n",
    "\n",
    "# Average score by ECO code (top 10 and bottom 10)\n",
    "print(f\"\\n9Ô∏è‚É£  Average score by ECO code:\")\n",
    "eco_scores = clean_data.groupby('eco')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\n   Top 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.head(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(f\"\\n   Bottom 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.tail(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# ECO codes with high variance in scores\n",
    "print(f\"\\nüîü  ECO codes with highest score variance (may indicate difficulty):\")\n",
    "eco_variance = clean_data.groupby('eco')['score'].agg(['var', 'std', 'count']).sort_values('var', ascending=False).head(10)\n",
    "print(f\"\\n   {'ECO':<6} {'Variance':<12} {'Std Dev':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_variance.iterrows():\n",
    "    print(f\"   {eco:<6} {row['var']:<12.4f} {row['std']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ECO CODE STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Key takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Most common family: {eco_families.idxmax()} ({eco_families.max():,} entries)\")\n",
    "print(f\"   ‚Ä¢ Most common ECO: {top_eco.index[0]} ({top_eco.iloc[0]:,} entries)\")\n",
    "print(f\"   ‚Ä¢ ECO codes appear in all splits (good for training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbad18",
   "metadata": {},
   "source": [
    "## 4b. Create ECO Side Information\n",
    "\n",
    "**Why ECO is Side Information:**\n",
    "- ECO codes describe **opening characteristics**, not individual player-opening interactions\n",
    "- Similar to how player ratings describe players, ECO describes openings\n",
    "- Each opening has ONE ECO code (not per player-opening pair)\n",
    "\n",
    "**Implementation Strategy:**\n",
    "- Split ECO codes into two categorical features:\n",
    "  - `eco_letter`: A, B, C, D, or E ‚Üí encoded as integers 0-4\n",
    "  - `eco_number`: The numeric part (e.g., \"21\" from \"C21\") ‚Üí encoded as sequential integers\n",
    "- Store in a separate `opening_side_info` lookup table (indexed by opening_id)\n",
    "- Remove `eco` from train/test/val DataFrames (it's not interaction data)\n",
    "- During training, model will lookup opening_id ‚Üí (eco_letter, eco_number)\n",
    "\n",
    "**Why Split ECO into Letter and Number:**\n",
    "- ECO families (A-E) represent fundamentally different opening types:\n",
    "  - **A**: Flank openings (English, R√©ti, Bird's, etc.)\n",
    "  - **B**: Semi-Open games (Sicilian, French, Caro-Kann, etc.)\n",
    "  - **C**: Open games (King's pawn openings, Spanish, Italian, etc.)\n",
    "  - **D**: Closed games (Queen's Gambit variations)\n",
    "  - **E**: Indian defenses (King's Indian, Nimzo-Indian, etc.)\n",
    "- Numbers within each family represent variations (C20-C29, C30-C39, etc.)\n",
    "- Model can learn separate embeddings for family vs variation\n",
    "\n",
    "**Categorical Encoding:**\n",
    "- Both features will be treated as categorical (not ordinal)\n",
    "- Higher numbers don't mean \"better\" openings\n",
    "- Model will learn embedding vectors for each category\n",
    "- This allows the model to capture non-linear relationships between ECO codes and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64124475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b. Create ECO side information and remove ECO from train/test/val DataFrames\n",
    "\n",
    "# Check if ECO processing has already been done\n",
    "if 'eco' not in X_train.columns and 'opening_side_info' in globals():\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 4B: ECO SIDE INFORMATION CREATION\")\n",
    "    print(\"\\n‚úì ECO column already removed from train/test/val data\")\n",
    "    print(f\"\\nOpening side info shape: {opening_side_info.shape}\")\n",
    "    print(f\"Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "    print(\"opening side info:\", opening_side_info.head().to_string())\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing ECO encoding statistics:\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_letter values: {opening_side_info['eco_letter_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_number values: {opening_side_info['eco_number_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ eco_letter range: [{opening_side_info['eco_letter_cat'].min()}, {opening_side_info['eco_letter_cat'].max()}]\")\n",
    "    print(f\"   ‚Ä¢ eco_number range: [{opening_side_info['eco_number_cat'].min()}, {opening_side_info['eco_number_cat'].max()}]\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing ECO encoding:\")\n",
    "    sample_data = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "    print(sample_data.to_string())\n",
    "else:\n",
    "    def create_eco_side_information(clean_data_df, X_train_df, X_val_df, X_test_df):\n",
    "        \"\"\"\n",
    "        Create ECO side information table and remove ECO from train/test/val DataFrames.\n",
    "        \n",
    "        ECO codes are opening-level features, not player-opening interaction features.\n",
    "        We split each ECO code (e.g., \"C21\") into:\n",
    "        - eco_letter: The letter part (A, B, C, D, or E)\n",
    "        - eco_number: The numeric part (e.g., 21)\n",
    "        \n",
    "        Both are encoded as sequential integers for use as categorical features in embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        clean_data_df : pd.DataFrame\n",
    "            Full cleaned data with opening_id and eco columns\n",
    "        X_train_df, X_val_df, X_test_df : pd.DataFrame\n",
    "            Train/val/test feature DataFrames (will be modified to remove 'eco')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 4B: CREATE ECO SIDE INFORMATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Extract unique opening ‚Üí ECO mappings\n",
    "        opening_eco_map = clean_data_df[['opening_id', 'eco']].drop_duplicates().set_index('opening_id')\n",
    "        print(f\"   ‚úì Extracted {len(opening_eco_map):,} unique openings\")\n",
    "        \n",
    "        # Verify one-to-one mapping\n",
    "        eco_per_opening = clean_data_df.groupby('opening_id')['eco'].nunique()\n",
    "        if (eco_per_opening > 1).any():\n",
    "            problematic = eco_per_opening[eco_per_opening > 1]\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(problematic)} openings have multiple ECO codes!\")\n",
    "            print(f\"   Problematic opening IDs: {problematic.index.tolist()[:10]}...\")\n",
    "        \n",
    "        # Split ECO into letter and number components\n",
    "        opening_eco_map['eco_letter_str'] = opening_eco_map['eco'].str[0]  # First character (A-E)\n",
    "        opening_eco_map['eco_number_str'] = opening_eco_map['eco'].str[1:]  # Remaining characters (numeric)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted letter and number components\")\n",
    "        print(f\"   ‚Ä¢ Unique letters: {opening_eco_map['eco_letter_str'].unique()}\")\n",
    "        print(f\"   ‚Ä¢ Unique numbers: {opening_eco_map['eco_number_str'].nunique()}\")\n",
    "        \n",
    "        # Create encoding mappings for eco_letter (A-E ‚Üí 0-4)\n",
    "        unique_letters = sorted(opening_eco_map['eco_letter_str'].unique())\n",
    "        eco_letter_to_int = {letter: idx for idx, letter in enumerate(unique_letters)}\n",
    "        eco_int_to_letter = {idx: letter for letter, idx in eco_letter_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_letter_cat'] = opening_eco_map['eco_letter_str'].map(eco_letter_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Letter encoding created:\")\n",
    "        for letter, idx in sorted(eco_letter_to_int.items()):\n",
    "            count = (opening_eco_map['eco_letter_str'] == letter).sum()\n",
    "            print(f\"      '{letter}' ‚Üí {idx} ({count:,} openings)\")\n",
    "        \n",
    "        # Create encoding mappings for eco_number (00-99 ‚Üí sequential integers)\n",
    "        unique_numbers = sorted(opening_eco_map['eco_number_str'].unique())\n",
    "        eco_number_to_int = {num: idx for idx, num in enumerate(unique_numbers)}\n",
    "        eco_int_to_number = {idx: num for num, idx in eco_number_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_number_cat'] = opening_eco_map['eco_number_str'].map(eco_number_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Number encoding created:\")\n",
    "        print(f\"      {len(unique_numbers)} unique numbers mapped to [0, {len(unique_numbers)-1}]\")\n",
    "        print(f\"      Range: '{unique_numbers[0]}' ‚Üí 0, ..., '{unique_numbers[-1]}' ‚Üí {len(unique_numbers)-1}\")\n",
    "        \n",
    "        # Show distribution of numbers\n",
    "        print(f\"\\n   Distribution of ECO numbers (top 10):\")\n",
    "        number_counts = opening_eco_map['eco_number_str'].value_counts().head(10)\n",
    "        for num, count in number_counts.items():\n",
    "            encoded = eco_number_to_int[num]\n",
    "            print(f\"      '{num}' (‚Üí {encoded:>2}): {count:>3} openings\")\n",
    "        \n",
    "        # Create final opening_side_info table\n",
    "        # Only keep the encoded categorical columns and rename them for clarity\n",
    "        opening_side_info = opening_eco_map[['eco_letter_cat', 'eco_number_cat']].copy()\n",
    "\n",
    "        print(f\"   ‚úì Created opening_side_info\")\n",
    "        print(f\"      ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"      ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "        # Verify all openings in train/val/test have ECO info\n",
    "        all_openings = set(X_train_df['opening_id'].unique()) | \\\n",
    "                       set(X_val_df['opening_id'].unique()) | \\\n",
    "                       set(X_test_df['opening_id'].unique())\n",
    "        \n",
    "        missing_openings = all_openings - set(opening_side_info.index)\n",
    "        if len(missing_openings) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits are missing ECO info!\")\n",
    "            print(f\"   Missing opening IDs: {sorted(list(missing_openings))[:10]}...\")\n",
    "        \n",
    "        # Remove ECO from train/val/test DataFrames\n",
    "        print(f\"   ‚Ä¢ X_train before: {X_train_df.shape}, columns: {list(X_train_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val before: {X_val_df.shape}, columns: {list(X_val_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test before: {X_test_df.shape}, columns: {list(X_test_df.columns)}\")\n",
    "        \n",
    "        X_train_clean = X_train_df.drop(columns=['eco'])\n",
    "        X_val_clean = X_val_df.drop(columns=['eco'])\n",
    "        X_test_clean = X_test_df.drop(columns=['eco'])\n",
    "        \n",
    "        # Sample data showing the transformation\n",
    "        print(f\"\\n8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\")\n",
    "        sample_openings = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "        \n",
    "        # For display purposes, reconstruct original values from the categorical encodings\n",
    "        print(f\"\\n   {'Opening ID':<12} {'Letter (Cat)':<15} {'Number (Cat)':<15} {'Reconstructed ECO':<18}\")\n",
    "        print(f\"   {'-'*12} {'-'*15} {'-'*15} {'-'*18}\")\n",
    "        for idx, row in sample_openings.iterrows():\n",
    "            letter_str = eco_int_to_letter[row['eco_letter_cat']]\n",
    "            number_str = eco_int_to_number[row['eco_number_cat']]\n",
    "            reconstructed = f\"{letter_str}{number_str}\"\n",
    "            print(f\"   {idx:<12} {row['eco_letter_cat']:<15} {row['eco_number_cat']:<15} {reconstructed:<18}\")\n",
    "        \n",
    "        # Show ECO family distribution\n",
    "        print(f\"\\n9Ô∏è‚É£  ECO family distribution in opening_side_info:\")\n",
    "        letter_dist = opening_side_info['eco_letter_cat'].value_counts().sort_index()\n",
    "        print(f\"\\n   {'Encoded':<8} {'Letter':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "        print(f\"   {'-'*8} {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "        for encoded_val, count in letter_dist.items():\n",
    "            letter = eco_int_to_letter[encoded_val]\n",
    "            pct = 100 * count / len(opening_side_info)\n",
    "            bar_length = int(pct * 0.4)\n",
    "            bar = '‚ñà' * bar_length\n",
    "            print(f\"   {encoded_val:<8} {letter:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "        \n",
    "        print(f\"\\nCreated: opening_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape[0]:,} rows, {X_train_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape[0]:,} rows, {X_val_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape[0]:,} rows, {X_test_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ opening_side_info: {len(opening_side_info):,} openings (one per opening)\")\n",
    "        print(f\"   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\")\n",
    "        print(f\"   ‚Ä¢ eco_letter_to_int: {eco_letter_to_int}\")\n",
    "        print(f\"   ‚Ä¢ eco_number_to_int: (dict with {len(eco_number_to_int)} entries)\")\n",
    "        print(f\"\\n   You'll need them to encode ECO codes for new openings at inference time.\")\n",
    "\n",
    "        return opening_side_info, eco_letter_to_int, eco_number_to_int, X_train_clean, X_val_clean, X_test_clean\n",
    "    \n",
    "    # Call the function\n",
    "    opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test = create_eco_side_information(\n",
    "        clean_data, X_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Create reverse mappings for decoding (needed for verification and debugging)\n",
    "    eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "    eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "    print(f\"\\n‚úì Created reverse mappings for ECO decoding:\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_letter: {len(eco_int_to_letter)} entries\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_number: {len(eco_int_to_number)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify final data structure after ECO processing\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION: FINAL DATA STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\")\n",
    "print(f\"\\n   X_train:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_train.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(X_train.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   X_val:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_val.columns)}\")\n",
    "\n",
    "print(f\"\\n   X_test:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_test.columns)}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Side Information Tables:\")\n",
    "\n",
    "print(f\"\\n   player_side_info (indexed by player_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(player_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   opening_side_info (indexed by opening_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(opening_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Encoding Mappings (for inference):\")\n",
    "print(f\"\\n   eco_letter_map:\")\n",
    "for k, v in sorted(eco_letter_map.items()):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "\n",
    "print(f\"\\n   eco_number_map (first 10):\")\n",
    "for i, (k, v) in enumerate(sorted(eco_number_map.items())[:10]):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "print(f\"      ... ({len(eco_number_map)} total)\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Example: Lookup flow for a random train sample:\")\n",
    "sample = X_train.sample(1, random_state=42).iloc[0]\n",
    "player_id = sample['player_id']\n",
    "opening_id = sample['opening_id']\n",
    "\n",
    "print(f\"\\n   Sample interaction:\")\n",
    "print(f\"   ‚Ä¢ player_id: {player_id}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {opening_id}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Player side info lookup:\")\n",
    "player_info = player_side_info.loc[player_id]\n",
    "print(f\"   ‚Ä¢ rating_z: {player_info['rating_z']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Opening side info lookup:\")\n",
    "opening_info = opening_side_info.loc[opening_id]\n",
    "print(f\"   ‚Ä¢ eco_letter_cat: {opening_info['eco_letter_cat']} (letter: {eco_int_to_letter[opening_info['eco_letter_cat']]})\")\n",
    "print(f\"   ‚Ä¢ eco_number_cat: {opening_info['eco_number_cat']} (number: {eco_int_to_number[opening_info['eco_number_cat']]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Ready for PyTorch tensor conversion:\")\n",
    "print(f\"   ‚Ä¢ Features: player_id, opening_id, confidence\")\n",
    "print(f\"   ‚Ä¢ Target: score (in y_train, y_val, y_test)\")\n",
    "print(f\"   ‚Ä¢ Player side info: rating_z\")\n",
    "print(f\"   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\")\n",
    "print(f\"\\n   All ECO and rating data is now properly separated as side information!\")\n",
    "print(f\"\\n   Side info tables are clean - only contain necessary model inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43752aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4c: Correlation Analysis of Processed Data\n",
    "\n",
    "%pip install seaborn --quiet\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Start with the main interaction data (already has remapped IDs, adjusted score, confidence)\n",
    "corr_df = clean_data[[\"player_id\", \"opening_id\", \"score\", \"confidence\"]].copy()\n",
    "\n",
    "# Merge player side information (rating_z)\n",
    "# player_side_info is indexed by the remapped player_id\n",
    "corr_df = corr_df.merge(\n",
    "    player_side_info[[\"rating_z\"]], left_on=\"player_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# Merge opening side information (eco categories)\n",
    "# opening_side_info is indexed by the remapped opening_id\n",
    "corr_df = corr_df.merge(\n",
    "    opening_side_info[[\"eco_letter_cat\", \"eco_number_cat\"]],\n",
    "    left_on=\"opening_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(f\"Final DataFrame for correlation created.\")\n",
    "print(f\"   ‚Ä¢ Columns: {corr_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "correlation_matrix = corr_df.corr().drop(columns=['player_id', 'opening_id']).drop(index=['player_id', 'opening_id'])\n",
    "\n",
    "\n",
    "# 3. Visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Model Features\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 4. Analyze correlations with the target variable 'score'\n",
    "print(\"CORRELATIONS WITH TARGET VARIABLE (score)\")\n",
    "print(\"=\" * 80)\n",
    "score_correlations = (\n",
    "    correlation_matrix[\"score\"].drop(\"score\").sort_values(ascending=False)\n",
    ")\n",
    "print(score_correlations.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831d7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017515d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Sample player-opening pairs with reconstructed ECO codes and opening names\n",
    "# Doing this to make sure that our ECO encoding/decoding is correct\n",
    "\n",
    "print(\"VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Sample random player-opening pairs from training data\n",
    "sample_size = 100\n",
    "sample_data = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "\n",
    "# Get unique opening IDs from sample (these are NEW remapped IDs)\n",
    "new_opening_ids = sample_data['opening_id'].unique()\n",
    "\n",
    "# Convert NEW opening IDs back to OLD database IDs for query\n",
    "if opening_remapped:\n",
    "    old_opening_ids = [opening_idx_to_id[int(new_id)] for new_id in new_opening_ids]\n",
    "    print(f\"   ‚Ä¢ Converting {len(new_opening_ids)} NEW opening IDs to OLD database IDs for query\")\n",
    "    print(f\"   ‚Ä¢ Example: NEW ID {new_opening_ids[0]} ‚Üí OLD ID {old_opening_ids[0]}\")\n",
    "else:\n",
    "    old_opening_ids = new_opening_ids\n",
    "    print(f\"   ‚Ä¢ No remapping was done - using opening IDs directly\")\n",
    "\n",
    "opening_ids_str = ','.join(map(str, old_opening_ids))\n",
    "\n",
    "# Query database for opening names using OLD IDs\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    opening_query = f\"\"\"\n",
    "        SELECT id, name, eco\n",
    "        FROM opening\n",
    "        WHERE id IN ({opening_ids_str})\n",
    "    \"\"\"\n",
    "    opening_names = pd.DataFrame(con.execute(opening_query).df()).set_index('id')\n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Create reverse mappings for ECO decoding\n",
    "eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "\n",
    "# Build verification table\n",
    "print(f\"{'#':<4} {'Player':<8} {'Opening':<9} {'ECO (DB)':<10} {'Reconstructed':<13} {'Match':<6} {'Opening Name':<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "matches = 0\n",
    "for i, (idx, row) in enumerate(sample_data.iterrows(), 1):\n",
    "    player_id = int(row['player_id'])\n",
    "    new_opening_id = int(row['opening_id'])\n",
    "    \n",
    "    # Convert NEW opening ID to OLD database ID for lookup\n",
    "    if opening_remapped:\n",
    "        old_opening_id = opening_idx_to_id[new_opening_id]\n",
    "    else:\n",
    "        old_opening_id = new_opening_id\n",
    "    \n",
    "    # Lookup opening side info using NEW ID\n",
    "    opening_info = opening_side_info.loc[new_opening_id]\n",
    "    \n",
    "    # Reconstruct ECO from encoded categorical values\n",
    "    eco_letter_encoded = opening_info['eco_letter_cat']\n",
    "    eco_number_encoded = opening_info['eco_number_cat']\n",
    "    \n",
    "    eco_letter_decoded = eco_int_to_letter[eco_letter_encoded]\n",
    "    eco_number_decoded = eco_int_to_number[eco_number_encoded]\n",
    "    \n",
    "    reconstructed_eco = f\"{eco_letter_decoded}{eco_number_decoded}\"\n",
    "    \n",
    "    # Get original ECO from database using OLD ID\n",
    "    db_eco = opening_names.loc[old_opening_id, 'eco']\n",
    "    opening_name = opening_names.loc[old_opening_id, 'name']\n",
    "    \n",
    "    # Check if they match\n",
    "    match = \"‚úì\" if reconstructed_eco == db_eco else \"‚úó\"\n",
    "    if reconstructed_eco == db_eco:\n",
    "        matches += 1\n",
    "    \n",
    "    # Truncate opening name if too long\n",
    "    if len(opening_name) > 48:\n",
    "        opening_name = opening_name[:45] + \"...\"\n",
    "    \n",
    "    # Display using NEW opening ID (what's in the data now)\n",
    "    print(f\"{i:<4} {player_id:<8} {new_opening_id:<9} {db_eco:<10} {reconstructed_eco:<13} {match:<6} {opening_name:<50}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n‚úÖ Verification Results:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(sample_data)}\")\n",
    "print(f\"   ‚Ä¢ Matches: {matches}/{len(sample_data)} ({100*matches/len(sample_data):.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Mismatches: {len(sample_data) - matches}\")\n",
    "\n",
    "if matches == len(sample_data):\n",
    "    print(f\"\\n All ECO codes reconstructed correctly\")\n",
    "    print(f\"   ‚Ä¢ ID remapping preserved all ECO code mappings\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Some ECO codes did not match. Investigate mismatches above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf4878",
   "metadata": {},
   "source": [
    "## 5. Data Verification and Examination\n",
    "We're almost there. Let's examine our data structures to check for any obvious flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train \\n\", X_train.head())\n",
    "print(\"=\"*60)\n",
    "print(\"X_val \\n\", X_val.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"X_test \\n\", X_test.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_train \\n\", y_train.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_val \\n\", y_val.head())\n",
    "print(\"=\" * 60) \n",
    "print(\"y_test \\n\", y_test.head())\n",
    "\n",
    "# Now side information\n",
    "print(\"player_side_info \\n\", player_side_info.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"opening_side_info \\n\", opening_side_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52936cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification: Display cleaned side info tables\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANED SIDE INFORMATION TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä player_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: player_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(player_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\nüìä opening_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: opening_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(opening_side_info.head().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19883cba",
   "metadata": {},
   "source": [
    "## Step 5: Convert to PyTorch Tensors\n",
    "\n",
    "**What are tensors?**\n",
    "Tensors are PyTorch's version of arrays - just multi-dimensional data structures optimized for deep learning. Think of them like fancy NumPy arrays that can run on GPUs.\n",
    "\n",
    "**What we need to convert:**\n",
    "\n",
    "1. **Main features** (X_train, X_val, X_test):\n",
    "   - `player_id` ‚Üí long tensor (integer IDs)\n",
    "   - `opening_id` ‚Üí long tensor (integer IDs)\n",
    "   - `confidence` ‚Üí float tensor (weights for loss function)\n",
    "\n",
    "2. **Targets** (y_train, y_val, y_test):\n",
    "   - `score` ‚Üí float tensor (what we're predicting)\n",
    "\n",
    "3. **Player side info** (player_side_info):\n",
    "   - `rating_z` ‚Üí float tensor (normalized ratings)\n",
    "   - Indexed by player_id for fast lookup\n",
    "\n",
    "4. **Opening side info** (opening_side_info):\n",
    "   - `eco_letter_cat` ‚Üí long tensor (categorical)\n",
    "   - `eco_number_cat` ‚Üí long tensor (categorical)\n",
    "   - Indexed by opening_id for fast lookup\n",
    "\n",
    "**Why these data types?**\n",
    "- `long` (int64): For IDs and categorical features that will be embedded\n",
    "- `float` (float32): For continuous values like scores, confidence, and normalized ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350be514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# It says subprocess is unused but complains at me if I don't import it\n",
    "# Something to do with incompatible versions of blah blah blah\n",
    "# Don't really care as long as it works\n",
    "import subprocess\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda01b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Convert all data to PyTorch tensors\n",
    "\n",
    "print(\"STEP 5: CONVERT TO PYTORCH TENSORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\")\n",
    "\n",
    "# 0. Index alignment sanity checks\n",
    "print(f\"\\n0Ô∏è‚É£  Index alignment sanity checks...\")\n",
    "\n",
    "# Check player_side_info is properly indexed\n",
    "player_ids_sorted = sorted(player_side_info.index.values)\n",
    "if player_ids_sorted != list(player_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  player_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì player_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{player_side_info.index.min()}, {player_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {player_side_info.index.dtype}\")\n",
    "\n",
    "# Check opening_side_info is properly indexed\n",
    "opening_ids_sorted = sorted(opening_side_info.index.values)\n",
    "if opening_ids_sorted != list(opening_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì opening_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{opening_side_info.index.min()}, {opening_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {opening_side_info.index.dtype}\")\n",
    "\n",
    "# CRITICAL: Check if indices are contiguous 0-based\n",
    "# If opening_side_info.index = [0, 1, 2, ..., N-1], then we can use opening_id as direct array index\n",
    "# If not (e.g., [10, 15, 23, ...]), we'll need a mapping dictionary\n",
    "print(f\"\\n   Checking if indices are contiguous 0-based...\")\n",
    "player_ids_are_contiguous = (player_side_info.index == range(len(player_side_info))).all()\n",
    "opening_ids_are_contiguous = (opening_side_info.index == range(len(opening_side_info))).all()\n",
    "\n",
    "\n",
    "if not player_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Player IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "if not opening_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "\n",
    "# 1. Convert main features (train/val/test)\n",
    "\n",
    "# Train set\n",
    "player_ids_train = torch.tensor(X_train['player_id'].values, dtype=torch.long)\n",
    "opening_ids_train = torch.tensor(X_train['opening_id'].values, dtype=torch.long)\n",
    "confidence_train = torch.tensor(X_train['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ Training tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_train: {player_ids_train.shape}, dtype={player_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_train: {opening_ids_train.shape}, dtype={opening_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_train: {confidence_train.shape}, dtype={confidence_train.dtype}\")\n",
    "\n",
    "# Validation set\n",
    "player_ids_val = torch.tensor(X_val['player_id'].values, dtype=torch.long)\n",
    "opening_ids_val = torch.tensor(X_val['opening_id'].values, dtype=torch.long)\n",
    "confidence_val = torch.tensor(X_val['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Validation tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_val: {player_ids_val.shape}, dtype={player_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_val: {opening_ids_val.shape}, dtype={opening_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_val: {confidence_val.shape}, dtype={confidence_val.dtype}\")\n",
    "\n",
    "# Test set\n",
    "player_ids_test = torch.tensor(X_test['player_id'].values, dtype=torch.long)\n",
    "opening_ids_test = torch.tensor(X_test['opening_id'].values, dtype=torch.long)\n",
    "confidence_test = torch.tensor(X_test['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Test tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_test: {player_ids_test.shape}, dtype={player_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_test: {opening_ids_test.shape}, dtype={opening_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_test: {confidence_test.shape}, dtype={confidence_test.dtype}\")\n",
    "\n",
    "# 2. Convert targets (scores)\n",
    "print(f\"\\n2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\")\n",
    "\n",
    "scores_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "scores_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "scores_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ scores_train: {scores_train.shape}, dtype={scores_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_val: {scores_val.shape}, dtype={scores_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_test: {scores_test.shape}, dtype={scores_test.dtype}\")\n",
    "\n",
    "print(f\"\\n   Score ranges (sanity check):\")\n",
    "print(f\"   ‚Ä¢ Train: [{scores_train.min():.4f}, {scores_train.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Val: [{scores_val.min():.4f}, {scores_val.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Test: [{scores_test.min():.4f}, {scores_test.max():.4f}]\")\n",
    "\n",
    "# 3. Convert player side information\n",
    "\n",
    "# Create tensor of all player ratings (indexed by player_id)\n",
    "# Since player_side_info is indexed by player_id, we need to ensure coverage\n",
    "player_ratings_tensor = torch.tensor(player_side_info['rating_z'].values, dtype=torch.float32)\n",
    "player_ids_in_side_info = torch.tensor(player_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ player_ratings_tensor: {player_ratings_tensor.shape}, dtype={player_ratings_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ player_ids_in_side_info: {player_ids_in_side_info.shape}, dtype={player_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings_tensor.min():.4f}, {player_ratings_tensor.max():.4f}]\")\n",
    "\n",
    "# Verify all player IDs in train/val/test are covered\n",
    "all_player_ids = torch.cat([player_ids_train, player_ids_val, player_ids_test]).unique()\n",
    "missing_players = set(all_player_ids.tolist()) - set(player_ids_in_side_info.tolist())\n",
    "if len(missing_players) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in splits missing from side_info!\")\n",
    "\n",
    "# 4. Convert opening side information\n",
    "print(f\"\\n4Ô∏è‚É£  Converting opening side information...\")\n",
    "\n",
    "# Verify column names exist (using eco_letter_cat and eco_number_cat consistently)\n",
    "if 'eco_letter_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_letter_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "if 'eco_number_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_number_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "\n",
    "# Create tensors for opening ECO features (indexed by opening_id)\n",
    "opening_eco_letter_tensor = torch.tensor(opening_side_info['eco_letter_cat'].values, dtype=torch.long)\n",
    "opening_eco_number_tensor = torch.tensor(opening_side_info['eco_number_cat'].values, dtype=torch.long)\n",
    "opening_ids_in_side_info = torch.tensor(opening_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ opening_eco_letter_tensor: {opening_eco_letter_tensor.shape}, dtype={opening_eco_letter_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_eco_number_tensor: {opening_eco_number_tensor.shape}, dtype={opening_eco_number_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_in_side_info: {opening_ids_in_side_info.shape}, dtype={opening_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ ECO letter range: [{opening_eco_letter_tensor.min()}, {opening_eco_letter_tensor.max()}]\")\n",
    "print(f\"   ‚Ä¢ ECO number range: [{opening_eco_number_tensor.min()}, {opening_eco_number_tensor.max()}]\")\n",
    "\n",
    "# Verify all opening IDs in train/val/test are covered\n",
    "all_opening_ids = torch.cat([opening_ids_train, opening_ids_val, opening_ids_test]).unique()\n",
    "missing_openings = set(all_opening_ids.tolist()) - set(opening_ids_in_side_info.tolist())\n",
    "if len(missing_openings) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits missing from side_info!\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(f\"\\n5Ô∏è‚É£  Summary statistics:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(scores_train):,} samples\")\n",
    "print(f\"   ‚Ä¢ Val: {len(scores_val):,} samples\")\n",
    "print(f\"   ‚Ä¢ Test: {len(scores_test):,} samples\")\n",
    "\n",
    "print(f\"\\n   Unique entities:\")\n",
    "print(f\"   ‚Ä¢ Players: {len(player_ids_in_side_info):,}\")\n",
    "print(f\"   ‚Ä¢ Openings: {len(opening_ids_in_side_info):,}\")\n",
    "\n",
    "print(f\"\\n   Vocabulary sizes (for embedding layers):\")\n",
    "print(f\"   ‚Ä¢ num_players: {player_ids_in_side_info.max() + 1} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_openings: {opening_ids_in_side_info.max() + 1} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_letters: {opening_eco_letter_tensor.max() + 1} (max eco_letter_cat + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_numbers: {opening_eco_number_tensor.max() + 1} (max eco_number_cat + 1)\")\n",
    "\n",
    "# 6. Memory usage (approximate - using simple calculation)\n",
    "print(f\"\\n6Ô∏è‚É£  Approximate memory usage:\")\n",
    "def tensor_memory_mb(t):\n",
    "    \"\"\"Calculate tensor memory in MB\"\"\"\n",
    "    return (t.element_size() * t.nelement()) / (1024 * 1024)\n",
    "\n",
    "tensors = [\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train,\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val,\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test,\n",
    "    player_ratings_tensor, opening_eco_letter_tensor, opening_eco_number_tensor\n",
    "]\n",
    "total_memory_mb = sum(tensor_memory_mb(t) for t in tensors)\n",
    "print(f\"   ‚Ä¢ Total tensor memory: {total_memory_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TENSOR CONVERSION COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fc308",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup\n",
    "\n",
    "Define all constants, loss functions, optimizer, and helper functions for training.\n",
    "\n",
    "**What we need to set up:**\n",
    "\n",
    "1. **Constants and Hyperparameters:**\n",
    "   - `NUM_FACTORS`: Dimensionality of latent factor embeddings (e.g., 50)\n",
    "   - `LEARNING_RATE`: Step size for SGD optimizer (e.g., 0.01)\n",
    "   - `BATCH_SIZE`: Number of samples per training batch (e.g., 1024)\n",
    "   - `N_EPOCHS`: Number of full passes through training data (e.g., 10)\n",
    "   - Random seeds for reproducibility\n",
    "\n",
    "2. **Loss Functions:**\n",
    "   - MSE (Mean Squared Error): Main training loss\n",
    "   - RMSE (Root Mean Squared Error): Evaluation metric\n",
    "   - Confidence-weighted loss: Down-weight uncertain predictions\n",
    "\n",
    "3. **Optimizer:**\n",
    "   - SGD (Stochastic Gradient Descent) with momentum\n",
    "\n",
    "4. **Helper Functions:**\n",
    "   - `train_one_epoch()`: Train for one epoch\n",
    "   - `evaluate_model()`: Evaluate on validation/test set\n",
    "   - `calculate_rmse()`: Compute RMSE metric\n",
    "   - `save_checkpoint()`: Save model state\n",
    "\n",
    "5. **Logging:**\n",
    "   - Progress bars with ETA\n",
    "   - Loss tracking per epoch\n",
    "   - Model checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d13c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6a: Define Hyperparameters and Constants\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HYPERPARAMETERS AND CONSTANTS CONFIG\n",
    "# ========================================\n",
    "\n",
    "NUM_FACTORS = 50  # Dimensionality of latent factor embeddings\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9  # SGD momentum\n",
    "BATCH_SIZE = 1024 \n",
    "N_EPOCHS = 20\n",
    "\n",
    "# Regularization (if needed)\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_SAVE_DIR = Path.cwd().parent / \"data\" / \"models\"  # Saves to projectroot/data/models\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"\\nüìã Hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ NUM_FACTORS: {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ LEARNING_RATE: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ MOMENTUM: {MOMENTUM}\")\n",
    "print(f\"   ‚Ä¢ BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ N_EPOCHS: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ WEIGHT_DECAY: {WEIGHT_DECAY}\")\n",
    "\n",
    "print(f\"\\nüé≤ Random Seed:\")\n",
    "print(f\"   ‚Ä¢ RANDOM_SEED: {RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\nüíª Device Configuration:\")\n",
    "print(f\"   ‚Ä¢ DEVICE: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ‚Ä¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüíæ Model Saving:\")\n",
    "print(f\"   ‚Ä¢ Save directory: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# ========================================\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# ========================================\n",
    "\n",
    "# Python random\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì Python random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì NumPy random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# PyTorch random\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì PyTorch CPU random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU\n",
    "    print(f\"   ‚úì PyTorch GPU random seed set to {RANDOM_SEED}\")\n",
    "    \n",
    "    # Additional CUDA reproducibility settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"   ‚úì CUDA deterministic mode enabled\")\n",
    "\n",
    "# ========================================\n",
    "# CALCULATE VOCABULARY SIZES\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Sizes (for embedding layers):\")\n",
    "\n",
    "# Calculate from our tensors\n",
    "NUM_PLAYERS = int(player_ids_in_side_info.max()) + 1\n",
    "NUM_OPENINGS = int(opening_ids_in_side_info.max()) + 1\n",
    "NUM_ECO_LETTERS = int(opening_eco_letter_tensor.max()) + 1\n",
    "NUM_ECO_NUMBERS = int(opening_eco_number_tensor.max()) + 1\n",
    "\n",
    "print(f\"   ‚Ä¢ NUM_PLAYERS: {NUM_PLAYERS:,} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_OPENINGS: {NUM_OPENINGS:,} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_LETTERS: {NUM_ECO_LETTERS} (ECO letter categories)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_NUMBERS: {NUM_ECO_NUMBERS} (ECO number categories)\")\n",
    "\n",
    "# ========================================\n",
    "# DATASET STATISTICS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Train samples: {len(scores_train):,}\")\n",
    "print(f\"   ‚Ä¢ Validation samples: {len(scores_val):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(scores_test):,}\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(scores_train) + len(scores_val) + len(scores_test):,}\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Batches per epoch: {len(scores_train) // BATCH_SIZE:,}\")\n",
    "print(f\"   ‚Ä¢ Training iterations (total): {(len(scores_train) // BATCH_SIZE) * N_EPOCHS:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a93e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6b: Define Loss Functions\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "print(\"STEP 6B: DEFINE LOSS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "def mse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss with optional confidence weighting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        Higher confidence = larger loss contribution\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Compute squared error\n",
    "    squared_error = (predictions - targets) ** 2\n",
    "    \n",
    "    # Apply confidence weighting if provided\n",
    "    if confidence_weights is not None:\n",
    "        weighted_squared_error = squared_error * confidence_weights\n",
    "        # Average over all samples (sum of weighted errors / sum of weights)\n",
    "        loss = weighted_squared_error.sum() / confidence_weights.sum()\n",
    "    else:\n",
    "        # Standard MSE (average over all samples)\n",
    "        loss = squared_error.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def rmse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error loss.\n",
    "    \n",
    "    This is the square root of MSE and is our primary evaluation metric.\n",
    "    RMSE is in the same units as the target (score), making it interpretable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar RMSE value\n",
    "    \"\"\"\n",
    "    mse = mse_loss(predictions, targets, confidence_weights)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def calculate_rmse(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Calculate RMSE metric (convenience function for evaluation).\n",
    "    \n",
    "    This is the same as rmse_loss but with clearer naming for evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        RMSE value (Python float, not tensor)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Don't compute gradients for evaluation\n",
    "        rmse = rmse_loss(predictions, targets, confidence_weights)\n",
    "    return rmse.item()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüß™ Testing loss functions with dummy data...\")\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_predictions = torch.tensor([0.5, 0.6, 0.7, 0.8], dtype=torch.float32)\n",
    "dummy_targets = torch.tensor([0.55, 0.62, 0.68, 0.75], dtype=torch.float32)\n",
    "dummy_confidence = torch.tensor([0.8, 0.9, 0.7, 1.0], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Dummy data:\")\n",
    "print(f\"   ‚Ä¢ Predictions: {dummy_predictions.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Targets: {dummy_targets.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Confidence: {dummy_confidence.tolist()}\")\n",
    "\n",
    "# Test MSE without confidence weighting\n",
    "mse_unweighted = mse_loss(dummy_predictions, dummy_targets)\n",
    "print(f\"\\n   MSE (unweighted): {mse_unweighted.item():.6f}\")\n",
    "\n",
    "# Test MSE with confidence weighting\n",
    "mse_weighted = mse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"   MSE (weighted): {mse_weighted.item():.6f}\")\n",
    "\n",
    "# Test RMSE\n",
    "rmse_unweighted = rmse_loss(dummy_predictions, dummy_targets)\n",
    "rmse_weighted = rmse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   RMSE (unweighted): {rmse_unweighted.item():.6f}\")\n",
    "print(f\"   RMSE (weighted): {rmse_weighted.item():.6f}\")\n",
    "\n",
    "# Test calculate_rmse\n",
    "rmse_eval = calculate_rmse(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   calculate_rmse() result: {rmse_eval:.6f}\")\n",
    "print(f\"   ‚úì Returns Python float (not tensor): {type(rmse_eval)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LOSS FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e00b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6c: Create PyTorch Dataset and DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# CUSTOM DATASET CLASS\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for chess player-opening interactions.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - player_id: Integer ID for embedding lookup\n",
    "    - opening_id: Integer ID for embedding lookup\n",
    "    - confidence: Confidence weight for loss function\n",
    "    - score: Target value (win rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, player_ids, opening_ids, confidence, scores):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (long tensor)\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (long tensor)\n",
    "        confidence : torch.Tensor\n",
    "            Confidence weights (float tensor)\n",
    "        scores : torch.Tensor\n",
    "            Target scores (float tensor)\n",
    "        \"\"\"\n",
    "        self.player_ids = player_ids\n",
    "        self.opening_ids = opening_ids\n",
    "        self.confidence = confidence\n",
    "        self.scores = scores\n",
    "        \n",
    "        # Validate shapes\n",
    "        assert len(player_ids) == len(opening_ids) == len(confidence) == len(scores), \\\n",
    "            \"All tensors must have the same length\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.player_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with keys: 'player_id', 'opening_id', 'confidence', 'score'\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'player_id': self.player_ids[idx],\n",
    "            'opening_id': self.opening_ids[idx],\n",
    "            'confidence': self.confidence[idx],\n",
    "            'score': self.scores[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATASETS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Creating PyTorch Datasets...\")\n",
    "\n",
    "train_dataset = ChessOpeningDataset(\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train\n",
    ")\n",
    "val_dataset = ChessOpeningDataset(\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val\n",
    ")\n",
    "test_dataset = ChessOpeningDataset(\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"   ‚úì Validation dataset: {len(val_dataset):,} samples\")\n",
    "print(f\"   ‚úì Test dataset: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Test dataset access\n",
    "print(f\"\\n2Ô∏è‚É£  Testing dataset access...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"   Sample from train_dataset[0]:\")\n",
    "print(f\"   ‚Ä¢ player_id: {sample['player_id']}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {sample['opening_id']}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "print(f\"   ‚Ä¢ score: {sample['score']:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATALOADERS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Creating PyTorch DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle training data each epoch\n",
    "    num_workers=0,  # Number of subprocesses for data loading (0 = main process)\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle test data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fbb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6d: Define Helper Functions for Training\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"STEP 6D: DEFINE HELPER FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Save Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to save\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer state to save\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    train_loss : float\n",
    "        Training loss for this epoch\n",
    "    val_loss : float\n",
    "        Validation loss for this epoch\n",
    "    filepath : Path or str\n",
    "        Where to save the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        \n",
    "        # Save hyperparameters for reproducibility\n",
    "        'hyperparameters': {\n",
    "            'num_factors': NUM_FACTORS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'momentum': MOMENTUM,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'num_players': NUM_PLAYERS,\n",
    "            'num_openings': NUM_OPENINGS,\n",
    "            'num_eco_letters': NUM_ECO_LETTERS,\n",
    "            'num_eco_numbers': NUM_ECO_NUMBERS,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"   Checkpoint saved to: {filepath}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Load Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to load weights into\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to load state into\n",
    "    filepath : Path or str\n",
    "        Where to load the checkpoint from\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        The epoch number from the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"    Checkpoint loaded from: {filepath}\")\n",
    "    print(f\"   ‚Ä¢ Epoch: {epoch}\")\n",
    "    print(f\"   ‚Ä¢ Train loss: {checkpoint['train_loss']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Val loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    return epoch\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Format Time\n",
    "# ========================================\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"\n",
    "    Format seconds as human-readable time string.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seconds : float\n",
    "        Number of seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted time string (e.g., \"1h 23m 45s\")\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Print Progress\n",
    "# ========================================\n",
    "\n",
    "def print_progress(epoch, batch_idx, total_batches, loss, elapsed_time):\n",
    "    \"\"\"\n",
    "    Print training progress with ETA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    batch_idx : int\n",
    "        Current batch index (0-based)\n",
    "    total_batches : int\n",
    "        Total number of batches per epoch\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    elapsed_time : float\n",
    "        Elapsed time in seconds since epoch start\n",
    "    \"\"\"\n",
    "    # Calculate progress\n",
    "    progress_pct = 100.0 * (batch_idx + 1) / total_batches\n",
    "    \n",
    "    # Estimate time remaining\n",
    "    time_per_batch = elapsed_time / (batch_idx + 1)\n",
    "    remaining_batches = total_batches - (batch_idx + 1)\n",
    "    eta_seconds = time_per_batch * remaining_batches\n",
    "    \n",
    "    # Print progress bar\n",
    "    bar_length = 40\n",
    "    filled_length = int(bar_length * (batch_idx + 1) / total_batches)\n",
    "    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)\n",
    "    \n",
    "    print(f\"   Epoch {epoch} [{bar}] {progress_pct:>6.2f}% | \"\n",
    "          f\"Loss: {loss:.6f} | \"\n",
    "          f\"ETA: {format_time(eta_seconds)}\", end='\\r')\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST HELPER FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "# Test format_time\n",
    "print(f\"\\n   format_time() tests:\")\n",
    "test_times = [30, 90, 300, 3661, 7384]\n",
    "for t in test_times:\n",
    "    print(f\"   ‚Ä¢ {t} seconds ‚Üí {format_time(t)}\")\n",
    "\n",
    "# Test print_progress (will overwrite line)\n",
    "print(f\"\\n   print_progress() test (simulating batch progress):\")\n",
    "for i in range(0, 101, 20):\n",
    "    total = 100\n",
    "    loss = 0.05 - i * 0.0003\n",
    "    elapsed = i * 0.5\n",
    "    print_progress(epoch=1, batch_idx=i, total_batches=total, loss=loss, elapsed_time=elapsed)\n",
    "    time.sleep(0.1)  # Brief pause to show animation\n",
    "print()  # New line after progress bar\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HELPER FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6e: Define Training and Evaluation Functions\n",
    "\n",
    "print(\"STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# TRAINING FUNCTION: Train One Epoch\n",
    "# ========================================\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, device, epoch_num):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to train\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for updating model parameters\n",
    "    device : torch.device\n",
    "        Device to train on (CPU or CUDA)\n",
    "    epoch_num : int\n",
    "        Current epoch number (for logging)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (average_loss, elapsed_time)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move batch to device\n",
    "        player_ids = batch['player_id'].to(device)\n",
    "        opening_ids = batch['opening_id'].to(device)\n",
    "        confidence = batch['confidence'].to(device)\n",
    "        targets = batch['score'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(player_ids, opening_ids)\n",
    "        \n",
    "        # Compute loss with confidence weighting\n",
    "        loss = mse_loss(predictions, targets, confidence)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 10 batches or on last batch\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print_progress(epoch_num, batch_idx, num_batches, loss.item(), elapsed)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    elapsed_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Clear progress line and print summary\n",
    "    print()  # New line after progress bar\n",
    "    \n",
    "    return avg_loss, elapsed_time\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# EVALUATION FUNCTION: Evaluate Model\n",
    "# ========================================\n",
    "\n",
    "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to evaluate\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for evaluation data\n",
    "    device : torch.device\n",
    "        Device to evaluate on (CPU or CUDA)\n",
    "    dataset_name : str\n",
    "        Name of dataset for logging (e.g., \"Validation\", \"Test\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (mse, rmse)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_mse = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in data_loader:\n",
    "            # Move batch to device\n",
    "            player_ids = batch['player_id'].to(device)\n",
    "            opening_ids = batch['opening_id'].to(device)\n",
    "            confidence = batch['confidence'].to(device)\n",
    "            targets = batch['score'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(player_ids, opening_ids)\n",
    "            \n",
    "            # Compute MSE with confidence weighting\n",
    "            batch_mse = mse_loss(predictions, targets, confidence)\n",
    "            \n",
    "            # Track weighted sum of MSE\n",
    "            total_mse += batch_mse.item() * len(targets)\n",
    "            total_samples += len(targets)\n",
    "    \n",
    "    # Calculate average MSE and RMSE\n",
    "    avg_mse = total_mse / total_samples\n",
    "    avg_rmse = avg_mse ** 0.5\n",
    "    \n",
    "    return avg_mse, avg_rmse\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST FUNCTIONS (with dummy model)\n",
    "# ========================================\n",
    "\n",
    "print(\" TRAINING AND EVALUATION FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã Training workflow:\")\n",
    "print(f\"   1. Initialize model and optimizer\")\n",
    "print(f\"   2. For each epoch:\")\n",
    "print(f\"      a. Call train_one_epoch()\")\n",
    "print(f\"      b. Call evaluate_model() on validation set\")\n",
    "print(f\"      c. Save checkpoint with save_checkpoint()\")\n",
    "print(f\"      d. Log metrics\")\n",
    "print(f\"   3. After training, evaluate on test set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6f: Define Model Architecture\n",
    "\n",
    "print(\"STEP 6F: DEFINE MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Chess Opening Recommender Model\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningRecommender(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for chess opening recommendations.\n",
    "    \n",
    "    The model learns latent factors for players and openings, incorporating\n",
    "    side information:\n",
    "    - Player ratings (normalized)\n",
    "    - Opening ECO codes (letter and number as categorical features)\n",
    "    \n",
    "    Architecture:\n",
    "    - Player embedding: learnable latent factors\n",
    "    - Opening embedding: learnable latent factors\n",
    "    - Player rating: fixed side information (no embedding)\n",
    "    - ECO letter/number: categorical embeddings\n",
    "    \n",
    "    Prediction: dot product of player and opening representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_players, num_openings, num_factors,\n",
    "                 player_ratings, opening_eco_letters, opening_eco_numbers,\n",
    "                 num_eco_letters, num_eco_numbers, eco_embed_dim=4):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_players : int\n",
    "            Total number of unique players\n",
    "        num_openings : int\n",
    "            Total number of unique openings\n",
    "        num_factors : int\n",
    "            Dimensionality of latent factor embeddings\n",
    "        player_ratings : torch.Tensor\n",
    "            Z-score normalized ratings for all players (shape: [num_players])\n",
    "        opening_eco_letters : torch.Tensor\n",
    "            ECO letter categories for all openings (shape: [num_openings])\n",
    "        opening_eco_numbers : torch.Tensor\n",
    "            ECO number categories for all openings (shape: [num_openings])\n",
    "        num_eco_letters : int\n",
    "            Number of unique ECO letter categories\n",
    "        num_eco_numbers : int\n",
    "            Number of unique ECO number categories\n",
    "        eco_embed_dim : int\n",
    "            Dimensionality of ECO categorical embeddings (default: 4)\n",
    "        \"\"\"\n",
    "        super(ChessOpeningRecommender, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_players = num_players\n",
    "        self.num_openings = num_openings\n",
    "        self.num_factors = num_factors\n",
    "        self.eco_embed_dim = eco_embed_dim\n",
    "        \n",
    "        # ========================================\n",
    "        # Player Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.player_factors = torch.nn.Embedding(num_players, num_factors)\n",
    "        \n",
    "        self.player_biases = torch.nn.Embedding(num_players, 1)\n",
    "        \n",
    "        # Player ratings (fixed side information - registered as buffer, not parameter)\n",
    "        self.register_buffer('player_ratings', player_ratings)\n",
    "        \n",
    "        # ========================================\n",
    "        # Opening Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.opening_factors = torch.nn.Embedding(num_openings, num_factors)\n",
    "        \n",
    "        self.opening_biases = torch.nn.Embedding(num_openings, 1)\n",
    "        \n",
    "        # ECO letter and number (fixed side information - registered as buffers)\n",
    "        self.register_buffer('opening_eco_letters', opening_eco_letters)\n",
    "        self.register_buffer('opening_eco_numbers', opening_eco_numbers)\n",
    "        \n",
    "        # ECO embeddings (learnable)\n",
    "        self.eco_letter_embedding = torch.nn.Embedding(num_eco_letters, eco_embed_dim)\n",
    "        self.eco_number_embedding = torch.nn.Embedding(num_eco_numbers, eco_embed_dim)\n",
    "        \n",
    "        # ========================================\n",
    "        # Combination Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Combine player latent factors with rating\n",
    "        # Input: [num_factors + 1] ‚Üí Output: [num_factors]\n",
    "        self.player_combiner = torch.nn.Linear(num_factors + 1, num_factors)\n",
    "        \n",
    "        # Combine opening latent factors with ECO embeddings\n",
    "        # Input: [num_factors + 2*eco_embed_dim] ‚Üí Output: [num_factors]\n",
    "        self.opening_combiner = torch.nn.Linear(num_factors + 2 * eco_embed_dim, num_factors)\n",
    "        \n",
    "        # ========================================\n",
    "        # Global Bias\n",
    "        # ========================================\n",
    "        \n",
    "        # Global bias term (learnable scalar)\n",
    "        self.global_bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # ========================================\n",
    "        # Initialize Weights\n",
    "        # ========================================\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        torch.nn.init.normal_(self.player_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.opening_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_letter_embedding.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_number_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        torch.nn.init.zeros_(self.player_biases.weight)\n",
    "        torch.nn.init.zeros_(self.opening_biases.weight)\n",
    "        \n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        torch.nn.init.xavier_uniform_(self.player_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.player_combiner.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.opening_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.opening_combiner.bias)\n",
    "    \n",
    "    def forward(self, player_ids, opening_ids):\n",
    "        \"\"\"\n",
    "        Forward pass: predict player-opening scores.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (shape: [batch_size])\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (shape: [batch_size])\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Predicted scores (shape: [batch_size])\n",
    "        \"\"\"\n",
    "        # ========================================\n",
    "        # Get Player Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get player latent factors [batch_size, num_factors]\n",
    "        player_embed = self.player_factors(player_ids)\n",
    "        \n",
    "        # Get player ratings [batch_size, 1]\n",
    "        player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "        \n",
    "        # Concatenate player factors with rating [batch_size, num_factors + 1]\n",
    "        player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "        \n",
    "        # Combine into final player representation [batch_size, num_factors]\n",
    "        player_repr = self.player_combiner(player_concat)\n",
    "        \n",
    "        # Get player bias [batch_size]\n",
    "        player_bias = self.player_biases(player_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Get Opening Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get opening latent factors [batch_size, num_factors]\n",
    "        opening_embed = self.opening_factors(opening_ids)\n",
    "        \n",
    "        # Get ECO embeddings\n",
    "        eco_letters = self.opening_eco_letters[opening_ids]  # [batch_size]\n",
    "        eco_numbers = self.opening_eco_numbers[opening_ids]  # [batch_size]\n",
    "        \n",
    "        eco_letter_embed = self.eco_letter_embedding(eco_letters)  # [batch_size, eco_embed_dim]\n",
    "        eco_number_embed = self.eco_number_embedding(eco_numbers)  # [batch_size, eco_embed_dim]\n",
    "        \n",
    "        # Concatenate opening factors with ECO embeddings\n",
    "        # [batch_size, num_factors + 2*eco_embed_dim]\n",
    "        opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "        \n",
    "        # Combine into final opening representation [batch_size, num_factors]\n",
    "        opening_repr = self.opening_combiner(opening_concat)\n",
    "        \n",
    "        # Get opening bias [batch_size]\n",
    "        opening_bias = self.opening_biases(opening_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Compute Prediction\n",
    "        # ========================================\n",
    "        \n",
    "        # Dot product of player and opening representations [batch_size]\n",
    "        interaction = (player_repr * opening_repr).sum(dim=1)\n",
    "        \n",
    "        # Add biases and global bias\n",
    "        prediction = interaction + player_bias + opening_bias + self.global_bias\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] range (since scores are win rates)\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def get_player_embedding(self, player_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific player.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_id : int\n",
    "            Player ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Player representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            player_ids = torch.tensor([player_id], device=self.player_factors.weight.device)\n",
    "            player_embed = self.player_factors(player_ids)\n",
    "            player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "            player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "            player_repr = self.player_combiner(player_concat)\n",
    "            return player_repr.squeeze()\n",
    "    \n",
    "    def get_opening_embedding(self, opening_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific opening.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        opening_id : int\n",
    "            Opening ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Opening representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            opening_ids = torch.tensor([opening_id], device=self.opening_factors.weight.device)\n",
    "            opening_embed = self.opening_factors(opening_ids)\n",
    "            \n",
    "            eco_letters = self.opening_eco_letters[opening_ids]\n",
    "            eco_numbers = self.opening_eco_numbers[opening_ids]\n",
    "            \n",
    "            eco_letter_embed = self.eco_letter_embedding(eco_letters)\n",
    "            eco_number_embed = self.eco_number_embedding(eco_numbers)\n",
    "            \n",
    "            opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "            opening_repr = self.opening_combiner(opening_concat)\n",
    "            return opening_repr.squeeze()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model Summary\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ ChessOpeningRecommender model class defined\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Player Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_PLAYERS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_PLAYERS}, 1)\")\n",
    "print(f\"      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 1}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Opening Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_OPENINGS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_OPENINGS}, 1)\")\n",
    "print(f\"      ‚Ä¢ ECO letter embedding: Embedding({NUM_ECO_LETTERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ ECO number embedding: Embedding({NUM_ECO_NUMBERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 8}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Prediction:\")\n",
    "print(f\"      ‚Ä¢ Dot product of player and opening representations\")\n",
    "print(f\"      ‚Ä¢ + player bias + opening bias + global bias\")\n",
    "print(f\"      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\")\n",
    "\n",
    "# Calculate total parameters\n",
    "player_factor_params = NUM_PLAYERS * NUM_FACTORS\n",
    "player_bias_params = NUM_PLAYERS\n",
    "player_combiner_params = (NUM_FACTORS + 1) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "opening_factor_params = NUM_OPENINGS * NUM_FACTORS\n",
    "opening_bias_params = NUM_OPENINGS\n",
    "eco_letter_params = NUM_ECO_LETTERS * 4\n",
    "eco_number_params = NUM_ECO_NUMBERS * 4\n",
    "opening_combiner_params = (NUM_FACTORS + 8) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "global_bias_params = 1\n",
    "\n",
    "total_params = (player_factor_params + player_bias_params + player_combiner_params +\n",
    "                opening_factor_params + opening_bias_params + \n",
    "                eco_letter_params + eco_number_params + opening_combiner_params +\n",
    "                global_bias_params)\n",
    "\n",
    "print(f\"\\n Model Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total trainable parameters: {total_params:,}\")\n",
    "print(f\"   ‚Ä¢ Player parameters: {player_factor_params + player_bias_params + player_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Opening parameters: {opening_factor_params + opening_bias_params + eco_letter_params + eco_number_params + opening_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Global parameters: {global_bias_params}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08708d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 CROSS VALIDATION CONFIGURATION & DATA SUBSETTING\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"STEP 6.1: CV CONFIGURATION & DATA SUBSETTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# 1. Configuration\n",
    "# ========================================\n",
    "NUM_CV_PLAYERS = 7_500\n",
    "K_FOLDS = 3\n",
    "CV_EPOCHS = 25 # Number of epochs to train for each fold/config\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"num_factors\": [5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2],\n",
    "    \"batch_size\": [512, 1024, 2048],\n",
    "}\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 2. Balanced Player Sampling\n",
    "# ========================================\n",
    "# Sample players in a balanced way by rating and number of games\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get player-level statistics\n",
    "player_stats = (\n",
    "    clean_data.groupby(\"player_id\")\n",
    "    .agg({\"num_games\": \"sum\"})  # Sum all games across all openings\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge with player_side_info to get rating_z\n",
    "player_stats = player_stats.merge(\n",
    "    player_side_info.reset_index()[[\"player_id\", \"rating_z\"]],\n",
    "    on=\"player_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Create stratification bins\n",
    "# Bin by rating (quartiles)\n",
    "player_stats['rating_bin'] = pd.qcut(player_stats['rating_z'], q=4, labels=['low', 'med_low', 'med_high', 'high'], duplicates='drop')\n",
    "\n",
    "# Bin by number of games (quartiles)\n",
    "player_stats['games_bin'] = pd.qcut(player_stats['num_games'], q=4, labels=['few', 'some', 'many', 'lots'], duplicates='drop')\n",
    "\n",
    "# Create combined stratification key\n",
    "player_stats['strata'] = player_stats['rating_bin'].astype(str) + '_' + player_stats['games_bin'].astype(str)\n",
    "\n",
    "print(f\"Total players: {len(player_stats):,}\")\n",
    "print(f\"Unique strata: {player_stats['strata'].nunique()}\")\n",
    "print(\"\\nStrata distribution:\")\n",
    "print(player_stats['strata'].value_counts().sort_index())\n",
    "\n",
    "# Sample proportionally from each stratum\n",
    "cv_player_ids = []\n",
    "for stratum in player_stats['strata'].unique():\n",
    "    stratum_players = player_stats[player_stats['strata'] == stratum]['player_id'].values\n",
    "    n_sample = max(1, int(len(stratum_players) * NUM_CV_PLAYERS / len(player_stats)))\n",
    "    n_sample = min(n_sample, len(stratum_players))  # Don't sample more than available\n",
    "    sampled = np.random.choice(stratum_players, size=n_sample, replace=False)\n",
    "    cv_player_ids.extend(sampled)\n",
    "\n",
    "cv_player_ids = np.array(cv_player_ids)\n",
    "print(f\"\\nSampled {len(cv_player_ids):,} players for CV (target was {NUM_CV_PLAYERS:,})\")\n",
    "\n",
    "# Create a dataframe containing only the interactions from the selected players\n",
    "cv_data = clean_data[clean_data[\"player_id\"].isin(cv_player_ids)].copy()\n",
    "\n",
    "# Verify balanced distribution\n",
    "cv_player_stats = player_stats[player_stats['player_id'].isin(cv_player_ids)]\n",
    "print(f\"\\nCV subset player statistics:\")\n",
    "print(f\"  Players: {len(cv_player_ids):,}\")\n",
    "print(f\"  Interactions: {len(cv_data):,}\")\n",
    "print(f\"  Rating range: {cv_player_stats['rating_z'].min():.0f} - {cv_player_stats['rating_z'].max():.0f}\")\n",
    "print(f\"  Rating mean: {cv_player_stats['rating_z'].mean():.1f} (full data: {player_stats['rating_z'].mean():.1f})\")\n",
    "print(f\"  Games range: {cv_player_stats['num_games'].min()} - {cv_player_stats['num_games'].max()}\")\n",
    "print(f\"  Games mean: {cv_player_stats['num_games'].mean():.1f} (full data: {player_stats['num_games'].mean():.1f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ BALANCED SAMPLING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 K-FOLD CROSS-VALIDATION LOOP WITH RANDOM SEARCH\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import spearmanr\n",
    "import scipy.stats\n",
    "\n",
    "print(\"STEP 6.2: K-FOLD CROSS-VALIDATION LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Random Search Configuration\n",
    "# ========================================\n",
    "N_RANDOM_SAMPLES = 25  # Number of random hyperparameter combinations to try\n",
    "random_state = 42\n",
    "\n",
    "# Use sklearn's ParameterSampler for random search\n",
    "param_sampler = ParameterSampler(\n",
    "    param_grid, n_iter=N_RANDOM_SAMPLES, random_state=random_state\n",
    ")\n",
    "\n",
    "print(f\"Using Random Search with {N_RANDOM_SAMPLES} hyperparameter combinations\")\n",
    "print(f\"Total CV runs: {N_RANDOM_SAMPLES * K_FOLDS}\")\n",
    "print()\n",
    "\n",
    "# ========================================\n",
    "# K-Fold Cross-Validation Loop\n",
    "# ========================================\n",
    "\n",
    "# Store results\n",
    "cv_results = []\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Loop through random hyperparameter combinations\n",
    "for config_idx, params in enumerate(param_sampler):\n",
    "    num_factors = params['num_factors']\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    config_str = f\"factors={num_factors}, lr={learning_rate:.5f}, batch={batch_size}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CONFIG {config_idx+1}/{N_RANDOM_SAMPLES}: {config_str}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    fold_metrics = {'rmse': [], 'mae': [], 'huber': [], 'weighted_rmse': [], 'spearman': [], 'baseline_delta': [], 'time': []}\n",
    "    \n",
    "    # K-Fold CV for this configuration\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(cv_data)):\n",
    "        fold_start_time = time.time()\n",
    "        print(f\"  Fold {fold_idx+1}/{K_FOLDS}...\", end=\" \", flush=True)\n",
    "\n",
    "        # --- Data preparation for this fold ---\n",
    "        train_fold_df = cv_data.iloc[train_idx]\n",
    "        val_fold_df = cv_data.iloc[val_idx]\n",
    "\n",
    "        # --- Convert to Tensors for this fold ---\n",
    "        train_dataset_f = ChessOpeningDataset(\n",
    "            torch.tensor(train_fold_df['player_id'].values, dtype=torch.long),\n",
    "            torch.tensor(train_fold_df['opening_id'].values, dtype=torch.long),\n",
    "            torch.tensor(train_fold_df['confidence'].values, dtype=torch.float32),\n",
    "            torch.tensor(train_fold_df['score'].values, dtype=torch.float32)\n",
    "        )\n",
    "        val_dataset_f = ChessOpeningDataset(\n",
    "            torch.tensor(val_fold_df['player_id'].values, dtype=torch.long),\n",
    "            torch.tensor(val_fold_df['opening_id'].values, dtype=torch.long),\n",
    "            torch.tensor(val_fold_df['confidence'].values, dtype=torch.float32),\n",
    "            torch.tensor(val_fold_df['score'].values, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        train_loader_f = DataLoader(train_dataset_f, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_f = DataLoader(val_dataset_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # --- Model and Optimizer for this fold ---\n",
    "        model_f = ChessOpeningRecommender(\n",
    "            num_players=NUM_PLAYERS, \n",
    "            num_openings=NUM_OPENINGS, \n",
    "            num_factors=num_factors,\n",
    "            player_ratings=player_ratings_tensor, \n",
    "            opening_eco_letters=opening_eco_letter_tensor,\n",
    "            opening_eco_numbers=opening_eco_number_tensor, \n",
    "            num_eco_letters=NUM_ECO_LETTERS,\n",
    "            num_eco_numbers=NUM_ECO_NUMBERS\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        optimizer_f = torch.optim.SGD(model_f.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "        # --- Training loop for this fold (silent mode) ---\n",
    "        model_f.train()\n",
    "        for epoch in range(1, CV_EPOCHS + 1):\n",
    "            for batch in train_loader_f:\n",
    "                optimizer_f.zero_grad()\n",
    "                predictions = model_f(\n",
    "                    batch['player_id'].to(DEVICE), \n",
    "                    batch['opening_id'].to(DEVICE)\n",
    "                )\n",
    "                loss = mse_loss(predictions, batch['score'].to(DEVICE), batch['confidence'].to(DEVICE))\n",
    "                loss.backward()\n",
    "                optimizer_f.step()\n",
    "        \n",
    "        # --- Evaluation for this fold ---\n",
    "        val_mse, val_rmse = evaluate_model(model_f, val_loader_f, DEVICE)\n",
    "        \n",
    "        # Calculate MAE and collect predictions/targets\n",
    "        model_f.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_f:\n",
    "                preds = model_f(batch['player_id'].to(DEVICE), batch['opening_id'].to(DEVICE))\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_targets.append(batch['score'].cpu())\n",
    "        \n",
    "        # Convert to numpy for metric calculations\n",
    "        preds_all = torch.cat(all_preds).numpy()\n",
    "        targets_all = torch.cat(all_targets).numpy()\n",
    "        conf_all = val_fold_df['confidence'].values\n",
    "        \n",
    "        # MAE\n",
    "        val_mae = torch.nn.functional.l1_loss(torch.cat(all_preds), torch.cat(all_targets)).item()\n",
    "        \n",
    "        # ============================================================\n",
    "        # ADDITIONAL METRICS\n",
    "        # ============================================================\n",
    "        \n",
    "        # 1. Huber Loss (robust to outliers)\n",
    "        huber_fn = torch.nn.HuberLoss(delta=1.0)\n",
    "        val_huber = huber_fn(torch.cat(all_preds), torch.cat(all_targets)).item()\n",
    "        \n",
    "        # 2. Weighted RMSE (using confidence as weights)\n",
    "        weighted_mse = np.average((preds_all - targets_all)**2, weights=conf_all)\n",
    "        val_weighted_rmse = np.sqrt(weighted_mse)\n",
    "        \n",
    "        # 3. Spearman correlation (rank correlation between predictions and true scores)\n",
    "        val_spearman, _ = spearmanr(preds_all, targets_all)\n",
    "        \n",
    "        # 4. Baseline comparison: RMSE if we predicted the mean target\n",
    "        mean_target = targets_all.mean()\n",
    "        baseline_mse = np.mean((targets_all - mean_target)**2)\n",
    "        baseline_rmse = np.sqrt(baseline_mse)\n",
    "        val_baseline_delta = baseline_rmse - val_rmse  # positive means we beat the baseline\n",
    "\n",
    "        fold_time = time.time() - fold_start_time\n",
    "        fold_metrics['rmse'].append(val_rmse)\n",
    "        fold_metrics['mae'].append(val_mae)\n",
    "        fold_metrics['huber'].append(val_huber)\n",
    "        fold_metrics['weighted_rmse'].append(val_weighted_rmse)\n",
    "        fold_metrics['spearman'].append(val_spearman)\n",
    "        fold_metrics['baseline_delta'].append(val_baseline_delta)\n",
    "        fold_metrics['time'].append(fold_time)\n",
    "        \n",
    "        print(f\"RMSE: {val_rmse:.4f}, Spearman: {val_spearman:.3f}, Time: {fold_time:.1f}s\")\n",
    "\n",
    "    # --- Aggregate metrics for this hyperparameter config ---\n",
    "    avg_rmse = np.mean(fold_metrics['rmse'])\n",
    "    std_rmse = np.std(fold_metrics['rmse'])\n",
    "    avg_mae = np.mean(fold_metrics['mae'])\n",
    "    std_mae = np.std(fold_metrics['mae'])\n",
    "    avg_huber = np.mean(fold_metrics['huber'])\n",
    "    avg_weighted_rmse = np.mean(fold_metrics['weighted_rmse'])\n",
    "    avg_spearman = np.mean(fold_metrics['spearman'])\n",
    "    avg_baseline_delta = np.mean(fold_metrics['baseline_delta'])\n",
    "    avg_time = np.mean(fold_metrics['time'])\n",
    "    \n",
    "    cv_results.append({\n",
    "        'num_factors': num_factors,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'mean_rmse': avg_rmse,\n",
    "        'std_rmse': std_rmse,\n",
    "        'mean_mae': avg_mae,\n",
    "        'std_mae': std_mae,\n",
    "        'mean_huber': avg_huber,\n",
    "        'mean_weighted_rmse': avg_weighted_rmse,\n",
    "        'mean_spearman': avg_spearman,\n",
    "        'mean_baseline_delta': avg_baseline_delta,\n",
    "        'mean_time_per_fold': avg_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úì Config {config_idx+1} complete: Avg RMSE = {avg_rmse:.4f} ¬± {std_rmse:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ CROSS-VALIDATION LOOP COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b79ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================   =================\n",
    "# 3. Analyze and Visualize Results\n",
    "# ========================================\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "results_df = results_df.sort_values(\"mean_rmse\", ascending=True)\n",
    "\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string())\n",
    "\n",
    "best_params = results_df.iloc[0]\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"  - Num Factors: {best_params['num_factors']}\")\n",
    "print(f\"  - Learning Rate: {best_params['learning_rate']}\")\n",
    "print(f\"  - Batch Size: {best_params['batch_size']}\")\n",
    "print(f\"  - Best Mean RMSE: {best_params['mean_rmse']:.4f}\")\n",
    "print(f\"  - RMSE: {best_params['mean_rmse']:.4f} ¬± {best_params['std_rmse']:.4f}\")\n",
    "\n",
    "# --- Visualization ---\n",
    "print(\"\\nVisualizing results...\")\n",
    "\n",
    "# Plot RMSE vs. Number of Factors\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data=results_df,\n",
    "    x=\"num_factors\",\n",
    "    y=\"mean_rmse\",\n",
    "    hue=\"learning_rate\",\n",
    "    style=\"batch_size\",\n",
    "    markers=True,\n",
    "    dashes=False,\n",
    ")\n",
    "plt.title(\"Mean RMSE vs. Number of Factors\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of Factors\")\n",
    "plt.ylabel(\"Mean RMSE\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend(title=\"Hyperparameters\")\n",
    "plt.show()\n",
    "\n",
    "# Heatmap for best learning rate\n",
    "best_lr = best_params[\"learning_rate\"]\n",
    "pivot_df = results_df[results_df[\"learning_rate\"] == best_lr].pivot(\n",
    "    index=\"num_factors\", columns=\"batch_size\", values=\"mean_rmse\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(pivot_df, annot=True, fmt=\".4f\", cmap=\"viridis_r\", linewidths=0.5)\n",
    "plt.title(f\"Hyperparameter Heatmap for RMSE (lr={best_lr})\")\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Number of Factors\")\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(\"plots/cross_val/rmse_vs_factors.png\", bbox_inches=\"tight\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03690a7e",
   "metadata": {},
   "source": [
    "## Step 6: Hyperparameter Tuning with K-Fold Cross-Validation\n",
    "\n",
    "Before training the final model, we will perform k-fold cross-validation to find the optimal hyperparameters. This provides a more robust evaluation than a single validation set.\n",
    "\n",
    "**Process:**\n",
    "1.  **Subset Data**: We'll use a smaller, representative sample of players to make the tuning process faster.\n",
    "2.  **K-Fold Split**: The player data will be split into `k` folds. For each fold, we train on `k-1` folds and validate on the remaining one.\n",
    "3.  **Hyperparameter Search**: We will iterate through different combinations of `NUM_FACTORS`, `LEARNING_RATE`, and `BATCH_SIZE`.\n",
    "4.  **Aggregate & Evaluate**: We'll average the performance metrics (like RMSE) across all folds for each hyperparameter combination to determine the best set.\n",
    "5.  **Visualize**: Results will be plotted to visualize the impact of each hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc2c6d",
   "metadata": {},
   "source": [
    "## Step 7: Final Model Training Setup\n",
    "\n",
    "Now we'll set up the training components for the **final model**: the model instance, optimizer, and learning rate scheduler.\n",
    "\n",
    "We'll use the best hyperparameters found during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 7: FINAL MODEL TRAINING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Hyperparameters for Final Training\n",
    "# ==================================\n",
    "# These would be set by the results from the CV step.\n",
    "# For now, we'll use the initial defaults.\n",
    "NUM_FACTORS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# ==================================\n",
    "# Model Instantiation\n",
    "# ==================================\n",
    "final_model = ChessOpeningRecommender(\n",
    "    num_players=NUM_PLAYERS,\n",
    "    num_openings=NUM_OPENINGS,\n",
    "    num_factors=NUM_FACTORS,\n",
    "    player_ratings=player_ratings_tensor,\n",
    "    opening_eco_letters=opening_eco_letter_tensor,\n",
    "    opening_eco_numbers=opening_eco_number_tensor,\n",
    "    num_eco_letters=NUM_ECO_LETTERS,\n",
    "    num_eco_numbers=NUM_ECO_NUMBERS\n",
    ").to(DEVICE)\n",
    "\n",
    "# ==================================\n",
    "# Optimizer\n",
    "# ==================================\n",
    "# We'll use SGD with momentum, a classic choice for matrix factorization.\n",
    "optimizer = torch.optim.SGD(final_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# ==================================\n",
    "# Learning Rate Scheduler\n",
    "# ==================================\n",
    "# Reduces the learning rate when a metric has stopped improving.\n",
    "# This helps to fine-tune the model in the later stages of training.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',      # The scheduler will step when the quantity monitored has stopped decreasing\n",
    "    factor=0.1,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "    patience=2,      # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=True     # If True, prints a message to stdout for each update\n",
    ")\n",
    "\n",
    "# ==================================\n",
    "# Checkpoint and Model Save Paths\n",
    "# ==================================\n",
    "CHECKPOINT_DIR = \"data/models\"\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.pt\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Final model hyperparameters:\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Num Factors: {NUM_FACTORS}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Num Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"\\nModel, optimizer, and scheduler initialized.\")\n",
    "print(f\"Checkpoints will be saved in: '{CHECKPOINT_DIR}'\")\n",
    "print(f\"Best model will be saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ SETUP COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becba99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0dc479f",
   "metadata": {},
   "source": [
    "## Step 8: Final Model Training Loop\n",
    "\n",
    "Here's the main training loop for the final model. We'll iterate for a specified number of epochs, training the model and evaluating its performance on the validation set periodically. We'll also save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"STEP 8: TRAINING THE FINAL MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Training State Tracking\n",
    "# ==================================\n",
    "history = defaultdict(list)\n",
    "best_val_rmse = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "\n",
    "# ==================================\n",
    "# Main Training Loop\n",
    "# ==================================\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    train_loss, train_rmse = train_one_epoch(final_model, train_loader, optimizer, DEVICE, epoch)\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    val_loss, val_rmse = evaluate_model(final_model, val_loader, DEVICE)\n",
    "    \n",
    "    # --- Learning Rate Scheduler Step ---\n",
    "    scheduler.step(val_rmse)\n",
    "    \n",
    "    # --- Logging ---\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_rmse'].append(train_rmse)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    \n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f} | \"\n",
    "          f\"Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "    # --- Checkpoint Saving ---\n",
    "    # Save a checkpoint every epoch\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch:03d}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_rmse': val_rmse,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Save the best model based on validation RMSE\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        epochs_since_improvement = 0\n",
    "        torch.save(final_model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  -> New best model saved with Val RMSE: {val_rmse:.4f}\")\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "\n",
    "# ==================================\n",
    "# Post-Training Summary\n",
    "# ==================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ FINAL TRAINING COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best Validation RMSE: {best_val_rmse:.4f}\")\n",
    "print(f\"Best model saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(f\"Last checkpoint saved to: '{checkpoint_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68202800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
