{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e163a7",
   "metadata": {},
   "source": [
    "# Notebook 26 ‚Äî Opening Recommender Model: Training Pipeline\n",
    "\n",
    "### 0. Overview and Goals\n",
    "\n",
    "This notebook defines the full pipeline for training the chess opening recommender model.  \n",
    "The objective is to predict **player‚Äìopening performance scores** ((wins + (0.5 * draws) / num games)) for openings a player hasn‚Äôt yet played, based on their results in the openings they *have* played.  \n",
    "\n",
    "The model will use **matrix factorization** with **stochastic gradient descent (SGD)** to learn latent factors representing player and opening characteristics.  \n",
    "All computations will be implemented in **PyTorch**, with data loaded from my local **DuckDB** database.\n",
    "\n",
    "**High-level specs:**\n",
    "- Use only *White* openings initially (we‚Äôll extend to Black later).  \n",
    "- Data source: processed player‚Äìopening stats from local DuckDB.  \n",
    "- Predict: normalized ‚Äúscore‚Äù = win rate ((wins + 0.5 x draws) / total games).  \n",
    "- Filter: only include entries with ‚â• `MIN_GAMES_THRESHOLD` (default = 50).  \n",
    "- Ignore: rating differences, time controls, and other metadata.  \n",
    "- Model parameters (to be defined in appropriate places for easy editing):  \n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_PLAYERS_TO_PROCESS`  \n",
    "- Logging and checkpoints throughout for reproducibility.  \n",
    "- All random operations seeded for deterministic runs.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Extraction\n",
    "- Connect to local DuckDB\n",
    "- Pull all processed player‚Äìopening statistics from\n",
    "- Verify schema consistency:  \n",
    "  - Required columns: `player_id`, `opening_id`, `eco`, `num_games`, `wins`, `draws`, `losses`.  \n",
    "- Include a row-count sanity check.\n",
    "- Only players with ratings above 1200\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Sanitization & Normalization\n",
    "- Optionally normalize scores if needed for MF convergence.  \n",
    "- Drop players with no qualifying openings and openings with no qualifying players.  \n",
    "  - I believe there shouldn't be any but we'll double check.\n",
    "- Resequence player_id and opening_id to be sequential integers - right now there are gaps because of entries we deleted from the DB \n",
    "- Check for sparsity consistency (no implicit zeros yet).  \n",
    "- Note that this data has already been split in to white and black games further up the pipeline\n",
    "\n",
    "### Data Quality\n",
    "- Drop entries with fewer than `MIN_GAMES_THRESHOLD` games\n",
    "- Handle any duplicate `(player_id, opening_id)` combinations\n",
    "- Remove players with no qualifying openings\n",
    "- Remove openings with no qualifying players\n",
    "- Verify no null values remain\n",
    "\n",
    "### ECO Codes\n",
    "- Keep ECO codes for later categorical encoding (Step 4)\n",
    "- ECO will be used as opening side information (similar to rating for players)\n",
    "\n",
    "### Confidence Weighting\n",
    "- Use `MIN_GAMES_THRESHOLD = 10` to keep more data\n",
    "- Add a **confidence weight** column: `confidence = num_games / (num_games + K)` where K ‚âà 50\n",
    "- This weight will be used in the loss function to down-weight uncertain predictions\n",
    "- High-game-count entries ‚Üí high confidence ‚Üí larger loss impact\n",
    "- Low-game-count entries ‚Üí low confidence ‚Üí smaller loss impact\n",
    "\n",
    "### Player Rating (Side Information)\n",
    "- **Player ratings are side information** - they describe player characteristics, not individual player-opening interactions\n",
    "- Ratings will be stored separately and joined to player embeddings during training\n",
    "- We'll **normalize ratings** (likely z-score normalization) to avoid scaling issues with the embedding layer\n",
    "- Rating normalization will be done once after extraction, not per-row\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Splits\n",
    "- Split into train/test/val sets.  \n",
    "- Ensure every player and every opening appears at least once in the training data.  \n",
    "- Strategy:  \n",
    "  - Sample unique players and openings to guarantee coverage in train.  \n",
    "  - Remaining data ‚Üí stratified random split into train/test.  \n",
    "  - Deduplicate and merge unique IDs back into train if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Enumerate Categorical Variables\n",
    "- Enumerate `eco` (if included) as an integer categorical variable.  \n",
    "- Confirm all columns are numeric and compatible with PyTorch tensors.  \n",
    "- Verify no missing or out-of-range IDs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Data Structure\n",
    "- Each row: one `(player_id, opening_id, score)` record.\n",
    "- Include other fields- eco, num games etc\n",
    "- Convert DataFrame to PyTorch tensors (`torch.long` for IDs, `torch.float` for scores).  \n",
    "- Log dataset shapes and sparsity metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Training Setup\n",
    "Define constants:\n",
    "- `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_FACTORS`  \n",
    "- Loss functions: MSE and RMSE  \n",
    "- Activation: sigmoid or none (depending on score normalization)  \n",
    "- Optimizer: SGD  \n",
    "- Figure out if there's anything else we need to design or specify\n",
    "\n",
    "Implement helper functions:\n",
    "- `train_one_epoch()`\n",
    "- `evaluate_model()`\n",
    "- `calculate_rmse()`\n",
    "- `save_checkpoint()`  \n",
    "\n",
    "Ensure detailed logging, ETA reporting, and reproducible random seeds.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Training Loop\n",
    "- Initialize player and opening embeddings.  \n",
    "- Iterate through epochs with mini-batch SGD (`BATCH_SIZE = 1024`).  \n",
    "- Compute and log MSE/RMSE per epoch.  \n",
    "- Save model checkpoints locally after each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Evaluation\n",
    "- Evaluate on test set.  \n",
    "- Report MSE, RMSE, and visual diagnostics (predicted vs actual score).  \n",
    "- Inspect a few player and opening latent factors for sanity.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Cross-Validation & Hyperparameter Tuning\n",
    "- Define ranges for:  \n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`  \n",
    "- Perform small-scale grid or random search for best configuration.  \n",
    "- Compare validation RMSE across runs.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Next Steps\n",
    "- Extend model to include Black openings.  \n",
    "- Experiment with hybrid inputs (player rating, ECO grouping).  \n",
    "- Consider implicit feedback handling (unplayed openings as zeros).  \n",
    "- Integrate trained model into API for recommendation output.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**  \n",
    "- Every random seed and parameter definition will be explicit.  \n",
    "- Every major step includes row-count, schema, and type validation.  \n",
    "- Model artifacts and logs will be saved locally for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d823",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "Connect to DuckDB and extract all player-opening statistics.\n",
    "Verify schema and perform sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcea569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA EXTRACTION\n",
      "============================================================\n",
      "\n",
      "üìÅ Database: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "üìÅ Database exists: True\n",
      "üé® Color filter: White\n",
      "üîí Minimum holdout players: 1,000\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path.cwd() / 'utils'))\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = Path.cwd().parent / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "COLOR_FILTER = 'w'  # 'w' for white, 'b' for black\n",
    "MIN_HOLDOUT_PLAYERS = 1000  # Minimum number of players to reserve for fold-in verification. These will not be used at all in this notebook for training, test/val or anything else.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: DATA EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìÅ Database: {DB_PATH}\")\n",
    "print(f\"üìÅ Database exists: {DB_PATH.exists()}\")\n",
    "print(f\"üé® Color filter: {'White' if COLOR_FILTER == 'w' else 'Black'}\")\n",
    "print(f\"üîí Minimum holdout players: {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9435033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£  Extracting player-opening statistics (color: 'w')...\n",
      "   ‚Ä¢ Minimum rating filter: 1200\n",
      "\n",
      "2Ô∏è‚É£  Selecting holdout players for fold-in verification...\n",
      "   ‚Ä¢ Holdout size: 1,000 players minimum\n",
      "   ‚Ä¢ Total eligible players: 5,000\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 4,000\n",
      "   ‚Ä¢ Holdout percentage: 20.0%\n",
      "\n",
      "3Ô∏è‚É£  Extracting training data (excluding holdout players)...\n",
      "   ‚Ä¢ Total eligible players: 5,000\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 4,000\n",
      "   ‚Ä¢ Holdout percentage: 20.0%\n",
      "\n",
      "3Ô∏è‚É£  Extracting training data (excluding holdout players)...\n",
      "   ‚úì Extracted 997,215 rows\n",
      "\n",
      "   üíæ Saved holdout_players_df with 1,000 player IDs\n",
      "   ‚Ä¢ These players are COMPLETELY UNSEEN by the training process\n",
      "   ‚Ä¢ Use them later for fold-in verification\n",
      "\n",
      "4Ô∏è‚É£  Verifying schema...\n",
      "   ‚úì All required columns present: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "5Ô∏è‚É£  Checking data types...\n",
      "   ‚Ä¢ player_id: int32\n",
      "   ‚Ä¢ opening_id: int32\n",
      "   ‚Ä¢ num_games: int32\n",
      "   ‚Ä¢ score: float64\n",
      "   ‚Ä¢ eco: object\n",
      "\n",
      "6Ô∏è‚É£  Data statistics...\n",
      "   ‚Ä¢ Total rows: 997,215\n",
      "   ‚Ä¢ Unique players: 4,000\n",
      "   ‚Ä¢ Unique openings: 2,988\n",
      "   ‚Ä¢ Total games (sum): 19,283,696\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 43\n",
      "   ‚Ä¢ Max: 49977\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 9648\n",
      "   ‚Ä¢ Mean: 19.3\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5012\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0         43         183          1  0.000000  A04\n",
      "1         43         197          4  0.625000  A04\n",
      "2         43         207          8  0.500000  A05\n",
      "3         43         218         20  0.350000  A06\n",
      "4         43         309          3  0.000000  A20\n",
      "5         43         370          1  0.000000  A40\n",
      "6         43         431          1  1.000000  A41\n",
      "7         43         445         19  0.526316  A43\n",
      "8         43         471          1  0.000000  A45\n",
      "9         43         504          1  0.000000  A46\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (997215, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: 1,000 players held out for fold-in verification\n",
      "   ‚Ä¢ Access via: holdout_players_df\n",
      "   ‚Ä¢ These players will NOT appear in any training, validation, or test splits\n",
      "   ‚úì Extracted 997,215 rows\n",
      "\n",
      "   üíæ Saved holdout_players_df with 1,000 player IDs\n",
      "   ‚Ä¢ These players are COMPLETELY UNSEEN by the training process\n",
      "   ‚Ä¢ Use them later for fold-in verification\n",
      "\n",
      "4Ô∏è‚É£  Verifying schema...\n",
      "   ‚úì All required columns present: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "5Ô∏è‚É£  Checking data types...\n",
      "   ‚Ä¢ player_id: int32\n",
      "   ‚Ä¢ opening_id: int32\n",
      "   ‚Ä¢ num_games: int32\n",
      "   ‚Ä¢ score: float64\n",
      "   ‚Ä¢ eco: object\n",
      "\n",
      "6Ô∏è‚É£  Data statistics...\n",
      "   ‚Ä¢ Total rows: 997,215\n",
      "   ‚Ä¢ Unique players: 4,000\n",
      "   ‚Ä¢ Unique openings: 2,988\n",
      "   ‚Ä¢ Total games (sum): 19,283,696\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 43\n",
      "   ‚Ä¢ Max: 49977\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 9648\n",
      "   ‚Ä¢ Mean: 19.3\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5012\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0         43         183          1  0.000000  A04\n",
      "1         43         197          4  0.625000  A04\n",
      "2         43         207          8  0.500000  A05\n",
      "3         43         218         20  0.350000  A06\n",
      "4         43         309          3  0.000000  A20\n",
      "5         43         370          1  0.000000  A40\n",
      "6         43         431          1  1.000000  A41\n",
      "7         43         445         19  0.526316  A43\n",
      "8         43         471          1  0.000000  A45\n",
      "9         43         504          1  0.000000  A46\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (997215, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: 1,000 players held out for fold-in verification\n",
      "   ‚Ä¢ Access via: holdout_players_df\n",
      "   ‚Ä¢ These players will NOT appear in any training, validation, or test splits\n",
      "\n",
      "‚úì Database connection closed\n",
      "\n",
      "‚úì Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# Connect to DuckDB and extract player-opening statistics\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "MAX_PLAYERS = 5_000 # for testing, will increase later\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player-opening statistics (color: '{COLOR_FILTER}')...\")\n",
    "    \n",
    "    # Extract stats with calculated score and num_games\n",
    "    # Filter by color, minimum rating, and calculate score in the database\n",
    "    MIN_RATING = 1200\n",
    "    print(f\"   ‚Ä¢ Minimum rating filter: {MIN_RATING}\")\n",
    "    \n",
    "    # First, get all eligible players and randomly select holdout set\n",
    "    print(f\"\\n2Ô∏è‚É£  Selecting holdout players for fold-in verification...\")\n",
    "    print(f\"   ‚Ä¢ Holdout size: {MIN_HOLDOUT_PLAYERS:,} players minimum\")\n",
    "    \n",
    "    # Get all players with sufficient data\n",
    "    player_query = f\"\"\"\n",
    "        SELECT DISTINCT p.id as player_id\n",
    "        FROM player p\n",
    "        JOIN player_opening_stats pos ON p.id = pos.player_id\n",
    "        WHERE p.rating >= {MIN_RATING}\n",
    "        AND pos.color = '{COLOR_FILTER}'\n",
    "        LIMIT {MAX_PLAYERS}\n",
    "    \"\"\"\n",
    "    \n",
    "    all_eligible_players = pd.DataFrame(con.execute(player_query).df())\n",
    "    total_eligible = len(all_eligible_players)\n",
    "    print(f\"   ‚Ä¢ Total eligible players: {total_eligible:,}\")\n",
    "    \n",
    "    if total_eligible < MIN_HOLDOUT_PLAYERS:\n",
    "        raise ValueError(f\"Not enough eligible players ({total_eligible:,}) to create holdout set of {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "    \n",
    "    # Randomly sample holdout players (deterministic with seed)\n",
    "    import numpy as np\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    holdout_player_ids = np.random.choice(\n",
    "        all_eligible_players['player_id'].values,\n",
    "        size=MIN_HOLDOUT_PLAYERS,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    training_player_ids = set(all_eligible_players['player_id'].values) - set(holdout_player_ids)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Holdout players selected: {len(holdout_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Training players available: {len(training_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Holdout percentage: {100 * len(holdout_player_ids) / total_eligible:.1f}%\")\n",
    "    \n",
    "    # Convert training player IDs to SQL-friendly string\n",
    "    training_player_ids_str = ','.join(map(str, training_player_ids))\n",
    "    \n",
    "    # Extract data ONLY for training players\n",
    "    print(f\"\\n3Ô∏è‚É£  Extracting training data (excluding holdout players)...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            pos.player_id,\n",
    "            pos.opening_id,\n",
    "            pos.num_wins + pos.num_draws + pos.num_losses as num_games,\n",
    "            (pos.num_wins + (pos.num_draws * 0.5)) / \n",
    "                NULLIF(pos.num_wins + pos.num_draws + pos.num_losses, 0) as score,\n",
    "            o.eco\n",
    "        FROM player_opening_stats pos\n",
    "        JOIN opening o ON pos.opening_id = o.id\n",
    "        JOIN player p ON pos.player_id = p.id\n",
    "        WHERE pos.color = '{COLOR_FILTER}'\n",
    "        AND p.rating >= {MIN_RATING}\n",
    "        AND pos.player_id IN ({training_player_ids_str})\n",
    "        ORDER BY pos.player_id, pos.opening_id\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_data = pd.DataFrame(con.execute(query).df())\n",
    "    \n",
    "    print(f\"   ‚úì Extracted {len(raw_data):,} rows\")\n",
    "    \n",
    "    # Also save holdout player IDs for later use\n",
    "    holdout_players_df = pd.DataFrame({'player_id': holdout_player_ids})\n",
    "    print(f\"\\n   üíæ Saved holdout_players_df with {len(holdout_players_df):,} player IDs\")\n",
    "    print(f\"   ‚Ä¢ These players are COMPLETELY UNSEEN by the training process\")\n",
    "    print(f\"   ‚Ä¢ Use them later for fold-in verification\")\n",
    "    \n",
    "    # Schema verification\n",
    "    print(\"\\n4Ô∏è‚É£  Verifying schema...\")\n",
    "    required_columns = ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
    "    \n",
    "    for col in required_columns:\n",
    "        if col not in raw_data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    print(f\"   ‚úì All required columns present: {required_columns}\")\n",
    "    \n",
    "    # Data types verification\n",
    "    print(\"\\n5Ô∏è‚É£  Checking data types...\")\n",
    "    print(f\"   ‚Ä¢ player_id: {raw_data['player_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ opening_id: {raw_data['opening_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ num_games: {raw_data['num_games'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ score: {raw_data['score'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ eco: {raw_data['eco'].dtype}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n6Ô∏è‚É£  Data statistics...\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {len(raw_data):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Total games (sum): {raw_data['num_games'].sum():,}\")\n",
    "    \n",
    "    # Player ID range\n",
    "    print(f\"\\n   Player ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['player_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['player_id'].max()}\")\n",
    "    \n",
    "    # Opening ID range\n",
    "    print(f\"\\n   Opening ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['opening_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['opening_id'].max()}\")\n",
    "    \n",
    "    # Games per entry statistics\n",
    "    print(f\"\\n   Games per entry:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['num_games'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['num_games'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['num_games'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['num_games'].median():.0f}\")\n",
    "    \n",
    "    # Score statistics\n",
    "    print(f\"\\n   Score distribution:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['score'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['score'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['score'].mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['score'].median():.4f}\")\n",
    "    \n",
    "    # Check for null values\n",
    "    print(\"\\n7Ô∏è‚É£  Checking for null values...\")\n",
    "    null_counts = raw_data.isnull().sum()\n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"   ‚úì No null values found\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(\"\\n8Ô∏è‚É£  Sample of extracted data (first 10 rows):\")\n",
    "    print(raw_data.head(10).to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nData shape: {raw_data.shape}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  IMPORTANT: {len(holdout_player_ids):,} players held out for fold-in verification\")\n",
    "    print(f\"   ‚Ä¢ Access via: holdout_players_df\")\n",
    "    print(f\"   ‚Ä¢ These players will NOT appear in any training, validation, or test splits\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"\\n‚úì Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad110b85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "519d8f2f",
   "metadata": {},
   "source": [
    "## Step 2: Data Sanitization & Normalization\n",
    "\n",
    "Filter low-quality data, handle duplicates, and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74f0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: DATA SANITIZATION & NORMALIZATION\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ MIN_GAMES_THRESHOLD: 10\n",
      "\n",
      "üìä Starting data shape: (997215, 5)\n",
      "   ‚Ä¢ Rows: 997,215\n",
      "   ‚Ä¢ Unique players: 4,000\n",
      "   ‚Ä¢ Unique openings: 2,988\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Before: 997,215 rows\n",
      "   ‚Ä¢ After: 246,703 rows\n",
      "   ‚Ä¢ Filtered out: 750,512 rows (75.3%)\n",
      "\n",
      "2Ô∏è‚É£  Checking for duplicate (player_id, opening_id) combinations...\n",
      "   ‚úì No duplicates found\n",
      "\n",
      "3Ô∏è‚É£  Removing players with no qualifying openings...\n",
      "   ‚Ä¢ Players before: 3,994\n",
      "   ‚Ä¢ Players after: 3,994\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "4Ô∏è‚É£  Removing openings with no qualifying players...\n",
      "   ‚Ä¢ Openings before: 2,324\n",
      "   ‚Ä¢ Openings after: 2,324\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "5Ô∏è‚É£  Verifying no null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Final data statistics:\n",
      "   ‚Ä¢ Total rows: 246,703\n",
      "   ‚Ä¢ Unique players: 3,994\n",
      "   ‚Ä¢ Unique openings: 2,324\n",
      "   ‚Ä¢ Total games: 17,330,300\n",
      "   ‚Ä¢ Avg games per entry: 70.2\n",
      "   ‚Ä¢ Avg openings per player: 61.8\n",
      "   ‚Ä¢ Avg players per opening: 106.2\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5131\n",
      "   ‚Ä¢ 75th percentile: 0.5753\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5114\n",
      "   ‚Ä¢ Std: 0.1085\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "        player_id  opening_id  num_games     score  eco\n",
      "76909       14599        1914         72  0.437500  C42\n",
      "163010      31526          35         29  0.431034  A00\n",
      "240818      47640        1664         31  0.596774  C30\n",
      "94882       18264         758         11  0.363636  B01\n",
      "174063      33669        1374         19  0.605263  C00\n",
      "200481      38974         321         41  0.548780  A22\n",
      "176817      34421         737         91  0.461538  B00\n",
      "83675       15910         704         17  0.470588  B00\n",
      "222311      43396        2052         68  0.500000  C50\n",
      "200479      38974         309         63  0.642857  A20\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (246703, 5)\n",
      "Data reduction: 75.3%\n",
      "   ‚Ä¢ Avg players per opening: 106.2\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5131\n",
      "   ‚Ä¢ 75th percentile: 0.5753\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5114\n",
      "   ‚Ä¢ Std: 0.1085\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "        player_id  opening_id  num_games     score  eco\n",
      "76909       14599        1914         72  0.437500  C42\n",
      "163010      31526          35         29  0.431034  A00\n",
      "240818      47640        1664         31  0.596774  C30\n",
      "94882       18264         758         11  0.363636  B01\n",
      "174063      33669        1374         19  0.605263  C00\n",
      "200481      38974         321         41  0.548780  A22\n",
      "176817      34421         737         91  0.461538  B00\n",
      "83675       15910         704         17  0.470588  B00\n",
      "222311      43396        2052         68  0.500000  C50\n",
      "200479      38974         309         63  0.642857  A20\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (246703, 5)\n",
      "Data reduction: 75.3%\n"
     ]
    }
   ],
   "source": [
    "# 2a. Filter low-quality data, handle duplicates, and prepare for training.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "MIN_GAMES_THRESHOLD = 10\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: DATA SANITIZATION & NORMALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   ‚Ä¢ MIN_GAMES_THRESHOLD: {MIN_GAMES_THRESHOLD}\")\n",
    "\n",
    "# Start with raw_data from Step 1\n",
    "print(f\"\\nüìä Starting data shape: {raw_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Rows: {len(raw_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "\n",
    "# 1. Filter by minimum games threshold\n",
    "print(f\"\\n1Ô∏è‚É£  Filtering entries with < {MIN_GAMES_THRESHOLD} games...\")\n",
    "before_filter = len(raw_data)\n",
    "clean_data = raw_data.query(f'num_games >= {MIN_GAMES_THRESHOLD}').copy()\n",
    "num_rows_after_filter = len(clean_data)\n",
    "num_rows_filtered_out = before_filter - num_rows_after_filter\n",
    "\n",
    "print(f\"   ‚Ä¢ Before: {before_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ After: {num_rows_after_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ Filtered out: {num_rows_filtered_out:,} rows ({100*num_rows_filtered_out/before_filter:.1f}%)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "print(f\"\\n2Ô∏è‚É£  Checking for duplicate (player_id, opening_id) combinations...\")\n",
    "num_duplicates = clean_data.duplicated(subset=['player_id', 'opening_id']).sum()\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {num_duplicates} duplicate entries\")\n",
    "    dup_mask = clean_data.duplicated(subset=['player_id', 'opening_id'], keep=False)\n",
    "    print(\"\\n   Sample of duplicates:\")\n",
    "    print(clean_data[dup_mask].head(10).to_string())\n",
    "    \n",
    "    # Keep only first occurrence of any duplicate player-opening pair\n",
    "    print(\"\\n   Removing duplicates (keeping first occurrence)...\")\n",
    "    clean_data = pd.DataFrame.drop_duplicates(clean_data, subset=['player_id', 'opening_id'], keep='first')\n",
    "    print(f\"   ‚úì After deduplication: {len(clean_data):,} rows\")\n",
    "else:\n",
    "    print(f\"   ‚úì No duplicates found\")\n",
    "\n",
    "# 3. Remove players with no qualifying openings\n",
    "print(f\"\\n3Ô∏è‚É£  Removing players with no qualifying openings...\") # Note that a few players only play stuff like the Van't Kruijs which we've excluded, so a small numer of players will be excluded here\n",
    "players_before = clean_data['player_id'].nunique()\n",
    "\n",
    "# Count openings per player\n",
    "num_openings_per_player = pd.DataFrame(clean_data.groupby('player_id').size(), columns=['count'])\n",
    "players_with_data = num_openings_per_player[num_openings_per_player['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter\n",
    "clean_data = clean_data[clean_data['player_id'].isin(players_with_data)]\n",
    "players_after = clean_data['player_id'].nunique()\n",
    "\n",
    "print(f\"   ‚Ä¢ Players before: {players_before:,}\")\n",
    "print(f\"   ‚Ä¢ Players after: {players_after:,}\")\n",
    "print(f\"   ‚Ä¢ Removed: {players_before - players_after}\")\n",
    "\n",
    "# 4. Remove openings with no qualifying players\n",
    "print(f\"\\n4Ô∏è‚É£  Removing openings with no qualifying players...\")\n",
    "num_openings_before = clean_data['opening_id'].nunique()\n",
    "\n",
    "# Use pd.DataFrame.groupby() to count players per opening\n",
    "num_players_per_opening = pd.DataFrame(clean_data.groupby('opening_id').size(), columns=['count'])\n",
    "openings_with_data = num_players_per_opening[num_players_per_opening['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter using pd.DataFrame.isin()\n",
    "clean_data = clean_data[clean_data['opening_id'].isin(openings_with_data)]\n",
    "openings_after = clean_data['opening_id'].nunique()\n",
    "\n",
    "print(f\"   ‚Ä¢ Openings before: {num_openings_before:,}\")\n",
    "print(f\"   ‚Ä¢ Openings after: {openings_after:,}\")\n",
    "print(f\"   ‚Ä¢ Removed: {num_openings_before - openings_after}\")\n",
    "\n",
    "# 5. Verify no null values using pd.isna()\n",
    "print(f\"\\n5Ô∏è‚É£  Verifying no null values...\")\n",
    "null_counts = pd.DataFrame.isna(clean_data).sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"   ‚úì No null values found\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "    for col, count in null_counts[null_counts > 0].items():\n",
    "        print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    # Drop rows with nulls using pd.DataFrame.dropna()\n",
    "    clean_data = pd.DataFrame.dropna(clean_data)\n",
    "    print(f\"   ‚úì Dropped null rows. New shape: {clean_data.shape}\")\n",
    "\n",
    "# TODO: Add confidence weighting column\n",
    "# TODO: Extract and normalize player ratings (side information)\n",
    "\n",
    "# Reset index using pd.DataFrame.reset_index()\n",
    "clean_data = pd.DataFrame.reset_index(clean_data, drop=True)\n",
    "\n",
    "# Final statistics using pd functions\n",
    "print(f\"\\n6Ô∏è‚É£  Final data statistics:\")\n",
    "print(f\"   ‚Ä¢ Total rows: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {pd.Series.nunique(clean_data['player_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {pd.Series.nunique(clean_data['opening_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Total games: {pd.Series.sum(clean_data['num_games']):,}\")\n",
    "print(f\"   ‚Ä¢ Avg games per entry: {pd.Series.mean(clean_data['num_games']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg openings per player: {len(clean_data) / pd.Series.nunique(clean_data['player_id']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg players per opening: {len(clean_data) / pd.Series.nunique(clean_data['opening_id']):.1f}\")\n",
    "\n",
    "# Score distribution using pd functions\n",
    "print(f\"\\n   Score statistics:\")\n",
    "print(f\"   ‚Ä¢ Min: {pd.Series.min(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {pd.Series.quantile(clean_data['score'], 0.25):.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {pd.Series.median(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {pd.Series.quantile(clean_data['score'], 0.75):.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {pd.Series.max(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {pd.Series.mean(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {pd.Series.std(clean_data['score']):.4f}\")\n",
    "\n",
    "# Sample of cleaned data using pd.DataFrame.sample()\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of cleaned data (10 random rows):\")\n",
    "print(pd.DataFrame.sample(clean_data, min(10, len(clean_data)), random_state=42).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SANITIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCleaned data shape: {clean_data.shape}\")\n",
    "print(f\"Data reduction: {100 * (1 - len(clean_data)/len(raw_data)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e671b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ K_PLAYER (shrinkage constant): 50\n",
      "   ‚Ä¢ Method: Two-level empirical Bayes shrinkage\n",
      "   ‚Ä¢ Level 1: Calculate opening-specific means\n",
      "   ‚Ä¢ Level 2: Shrink player scores toward opening means\n",
      "\n",
      "üìä Global statistics:\n",
      "   ‚Ä¢ Global mean score: 0.5114\n",
      "   ‚Ä¢ Total entries: 246,703\n",
      "   ‚Ä¢ Unique openings: 2,324\n",
      "\n",
      "1Ô∏è‚É£  LEVEL 1: Calculating opening-specific means...\n",
      "   ‚úì Calculated means for 2,324 openings\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1753\n",
      "   ‚Ä¢ 25th percentile: 0.4901\n",
      "   ‚Ä¢ Median: 0.5154\n",
      "   ‚Ä¢ 75th percentile: 0.5404\n",
      "   ‚Ä¢ Max: 0.9091\n",
      "   ‚Ä¢ Std: 0.0613\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 722\n",
      "   ‚Ä¢ Players per opening (median): 23\n",
      "   ‚Ä¢ Total games range: [10, 424246]\n",
      "   ‚Ä¢ Players range: [1, 3536]\n",
      "\n",
      "2Ô∏è‚É£  LEVEL 2: Shrinking player scores toward opening means...\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 246,703 entries\n",
      "\n",
      "3Ô∏è‚É£  Calculating confidence weights...\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Formula: confidence = num_games / (num_games + 50)\n",
      "   ‚Ä¢ Range: [0.1667, 0.9948]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000953\n",
      "   ‚Ä¢ Std adjustment: 0.076576\n",
      "   ‚Ä¢ Max adjustment: 0.459737\n",
      "   ‚Ä¢ Min adjustment: -0.448163\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003771\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001804\n",
      "   ‚Ä¢ 75th percentile (n=65 games): avg adjustment = -0.000684\n",
      "   ‚Ä¢ >75th percentile (n>65 games): avg adjustment = -0.001303\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1361\n",
      "   ‚Ä¢ 25th percentile: 0.4849\n",
      "   ‚Ä¢ Median: 0.5123\n",
      "   ‚Ä¢ 75th percentile: 0.5394\n",
      "   ‚Ä¢ Max: 0.9091\n",
      "   ‚Ä¢ Mean: 0.5124\n",
      "   ‚Ä¢ Std: 0.0421\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   Player 47819 | Opening 2052 | Games:  12 | Opening mean: 0.5334 | Original: 0.4167 ‚Üí Adjusted: 0.5108 | Diff: +0.0941 | Confidence: 0.194\n",
      "   Player  9958 | Opening  704 | Games:  11 | Opening mean: 0.4731 | Original: 0.2727 ‚Üí Adjusted: 0.4370 | Diff: +0.1643 | Confidence: 0.180\n",
      "   Player 10015 | Opening 3027 | Games:  10 | Opening mean: 0.5139 | Original: 0.8000 ‚Üí Adjusted: 0.5616 | Diff: -0.2384 | Confidence: 0.167\n",
      "   Player 44709 | Opening 2611 | Games:  10 | Opening mean: 0.5241 | Original: 0.6000 ‚Üí Adjusted: 0.5367 | Diff: -0.0633 | Confidence: 0.167\n",
      "   Player 20503 | Opening   51 | Games:  13 | Opening mean: 0.5078 | Original: 0.4231 ‚Üí Adjusted: 0.4903 | Diff: +0.0672 | Confidence: 0.206\n",
      "   Player 11448 | Opening 1325 | Games:  11 | Opening mean: 0.4834 | Original: 0.2727 ‚Üí Adjusted: 0.4454 | Diff: +0.1727 | Confidence: 0.180\n",
      "   Player 35838 | Opening  518 | Games:  12 | Opening mean: 0.5192 | Original: 0.4167 ‚Üí Adjusted: 0.4994 | Diff: +0.0827 | Confidence: 0.194\n",
      "   Player 15101 | Opening 2110 | Games:  14 | Opening mean: 0.5256 | Original: 0.5714 ‚Üí Adjusted: 0.5356 | Diff: -0.0358 | Confidence: 0.219\n",
      "   Player 11301 | Opening 1314 | Games:  16 | Opening mean: 0.4878 | Original: 0.3438 ‚Üí Adjusted: 0.4529 | Diff: +0.1092 | Confidence: 0.242\n",
      "   Player 42738 | Opening  496 | Games:  19 | Opening mean: 0.5178 | Original: 0.4737 ‚Üí Adjusted: 0.5057 | Diff: +0.0320 | Confidence: 0.275\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 35773 | Opening  376 | Games:  53 | Opening mean: 0.5191 | Original: 0.4434 ‚Üí Adjusted: 0.4802 | Diff: +0.0368 | Confidence: 0.515\n",
      "   Player 31777 | Opening 2465 | Games:  53 | Opening mean: 0.5212 | Original: 0.3868 ‚Üí Adjusted: 0.4520 | Diff: +0.0652 | Confidence: 0.515\n",
      "   Player  1073 | Opening  838 | Games:  53 | Opening mean: 0.5089 | Original: 0.4528 ‚Üí Adjusted: 0.4801 | Diff: +0.0272 | Confidence: 0.515\n",
      "   Player 46022 | Opening  773 | Games:  84 | Opening mean: 0.5019 | Original: 0.4702 ‚Üí Adjusted: 0.4821 | Diff: +0.0118 | Confidence: 0.627\n",
      "   Player 22809 | Opening  376 | Games:  90 | Opening mean: 0.5191 | Original: 0.5611 ‚Üí Adjusted: 0.5461 | Diff: -0.0150 | Confidence: 0.643\n",
      "   Player 43666 | Opening 3252 | Games:  53 | Opening mean: 0.4312 | Original: 0.4245 ‚Üí Adjusted: 0.4278 | Diff: +0.0032 | Confidence: 0.515\n",
      "   Player  1567 | Opening  772 | Games:  58 | Opening mean: 0.5200 | Original: 0.4483 ‚Üí Adjusted: 0.4815 | Diff: +0.0332 | Confidence: 0.537\n",
      "   Player 28303 | Opening 1137 | Games:  57 | Opening mean: 0.5057 | Original: 0.6316 ‚Üí Adjusted: 0.5727 | Diff: -0.0588 | Confidence: 0.533\n",
      "   Player  5407 | Opening 1643 | Games:  50 | Opening mean: 0.4987 | Original: 0.6100 ‚Üí Adjusted: 0.5543 | Diff: -0.0557 | Confidence: 0.500\n",
      "   Player 12906 | Opening 2050 | Games:  98 | Opening mean: 0.4906 | Original: 0.5408 ‚Üí Adjusted: 0.5239 | Diff: -0.0170 | Confidence: 0.662\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player  8612 | Opening 2731 | Games: 236 | Opening mean: 0.5383 | Original: 0.5169 ‚Üí Adjusted: 0.5207 | Diff: +0.0037 | Confidence: 0.825\n",
      "   Player 22458 | Opening 1986 | Games: 416 | Opening mean: 0.5255 | Original: 0.5661 ‚Üí Adjusted: 0.5617 | Diff: -0.0044 | Confidence: 0.893\n",
      "   Player 35435 | Opening  236 | Games: 618 | Opening mean: 0.5601 | Original: 0.6432 ‚Üí Adjusted: 0.6370 | Diff: -0.0062 | Confidence: 0.925\n",
      "   Player 47475 | Opening  751 | Games: 472 | Opening mean: 0.5076 | Original: 0.4428 ‚Üí Adjusted: 0.4490 | Diff: +0.0062 | Confidence: 0.904\n",
      "   Player 40889 | Opening 1945 | Games: 872 | Opening mean: 0.5150 | Original: 0.5550 ‚Üí Adjusted: 0.5529 | Diff: -0.0022 | Confidence: 0.946\n",
      "   Player 15101 | Opening 1356 | Games: 211 | Opening mean: 0.5061 | Original: 0.4953 ‚Üí Adjusted: 0.4973 | Diff: +0.0021 | Confidence: 0.808\n",
      "   Player 26268 | Opening  838 | Games: 224 | Opening mean: 0.5089 | Original: 0.5513 ‚Üí Adjusted: 0.5436 | Diff: -0.0077 | Confidence: 0.818\n",
      "   Player 13984 | Opening 1356 | Games: 230 | Opening mean: 0.5061 | Original: 0.5674 ‚Üí Adjusted: 0.5565 | Diff: -0.0109 | Confidence: 0.821\n",
      "   Player  9229 | Opening 1538 | Games: 801 | Opening mean: 0.5386 | Original: 0.5325 ‚Üí Adjusted: 0.5328 | Diff: +0.0004 | Confidence: 0.941\n",
      "   Player  7320 | Opening 1314 | Games: 247 | Opening mean: 0.4878 | Original: 0.6012 ‚Üí Adjusted: 0.5821 | Diff: -0.0191 | Confidence: 0.832\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Opening 2769 (D60): mean = 0.9091 (+0.3977 vs global) | 1 player entries\n",
      "   Opening  686 (B00): mean = 0.9000 (+0.3886 vs global) | 1 player entries\n",
      "   Opening 1697 (C33): mean = 0.8500 (+0.3386 vs global) | 1 player entries\n",
      "   Opening 1795 (C39): mean = 0.8333 (+0.3219 vs global) | 1 player entries\n",
      "   Opening 3345 (B01): mean = 0.8214 (+0.3100 vs global) | 1 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening  614 (A80): mean = 0.1753 (-0.3361 vs global) | 2 player entries\n",
      "   Opening 1239 (B58): mean = 0.1923 (-0.3191 vs global) | 1 player entries\n",
      "   Opening  669 (A99): mean = 0.2083 (-0.3031 vs global) | 1 player entries\n",
      "   Opening 1675 (C31): mean = 0.2154 (-0.2960 vs global) | 2 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 48020 | Opening 3537 (E45) | Games: 10 | Opening mean: 0.7500 | Original: 0.7500 ‚Üí 0.7500\n",
      "      If we'd shrunk to global mean: 0.5512 (would lose +0.1988 of deserved credit)\n",
      "   Player 26645 | Opening 2130 (C54) | Games: 18 | Opening mean: 0.7285 | Original: 0.8333 ‚Üí 0.7562\n",
      "      If we'd shrunk to global mean: 0.5966 (would lose +0.1596 of deserved credit)\n",
      "   Player 41233 | Opening 1455 (C12) | Games: 10 | Opening mean: 0.7111 | Original: 0.7000 ‚Üí 0.7093\n",
      "      If we'd shrunk to global mean: 0.5428 (would lose +0.1664 of deserved credit)\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player 19116 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3368 | Original: 0.2000 ‚Üí 0.3140\n",
      "      If we'd shrunk to global mean: 0.4595 (would unfairly boost by +0.1455)\n",
      "   Player 39247 | Opening 1989 (C45) | Games: 10 | Opening mean: 0.3137 | Original: 0.0000 ‚Üí 0.2614\n",
      "      If we'd shrunk to global mean: 0.4262 (would unfairly boost by +0.1647)\n",
      "   Player 44048 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3368 | Original: 0.4000 ‚Üí 0.3474\n",
      "      If we'd shrunk to global mean: 0.4928 (would unfairly boost by +0.1455)\n",
      "\n",
      "9Ô∏è‚É£  Cleaning up...\n",
      "   ‚úì Removed temporary columns\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (246703, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n",
      "   Opening 2769 (D60): mean = 0.9091 (+0.3977 vs global) | 1 player entries\n",
      "   Opening  686 (B00): mean = 0.9000 (+0.3886 vs global) | 1 player entries\n",
      "   Opening 1697 (C33): mean = 0.8500 (+0.3386 vs global) | 1 player entries\n",
      "   Opening 1795 (C39): mean = 0.8333 (+0.3219 vs global) | 1 player entries\n",
      "   Opening 3345 (B01): mean = 0.8214 (+0.3100 vs global) | 1 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening  614 (A80): mean = 0.1753 (-0.3361 vs global) | 2 player entries\n",
      "   Opening 1239 (B58): mean = 0.1923 (-0.3191 vs global) | 1 player entries\n",
      "   Opening  669 (A99): mean = 0.2083 (-0.3031 vs global) | 1 player entries\n",
      "   Opening 1675 (C31): mean = 0.2154 (-0.2960 vs global) | 2 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 48020 | Opening 3537 (E45) | Games: 10 | Opening mean: 0.7500 | Original: 0.7500 ‚Üí 0.7500\n",
      "      If we'd shrunk to global mean: 0.5512 (would lose +0.1988 of deserved credit)\n",
      "   Player 26645 | Opening 2130 (C54) | Games: 18 | Opening mean: 0.7285 | Original: 0.8333 ‚Üí 0.7562\n",
      "      If we'd shrunk to global mean: 0.5966 (would lose +0.1596 of deserved credit)\n",
      "   Player 41233 | Opening 1455 (C12) | Games: 10 | Opening mean: 0.7111 | Original: 0.7000 ‚Üí 0.7093\n",
      "      If we'd shrunk to global mean: 0.5428 (would lose +0.1664 of deserved credit)\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player 19116 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3368 | Original: 0.2000 ‚Üí 0.3140\n",
      "      If we'd shrunk to global mean: 0.4595 (would unfairly boost by +0.1455)\n",
      "   Player 39247 | Opening 1989 (C45) | Games: 10 | Opening mean: 0.3137 | Original: 0.0000 ‚Üí 0.2614\n",
      "      If we'd shrunk to global mean: 0.4262 (would unfairly boost by +0.1647)\n",
      "   Player 44048 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3368 | Original: 0.4000 ‚Üí 0.3474\n",
      "      If we'd shrunk to global mean: 0.4928 (would unfairly boost by +0.1455)\n",
      "\n",
      "9Ô∏è‚É£  Cleaning up...\n",
      "   ‚úì Removed temporary columns\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (246703, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n"
     ]
    }
   ],
   "source": [
    "# 2b. Apply hierarchical Bayesian shrinkage to adjust scores based on sample size confidence\n",
    "\n",
    "# Check if confidence already exists - if so, skip this processing\n",
    "if 'confidence' in clean_data.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'confidence' column already exists in data\")\n",
    "    print(\"   This indicates hierarchical Bayesian processing has already been applied.\")\n",
    "    print(f\"\\nCurrent data shape: {clean_data.shape}\")\n",
    "    print(f\"Confidence range: [{clean_data['confidence'].min():.4f}, {clean_data['confidence'].max():.4f}]\")\n",
    "else:\n",
    "    # Define the processing function\n",
    "    # This is a long function, I recommend you fold it down in your editor\n",
    "    def apply_hierarchical_bayesian_shrinkage(data, k_player=50):\n",
    "        \"\"\"\n",
    "        Apply two-level hierarchical Bayesian shrinkage to adjust scores.\n",
    "        \n",
    "        A lot of our player-opening entries have a small number of games played, because openings are so specific.\n",
    "        This introduces sample size issues.\n",
    "        \n",
    "        We use TWO-LEVEL shrinkage:\n",
    "        Level 1: Calculate opening-specific means (these are our \"ground truth\" for each opening)\n",
    "        Level 2: Shrink individual player-opening scores toward their opening's mean\n",
    "        This is better than shrinking toward global mean because different openings have different baseline win rates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Clean data with columns: player_id, opening_id, score, num_games, eco\n",
    "        k_player : int\n",
    "            Shrinkage constant for player-opening scores (default: 50)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Data with adjusted scores and new 'confidence' column\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "        print(f\"   ‚Ä¢ K_PLAYER (shrinkage constant): {k_player}\")\n",
    "        print(f\"   ‚Ä¢ Method: Two-level empirical Bayes shrinkage\")\n",
    "        print(f\"   ‚Ä¢ Level 1: Calculate opening-specific means\")\n",
    "        print(f\"   ‚Ä¢ Level 2: Shrink player scores toward opening means\")\n",
    "        \n",
    "        # Calculate global mean score for comparison\n",
    "        global_mean_score = data[\"score\"].mean()\n",
    "        print(f\"\\nüìä Global statistics:\")\n",
    "        print(f\"   ‚Ä¢ Global mean score: {global_mean_score:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Total entries: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique openings: {data['opening_id'].nunique():,}\")\n",
    "        \n",
    "        # Store original scores for comparison\n",
    "        data = data.copy()  # Best practice: work on a copy\n",
    "        data[\"score_original\"] = data[\"score\"].copy()\n",
    "        \n",
    "        # LEVEL 1: Calculate opening-specific means and statistics\n",
    "        print(f\"\\n1Ô∏è‚É£  LEVEL 1: Calculating opening-specific means...\")\n",
    "        \n",
    "        opening_stats = (\n",
    "            data.groupby(\"opening_id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"score\": \"mean\",\n",
    "                    \"num_games\": \"sum\",\n",
    "                    \"player_id\": \"count\",  # Number of players who played this opening\n",
    "                }\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"score\": \"opening_mean\",\n",
    "                    \"num_games\": \"opening_total_games\",\n",
    "                    \"player_id\": \"opening_num_players\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úì Calculated means for {len(opening_stats):,} openings\")\n",
    "        \n",
    "        # Opening mean statistics\n",
    "        print(f\"\\n   Opening mean score distribution:\")\n",
    "        print(f\"   ‚Ä¢ Min: {opening_stats['opening_mean'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {opening_stats['opening_mean'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {opening_stats['opening_mean'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {opening_stats['opening_mean'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {opening_stats['opening_mean'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {opening_stats['opening_mean'].std():.4f}\")\n",
    "        \n",
    "        # Show distribution of opening sizes\n",
    "        print(f\"\\n   Opening sample size distribution:\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games per opening (median): {opening_stats['opening_total_games'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players per opening (median): {opening_stats['opening_num_players'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games range: [{opening_stats['opening_total_games'].min():.0f}, {opening_stats['opening_total_games'].max():.0f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players range: [{opening_stats['opening_num_players'].min():.0f}, {opening_stats['opening_num_players'].max():.0f}]\"\n",
    "        )\n",
    "        \n",
    "        # Merge opening means back into main dataframe\n",
    "        data = data.merge(\n",
    "            opening_stats[[\"opening_mean\"]], left_on=\"opening_id\", right_index=True, how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # LEVEL 2: Shrink player-opening scores toward opening-specific means\n",
    "        print(f\"\\n2Ô∏è‚É£  LEVEL 2: Shrinking player scores toward opening means...\")\n",
    "        print(\n",
    "            f\"   Formula: adjusted_score = (num_games √ó player_score + {k_player} √ó opening_mean) / (num_games + {k_player})\"\n",
    "        )\n",
    "        \n",
    "        numerator = (data[\"num_games\"] * data[\"score_original\"]) + (\n",
    "            k_player * data[\"opening_mean\"]\n",
    "        )\n",
    "        denominator = data[\"num_games\"] + k_player\n",
    "        data[\"score\"] = numerator / denominator\n",
    "        \n",
    "        print(f\"   ‚úì Scores adjusted for {len(data):,} entries\")\n",
    "        \n",
    "        # Calculate confidence weights (will be used in loss function later)\n",
    "        print(f\"\\n3Ô∏è‚É£  Calculating confidence weights...\")\n",
    "        data[\"confidence\"] = data[\"num_games\"] / (\n",
    "            data[\"num_games\"] + k_player\n",
    "        )\n",
    "        print(f\"   ‚úì Confidence weights calculated\")\n",
    "        print(f\"   ‚Ä¢ Formula: confidence = num_games / (num_games + {k_player})\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Range: [{data['confidence'].min():.4f}, {data['confidence'].max():.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # Statistics on the adjustment\n",
    "        score_diff = data[\"score\"] - data[\"score_original\"]\n",
    "        print(f\"\\n4Ô∏è‚É£  Adjustment statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean adjustment: {score_diff.mean():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Std adjustment: {score_diff.std():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Max adjustment: {score_diff.max():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Min adjustment: {score_diff.min():.6f}\")\n",
    "        \n",
    "        # Show distribution of adjustments\n",
    "        print(f\"\\n   Adjustment by num_games quartiles:\")\n",
    "        quartiles = data[\"num_games\"].quantile([0.25, 0.5, 0.75])\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 25th percentile (n={quartiles[0.25]:.0f} games): avg adjustment = {score_diff[data['num_games'] <= quartiles[0.25]].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 50th percentile (n={quartiles[0.5]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.25]) & (data['num_games'] <= quartiles[0.5])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 75th percentile (n={quartiles[0.75]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.5]) & (data['num_games'] <= quartiles[0.75])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ >75th percentile (n>{quartiles[0.75]:.0f} games): avg adjustment = {score_diff[data['num_games'] > quartiles[0.75]].mean():.6f}\"\n",
    "        )\n",
    "        \n",
    "        # New score distribution after adjustment\n",
    "        print(f\"\\n5Ô∏è‚É£  Adjusted score statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {data['score'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {data['score'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {data['score'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {data['score'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {data['score'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {data['score'].mean():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {data['score'].std():.4f}\")\n",
    "        \n",
    "        # Detailed sample showing the effect across different game counts\n",
    "        print(f\"\\n6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\")\n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        low_game_sample = data[\n",
    "            (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in low_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Medium-game entries (50-100 games) - MODERATE shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        med_game_sample = data[\n",
    "            (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in med_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   High-game entries (200+ games) - LOW shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        high_game_sample = data[data[\"num_games\"] >= 200].sample(\n",
    "            min(10, len(data[data[\"num_games\"] >= 200])), random_state=42\n",
    "        )\n",
    "        for idx, row in high_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        # Show extreme cases - comparing to both opening mean AND global mean\n",
    "        print(f\"\\n7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\")\n",
    "        \n",
    "        # Find entries where opening mean differs significantly from global mean\n",
    "        data[\"opening_deviation_from_global\"] = (\n",
    "            data[\"opening_mean\"] - global_mean_score\n",
    "        ).abs()\n",
    "        \n",
    "        print(f\"\\n   Openings with HIGHEST win rates (strong for White):\")\n",
    "        strong_openings = data.nlargest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in strong_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"(+{deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   Openings with LOWEST win rates (weak for White):\")\n",
    "        weak_openings = data.nsmallest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in weak_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"({deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        # Show specific examples where hierarchical shrinkage made a difference\n",
    "        print(f\"\\n8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\")\n",
    "        \n",
    "        # Find entries with strong openings where player did well\n",
    "        strong_opening_ids = data.nlargest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        strong_examples = data[\n",
    "            (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] > 0.6)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] > 0.6)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"\\n   Strong opening + good player performance (shrunk toward HIGH opening mean):\"\n",
    "        )\n",
    "        for idx, row in strong_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would lose {difference:+.4f} of deserved credit)\"\n",
    "            )\n",
    "        \n",
    "        # Find entries with weak openings where player did poorly\n",
    "        weak_opening_ids = data.nsmallest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        weak_examples = data[\n",
    "            (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] < 0.45)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] < 0.45)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Weak opening + poor player performance (shrunk toward LOW opening mean):\")\n",
    "        for idx, row in weak_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would unfairly boost by {-difference:+.4f})\"\n",
    "            )\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        print(f\"\\n9Ô∏è‚É£  Cleaning up...\")\n",
    "        data = data.drop(\n",
    "            columns=[\"score_original\", \"opening_mean\", \"opening_deviation_from_global\"]\n",
    "        )\n",
    "        print(f\"   ‚úì Removed temporary columns\")\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nFinal data shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        print(f\"\\nNew columns added:\")\n",
    "        print(f\"   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\")\n",
    "        print(f\"   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\")\n",
    "        print(f\"\\nKey improvement over simple shrinkage:\")\n",
    "        print(f\"   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\")\n",
    "        print(f\"   ‚Ä¢ Preserves opening difficulty differences\")\n",
    "        print(f\"   ‚Ä¢ More accurate for both strong and weak openings\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Configuration for Bayesian shrinkage\n",
    "    K_PLAYER = 50  # Shrinkage constant for player-opening scores\n",
    "    \n",
    "    # Call the function\n",
    "    clean_data = apply_hierarchical_bayesian_shrinkage(clean_data, k_player=K_PLAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73319d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        player_id  opening_id  num_games     score  eco  confidence\n",
      "152264      29473         382         26  0.486783  A40    0.342105\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2C: PLAYER RATING STATISTICS\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Extracting player ratings from database...\n",
      "   ‚úì Retrieved ratings for 3,994 players\n",
      "   ‚úì Database connection closed\n",
      "\n",
      "2Ô∏è‚É£  Merging ratings with clean_data...\n",
      "   ‚úì Merged successfully\n",
      "   ‚úì All entries have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 3,994\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1201\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1784.55\n",
      "   ‚Ä¢ Median: 1787\n",
      "   ‚Ä¢ Std Dev: 248.79\n",
      "\n",
      "4Ô∏è‚É£  Quartile statistics:\n",
      "   ‚Ä¢ 25th percentile: 1600\n",
      "   ‚Ä¢ 50th percentile (median): 1787\n",
      "   ‚Ä¢ 75th percentile: 1950\n",
      "\n",
      "5Ô∏è‚É£  Detailed percentile distribution (5% increments):\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1201    \n",
      "       5%          1381    ‚ñà‚ñà‚ñà‚ñà\n",
      "      10%          1458    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1514    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1558    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1600    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1644    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1682    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1718    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1755    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1787    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1820    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1856    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1884    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1915    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1950    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1996    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2039    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2102    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2202    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400         238      5.96%      ‚ñà‚ñà\n",
      "   1400-1600         756     18.93%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800       1,078     26.99%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000       1,142     28.59%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200         569     14.25%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400         172      4.31%      ‚ñà\n",
      "   2400-2600          37      0.93%      \n",
      "   2600-3000           2      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1622\n",
      "   ‚Ä¢ Interquartile Range (IQR): 350\n",
      "   ‚Ä¢ 10th-90th percentile range: 644\n",
      "   1600-1800       1,078     26.99%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000       1,142     28.59%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200         569     14.25%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400         172      4.31%      ‚ñà\n",
      "   2400-2600          37      0.93%      \n",
      "   2600-3000           2      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1622\n",
      "   ‚Ä¢ Interquartile Range (IQR): 350\n",
      "   ‚Ä¢ 10th-90th percentile range: 644\n",
      "\n",
      "8Ô∏è‚É£  Distribution shape:\n",
      "   ‚Ä¢ Skewness: 0.1537 (right-skewed)\n",
      "   ‚Ä¢ Kurtosis: -0.2527 (light-tailed)\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1458):\n",
      "      Player 22465: abritram - Rating: 1458\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1600):\n",
      "      Player 12533: MANPU - Rating: 1600\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1787):\n",
      "      Player 31928: jonascmunhoz - Rating: 1787\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1950):\n",
      "      Player 19841: Supercerebraco - Rating: 1950\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2102):\n",
      "      Player 32230: junison07 - Rating: 2102\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Key takeaways:\n",
      "   ‚Ä¢ Total players: 3,994\n",
      "   ‚Ä¢ Rating range: [1201, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1785 ¬± 249\n",
      "   ‚Ä¢ Median: 1787\n",
      "\n",
      "   Next steps: Normalize ratings for model input\n",
      "\n",
      "8Ô∏è‚É£  Distribution shape:\n",
      "   ‚Ä¢ Skewness: 0.1537 (right-skewed)\n",
      "   ‚Ä¢ Kurtosis: -0.2527 (light-tailed)\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1458):\n",
      "      Player 22465: abritram - Rating: 1458\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1600):\n",
      "      Player 12533: MANPU - Rating: 1600\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1787):\n",
      "      Player 31928: jonascmunhoz - Rating: 1787\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1950):\n",
      "      Player 19841: Supercerebraco - Rating: 1950\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2102):\n",
      "      Player 32230: junison07 - Rating: 2102\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Key takeaways:\n",
      "   ‚Ä¢ Total players: 3,994\n",
      "   ‚Ä¢ Rating range: [1201, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1785 ¬± 249\n",
      "   ‚Ä¢ Median: 1787\n",
      "\n",
      "   Next steps: Normalize ratings for model input\n"
     ]
    }
   ],
   "source": [
    "# 2c. Gather player rating statistics (no mutation, just exploration)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2C: PLAYER RATING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to database and extract player ratings\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player ratings from database...\")\n",
    "    \n",
    "    # Get unique player IDs from our clean_data\n",
    "    unique_player_ids = clean_data['player_id'].unique()\n",
    "    player_ids_str = ','.join(map(str, unique_player_ids))\n",
    "    \n",
    "    # Query to get player ratings\n",
    "    rating_query = f\"\"\"\n",
    "        SELECT \n",
    "            id as player_id,\n",
    "            name,\n",
    "            title,\n",
    "            rating\n",
    "        FROM player\n",
    "        WHERE id IN ({player_ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    player_ratings = pd.DataFrame(con.execute(rating_query).df())\n",
    "    print(f\"   ‚úì Retrieved ratings for {len(player_ratings):,} players\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"   ‚úì Database connection closed\")\n",
    "\n",
    "# Merge ratings into clean_data for analysis\n",
    "print(f\"\\n2Ô∏è‚É£  Merging ratings with clean_data...\")\n",
    "clean_data_with_ratings = clean_data.merge(player_ratings[['player_id', 'rating']], on='player_id', how='left')\n",
    "print(f\"   ‚úì Merged successfully\")\n",
    "\n",
    "# Check for missing ratings\n",
    "num_missing_ratings = clean_data_with_ratings['rating'].isna().sum()\n",
    "if num_missing_ratings > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  {num_missing_ratings:,} entries ({100*num_missing_ratings/len(clean_data_with_ratings):.2f}%) have missing ratings\")\n",
    "else:\n",
    "    print(f\"   ‚úì All entries have ratings\")\n",
    "\n",
    "# Basic rating statistics\n",
    "print(f\"\\n3Ô∏è‚É£  Basic rating statistics:\")\n",
    "print(f\"   ‚Ä¢ Count: {player_ratings['rating'].notna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Missing: {player_ratings['rating'].isna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Min: {player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Max: {player_ratings['rating'].max():.0f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {player_ratings['rating'].mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"   ‚Ä¢ Std Dev: {player_ratings['rating'].std():.2f}\")\n",
    "\n",
    "# Quartile statistics\n",
    "print(f\"\\n4Ô∏è‚É£  Quartile statistics:\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {player_ratings['rating'].quantile(0.25):.0f}\")\n",
    "print(f\"   ‚Ä¢ 50th percentile (median): {player_ratings['rating'].quantile(0.50):.0f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {player_ratings['rating'].quantile(0.75):.0f}\")\n",
    "\n",
    "# Granular percentile statistics (5% increments)\n",
    "print(f\"\\n5Ô∏è‚É£  Detailed percentile distribution (5% increments):\")\n",
    "percentiles = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,\n",
    "               0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "print(f\"\\n   {'Percentile':<12} {'Rating':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*12} {'-'*10} {'-'*40}\")\n",
    "\n",
    "for p in percentiles:\n",
    "    rating_value = player_ratings['rating'].quantile(p)\n",
    "    # Create a simple bar visualization\n",
    "    bar_length = int((rating_value - player_ratings['rating'].min()) / \n",
    "                     (player_ratings['rating'].max() - player_ratings['rating'].min()) * 40)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {p*100:>5.0f}%       {rating_value:>7.0f}    {bar}\")\n",
    "\n",
    "# Rating ranges and counts\n",
    "print(f\"\\n6Ô∏è‚É£  Rating distribution by range:\")\n",
    "rating_ranges = [\n",
    "    (0, 1000), (1000, 1200), (1200, 1400), (1400, 1600), \n",
    "    (1600, 1800), (1800, 2000), (2000, 2200), (2200, 2400), \n",
    "    (2400, 2600), (2600, 3000)\n",
    "]\n",
    "\n",
    "print(f\"\\n   {'Range':<15} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*15} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "\n",
    "for low, high in rating_ranges:\n",
    "    count = len(player_ratings[(player_ratings['rating'] >= low) & (player_ratings['rating'] < high)])\n",
    "    pct = 100 * count / len(player_ratings)\n",
    "    bar_length = int(pct * 0.4)  # Scale for visualization\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {low:>4}-{high:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Interquartile range\n",
    "iqr = player_ratings['rating'].quantile(0.75) - player_ratings['rating'].quantile(0.25)\n",
    "print(f\"\\n7Ô∏è‚É£  Spread statistics:\")\n",
    "print(f\"   ‚Ä¢ Range: {player_ratings['rating'].max() - player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Interquartile Range (IQR): {iqr:.0f}\")\n",
    "print(f\"   ‚Ä¢ 10th-90th percentile range: {player_ratings['rating'].quantile(0.90) - player_ratings['rating'].quantile(0.10):.0f}\")\n",
    "\n",
    "# Skewness and kurtosis if available\n",
    "try:\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    skewness = skew(player_ratings['rating'].dropna())\n",
    "    kurt = kurtosis(player_ratings['rating'].dropna())\n",
    "    print(f\"\\n8Ô∏è‚É£  Distribution shape:\")\n",
    "    print(f\"   ‚Ä¢ Skewness: {skewness:.4f} {'(right-skewed)' if skewness > 0 else '(left-skewed)' if skewness < 0 else '(symmetric)'}\")\n",
    "    print(f\"   ‚Ä¢ Kurtosis: {kurt:.4f} {'(heavy-tailed)' if kurt > 0 else '(light-tailed)' if kurt < 0 else '(normal)'}\")\n",
    "except ImportError:\n",
    "    print(f\"\\n8Ô∏è‚É£  Distribution shape:\")\n",
    "    print(f\"   ‚Ä¢ scipy not available for skewness/kurtosis calculation\")\n",
    "\n",
    "# Sample of players at different rating levels\n",
    "print(f\"\\n9Ô∏è‚É£  Sample players at different rating levels:\")\n",
    "sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "for p in sample_percentiles:\n",
    "    rating_threshold = player_ratings['rating'].quantile(p)\n",
    "    # Get a player near this rating\n",
    "    sample_player = player_ratings.iloc[(player_ratings['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "    print(f\"\\n   ~{p*100:.0f}th percentile (rating ‚âà {rating_threshold:.0f}):\")\n",
    "    for idx, row in sample_player.iterrows():\n",
    "        # print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f} {f'({row['title']})' if pd.notna(row['title']) else ''}\")\n",
    "        title_str = f\" ({row['title']})\" if pd.notna(row['title']) else \"\"\n",
    "        print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f}{title_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ RATING STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nKey takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total players: {len(player_ratings):,}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings['rating'].min():.0f}, {player_ratings['rating'].max():.0f}]\")\n",
    "print(f\"   ‚Ä¢ Mean ¬± std: {player_ratings['rating'].mean():.0f} ¬± {player_ratings['rating'].std():.0f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"\\n   Next steps: Normalize ratings for model input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ab6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Normalization strategy: Z-score\n",
      "   ‚Ä¢ Formula: (rating - mean) / std\n",
      "   ‚Ä¢ Purpose: Scale ratings for use as side information in MF model\n",
      "   ‚Ä¢ Storage: SEPARATE lookup table, NOT merged into clean_data\n",
      "   ‚Ä¢ Usage: Model will lookup player_id ‚Üí rating_z during training\n",
      "\n",
      "1Ô∏è‚É£  Normalization parameters (calculated from 3,994 players):\n",
      "   ‚Ä¢ Mean: 1784.55\n",
      "   ‚Ä¢ Std Dev: 248.79\n",
      "\n",
      "2Ô∏è‚É£  Normalized rating statistics:\n",
      "   ‚Ä¢ Min: -2.3456\n",
      "   ‚Ä¢ Max: 4.1740\n",
      "   ‚Ä¢ Mean: -0.000000 (should be ~0)\n",
      "   ‚Ä¢ Std: 1.000000 (should be ~1)\n",
      "   ‚Ä¢ Range: [-2.35, 4.17]\n",
      "\n",
      "3Ô∏è‚É£  Sample normalized ratings across skill levels:\n",
      "   ~10th percentile: Player 1862 | Rating: 1458 ‚Üí Z-score: -1.313\n",
      "   ~25th percentile: Player 1049 | Rating: 1600 ‚Üí Z-score: -0.742\n",
      "   ~50th percentile: Player 2632 | Rating: 1787 ‚Üí Z-score:  0.010\n",
      "   ~75th percentile: Player 1630 | Rating: 1950 ‚Üí Z-score:  0.665\n",
      "   ~90th percentile: Player 2650 | Rating: 2102 ‚Üí Z-score:  1.276\n",
      "\n",
      "4Ô∏è‚É£  Interpretation guide:\n",
      "   ‚Ä¢ rating_z ‚âà -2.3: 1200 player (minimum)\n",
      "   ‚Ä¢ rating_z ‚âà -0.7: 1600 player (25th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà  0.0: 1785 player (mean)\n",
      "   ‚Ä¢ rating_z ‚âà 0.7: 1950 player (75th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà 4.2: 2823 player (maximum)\n",
      "\n",
      "5Ô∏è‚É£  Side information table structure:\n",
      "   ‚Ä¢ Shape: (3994, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'rating', 'rating_z']\n",
      "   ‚Ä¢ Indexing: Setting player_id as index for O(1) lookups\n",
      "\n",
      "6Ô∏è‚É£  Sample entries from side information table:\n",
      "   Player 24853 | Rating: 1531 ‚Üí Z-score: -1.019\n",
      "   Player 12861 | Rating: 1991 ‚Üí Z-score:  0.830\n",
      "   Player 20823 | Rating: 2365 ‚Üí Z-score:  2.333\n",
      "   Player  9841 | Rating: 1898 ‚Üí Z-score:  0.456\n",
      "   Player 48724 | Rating: 1733 ‚Üí Z-score: -0.207\n",
      "   Player 22507 | Rating: 1455 ‚Üí Z-score: -1.325\n",
      "   Player 24554 | Rating: 1572 ‚Üí Z-score: -0.854\n",
      "   Player  6943 | Rating: 1801 ‚Üí Z-score:  0.066\n",
      "   Player  2012 | Rating: 1714 ‚Üí Z-score: -0.284\n",
      "   Player 28288 | Rating: 2044 ‚Üí Z-score:  1.043\n",
      "\n",
      "7Ô∏è‚É£  Removing unnecessary columns...\n",
      "   ‚úì Dropped 'rating' column (only keeping 'rating_z')\n",
      "   ‚Ä¢ Final columns: ['rating_z']\n",
      "\n",
      "8Ô∏è‚É£  Verifying all clean_data players have ratings:\n",
      "   ‚úì All 3,994 players in clean_data have side information\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING NORMALIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Created: player_side_info\n",
      "   ‚Ä¢ Shape: (3994, 1)\n",
      "   ‚Ä¢ Index: player_id\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ clean_data: 246,703 rows (player-opening interactions)\n",
      "   ‚Ä¢ player_side_info: 3,994 rows (one per player)\n",
      "   ‚Ä¢ Rating storage: ONE value per player (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\n",
      "   RATING_MEAN = 1784.55\n",
      "   RATING_STD = 248.79\n",
      "\n",
      "   You'll need them to normalize ratings for new users at inference time.\n"
     ]
    }
   ],
   "source": [
    "# 2d. Normalize player ratings using z-score normalization (for use as side information in MF model)\n",
    "\n",
    "# Check if we've already created the player_side_info table\n",
    "if 'player_side_info' in globals() and 'rating_z' in player_side_info.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2D: RATING NORMALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'player_side_info' table already exists\")\n",
    "    print(\"   This indicates rating normalization has already been applied.\")\n",
    "    print(f\"\\nPlayer side info shape: {player_side_info.shape}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing normalized rating statistics:\")\n",
    "    print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "    print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing normalized ratings:\")\n",
    "    sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "    for idx, row in sample_data.iterrows():\n",
    "        print(f\"   Player {idx:>5} | {row['name']:<20} | \"\n",
    "              f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "else:\n",
    "    def normalize_player_ratings(player_ratings_df):\n",
    "        \"\"\"\n",
    "        Apply z-score normalization to player ratings for use as side information.\n",
    "        \n",
    "        This creates a SEPARATE table of player-level features, NOT merged into clean_data.\n",
    "        Rating is side information - it describes the player, not the player-opening interaction.\n",
    "        \n",
    "        During training, the model will LOOK UP each player's rating_z from this table.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ratings_df : pd.DataFrame\n",
    "            Player ratings with columns: player_id, name, title, rating\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (player_side_info DataFrame, RATING_MEAN, RATING_STD)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Normalization strategy: Z-score\")\n",
    "        print(f\"   ‚Ä¢ Formula: (rating - mean) / std\")\n",
    "        print(f\"   ‚Ä¢ Purpose: Scale ratings for use as side information in MF model\")\n",
    "        print(f\"   ‚Ä¢ Storage: SEPARATE lookup table, NOT merged into clean_data\")\n",
    "        print(f\"   ‚Ä¢ Usage: Model will lookup player_id ‚Üí rating_z during training\")\n",
    "        \n",
    "        # Calculate normalization parameters\n",
    "        RATING_MEAN = player_ratings_df['rating'].mean()\n",
    "        RATING_STD = player_ratings_df['rating'].std()\n",
    "        \n",
    "        print(f\"\\n1Ô∏è‚É£  Normalization parameters (calculated from {len(player_ratings_df):,} players):\")\n",
    "        print(f\"   ‚Ä¢ Mean: {RATING_MEAN:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Std Dev: {RATING_STD:.2f}\")\n",
    "        \n",
    "        # Create side information table - only keep player_id and rating for now\n",
    "        player_side_info = player_ratings_df[['player_id', 'rating']].copy()\n",
    "        player_side_info['rating_z'] = (player_side_info['rating'] - RATING_MEAN) / RATING_STD\n",
    "        \n",
    "        print(f\"\\n2Ô∏è‚É£  Normalized rating statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "        print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "        print(f\"   ‚Ä¢ Range: [{player_side_info['rating_z'].min():.2f}, {player_side_info['rating_z'].max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\n3Ô∏è‚É£  Sample normalized ratings across skill levels:\")\n",
    "        sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "        for p in sample_percentiles:\n",
    "            rating_threshold = player_side_info['rating'].quantile(p)\n",
    "            sample_player = player_side_info.iloc[(player_side_info['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "            for idx, row in sample_player.iterrows():\n",
    "                print(f\"   ~{p*100:.0f}th percentile: Player {idx} | \"\n",
    "                      f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n4Ô∏è‚É£  Interpretation guide:\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(1200 - RATING_MEAN)/RATING_STD:.1f}: 1200 player (minimum)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.25) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.25):.0f} player (25th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà  0.0: {RATING_MEAN:.0f} player (mean)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.75) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.75):.0f} player (75th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].max() - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].max():.0f} player (maximum)\")\n",
    "        \n",
    "        print(f\"\\n5Ô∏è‚É£  Side information table structure:\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        print(f\"   ‚Ä¢ Indexing: Setting player_id as index for O(1) lookups\")\n",
    "        \n",
    "        # Set player_id as index for fast lookups\n",
    "        player_side_info = player_side_info.set_index('player_id')\n",
    "        \n",
    "        print(f\"\\n6Ô∏è‚É£  Sample entries from side information table:\")\n",
    "        sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "        for idx, row in sample_data.iterrows():\n",
    "            print(f\"   Player {idx:>5} | Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n7Ô∏è‚É£  Removing unnecessary columns...\")\n",
    "        # Drop rating column - we only need rating_z for the model\n",
    "        player_side_info = player_side_info.drop(columns=['rating'])\n",
    "        print(f\"   ‚úì Dropped 'rating' column (only keeping 'rating_z')\")\n",
    "        print(f\"   ‚Ä¢ Final columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\n8Ô∏è‚É£  Verifying all clean_data players have ratings:\")\n",
    "        # This is important - make sure every player in clean_data has a rating\n",
    "        missing_players = set(clean_data['player_id'].unique()) - set(player_side_info.index)\n",
    "        if len(missing_players) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in clean_data are missing from side_info!\")\n",
    "            print(f\"   Missing player IDs: {sorted(list(missing_players))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(player_side_info):,} players in clean_data have side information\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ RATING NORMALIZATION COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nCreated: player_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: player_id\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ clean_data: {clean_data.shape[0]:,} rows (player-opening interactions)\")\n",
    "        print(f\"   ‚Ä¢ player_side_info: {len(player_side_info):,} rows (one per player)\")\n",
    "        print(f\"   ‚Ä¢ Rating storage: ONE value per player (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\")\n",
    "        print(f\"   RATING_MEAN = {RATING_MEAN:.2f}\")\n",
    "        print(f\"   RATING_STD = {RATING_STD:.2f}\")\n",
    "        print(f\"\\n   You'll need them to normalize ratings for new users at inference time.\")\n",
    "        \n",
    "        return player_side_info, RATING_MEAN, RATING_STD\n",
    "    \n",
    "    # Call the function\n",
    "    player_side_info, RATING_MEAN, RATING_STD = normalize_player_ratings(player_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14eba9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        player_id  opening_id  num_games     score  eco  confidence\n",
      "145404      28073         287         16  0.524285  A15    0.242424\n",
      "203831      39575         509        877  0.574809  A48    0.946063\n",
      "225653      44016         768         61  0.493575  B01    0.549550\n",
      "19670        3750        3190         19  0.492728  B50    0.275362\n",
      "143840      27731        3212        150  0.492735  B40    0.750000\n",
      "11842        2454        2069         16  0.467940  C51    0.242424\n",
      "234541      46022         349         11  0.554273  A31    0.180328\n",
      "176696      34413         671         10  0.528203  B00    0.166667\n",
      "238206      47055        1397         12  0.429408  C02    0.193548\n",
      "197269      38293         490         20  0.541486  A46    0.285714\n",
      "27924        5407        2019         19  0.555827  C47    0.275362\n",
      "203838      39575        3251         53  0.513160  A47    0.514563\n",
      "4822         1164        1767         29  0.469775  C37    0.367089\n",
      "202581      39390         910        228  0.500602  B10    0.820144\n",
      "174793      33822         430         11  0.544298  A41    0.180328\n",
      "162296      31338         510         27  0.514761  A48    0.350649\n",
      "227030      44206        2620         52  0.593609  D31    0.509804\n",
      "127416      24803        2520         26  0.466516  D10    0.342105\n",
      "155390      30293        2519        281  0.572920  D10    0.848943\n",
      "129846      25232          53         81  0.494713  A00    0.618321\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac119ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           rating_z\n",
      "player_id          \n",
      "7804      -1.887351\n",
      "34494      0.098274\n",
      "899        0.226898\n",
      "42048     -0.319752\n",
      "32255     -0.633272\n",
      "29040      0.150527\n",
      "19689     -1.537656\n",
      "39485     -0.138875\n",
      "37364      0.126410\n",
      "37964     -0.544843\n"
     ]
    }
   ],
   "source": [
    "print(player_side_info.sample(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30280db",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test/Val splits\n",
    "\n",
    "Here, I split my data and drop columns that are no longer needed. We're very close to being able to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf3849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: TRAIN/VALIDATION/TEST SPLIT\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ Train: 75%\n",
      "   ‚Ä¢ Validation: 15%\n",
      "   ‚Ä¢ Test: 10%\n",
      "   ‚Ä¢ Random seed: 42 (for reproducibility)\n",
      "\n",
      "1Ô∏è‚É£  Preparing data for split...\n",
      "   ‚Ä¢ Features (X): (246703, 4)\n",
      "   ‚Ä¢ Target (y): (246703,)\n",
      "   ‚Ä¢ Feature columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Cleaning player side information...\n",
      "   ‚Ä¢ Original player_side_info shape: (3994, 1)\n",
      "   ‚Ä¢ Cleaned player_side_info shape: (3994, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "3Ô∏è‚É£  Splitting data (optimized approach)...\n",
      "   ‚Ä¢ Train: 185,026 samples (75.0%)\n",
      "   ‚Ä¢ Validation: 37,006 samples (15.0%)\n",
      "   ‚Ä¢ Test: 24,671 samples (10.0%)\n",
      "\n",
      "4Ô∏è‚É£  Verification:\n",
      "   ‚Ä¢ Total samples: 246,703 (should equal 246,703)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "5Ô∏è‚É£  Computing coverage statistics (cached)...\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 3,989 unique players\n",
      "   ‚Ä¢ Val: 3,915 unique players\n",
      "   ‚Ä¢ Test: 3,841 unique players\n",
      "   ‚Ä¢ Total unique: 3,994 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,266 unique openings\n",
      "   ‚Ä¢ Val: 1,782 unique openings\n",
      "   ‚Ä¢ Test: 1,678 unique openings\n",
      "   ‚Ä¢ Total unique: 2,324 openings\n",
      "\n",
      "6Ô∏è‚É£  Cold start analysis (vectorized)...\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 2 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 33 (1.9%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 4 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 33 (2.0%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.5124\n",
      "   ‚Ä¢ Std: 0.0422\n",
      "   ‚Ä¢ Min: 0.1361\n",
      "   ‚Ä¢ Max: 0.9091\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.5121\n",
      "   ‚Ä¢ Std: 0.0417\n",
      "   ‚Ä¢ Min: 0.1906\n",
      "   ‚Ä¢ Max: 0.7326\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.5125\n",
      "   ‚Ä¢ Std: 0.0421\n",
      "   ‚Ä¢ Min: 0.2128\n",
      "   ‚Ä¢ Max: 0.8018\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.4130\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.4132\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.4128\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 185,026 samples (75%)\n",
      "   ‚Ä¢ Validation data: 37,006 samples (15%)\n",
      "   ‚Ä¢ Test data: 24,671 samples (10%)\n",
      "   ‚Ä¢ Player side info: 3,994 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n",
      "\n",
      "üì¶ Available datasets:\n",
      "   ‚Ä¢ X_train, y_train - Training features and targets\n",
      "   ‚Ä¢ X_val, y_val - Validation features and targets\n",
      "   ‚Ä¢ X_test, y_test - Test features and targets\n",
      "   ‚Ä¢ player_side_info_clean - Player ratings (indexed by player_id)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as categorical features\n",
      "   ‚Ä¢ Convert to PyTorch tensors\n",
      "   ‚Ä¢ Build matrix factorization model with side information\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train/Validation/Test Split (75/15/10) - OPTIMIZED\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: TRAIN/VALIDATION/TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   ‚Ä¢ Train: 75%\")\n",
    "print(f\"   ‚Ä¢ Validation: 15%\")\n",
    "print(f\"   ‚Ä¢ Test: 10%\")\n",
    "print(f\"   ‚Ä¢ Random seed: 42 (for reproducibility)\")\n",
    "\n",
    "# Prepare the data\n",
    "print(f\"\\n1Ô∏è‚É£  Preparing data for split...\")\n",
    "\n",
    "# Drop num_games from clean_data - we don't need it for training\n",
    "# Keep: player_id, opening_id, score, eco, confidence\n",
    "X = clean_data[[\"player_id\", \"opening_id\", \"eco\", \"confidence\"]].copy()\n",
    "y = clean_data[\"score\"].copy()\n",
    "\n",
    "print(f\"   ‚Ä¢ Features (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Target (y): {y.shape}\")\n",
    "print(f\"   ‚Ä¢ Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Clean up player_side_info - only keep rating_z\n",
    "print(f\"\\n2Ô∏è‚É£  Cleaning player side information...\")\n",
    "player_side_info_clean = player_side_info[[\"rating_z\"]].copy()\n",
    "print(f\"   ‚Ä¢ Original player_side_info shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Cleaned player_side_info shape: {player_side_info_clean.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info_clean.columns)}\")\n",
    "\n",
    "# OPTIMIZED: Use index-based splitting to avoid DataFrame copies\n",
    "print(f\"\\n3Ô∏è‚É£  Splitting data (optimized approach)...\")\n",
    "idx = np.arange(len(X))\n",
    "\n",
    "# First split: separate out test set (10%)\n",
    "idx_temp, idx_test = train_test_split(idx, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: split remaining into train (75%) and val (15%)\n",
    "# 15% of original = 15/90 ‚âà 0.1667 of temp\n",
    "idx_train, idx_val = train_test_split(idx_temp, test_size=15/90, random_state=42, shuffle=True)\n",
    "\n",
    "# Create splits using iloc (view, not copy)\n",
    "X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
    "X_test, y_test = X.iloc[idx_test], y.iloc[idx_test]\n",
    "\n",
    "print(f\"   ‚Ä¢ Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Validation: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split\n",
    "print(f\"\\n4Ô∏è‚É£  Verification:\")\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"   ‚Ä¢ Total samples: {total:,} (should equal {len(X):,})\")\n",
    "print(f\"   ‚Ä¢ Train %: {len(X_train)/total*100:.2f}% (target: 75%)\")\n",
    "print(f\"   ‚Ä¢ Val %: {len(X_val)/total*100:.2f}% (target: 15%)\")\n",
    "print(f\"   ‚Ä¢ Test %: {len(X_test)/total*100:.2f}% (target: 10%)\")\n",
    "\n",
    "# OPTIMIZED: Pre-compute unique arrays once\n",
    "print(f\"\\n5Ô∏è‚É£  Computing coverage statistics (cached)...\")\n",
    "players_train = X_train[\"player_id\"].unique()\n",
    "players_val = X_val[\"player_id\"].unique()\n",
    "players_test = X_test[\"player_id\"].unique()\n",
    "\n",
    "openings_train = X_train[\"opening_id\"].unique()\n",
    "openings_val = X_val[\"opening_id\"].unique()\n",
    "openings_test = X_test[\"opening_id\"].unique()\n",
    "\n",
    "print(f\"\\n   Players:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(players_train):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Val: {len(players_val):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Test: {len(players_test):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['player_id'].nunique():,} players\")\n",
    "\n",
    "print(f\"\\n   Openings:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(openings_train):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Val: {len(openings_val):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Test: {len(openings_test):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['opening_id'].nunique():,} openings\")\n",
    "\n",
    "# OPTIMIZED: Use NumPy setdiff1d for cold-start analysis (C-speed)\n",
    "print(f\"\\n6Ô∏è‚É£  Cold start analysis (vectorized)...\")\n",
    "\n",
    "val_cold_players = np.setdiff1d(players_val, players_train, assume_unique=True)\n",
    "val_cold_openings = np.setdiff1d(openings_val, openings_train, assume_unique=True)\n",
    "\n",
    "test_cold_players = np.setdiff1d(players_test, players_train, assume_unique=True)\n",
    "test_cold_openings = np.setdiff1d(openings_test, openings_train, assume_unique=True)\n",
    "\n",
    "print(f\"\\n   Validation set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(val_cold_players):,} ({len(val_cold_players)/len(players_val)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(val_cold_openings):,} ({len(val_cold_openings)/len(openings_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(test_cold_players):,} ({len(test_cold_players)/len(players_test)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(test_cold_openings):,} ({len(test_cold_openings)/len(openings_test)*100:.1f}%)\")\n",
    "\n",
    "# OPTIMIZED: Compute stats in one pass using describe()\n",
    "print(f\"\\n7Ô∏è‚É£  Score distribution across splits:\")\n",
    "\n",
    "y_train_stats = y_train.describe()\n",
    "y_val_stats = y_val.describe()\n",
    "y_test_stats = y_test.describe()\n",
    "\n",
    "print(f\"\\n   Train:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_train_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_train_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_train_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_val_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_val_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_val_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_test_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_test_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_test_stats['max']:.4f}\")\n",
    "\n",
    "# OPTIMIZED: Compute confidence stats in one pass\n",
    "print(f\"\\n8Ô∏è‚É£  Confidence distribution across splits:\")\n",
    "\n",
    "conf_train_stats = X_train['confidence'].describe()\n",
    "conf_val_stats = X_val['confidence'].describe()\n",
    "conf_test_stats = X_test['confidence'].describe()\n",
    "\n",
    "print(f\"\\n   Train:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_train_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_val_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_test_stats['50%']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SPLIT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   ‚Ä¢ Training data: {len(X_train):,} samples (75%)\")\n",
    "print(f\"   ‚Ä¢ Validation data: {len(X_val):,} samples (15%)\")\n",
    "print(f\"   ‚Ä¢ Test data: {len(X_test):,} samples (10%)\")\n",
    "print(f\"   ‚Ä¢ Player side info: {len(player_side_info_clean):,} players\")\n",
    "print(f\"   ‚Ä¢ Side info columns: {list(player_side_info_clean.columns)}\")\n",
    "\n",
    "print(f\"\\nüì¶ Available datasets:\")\n",
    "print(f\"   ‚Ä¢ X_train, y_train - Training features and targets\")\n",
    "print(f\"   ‚Ä¢ X_val, y_val - Validation features and targets\")\n",
    "print(f\"   ‚Ä¢ X_test, y_test - Test features and targets\")\n",
    "print(f\"   ‚Ä¢ player_side_info_clean - Player ratings (indexed by player_id)\")\n",
    "\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"   ‚Ä¢ Enumerate ECO codes as categorical features\")\n",
    "print(f\"   ‚Ä¢ Convert to PyTorch tensors\")\n",
    "print(f\"   ‚Ä¢ Build matrix factorization model with side information\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781a382",
   "metadata": {},
   "source": [
    "## Step 3b: Remap Player and Opening IDs to Sequential Integers\n",
    "\n",
    "**Why remap IDs?**\n",
    "- Database IDs may have gaps (e.g., [1, 5, 10, 15, ...]) from deleted entries\n",
    "- Embedding layers need 0-based contiguous indices for efficiency\n",
    "- Remapping saves memory (no unused embedding slots)\n",
    "\n",
    "**Process:**\n",
    "1. Check if IDs are already sequential (0 or 1-based with no gaps)\n",
    "2. If not, create mappings: old_id ‚Üí new_sequential_id\n",
    "3. Remap all DataFrames and side info tables\n",
    "4. Verify mappings with spot checks\n",
    "\n",
    "This ensures embeddings use minimal memory and indices align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b4c409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Processing player IDs...\n",
      "\n",
      "============================================================\n",
      "Checking player IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique players: 3994\n",
      "   ‚Ä¢ ID range: [43, 49977]\n",
      "   ‚ö†Ô∏è  player IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 45941\n",
      "   ‚Ä¢ Wasted embedding slots without remapping: 45941\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      player_id 43 ‚Üí 0\n",
      "      player_id 108 ‚Üí 1\n",
      "      player_id 109 ‚Üí 2\n",
      "      player_id 125 ‚Üí 3\n",
      "      player_id 126 ‚Üí 4\n",
      "      player_id 49893 ‚Üí 3989\n",
      "      player_id 49895 ‚Üí 3990\n",
      "      player_id 49918 ‚Üí 3991\n",
      "      player_id 49972 ‚Üí 3992\n",
      "      player_id 49977 ‚Üí 3993\n",
      "\n",
      "   Remapping 5 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "\n",
      "============================================================\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2324\n",
      "   ‚Ä¢ ID range: [2, 3575]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1250\n",
      "   ‚Ä¢ Wasted embedding slots without remapping: 1250\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 4 ‚Üí 1\n",
      "      opening_id 5 ‚Üí 2\n",
      "      opening_id 9 ‚Üí 3\n",
      "      opening_id 10 ‚Üí 4\n",
      "      opening_id 3552 ‚Üí 2319\n",
      "      opening_id 3554 ‚Üí 2320\n",
      "      opening_id 3564 ‚Üí 2321\n",
      "      opening_id 3572 ‚Üí 2322\n",
      "      opening_id 3575 ‚Üí 2323\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/4\n",
      "   ‚úì Remapped DataFrame 2/4\n",
      "   ‚úì Remapped DataFrame 3/4\n",
      "   ‚úì Remapped DataFrame 4/4\n",
      "\n",
      "   ‚úÖ Opening ID remapping complete!\n",
      "\n",
      "3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\n",
      "   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\n",
      "\n",
      "   Sampling 10 entries from X_train (before remapping was applied)...\n",
      "   ‚Ä¢ Sample indices: [0, 20558, 41116, 61674, 82232, 102790, 123348, 143906, 164464, 185025]\n",
      "   ‚Ä¢ These represent: first, evenly spaced middle rows, and last\n",
      "\n",
      "   Verification checks:\n",
      "   #    Row Idx    New Player   New Opening  Confidence   Status         \n",
      "   ---- ---------- ------------ ------------ ------------ ---------------\n",
      "   1    0          1759         2095         0.1803       ‚úì PASS         \n",
      "   2    20558      2673         509          0.1935       ‚úì PASS         \n",
      "   3    41116      1212         359          0.7312       ‚úì PASS         \n",
      "   4    61674      3421         637          0.8024       ‚úì PASS         \n",
      "   5    82232      2712         547          0.1803       ‚úì PASS         \n",
      "   6    102790     2936         1017         0.3976       ‚úì PASS         \n",
      "   7    123348     1266         533          0.6454       ‚úì PASS         \n",
      "   8    143906     640          163          0.4565       ‚úì PASS         \n",
      "   9    164464     2893         284          0.2063       ‚úì PASS         \n",
      "   10   185025     2322         206          0.7727       ‚úì PASS         \n",
      "\n",
      "   Reverse mapping verification (sample of 3 entries):\n",
      "   #    New‚ÜíOld Player                 New‚ÜíOld Opening               \n",
      "   ---- ------------------------------ ------------------------------\n",
      "   1    1759 ‚Üí 21270                   2095 ‚Üí 3180                   \n",
      "   6    2936 ‚Üí 35957                   1017 ‚Üí 1479                   \n",
      "   10   2322 ‚Üí 28096                   206 ‚Üí 300                     \n",
      "\n",
      "   ‚úÖ All spot checks passed! ID mappings are correct.\n",
      "\n",
      "4Ô∏è‚É£  Summary:\n",
      "\n",
      "   Player IDs:\n",
      "   ‚Ä¢ Original range: [43, 49977]\n",
      "   ‚Ä¢ New range: [0, 3993]\n",
      "   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\n",
      "\n",
      "   Opening IDs:\n",
      "   ‚Ä¢ Original range: [2, 3575]\n",
      "   ‚Ä¢ New range: [0, 2323]\n",
      "   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\n",
      "\n",
      "   Updated DataFrames:\n",
      "   ‚Ä¢ X_train: (185026, 4)\n",
      "   ‚Ä¢ X_val: (37006, 4)\n",
      "   ‚Ä¢ X_test: (24671, 4)\n",
      "   ‚Ä¢ clean_data: (246703, 6)\n",
      "   ‚Ä¢ player_side_info: (3994, 1)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ID REMAPPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Important:\n",
      "   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\n",
      "   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\n",
      "   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\n",
      "   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "\n",
      "============================================================\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2324\n",
      "   ‚Ä¢ ID range: [2, 3575]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1250\n",
      "   ‚Ä¢ Wasted embedding slots without remapping: 1250\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 4 ‚Üí 1\n",
      "      opening_id 5 ‚Üí 2\n",
      "      opening_id 9 ‚Üí 3\n",
      "      opening_id 10 ‚Üí 4\n",
      "      opening_id 3552 ‚Üí 2319\n",
      "      opening_id 3554 ‚Üí 2320\n",
      "      opening_id 3564 ‚Üí 2321\n",
      "      opening_id 3572 ‚Üí 2322\n",
      "      opening_id 3575 ‚Üí 2323\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/4\n",
      "   ‚úì Remapped DataFrame 2/4\n",
      "   ‚úì Remapped DataFrame 3/4\n",
      "   ‚úì Remapped DataFrame 4/4\n",
      "\n",
      "   ‚úÖ Opening ID remapping complete!\n",
      "\n",
      "3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\n",
      "   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\n",
      "\n",
      "   Sampling 10 entries from X_train (before remapping was applied)...\n",
      "   ‚Ä¢ Sample indices: [0, 20558, 41116, 61674, 82232, 102790, 123348, 143906, 164464, 185025]\n",
      "   ‚Ä¢ These represent: first, evenly spaced middle rows, and last\n",
      "\n",
      "   Verification checks:\n",
      "   #    Row Idx    New Player   New Opening  Confidence   Status         \n",
      "   ---- ---------- ------------ ------------ ------------ ---------------\n",
      "   1    0          1759         2095         0.1803       ‚úì PASS         \n",
      "   2    20558      2673         509          0.1935       ‚úì PASS         \n",
      "   3    41116      1212         359          0.7312       ‚úì PASS         \n",
      "   4    61674      3421         637          0.8024       ‚úì PASS         \n",
      "   5    82232      2712         547          0.1803       ‚úì PASS         \n",
      "   6    102790     2936         1017         0.3976       ‚úì PASS         \n",
      "   7    123348     1266         533          0.6454       ‚úì PASS         \n",
      "   8    143906     640          163          0.4565       ‚úì PASS         \n",
      "   9    164464     2893         284          0.2063       ‚úì PASS         \n",
      "   10   185025     2322         206          0.7727       ‚úì PASS         \n",
      "\n",
      "   Reverse mapping verification (sample of 3 entries):\n",
      "   #    New‚ÜíOld Player                 New‚ÜíOld Opening               \n",
      "   ---- ------------------------------ ------------------------------\n",
      "   1    1759 ‚Üí 21270                   2095 ‚Üí 3180                   \n",
      "   6    2936 ‚Üí 35957                   1017 ‚Üí 1479                   \n",
      "   10   2322 ‚Üí 28096                   206 ‚Üí 300                     \n",
      "\n",
      "   ‚úÖ All spot checks passed! ID mappings are correct.\n",
      "\n",
      "4Ô∏è‚É£  Summary:\n",
      "\n",
      "   Player IDs:\n",
      "   ‚Ä¢ Original range: [43, 49977]\n",
      "   ‚Ä¢ New range: [0, 3993]\n",
      "   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\n",
      "\n",
      "   Opening IDs:\n",
      "   ‚Ä¢ Original range: [2, 3575]\n",
      "   ‚Ä¢ New range: [0, 2323]\n",
      "   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\n",
      "\n",
      "   Updated DataFrames:\n",
      "   ‚Ä¢ X_train: (185026, 4)\n",
      "   ‚Ä¢ X_val: (37006, 4)\n",
      "   ‚Ä¢ X_test: (24671, 4)\n",
      "   ‚Ä¢ clean_data: (246703, 6)\n",
      "   ‚Ä¢ player_side_info: (3994, 1)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ID REMAPPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Important:\n",
      "   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\n",
      "   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\n",
      "   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\n",
      "   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\n"
     ]
    }
   ],
   "source": [
    "# Step 3b: Remap player and opening IDs to 0-based sequential integers\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def check_and_remap_ids(df_list, id_column, entity_name):\n",
    "    \"\"\"\n",
    "    Check if IDs are sequential starting from 0, and remap if not.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of DataFrames\n",
    "        List of DataFrames containing the ID column to check/remap\n",
    "    id_column : str\n",
    "        Name of the ID column ('player_id' or 'opening_id')\n",
    "    entity_name : str\n",
    "        Name for logging ('player' or 'opening')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (df_list with remapped IDs, id_to_idx mapping dict, needs_remapping bool)\n",
    "    \"\"\"\n",
    "    # Get all unique IDs across all dataframes\n",
    "    all_ids = pd.concat([df[id_column] for df in df_list]).unique()\n",
    "    all_ids_sorted = sorted(all_ids)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Checking {entity_name} IDs...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ‚Ä¢ Total unique {entity_name}s: {len(all_ids_sorted)}\")\n",
    "    print(f\"   ‚Ä¢ ID range: [{all_ids_sorted[0]}, {all_ids_sorted[-1]}]\")\n",
    "    \n",
    "    # Check if IDs are already 0-based sequential (0, 1, 2, ...)\n",
    "    expected_sequential = list(range(len(all_ids_sorted)))\n",
    "    is_sequential = (all_ids_sorted == expected_sequential)\n",
    "    \n",
    "    if is_sequential:\n",
    "        print(f\"   ‚úì {entity_name} IDs are already 0-based sequential - no remapping needed!\")\n",
    "        return df_list, None, False\n",
    "    \n",
    "    # Check if IDs are 1-based sequential (1, 2, 3, ...)\n",
    "    expected_sequential_1based = list(range(1, len(all_ids_sorted) + 1))\n",
    "    is_sequential_1based = (all_ids_sorted == expected_sequential_1based)\n",
    "    \n",
    "    if is_sequential_1based:\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs are 1-based sequential - will remap to 0-based\")\n",
    "    else:\n",
    "        # Calculate gaps\n",
    "        num_gaps = (all_ids_sorted[-1] - all_ids_sorted[0] + 1) - len(all_ids_sorted)\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs have gaps - will remap to 0-based sequential\")\n",
    "        print(f\"   ‚Ä¢ Number of gaps: {num_gaps}\")\n",
    "        print(f\"   ‚Ä¢ Wasted embedding slots without remapping: {num_gaps}\")\n",
    "    \n",
    "    # Create mapping: old_id -> new_idx (0-based)\n",
    "    id_to_idx = {old_id: new_idx for new_idx, old_id in enumerate(all_ids_sorted)}\n",
    "    idx_to_id = {new_idx: old_id for old_id, new_idx in id_to_idx.items()}\n",
    "    \n",
    "    print(f\"\\n   Creating mapping...\")\n",
    "    print(f\"   ‚Ä¢ Example mappings:\")\n",
    "    sample_ids = all_ids_sorted[:5] + all_ids_sorted[-5:]\n",
    "    for old_id in sample_ids[:10]:  # Show first 5 and last 5\n",
    "        print(f\"      {entity_name}_id {old_id} ‚Üí {id_to_idx[old_id]}\")\n",
    "    \n",
    "    # Remap all DataFrames\n",
    "    print(f\"\\n   Remapping {len(df_list)} DataFrames...\")\n",
    "    remapped_dfs = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[id_column] = df_copy[id_column].map(id_to_idx)\n",
    "        remapped_dfs.append(df_copy)\n",
    "        print(f\"   ‚úì Remapped DataFrame {i+1}/{len(df_list)}\")\n",
    "    \n",
    "    return remapped_dfs, (id_to_idx, idx_to_id), True\n",
    "\n",
    "# 1. Remap player IDs\n",
    "print(f\"\\n1Ô∏è‚É£  Processing player IDs...\")\n",
    "player_dfs = [X_train, X_val, X_test, clean_data, player_side_info.reset_index()]\n",
    "remapped_player_dfs, player_mappings, player_remapped = check_and_remap_ids(\n",
    "    player_dfs, 'player_id', 'player'\n",
    ")\n",
    "\n",
    "if player_remapped:\n",
    "    X_train, X_val, X_test, clean_data, player_side_info_remapped = remapped_player_dfs\n",
    "    player_id_to_idx, player_idx_to_id = player_mappings\n",
    "    player_side_info = player_side_info_remapped.set_index('player_id')\n",
    "    print(f\"\\n   ‚úÖ Player ID remapping complete!\")\n",
    "else:\n",
    "    player_id_to_idx, player_idx_to_id = None, None\n",
    "\n",
    "# 2. Remap opening IDs\n",
    "print(f\"\\n2Ô∏è‚É£  Processing opening IDs...\")\n",
    "opening_dfs = [X_train, X_val, X_test, clean_data]\n",
    "remapped_opening_dfs, opening_mappings, opening_remapped = check_and_remap_ids(\n",
    "    opening_dfs, 'opening_id', 'opening'\n",
    ")\n",
    "\n",
    "if opening_remapped:\n",
    "    X_train, X_val, X_test, clean_data = remapped_opening_dfs\n",
    "    opening_id_to_idx, opening_idx_to_id = opening_mappings\n",
    "    print(f\"\\n   ‚úÖ Opening ID remapping complete!\")\n",
    "else:\n",
    "    opening_id_to_idx, opening_idx_to_id = None, None\n",
    "\n",
    "# 3. Spot checks to verify mappings\n",
    "print(f\"\\n3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\")\n",
    "print(f\"   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\")\n",
    "\n",
    "# Sample 10 entries: first, last, and 8 in between\n",
    "print(f\"\\n   Sampling 10 entries from X_train (before remapping was applied)...\")\n",
    "total_rows = len(X_train)\n",
    "# Get indices: first, last, and 8 evenly spaced in between\n",
    "sample_indices = [0]  # First row\n",
    "step = (total_rows - 1) // 9  # Divide remaining rows into 9 parts\n",
    "sample_indices.extend([min(i * step, total_rows - 1) for i in range(1, 9)])\n",
    "sample_indices.append(total_rows - 1)  # Last row\n",
    "\n",
    "print(f\"   ‚Ä¢ Sample indices: {sample_indices}\")\n",
    "print(f\"   ‚Ä¢ These represent: first, evenly spaced middle rows, and last\")\n",
    "\n",
    "# Store samples with their NEW (remapped) IDs\n",
    "samples = []\n",
    "for idx in sample_indices:\n",
    "    row = X_train.iloc[idx]\n",
    "    samples.append({\n",
    "        'index': idx,\n",
    "        'new_player_id': row['player_id'],\n",
    "        'new_opening_id': row['opening_id'],\n",
    "        'confidence': row['confidence']\n",
    "    })\n",
    "\n",
    "print(f\"\\n   Verification checks:\")\n",
    "print(f\"   {'#':<4} {'Row Idx':<10} {'New Player':<12} {'New Opening':<12} {'Confidence':<12} {'Status':<15}\")\n",
    "print(f\"   {'-'*4} {'-'*10} {'-'*12} {'-'*12} {'-'*12} {'-'*15}\")\n",
    "\n",
    "all_checks_passed = True\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    new_player_id = sample['new_player_id']\n",
    "    new_opening_id = sample['new_opening_id']\n",
    "    confidence = sample['confidence']\n",
    "    idx = sample['index']\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: New player ID is valid (0-based sequential)\n",
    "    if player_remapped:\n",
    "        player_valid = 0 <= new_player_id < len(player_id_to_idx)\n",
    "        checks.append((\"player_id\", player_valid))\n",
    "    else:\n",
    "        checks.append((\"player_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 2: New opening ID is valid (0-based sequential)\n",
    "    if opening_remapped:\n",
    "        opening_valid = 0 <= new_opening_id < len(opening_id_to_idx)\n",
    "        checks.append((\"opening_id\", opening_valid))\n",
    "    else:\n",
    "        checks.append((\"opening_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 3: Player exists in player_side_info\n",
    "    player_exists = new_player_id in player_side_info.index\n",
    "    checks.append((\"in_side_info\", player_exists))\n",
    "    \n",
    "    # Check 4: Opening exists in clean_data\n",
    "    opening_exists = new_opening_id in clean_data['opening_id'].values\n",
    "    checks.append((\"in_clean\", opening_exists))\n",
    "    \n",
    "    # All checks must pass\n",
    "    all_pass = all(check[1] for check in checks)\n",
    "    status = \"‚úì PASS\" if all_pass else f\"‚úó FAIL ({','.join([c[0] for c in checks if not c[1]])})\"\n",
    "    \n",
    "    if not all_pass:\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(f\"   {i:<4} {idx:<10} {new_player_id:<12} {new_opening_id:<12} {confidence:<12.4f} {status:<15}\")\n",
    "\n",
    "# Additional verification: check if we can reverse map\n",
    "if player_remapped or opening_remapped:\n",
    "    print(f\"\\n   Reverse mapping verification (sample of 3 entries):\")\n",
    "    print(f\"   {'#':<4} {'New‚ÜíOld Player':<30} {'New‚ÜíOld Opening':<30}\")\n",
    "    print(f\"   {'-'*4} {'-'*30} {'-'*30}\")\n",
    "    \n",
    "    for i in [0, len(samples)//2, len(samples)-1]:  # First, middle, last\n",
    "        sample = samples[i]\n",
    "        \n",
    "        # Reverse map player\n",
    "        if player_remapped:\n",
    "            old_player = player_idx_to_id.get(sample['new_player_id'], 'NOT_FOUND')\n",
    "            player_str = f\"{sample['new_player_id']} ‚Üí {old_player}\"\n",
    "        else:\n",
    "            player_str = f\"{sample['new_player_id']} (unchanged)\"\n",
    "        \n",
    "        # Reverse map opening\n",
    "        if opening_remapped:\n",
    "            old_opening = opening_idx_to_id.get(sample['new_opening_id'], 'NOT_FOUND')\n",
    "            opening_str = f\"{sample['new_opening_id']} ‚Üí {old_opening}\"\n",
    "        else:\n",
    "            opening_str = f\"{sample['new_opening_id']} (unchanged)\"\n",
    "        \n",
    "        print(f\"   {i+1:<4} {player_str:<30} {opening_str:<30}\")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(f\"\\n   ‚úÖ All spot checks passed! ID mappings are correct.\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Some spot checks failed - investigate immediately!\")\n",
    "    raise ValueError(\"ID remapping verification failed!\")\n",
    "\n",
    "# 4. Summary\n",
    "print(f\"\\n4Ô∏è‚É£  Summary:\")\n",
    "print(f\"\\n   Player IDs:\")\n",
    "if player_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{player_idx_to_id[0]}, {player_idx_to_id[len(player_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(player_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "    \n",
    "print(f\"\\n   Opening IDs:\")\n",
    "if opening_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{opening_idx_to_id[0]}, {opening_idx_to_id[len(opening_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(opening_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "\n",
    "print(f\"\\n   Updated DataFrames:\")\n",
    "print(f\"   ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ X_val: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ clean_data: {clean_data.shape}\")\n",
    "print(f\"   ‚Ä¢ player_side_info: {player_side_info.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ID REMAPPING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Important:\")\n",
    "print(f\"   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\")\n",
    "print(f\"   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\")\n",
    "print(f\"   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\")\n",
    "print(f\"   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a580c0",
   "metadata": {},
   "source": [
    "## 4. Enumerate Categorical Variables\n",
    "\n",
    "I believe the only variable we need to enumerate here is `eco`. That's the broad categorization of a specific opening.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- One ECO code will have many openings\n",
    "- They're sorted by letter, then further by number. For instance, C21 and C44 are in the `C` family.\n",
    "- Maybe we make this side information?\n",
    "\n",
    "First, let's get some data on ECO codes to help us better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9504a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: ECO CODE STATISTICS\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Overall ECO statistics:\n",
      "   ‚Ä¢ Total unique ECO codes: 445\n",
      "   ‚Ä¢ Total entries: 246,703\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 554.4\n",
      "   ‚Ä¢ Median entries per ECO: 91.0\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 15803\n",
      "   ‚Ä¢ Std: 1517.6\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 554.4\n",
      "   ‚Ä¢ Median entries per ECO: 91.0\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 15803\n",
      "   ‚Ä¢ Std: 1517.6\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A         48,919     19.83%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B         84,136     34.10%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C         78,682     31.89%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D         27,494     11.14%      ‚ñà‚ñà‚ñà‚ñà\n",
      "   E          7,472      3.03%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00     15,803      6.41%      ‚ñà\n",
      "   2      B01     15,404      6.24%      ‚ñà\n",
      "   3      A40     11,974      4.85%      ‚ñà\n",
      "   4      C50      7,792      3.16%      \n",
      "   5      C00      7,194      2.92%      \n",
      "   6      C40      6,331      2.57%      \n",
      "   7      B06      5,986      2.43%      \n",
      "   8      C42      5,157      2.09%      \n",
      "   9      C44      4,923      2.00%      \n",
      "   10     B10      4,588      1.86%      \n",
      "   11     A00      4,556      1.85%      \n",
      "   12     C41      4,471      1.81%      \n",
      "   13     B40      4,298      1.74%      \n",
      "   14     B12      3,985      1.62%      \n",
      "   15     D00      3,895      1.58%      \n",
      "   16     C02      3,855      1.56%      \n",
      "   17     A43      3,291      1.33%      \n",
      "   18     A04      3,162      1.28%      \n",
      "   19     B21      3,160      1.28%      \n",
      "   20     B32      2,729      1.11%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      D68          1    ‚ñà\n",
      "   2      D62          1    ‚ñà\n",
      "   3      B62          1    ‚ñà\n",
      "   4      C83          1    ‚ñà\n",
      "   5      A99          1    ‚ñà\n",
      "   6      A72          1    ‚ñà\n",
      "   7      B58          1    ‚ñà\n",
      "   8      A97          1    ‚ñà\n",
      "   9      D48          1    ‚ñà\n",
      "   10     C81          1    ‚ñà\n",
      "   11     C94          1    ‚ñà\n",
      "   12     E79          1    ‚ñà\n",
      "   13     E31          1    ‚ñà\n",
      "   14     D49          1    ‚ñà\n",
      "   15     D81          1    ‚ñà\n",
      "   16     E89          1    ‚ñà\n",
      "   17     E58          1    ‚ñà\n",
      "   18     A64          1    ‚ñà\n",
      "   19     E78          1    ‚ñà\n",
      "   20     E45          1    ‚ñà\n",
      "\n",
      "6Ô∏è‚É£  ECO code format analysis:\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A         48,919     19.83%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B         84,136     34.10%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C         78,682     31.89%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D         27,494     11.14%      ‚ñà‚ñà‚ñà‚ñà\n",
      "   E          7,472      3.03%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00     15,803      6.41%      ‚ñà\n",
      "   2      B01     15,404      6.24%      ‚ñà\n",
      "   3      A40     11,974      4.85%      ‚ñà\n",
      "   4      C50      7,792      3.16%      \n",
      "   5      C00      7,194      2.92%      \n",
      "   6      C40      6,331      2.57%      \n",
      "   7      B06      5,986      2.43%      \n",
      "   8      C42      5,157      2.09%      \n",
      "   9      C44      4,923      2.00%      \n",
      "   10     B10      4,588      1.86%      \n",
      "   11     A00      4,556      1.85%      \n",
      "   12     C41      4,471      1.81%      \n",
      "   13     B40      4,298      1.74%      \n",
      "   14     B12      3,985      1.62%      \n",
      "   15     D00      3,895      1.58%      \n",
      "   16     C02      3,855      1.56%      \n",
      "   17     A43      3,291      1.33%      \n",
      "   18     A04      3,162      1.28%      \n",
      "   19     B21      3,160      1.28%      \n",
      "   20     B32      2,729      1.11%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      D68          1    ‚ñà\n",
      "   2      D62          1    ‚ñà\n",
      "   3      B62          1    ‚ñà\n",
      "   4      C83          1    ‚ñà\n",
      "   5      A99          1    ‚ñà\n",
      "   6      A72          1    ‚ñà\n",
      "   7      B58          1    ‚ñà\n",
      "   8      A97          1    ‚ñà\n",
      "   9      D48          1    ‚ñà\n",
      "   10     C81          1    ‚ñà\n",
      "   11     C94          1    ‚ñà\n",
      "   12     E79          1    ‚ñà\n",
      "   13     E31          1    ‚ñà\n",
      "   14     D49          1    ‚ñà\n",
      "   15     D81          1    ‚ñà\n",
      "   16     E89          1    ‚ñà\n",
      "   17     E58          1    ‚ñà\n",
      "   18     A64          1    ‚ñà\n",
      "   19     E78          1    ‚ñà\n",
      "   20     E45          1    ‚ñà\n",
      "\n",
      "6Ô∏è‚É£  ECO code format analysis:\n",
      "   ‚Ä¢ ECO code lengths:\n",
      "      3 characters: 246,703 (100.00%)\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   B50, B63, B72, B87, C43, C46, C87, C91, C96, C97, D04, D06, D31, D59, D95, E47, E67, E69, E74, E90\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   ‚Ä¢ ECO code lengths:\n",
      "      3 characters: 246,703 (100.00%)\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   B50, B63, B72, B87, C43, C46, C87, C91, C96, C97, D04, D06, D31, D59, D95, E47, E67, E69, E74, E90\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   ‚Ä¢ Unique ECO codes: 439\n",
      "   ‚Ä¢ Total entries: 185,026\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 399\n",
      "   ‚Ä¢ Total entries: 37,006\n",
      "   ‚Ä¢ ECO codes not in train: 1\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 387\n",
      "   ‚Ä¢ Total entries: 24,671\n",
      "   ‚Ä¢ ECO codes not in train: 5\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "   ‚Ä¢ Unique ECO codes: 439\n",
      "   ‚Ä¢ Total entries: 185,026\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 399\n",
      "   ‚Ä¢ Total entries: 37,006\n",
      "   ‚Ä¢ ECO codes not in train: 1\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 387\n",
      "   ‚Ä¢ Total entries: 24,671\n",
      "   ‚Ä¢ ECO codes not in train: 5\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   E45    0.7500             1\n",
      "   E31    0.7368             1\n",
      "   C81    0.7143             1\n",
      "   A75    0.6470             2\n",
      "   B71    0.6346            17\n",
      "   D48    0.6250             1\n",
      "   C94    0.6250             1\n",
      "   E23    0.6244             5\n",
      "   A76    0.6218             8\n",
      "   E99    0.6171             4\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   B62    0.4286             1\n",
      "   D28    0.4224             3\n",
      "   E89    0.4167             1\n",
      "   D68    0.4091             1\n",
      "   E58    0.3929             1\n",
      "   E78    0.3636             1\n",
      "   D62    0.3636             1\n",
      "   A64    0.3571             1\n",
      "   A99    0.2083             1\n",
      "   B58    0.1923             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   E99    0.0211       0.1454             4\n",
      "   C91    0.0150       0.1226            11\n",
      "   C14    0.0115       0.1073            14\n",
      "   D83    0.0082       0.0905             9\n",
      "   E54    0.0074       0.0860             6\n",
      "   C39    0.0069       0.0831           107\n",
      "   C12    0.0067       0.0819            53\n",
      "   B55    0.0061       0.0779             3\n",
      "   E23    0.0057       0.0755             5\n",
      "   C57    0.0054       0.0735           836\n",
      "\n",
      "1Ô∏è‚É£1Ô∏è‚É£  Openings per ECO code:\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   E45    0.7500             1\n",
      "   E31    0.7368             1\n",
      "   C81    0.7143             1\n",
      "   A75    0.6470             2\n",
      "   B71    0.6346            17\n",
      "   D48    0.6250             1\n",
      "   C94    0.6250             1\n",
      "   E23    0.6244             5\n",
      "   A76    0.6218             8\n",
      "   E99    0.6171             4\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   B62    0.4286             1\n",
      "   D28    0.4224             3\n",
      "   E89    0.4167             1\n",
      "   D68    0.4091             1\n",
      "   E58    0.3929             1\n",
      "   E78    0.3636             1\n",
      "   D62    0.3636             1\n",
      "   A64    0.3571             1\n",
      "   A99    0.2083             1\n",
      "   B58    0.1923             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   E99    0.0211       0.1454             4\n",
      "   C91    0.0150       0.1226            11\n",
      "   C14    0.0115       0.1073            14\n",
      "   D83    0.0082       0.0905             9\n",
      "   E54    0.0074       0.0860             6\n",
      "   C39    0.0069       0.0831           107\n",
      "   C12    0.0067       0.0819            53\n",
      "   B55    0.0061       0.0779             3\n",
      "   E23    0.0057       0.0755             5\n",
      "   C57    0.0054       0.0735           836\n",
      "\n",
      "1Ô∏è‚É£1Ô∏è‚É£  Openings per ECO code:\n",
      "   ‚Ä¢ Mean openings per ECO: 6.7\n",
      "   ‚Ä¢ Median openings per ECO: 3.0\n",
      "   ‚Ä¢ Max openings per ECO: 107\n",
      "   ‚Ä¢ Min openings per ECO: 1\n",
      "\n",
      "   Top 10 ECO codes by number of openings:\n",
      "\n",
      "   ECO    # Openings  \n",
      "   ------ ------------\n",
      "   A00           107\n",
      "   B00            87\n",
      "   D00            86\n",
      "   C42            59\n",
      "   A40            55\n",
      "   B01            45\n",
      "   C44            45\n",
      "   C00            42\n",
      "   B06            40\n",
      "   C33            38\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 445\n",
      "   ‚Ä¢ Most common family: B (84,136 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (15,803 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as integers for categorical encoding\n",
      "   ‚Ä¢ Consider ECO as opening-level side information (similar to player ratings)\n",
      "   ‚Ä¢ Verify all ECO codes in validation/test exist in training set\n",
      "   ‚Ä¢ Mean openings per ECO: 6.7\n",
      "   ‚Ä¢ Median openings per ECO: 3.0\n",
      "   ‚Ä¢ Max openings per ECO: 107\n",
      "   ‚Ä¢ Min openings per ECO: 1\n",
      "\n",
      "   Top 10 ECO codes by number of openings:\n",
      "\n",
      "   ECO    # Openings  \n",
      "   ------ ------------\n",
      "   A00           107\n",
      "   B00            87\n",
      "   D00            86\n",
      "   C42            59\n",
      "   A40            55\n",
      "   B01            45\n",
      "   C44            45\n",
      "   C00            42\n",
      "   B06            40\n",
      "   C33            38\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 445\n",
      "   ‚Ä¢ Most common family: B (84,136 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (15,803 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as integers for categorical encoding\n",
      "   ‚Ä¢ Consider ECO as opening-level side information (similar to player ratings)\n",
      "   ‚Ä¢ Verify all ECO codes in validation/test exist in training set\n"
     ]
    }
   ],
   "source": [
    "# Step 4: ECO Code Statistics (no mutations, just exploration)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: ECO CODE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic ECO statistics across all data\n",
    "print(f\"\\n1Ô∏è‚É£  Overall ECO statistics:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Missing ECO values: {clean_data['eco'].isna().sum()}\")\n",
    "\n",
    "# ECO value counts\n",
    "eco_counts = clean_data['eco'].value_counts().sort_index()\n",
    "print(f\"\\n2Ô∏è‚É£  Distribution of entries per ECO code:\")\n",
    "print(f\"   ‚Ä¢ Mean entries per ECO: {eco_counts.mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Median entries per ECO: {eco_counts.median():.1f}\")\n",
    "print(f\"   ‚Ä¢ Min entries: {eco_counts.min()}\")\n",
    "print(f\"   ‚Ä¢ Max entries: {eco_counts.max()}\")\n",
    "print(f\"   ‚Ä¢ Std: {eco_counts.std():.1f}\")\n",
    "\n",
    "# ECO by first letter (family)\n",
    "print(f\"\\n3Ô∏è‚É£  ECO families (by first letter):\")\n",
    "eco_families = clean_data['eco'].str[0].value_counts().sort_index()\n",
    "print(f\"\\n   {'Family':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "for family, count in eco_families.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.4)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {family:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Top 20 most common ECO codes\n",
    "print(f\"\\n4Ô∏è‚É£  Top 20 most common ECO codes:\")\n",
    "top_eco = clean_data['eco'].value_counts().head(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*12} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(top_eco.items(), 1):\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.3)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Bottom 20 least common ECO codes\n",
    "print(f\"\\n5Ô∏è‚É£  Bottom 20 least common ECO codes:\")\n",
    "bottom_eco = clean_data['eco'].value_counts().tail(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(bottom_eco.items(), 1):\n",
    "    bar_length = min(count, 30)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {bar}\")\n",
    "\n",
    "# ECO code format analysis\n",
    "print(f\"\\n6Ô∏è‚É£  ECO code format analysis:\")\n",
    "eco_lengths = clean_data['eco'].str.len().value_counts().sort_index()\n",
    "print(f\"   ‚Ä¢ ECO code lengths:\")\n",
    "for length, count in eco_lengths.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    print(f\"      {length} characters: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Check for any unusual ECO codes\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of ECO codes:\")\n",
    "sample_eco = clean_data['eco'].drop_duplicates().sample(min(20, clean_data['eco'].nunique()), random_state=42).sort_values()\n",
    "print(f\"   {', '.join(sample_eco.values)}\")\n",
    "\n",
    "# ECO statistics by split\n",
    "print(f\"\\n8Ô∏è‚É£  ECO distribution across splits:\")\n",
    "print(f\"\\n   Train split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_train['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\n   Validation split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_val['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_val):,}\")\n",
    "val_new_eco = set(X_val['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(val_new_eco)}\")\n",
    "\n",
    "print(f\"\\n   Test split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_test['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_test):,}\")\n",
    "test_new_eco = set(X_test['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(test_new_eco)}\")\n",
    "\n",
    "# Average score by ECO code (top 10 and bottom 10)\n",
    "print(f\"\\n9Ô∏è‚É£  Average score by ECO code:\")\n",
    "eco_scores = clean_data.groupby('eco')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\n   Top 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.head(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(f\"\\n   Bottom 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.tail(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# ECO codes with high variance in scores\n",
    "print(f\"\\nüîü  ECO codes with highest score variance (may indicate difficulty):\")\n",
    "eco_variance = clean_data.groupby('eco')['score'].agg(['var', 'std', 'count']).sort_values('var', ascending=False).head(10)\n",
    "print(f\"\\n   {'ECO':<6} {'Variance':<12} {'Std Dev':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_variance.iterrows():\n",
    "    print(f\"   {eco:<6} {row['var']:<12.4f} {row['std']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# Number of openings per ECO code\n",
    "print(f\"\\n1Ô∏è‚É£1Ô∏è‚É£  Openings per ECO code:\")\n",
    "# Connect to database to get opening counts\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    eco_opening_query = \"\"\"\n",
    "        SELECT eco, COUNT(DISTINCT id) as num_openings\n",
    "        FROM opening\n",
    "        GROUP BY eco\n",
    "        ORDER BY num_openings DESC\n",
    "    \"\"\"\n",
    "    eco_opening_counts = pd.DataFrame(con.execute(eco_opening_query).df())\n",
    "    \n",
    "    # Filter to only ECO codes in our data\n",
    "    eco_opening_counts = eco_opening_counts[eco_opening_counts['eco'].isin(clean_data['eco'].unique())]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Mean openings per ECO: {eco_opening_counts['num_openings'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median openings per ECO: {eco_opening_counts['num_openings'].median():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Max openings per ECO: {eco_opening_counts['num_openings'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Min openings per ECO: {eco_opening_counts['num_openings'].min()}\")\n",
    "    \n",
    "    print(f\"\\n   Top 10 ECO codes by number of openings:\")\n",
    "    print(f\"\\n   {'ECO':<6} {'# Openings':<12}\")\n",
    "    print(f\"   {'-'*6} {'-'*12}\")\n",
    "    for idx, row in eco_opening_counts.head(10).iterrows():\n",
    "        print(f\"   {row['eco']:<6} {int(row['num_openings']):>10}\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ECO CODE STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Key takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Most common family: {eco_families.idxmax()} ({eco_families.max():,} entries)\")\n",
    "print(f\"   ‚Ä¢ Most common ECO: {top_eco.index[0]} ({top_eco.iloc[0]:,} entries)\")\n",
    "print(f\"   ‚Ä¢ ECO codes appear in all splits (good for training)\")\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"   ‚Ä¢ Enumerate ECO codes as integers for categorical encoding\")\n",
    "print(f\"   ‚Ä¢ Consider ECO as opening-level side information (similar to player ratings)\")\n",
    "print(f\"   ‚Ä¢ Verify all ECO codes in validation/test exist in training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbad18",
   "metadata": {},
   "source": [
    "## 4b. Create ECO Side Information\n",
    "\n",
    "**Why ECO is Side Information:**\n",
    "- ECO codes describe **opening characteristics**, not individual player-opening interactions\n",
    "- Similar to how player ratings describe players, ECO describes openings\n",
    "- Each opening has ONE ECO code (not per player-opening pair)\n",
    "\n",
    "**Implementation Strategy:**\n",
    "- Split ECO codes into two categorical features:\n",
    "  - `eco_letter`: A, B, C, D, or E ‚Üí encoded as integers 0-4\n",
    "  - `eco_number`: The numeric part (e.g., \"21\" from \"C21\") ‚Üí encoded as sequential integers\n",
    "- Store in a separate `opening_side_info` lookup table (indexed by opening_id)\n",
    "- Remove `eco` from train/test/val DataFrames (it's not interaction data)\n",
    "- During training, model will lookup opening_id ‚Üí (eco_letter, eco_number)\n",
    "\n",
    "**Why Split ECO into Letter and Number:**\n",
    "- ECO families (A-E) represent fundamentally different opening types:\n",
    "  - **A**: Flank openings (English, R√©ti, Bird's, etc.)\n",
    "  - **B**: Semi-Open games (Sicilian, French, Caro-Kann, etc.)\n",
    "  - **C**: Open games (King's pawn openings, Spanish, Italian, etc.)\n",
    "  - **D**: Closed games (Queen's Gambit variations)\n",
    "  - **E**: Indian defenses (King's Indian, Nimzo-Indian, etc.)\n",
    "- Numbers within each family represent variations (C20-C29, C30-C39, etc.)\n",
    "- Model can learn separate embeddings for family vs variation\n",
    "\n",
    "**Categorical Encoding:**\n",
    "- Both features will be treated as categorical (not ordinal)\n",
    "- Higher numbers don't mean \"better\" openings\n",
    "- Model will learn embedding vectors for each category\n",
    "- This allows the model to capture non-linear relationships between ECO codes and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64124475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4B: CREATE ECO SIDE INFORMATION\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Strategy:\n",
      "   ‚Ä¢ Extract unique opening_id ‚Üí eco mappings from clean_data\n",
      "   ‚Ä¢ Split ECO codes: 'C21' ‚Üí letter='C', number='21'\n",
      "   ‚Ä¢ Encode as sequential integers (categorical, not ordinal)\n",
      "   ‚Ä¢ Store in opening_side_info lookup table\n",
      "   ‚Ä¢ Remove 'eco' column from X_train, X_val, X_test\n",
      "\n",
      "1Ô∏è‚É£  Extracting unique opening-ECO mappings...\n",
      "   ‚úì Extracted 2,324 unique openings\n",
      "   ‚úì Verified: Each opening has exactly one ECO code (good!)\n",
      "\n",
      "2Ô∏è‚É£  Splitting ECO codes into letter and number components...\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['A' 'B' 'C' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "\n",
      "3Ô∏è‚É£  Encoding ECO letters as categorical integers...\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (508 openings)\n",
      "      'B' ‚Üí 1 (510 openings)\n",
      "      'C' ‚Üí 2 (734 openings)\n",
      "      'D' ‚Üí 3 (373 openings)\n",
      "      'E' ‚Üí 4 (199 openings)\n",
      "\n",
      "4Ô∏è‚É£  Encoding ECO numbers as categorical integers...\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 248 openings\n",
      "      '40' (‚Üí 40):  83 openings\n",
      "      '01' (‚Üí  1):  65 openings\n",
      "      '02' (‚Üí  2):  62 openings\n",
      "      '20' (‚Üí 20):  61 openings\n",
      "      '21' (‚Üí 21):  58 openings\n",
      "      '45' (‚Üí 45):  53 openings\n",
      "      '10' (‚Üí 10):  53 openings\n",
      "      '15' (‚Üí 15):  53 openings\n",
      "      '42' (‚Üí 42):  51 openings\n",
      "\n",
      "5Ô∏è‚É£  Creating opening_side_info lookup table...\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2324, 2)\n",
      "      ‚Ä¢ Index: opening_id\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "      ‚Ä¢ eco_letter_cat: Categorical encoding of ECO letter (A-E ‚Üí 0-4)\n",
      "      ‚Ä¢ eco_number_cat: Categorical encoding of ECO number\n",
      "\n",
      "6Ô∏è‚É£  Verifying coverage of train/val/test openings...\n",
      "   ‚úì All 2,324 openings in train/val/test have ECO side information\n",
      "\n",
      "7Ô∏è‚É£  Removing 'eco' column from train/val/test DataFrames...\n",
      "   ‚Ä¢ X_train before: (185026, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (37006, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (24671, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "   After removing 'eco':\n",
      "   ‚Ä¢ X_train: (185026, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_val: (37006, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_test: (24671, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   569          1               4               B04               \n",
      "   1522         2               80              C80               \n",
      "   1543         2               89              C89               \n",
      "   2176         0               54              A54               \n",
      "   1123         2               29              C29               \n",
      "   445          0               84              A84               \n",
      "   719          1               21              B21               \n",
      "   1396         2               51              C51               \n",
      "   868          1               72              B72               \n",
      "   323          0               45              A45               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            508     21.86%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            510     21.94%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            734     31.58%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            373     16.05%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            199      8.56%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO SIDE INFORMATION CREATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2324, 2)\n",
      "   ‚Ä¢ Index: opening_id (for O(1) lookups)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 185,026 rows, 3 features\n",
      "   ‚Ä¢ X_val: 37,006 rows, 3 features\n",
      "   ‚Ä¢ X_test: 24,671 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,324 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "üí° Model usage:\n",
      "   During training, for each (player_id, opening_id) pair:\n",
      "   1. Lookup opening_id ‚Üí opening_side_info[opening_id]\n",
      "   2. Get eco_letter_cat and eco_number_cat (already encoded as integers)\n",
      "   3. Feed into categorical embedding layers\n",
      "   4. Combine with opening latent factors\n",
      "\n",
      "üßπ Final cleanup:\n",
      "   ‚Ä¢ Kept only encoded categorical columns: eco_letter_cat, eco_number_cat\n",
      "   ‚Ä¢ Removed raw ECO strings (eco, eco_letter_str, eco_number_str)\n",
      "   ‚Ä¢ Column names clearly indicate categorical encoding (_cat suffix)\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n",
      "   ‚úì Extracted 2,324 unique openings\n",
      "   ‚úì Verified: Each opening has exactly one ECO code (good!)\n",
      "\n",
      "2Ô∏è‚É£  Splitting ECO codes into letter and number components...\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['A' 'B' 'C' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "\n",
      "3Ô∏è‚É£  Encoding ECO letters as categorical integers...\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (508 openings)\n",
      "      'B' ‚Üí 1 (510 openings)\n",
      "      'C' ‚Üí 2 (734 openings)\n",
      "      'D' ‚Üí 3 (373 openings)\n",
      "      'E' ‚Üí 4 (199 openings)\n",
      "\n",
      "4Ô∏è‚É£  Encoding ECO numbers as categorical integers...\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 248 openings\n",
      "      '40' (‚Üí 40):  83 openings\n",
      "      '01' (‚Üí  1):  65 openings\n",
      "      '02' (‚Üí  2):  62 openings\n",
      "      '20' (‚Üí 20):  61 openings\n",
      "      '21' (‚Üí 21):  58 openings\n",
      "      '45' (‚Üí 45):  53 openings\n",
      "      '10' (‚Üí 10):  53 openings\n",
      "      '15' (‚Üí 15):  53 openings\n",
      "      '42' (‚Üí 42):  51 openings\n",
      "\n",
      "5Ô∏è‚É£  Creating opening_side_info lookup table...\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2324, 2)\n",
      "      ‚Ä¢ Index: opening_id\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "      ‚Ä¢ eco_letter_cat: Categorical encoding of ECO letter (A-E ‚Üí 0-4)\n",
      "      ‚Ä¢ eco_number_cat: Categorical encoding of ECO number\n",
      "\n",
      "6Ô∏è‚É£  Verifying coverage of train/val/test openings...\n",
      "   ‚úì All 2,324 openings in train/val/test have ECO side information\n",
      "\n",
      "7Ô∏è‚É£  Removing 'eco' column from train/val/test DataFrames...\n",
      "   ‚Ä¢ X_train before: (185026, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (37006, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (24671, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "   After removing 'eco':\n",
      "   ‚Ä¢ X_train: (185026, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_val: (37006, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_test: (24671, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   569          1               4               B04               \n",
      "   1522         2               80              C80               \n",
      "   1543         2               89              C89               \n",
      "   2176         0               54              A54               \n",
      "   1123         2               29              C29               \n",
      "   445          0               84              A84               \n",
      "   719          1               21              B21               \n",
      "   1396         2               51              C51               \n",
      "   868          1               72              B72               \n",
      "   323          0               45              A45               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            508     21.86%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            510     21.94%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            734     31.58%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            373     16.05%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            199      8.56%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO SIDE INFORMATION CREATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2324, 2)\n",
      "   ‚Ä¢ Index: opening_id (for O(1) lookups)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 185,026 rows, 3 features\n",
      "   ‚Ä¢ X_val: 37,006 rows, 3 features\n",
      "   ‚Ä¢ X_test: 24,671 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,324 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "üí° Model usage:\n",
      "   During training, for each (player_id, opening_id) pair:\n",
      "   1. Lookup opening_id ‚Üí opening_side_info[opening_id]\n",
      "   2. Get eco_letter_cat and eco_number_cat (already encoded as integers)\n",
      "   3. Feed into categorical embedding layers\n",
      "   4. Combine with opening latent factors\n",
      "\n",
      "üßπ Final cleanup:\n",
      "   ‚Ä¢ Kept only encoded categorical columns: eco_letter_cat, eco_number_cat\n",
      "   ‚Ä¢ Removed raw ECO strings (eco, eco_letter_str, eco_number_str)\n",
      "   ‚Ä¢ Column names clearly indicate categorical encoding (_cat suffix)\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n"
     ]
    }
   ],
   "source": [
    "# 4b. Create ECO side information and remove ECO from train/test/val DataFrames\n",
    "\n",
    "# Check if ECO processing has already been done\n",
    "if 'eco' not in X_train.columns and 'opening_side_info' in globals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 4B: ECO SIDE INFORMATION CREATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì ECO column already removed from train/test/val data\")\n",
    "    print(\"‚úì 'opening_side_info' table already exists\")\n",
    "    print(f\"\\nOpening side info shape: {opening_side_info.shape}\")\n",
    "    print(f\"Columns: {list(opening_side_info.columns)}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing ECO encoding statistics:\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_letter values: {opening_side_info['eco_letter'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_number values: {opening_side_info['eco_number'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ eco_letter range: [{opening_side_info['eco_letter'].min()}, {opening_side_info['eco_letter'].max()}]\")\n",
    "    print(f\"   ‚Ä¢ eco_number range: [{opening_side_info['eco_number'].min()}, {opening_side_info['eco_number'].max()}]\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing ECO encoding:\")\n",
    "    sample_data = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "    for idx, row in sample_data.iterrows():\n",
    "        print(f\"   Opening {idx:>4} | ECO: {row['eco']:>3} ‚Üí Letter: {row['eco_letter']} ({row['eco_letter_str']}), Number: {row['eco_number']:>2} ({row['eco_number_str']:>2})\")\n",
    "else:\n",
    "    def create_eco_side_information(clean_data_df, X_train_df, X_val_df, X_test_df):\n",
    "        \"\"\"\n",
    "        Create ECO side information table and remove ECO from train/test/val DataFrames.\n",
    "        \n",
    "        ECO codes are opening-level features, not player-opening interaction features.\n",
    "        We split each ECO code (e.g., \"C21\") into:\n",
    "        - eco_letter: The letter part (A, B, C, D, or E)\n",
    "        - eco_number: The numeric part (e.g., 21)\n",
    "        \n",
    "        Both are encoded as sequential integers for use as categorical features in embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        clean_data_df : pd.DataFrame\n",
    "            Full cleaned data with opening_id and eco columns\n",
    "        X_train_df, X_val_df, X_test_df : pd.DataFrame\n",
    "            Train/val/test feature DataFrames (will be modified to remove 'eco')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 4B: CREATE ECO SIDE INFORMATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Strategy:\")\n",
    "        print(f\"   ‚Ä¢ Extract unique opening_id ‚Üí eco mappings from clean_data\")\n",
    "        print(f\"   ‚Ä¢ Split ECO codes: 'C21' ‚Üí letter='C', number='21'\")\n",
    "        print(f\"   ‚Ä¢ Encode as sequential integers (categorical, not ordinal)\")\n",
    "        print(f\"   ‚Ä¢ Store in opening_side_info lookup table\")\n",
    "        print(f\"   ‚Ä¢ Remove 'eco' column from X_train, X_val, X_test\")\n",
    "        \n",
    "        # Extract unique opening ‚Üí ECO mappings\n",
    "        print(f\"\\n1Ô∏è‚É£  Extracting unique opening-ECO mappings...\")\n",
    "        opening_eco_map = clean_data_df[['opening_id', 'eco']].drop_duplicates().set_index('opening_id')\n",
    "        print(f\"   ‚úì Extracted {len(opening_eco_map):,} unique openings\")\n",
    "        \n",
    "        # Verify one-to-one mapping\n",
    "        eco_per_opening = clean_data_df.groupby('opening_id')['eco'].nunique()\n",
    "        if (eco_per_opening > 1).any():\n",
    "            problematic = eco_per_opening[eco_per_opening > 1]\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(problematic)} openings have multiple ECO codes!\")\n",
    "            print(f\"   Problematic opening IDs: {problematic.index.tolist()[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì Verified: Each opening has exactly one ECO code (good!)\")\n",
    "        \n",
    "        # Split ECO into letter and number components\n",
    "        print(f\"\\n2Ô∏è‚É£  Splitting ECO codes into letter and number components...\")\n",
    "        opening_eco_map['eco_letter_str'] = opening_eco_map['eco'].str[0]  # First character (A-E)\n",
    "        opening_eco_map['eco_number_str'] = opening_eco_map['eco'].str[1:]  # Remaining characters (numeric)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted letter and number components\")\n",
    "        print(f\"   ‚Ä¢ Unique letters: {opening_eco_map['eco_letter_str'].unique()}\")\n",
    "        print(f\"   ‚Ä¢ Unique numbers: {opening_eco_map['eco_number_str'].nunique()}\")\n",
    "        \n",
    "        # Create encoding mappings for eco_letter (A-E ‚Üí 0-4)\n",
    "        print(f\"\\n3Ô∏è‚É£  Encoding ECO letters as categorical integers...\")\n",
    "        unique_letters = sorted(opening_eco_map['eco_letter_str'].unique())\n",
    "        eco_letter_to_int = {letter: idx for idx, letter in enumerate(unique_letters)}\n",
    "        eco_int_to_letter = {idx: letter for letter, idx in eco_letter_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_letter'] = opening_eco_map['eco_letter_str'].map(eco_letter_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Letter encoding created:\")\n",
    "        for letter, idx in sorted(eco_letter_to_int.items()):\n",
    "            count = (opening_eco_map['eco_letter_str'] == letter).sum()\n",
    "            print(f\"      '{letter}' ‚Üí {idx} ({count:,} openings)\")\n",
    "        \n",
    "        # Create encoding mappings for eco_number (00-99 ‚Üí sequential integers)\n",
    "        print(f\"\\n4Ô∏è‚É£  Encoding ECO numbers as categorical integers...\")\n",
    "        unique_numbers = sorted(opening_eco_map['eco_number_str'].unique())\n",
    "        eco_number_to_int = {num: idx for idx, num in enumerate(unique_numbers)}\n",
    "        eco_int_to_number = {idx: num for num, idx in eco_number_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_number'] = opening_eco_map['eco_number_str'].map(eco_number_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Number encoding created:\")\n",
    "        print(f\"      {len(unique_numbers)} unique numbers mapped to [0, {len(unique_numbers)-1}]\")\n",
    "        print(f\"      Range: '{unique_numbers[0]}' ‚Üí 0, ..., '{unique_numbers[-1]}' ‚Üí {len(unique_numbers)-1}\")\n",
    "        \n",
    "        # Show distribution of numbers\n",
    "        print(f\"\\n   Distribution of ECO numbers (top 10):\")\n",
    "        number_counts = opening_eco_map['eco_number_str'].value_counts().head(10)\n",
    "        for num, count in number_counts.items():\n",
    "            encoded = eco_number_to_int[num]\n",
    "            print(f\"      '{num}' (‚Üí {encoded:>2}): {count:>3} openings\")\n",
    "        \n",
    "        # Create final opening_side_info table\n",
    "        print(f\"\\n5Ô∏è‚É£  Creating opening_side_info lookup table...\")\n",
    "        # Only keep the encoded categorical columns and rename them for clarity\n",
    "        opening_side_info = opening_eco_map[['eco_letter', 'eco_number']].copy()\n",
    "        opening_side_info = opening_side_info.rename(columns={\n",
    "            'eco_letter': 'eco_letter_cat',  # _cat suffix indicates categorical encoding\n",
    "            'eco_number': 'eco_number_cat'\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úì Created opening_side_info\")\n",
    "        print(f\"      ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"      ‚Ä¢ Index: opening_id\")\n",
    "        print(f\"      ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        print(f\"      ‚Ä¢ eco_letter_cat: Categorical encoding of ECO letter (A-E ‚Üí 0-4)\")\n",
    "        print(f\"      ‚Ä¢ eco_number_cat: Categorical encoding of ECO number\")\n",
    "        \n",
    "        # Verify all openings in train/val/test have ECO info\n",
    "        print(f\"\\n6Ô∏è‚É£  Verifying coverage of train/val/test openings...\")\n",
    "        all_openings = set(X_train_df['opening_id'].unique()) | \\\n",
    "                       set(X_val_df['opening_id'].unique()) | \\\n",
    "                       set(X_test_df['opening_id'].unique())\n",
    "        \n",
    "        missing_openings = all_openings - set(opening_side_info.index)\n",
    "        if len(missing_openings) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits are missing ECO info!\")\n",
    "            print(f\"   Missing opening IDs: {sorted(list(missing_openings))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(all_openings):,} openings in train/val/test have ECO side information\")\n",
    "        \n",
    "        # Remove ECO from train/val/test DataFrames\n",
    "        print(f\"\\n7Ô∏è‚É£  Removing 'eco' column from train/val/test DataFrames...\")\n",
    "        print(f\"   ‚Ä¢ X_train before: {X_train_df.shape}, columns: {list(X_train_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val before: {X_val_df.shape}, columns: {list(X_val_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test before: {X_test_df.shape}, columns: {list(X_test_df.columns)}\")\n",
    "        \n",
    "        X_train_clean = X_train_df.drop(columns=['eco'])\n",
    "        X_val_clean = X_val_df.drop(columns=['eco'])\n",
    "        X_test_clean = X_test_df.drop(columns=['eco'])\n",
    "        \n",
    "        print(f\"\\n   After removing 'eco':\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape}, columns: {list(X_train_clean.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape}, columns: {list(X_val_clean.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape}, columns: {list(X_test_clean.columns)}\")\n",
    "        \n",
    "        # Sample data showing the transformation\n",
    "        print(f\"\\n8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\")\n",
    "        sample_openings = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "        \n",
    "        # For display purposes, reconstruct original values from the categorical encodings\n",
    "        print(f\"\\n   {'Opening ID':<12} {'Letter (Cat)':<15} {'Number (Cat)':<15} {'Reconstructed ECO':<18}\")\n",
    "        print(f\"   {'-'*12} {'-'*15} {'-'*15} {'-'*18}\")\n",
    "        for idx, row in sample_openings.iterrows():\n",
    "            letter_str = eco_int_to_letter[row['eco_letter_cat']]\n",
    "            number_str = eco_int_to_number[row['eco_number_cat']]\n",
    "            reconstructed = f\"{letter_str}{number_str}\"\n",
    "            print(f\"   {idx:<12} {row['eco_letter_cat']:<15} {row['eco_number_cat']:<15} {reconstructed:<18}\")\n",
    "        \n",
    "        # Show ECO family distribution\n",
    "        print(f\"\\n9Ô∏è‚É£  ECO family distribution in opening_side_info:\")\n",
    "        letter_dist = opening_side_info['eco_letter_cat'].value_counts().sort_index()\n",
    "        print(f\"\\n   {'Encoded':<8} {'Letter':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "        print(f\"   {'-'*8} {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "        for encoded_val, count in letter_dist.items():\n",
    "            letter = eco_int_to_letter[encoded_val]\n",
    "            pct = 100 * count / len(opening_side_info)\n",
    "            bar_length = int(pct * 0.4)\n",
    "            bar = '‚ñà' * bar_length\n",
    "            print(f\"   {encoded_val:<8} {letter:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ ECO SIDE INFORMATION CREATION COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nCreated: opening_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: opening_id (for O(1) lookups)\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape[0]:,} rows, {X_train_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape[0]:,} rows, {X_val_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape[0]:,} rows, {X_test_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ opening_side_info: {len(opening_side_info):,} openings (one per opening)\")\n",
    "        print(f\"   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\")\n",
    "        print(f\"   ‚Ä¢ eco_letter_to_int: {eco_letter_to_int}\")\n",
    "        print(f\"   ‚Ä¢ eco_number_to_int: (dict with {len(eco_number_to_int)} entries)\")\n",
    "        print(f\"\\n   You'll need them to encode ECO codes for new openings at inference time.\")\n",
    "        \n",
    "        print(f\"\\nüí° Model usage:\")\n",
    "        print(f\"   During training, for each (player_id, opening_id) pair:\")\n",
    "        print(f\"   1. Lookup opening_id ‚Üí opening_side_info[opening_id]\")\n",
    "        print(f\"   2. Get eco_letter_cat and eco_number_cat (already encoded as integers)\")\n",
    "        print(f\"   3. Feed into categorical embedding layers\")\n",
    "        print(f\"   4. Combine with opening latent factors\")\n",
    "        \n",
    "        print(f\"\\nüßπ Final cleanup:\")\n",
    "        print(f\"   ‚Ä¢ Kept only encoded categorical columns: eco_letter_cat, eco_number_cat\")\n",
    "        print(f\"   ‚Ä¢ Removed raw ECO strings (eco, eco_letter_str, eco_number_str)\")\n",
    "        print(f\"   ‚Ä¢ Column names clearly indicate categorical encoding (_cat suffix)\")\n",
    "        \n",
    "        return opening_side_info, eco_letter_to_int, eco_number_to_int, X_train_clean, X_val_clean, X_test_clean\n",
    "    \n",
    "    # Call the function\n",
    "    opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test = create_eco_side_information(\n",
    "        clean_data, X_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Create reverse mappings for decoding (needed for verification and debugging)\n",
    "    eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "    eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "    print(f\"\\n‚úì Created reverse mappings for ECO decoding:\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_letter: {len(eco_int_to_letter)} entries\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_number: {len(eco_int_to_number)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df4b4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICATION: FINAL DATA STRUCTURE\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\n",
      "\n",
      "   X_train:\n",
      "   ‚Ä¢ Shape: (185026, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ Sample:\n",
      "        player_id  opening_id  confidence\n",
      "111085       1759        2095    0.180328\n",
      "234318       3746         244    0.206349\n",
      "34446         549         521    0.206349\n",
      "\n",
      "   X_val:\n",
      "   ‚Ä¢ Shape: (37006, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "   X_test:\n",
      "   ‚Ä¢ Shape: (24671, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Side Information Tables:\n",
      "\n",
      "   player_side_info (indexed by player_id):\n",
      "   ‚Ä¢ Shape: (3994, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Sample:\n",
      "           rating_z\n",
      "player_id          \n",
      "0         -0.082603\n",
      "1          0.600709\n",
      "2         -1.553734\n",
      "\n",
      "   opening_side_info (indexed by opening_id):\n",
      "   ‚Ä¢ Shape: (2324, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Sample:\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "142                      0               6\n",
      "302                      0              43\n",
      "464                      1               0\n",
      "\n",
      "3Ô∏è‚É£  Encoding Mappings (for inference):\n",
      "\n",
      "   eco_letter_map:\n",
      "      'A' ‚Üí 0\n",
      "      'B' ‚Üí 1\n",
      "      'C' ‚Üí 2\n",
      "      'D' ‚Üí 3\n",
      "      'E' ‚Üí 4\n",
      "\n",
      "   eco_number_map (first 10):\n",
      "      '00' ‚Üí 0\n",
      "      '01' ‚Üí 1\n",
      "      '02' ‚Üí 2\n",
      "      '03' ‚Üí 3\n",
      "      '04' ‚Üí 4\n",
      "      '05' ‚Üí 5\n",
      "      '06' ‚Üí 6\n",
      "      '07' ‚Üí 7\n",
      "      '08' ‚Üí 8\n",
      "      '09' ‚Üí 9\n",
      "      ... (100 total)\n",
      "\n",
      "4Ô∏è‚É£  Example: Lookup flow for a random train sample:\n",
      "\n",
      "   Sample interaction:\n",
      "   ‚Ä¢ player_id: 2108.0\n",
      "   ‚Ä¢ opening_id: 34.0\n",
      "   ‚Ä¢ confidence: 0.2063\n",
      "\n",
      "   Player side info lookup:\n",
      "   ‚Ä¢ rating_z: -0.3479\n",
      "\n",
      "   Opening side info lookup:\n",
      "   ‚Ä¢ eco_letter_cat: 0 (letter: A)\n",
      "   ‚Ä¢ eco_number_cat: 0 (number: 00)\n",
      "\n",
      "============================================================\n",
      "‚úÖ VERIFICATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Ready for PyTorch tensor conversion:\n",
      "   ‚Ä¢ Features: player_id, opening_id, confidence\n",
      "   ‚Ä¢ Target: score (in y_train, y_val, y_test)\n",
      "   ‚Ä¢ Player side info: rating_z\n",
      "   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\n",
      "\n",
      "   All ECO and rating data is now properly separated as side information!\n",
      "\n",
      "   Side info tables are clean - only contain necessary model inputs!\n"
     ]
    }
   ],
   "source": [
    "# Verify final data structure after ECO processing\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION: FINAL DATA STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\")\n",
    "print(f\"\\n   X_train:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_train.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(X_train.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   X_val:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_val.columns)}\")\n",
    "\n",
    "print(f\"\\n   X_test:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_test.columns)}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Side Information Tables:\")\n",
    "\n",
    "print(f\"\\n   player_side_info (indexed by player_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(player_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   opening_side_info (indexed by opening_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(opening_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Encoding Mappings (for inference):\")\n",
    "print(f\"\\n   eco_letter_map:\")\n",
    "for k, v in sorted(eco_letter_map.items()):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "\n",
    "print(f\"\\n   eco_number_map (first 10):\")\n",
    "for i, (k, v) in enumerate(sorted(eco_number_map.items())[:10]):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "print(f\"      ... ({len(eco_number_map)} total)\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Example: Lookup flow for a random train sample:\")\n",
    "sample = X_train.sample(1, random_state=42).iloc[0]\n",
    "player_id = sample['player_id']\n",
    "opening_id = sample['opening_id']\n",
    "\n",
    "print(f\"\\n   Sample interaction:\")\n",
    "print(f\"   ‚Ä¢ player_id: {player_id}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {opening_id}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Player side info lookup:\")\n",
    "player_info = player_side_info.loc[player_id]\n",
    "print(f\"   ‚Ä¢ rating_z: {player_info['rating_z']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Opening side info lookup:\")\n",
    "opening_info = opening_side_info.loc[opening_id]\n",
    "print(f\"   ‚Ä¢ eco_letter_cat: {opening_info['eco_letter_cat']} (letter: {eco_int_to_letter[opening_info['eco_letter_cat']]})\")\n",
    "print(f\"   ‚Ä¢ eco_number_cat: {opening_info['eco_number_cat']} (number: {eco_int_to_number[opening_info['eco_number_cat']]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Ready for PyTorch tensor conversion:\")\n",
    "print(f\"   ‚Ä¢ Features: player_id, opening_id, confidence\")\n",
    "print(f\"   ‚Ä¢ Target: score (in y_train, y_val, y_test)\")\n",
    "print(f\"   ‚Ä¢ Player side info: rating_z\")\n",
    "print(f\"   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\")\n",
    "print(f\"\\n   All ECO and rating data is now properly separated as side information!\")\n",
    "print(f\"\\n   Side info tables are clean - only contain necessary model inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "017515d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\n",
      "====================================================================================================\n",
      "\n",
      "Sampling 100 player-opening pairs for verification...\n",
      "\n",
      "   ‚Ä¢ Converting 90 NEW opening IDs to OLD database IDs for query\n",
      "   ‚Ä¢ Example: NEW ID 34 ‚Üí OLD ID 63\n",
      "   ‚úì Retrieved 90 opening names from database\n",
      "\n",
      "#    Player   Opening   ECO (DB)   Reconstructed Match  Opening Name                                      \n",
      "====================================================================================================\n",
      "1    2108     34        A00        A00           ‚úì      Lasker Simul Special                              \n",
      "2    3085     704       B21        B21           ‚úì      Sicilian Defense: McDonnell Attack                \n",
      "3    1881     606       B08        B08           ‚úì      Pirc Defense: Classical Variation                 \n",
      "4    1521     533       B01        B01           ‚úì      Scandinavian Defense: Panov Transfer              \n",
      "5    1675     2116      B01        B01           ‚úì      Scandinavian Defense: Valencian Variation         \n",
      "6    2596     1243      C41        C41           ‚úì      Philidor Defense                                  \n",
      "7    1915     1680      D20        D20           ‚úì      Queen's Gambit Accepted                           \n",
      "8    3177     606       B08        B08           ‚úì      Pirc Defense: Classical Variation                 \n",
      "9    3351     549       B02        B02           ‚úì      Alekhine Defense: Scandinavian Variation, Ges...  \n",
      "10   1016     1508      C71        C71           ‚úì      Ruy Lopez: Morphy Defense, Modern Steinitz De...  \n",
      "11   283      488       B00        B00           ‚úì      Nimzowitsch Defense: Mikenas Variation            \n",
      "12   3220     447       A84        A84           ‚úì      Dutch Defense: Rubinstein Variation               \n",
      "13   1956     1667      D15        D15           ‚úì      Slav Defense: Three Knights Variation             \n",
      "14   1868     2170      A08        A08           ‚úì      Zukertort Opening: Reversed Gr√ºnfeld              \n",
      "15   348      129       A04        A04           ‚úì      Zukertort Opening: Vos Gambit                     \n",
      "16   504      530       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "17   1965     1089      C25        C25           ‚úì      Vienna Game: Anderssen Defense                    \n",
      "18   733      103       A02        A02           ‚úì      Bird Opening: Schlechter Gambit                   \n",
      "19   614      642       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Short V...  \n",
      "20   2573     548       B02        B02           ‚úì      Alekhine Defense: Scandinavian Variation          \n",
      "21   1778     1089      C25        C25           ‚úì      Vienna Game: Anderssen Defense                    \n",
      "22   2543     637       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation              \n",
      "23   3773     743       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Fianchetto     \n",
      "24   3654     2086      D37        D37           ‚úì      Queen's Gambit Declined: Three Knights, Vienn...  \n",
      "25   3816     2295      A40        A40           ‚úì      Englund Gambit: Felbecker Gambit                  \n",
      "26   2937     829       B45        B45           ‚úì      Sicilian Defense: Taimanov Variation, Normal ...  \n",
      "27   3848     263       A40        A40           ‚úì      Englund Gambit                                    \n",
      "28   1528     825       B44        B44           ‚úì      Sicilian Defense: Taimanov Variation              \n",
      "29   2030     169       A10        A10           ‚úì      English Opening: Jaenisch Gambit                  \n",
      "30   97       1138      C31        C31           ‚úì      King's Gambit Declined: Falkbeer Countergambit    \n",
      "31   304      841       B52        B52           ‚úì      Sicilian Defense: Canal Attack, Main Line         \n",
      "32   2323     820       B41        B41           ‚úì      Sicilian Defense: Kan Variation, Mar√≥czy Bind...  \n",
      "33   969      810       B40        B40           ‚úì      Sicilian Defense: Kramnik Variation               \n",
      "34   546      742       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Dragon         \n",
      "35   1054     633       B10        B10           ‚úì      Caro-Kann Defense: Two Knights Attack             \n",
      "36   3578     464       B00        B00           ‚úì      Duras Gambit                                      \n",
      "37   920      1449      C60        C60           ‚úì      Ruy Lopez: Cozio Defense                          \n",
      "38   1599     619       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "39   3669     2154      C50        C50           ‚úì      Italian Game: Blackburne-Kostiƒá Gambit            \n",
      "40   3938     1225      C40        C40           ‚úì      Elephant Gambit: Paulsen Countergambit            \n",
      "41   3784     1374      C50        C50           ‚úì      Italian Game: Anti-Fried Liver Defense            \n",
      "42   0        464       B00        B00           ‚úì      Duras Gambit                                      \n",
      "43   1840     863       B70        B70           ‚úì      Sicilian Defense: Dragon Variation                \n",
      "44   827      606       B08        B08           ‚úì      Pirc Defense: Classical Variation                 \n",
      "45   2615     1605      D02        D02           ‚úì      Queen's Pawn Game: Krause Variation               \n",
      "46   445      274       A40        A40           ‚úì      Englund Gambit Declined: Reversed Krebs           \n",
      "47   2113     1667      D15        D15           ‚úì      Slav Defense: Three Knights Variation             \n",
      "48   1000     803       B40        B40           ‚úì      Sicilian Defense: Four Knights Variation          \n",
      "49   2886     1505      C70        C70           ‚úì      Ruy Lopez: Morphy Defense, Norwegian Variation    \n",
      "50   480      1628      D06        D06           ‚úì      Queen's Gambit Declined: Marshall Defense         \n",
      "51   2023     1603      D02        D02           ‚úì      Queen's Pawn Game: Anti-Torre                     \n",
      "52   1021     1813      D55        D55           ‚úì      Queen's Gambit Declined: Modern Variation, No...  \n",
      "53   2521     1273      C42        C42           ‚úì      Russian Game: Cozio Attack                        \n",
      "54   3518     1069      C22        C22           ‚úì      Center Game: Normal Variation                     \n",
      "55   3696     1758      D37        D37           ‚úì      Queen's Gambit Declined: Harrwitz Attack, Ort...  \n",
      "56   3346     481       B00        B00           ‚úì      Nimzowitsch Defense: Kennedy Variation, Links...  \n",
      "57   2611     602       B07        B07           ‚úì      Pirc Defense: Byrne Variation                     \n",
      "58   3038     1320      C44        C44           ‚úì      Scotch Game: Lolli Variation                      \n",
      "59   299      1453      C60        C60           ‚úì      Ruy Lopez: N√ºrnberg Variation                     \n",
      "60   3841     2109      B02        B02           ‚úì      Alekhine Defense: Two Pawns Attack, Tate Vari...  \n",
      "61   3448     421       A80        A80           ‚úì      Dutch Defense                                     \n",
      "62   3373     1244      C41        C41           ‚úì      Philidor Defense: Bird Gambit                     \n",
      "63   3130     354       A50        A50           ‚úì      Indian Defense: Normal Variation                  \n",
      "64   3199     1607      D02        D02           ‚úì      Queen's Pawn Game: London System                  \n",
      "65   2283     545       B02        B02           ‚úì      Alekhine Defense: Mar√≥czy Variation               \n",
      "66   3112     532       B01        B01           ‚úì      Scandinavian Defense: Modern Variation, Gipsl...  \n",
      "67   600      583       B06        B06           ‚úì      Modern Defense: Standard Line                     \n",
      "68   2048     1750      D35        D35           ‚úì      Queen's Gambit Declined: Exchange Variation, ...  \n",
      "69   2136     530       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "70   216      502       B00        B00           ‚úì      Owen Defense: Smith Gambit                        \n",
      "71   3888     1600      D01        D01           ‚úì      Richter-Veresov Attack                            \n",
      "72   2507     1358      C47        C47           ‚úì      Four Knights Game: Scotch Variation Accepted      \n",
      "73   3379     338       A46        A46           ‚úì      Indian Defense: Spielmann-Indian                  \n",
      "74   925      1469      C64        C64           ‚úì      Ruy Lopez: Classical Variation                    \n",
      "75   204      471       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "76   3373     1668      D15        D15           ‚úì      Slav Defense: Two Knights Attack                  \n",
      "77   1763     1348      C46        C46           ‚úì      Three Knights Opening                             \n",
      "78   462      481       B00        B00           ‚úì      Nimzowitsch Defense: Kennedy Variation, Links...  \n",
      "79   529      1246      C41        C41           ‚úì      Philidor Defense: Exchange Variation              \n",
      "80   1612     825       B44        B44           ‚úì      Sicilian Defense: Taimanov Variation              \n",
      "81   3183     1243      C41        C41           ‚úì      Philidor Defense                                  \n",
      "82   2514     119       A04        A04           ‚úì      Zukertort Opening: Pirc Invitation                \n",
      "83   1095     2039      E90        E90           ‚úì      King's Indian Defense: Normal Variation, Rare...  \n",
      "84   3252     1342      C45        C45           ‚úì      Scotch Game: Modern Defense                       \n",
      "85   3663     261       A40        A40           ‚úì      English Defense                                   \n",
      "86   1973     1500      C69        C69           ‚úì      Ruy Lopez: Exchange Variation, Normal Variation   \n",
      "87   1916     531       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "88   2031     80        A01        A01           ‚úì      Nimzo-Larsen Attack                               \n",
      "89   2174     694       B20        B20           ‚úì      Sicilian Defense: Snyder Variation                \n",
      "90   2499     2288      C42        C42           ‚úì      Petrov's Defense: Three Knights Game              \n",
      "91   2798     951       C02        C02           ‚úì      French Defense: Advance Variation, Nimzowitsc...  \n",
      "92   154      963       C05        C05           ‚úì      French Defense: Tarrasch Variation, Closed Va...  \n",
      "93   207      284       A40        A40           ‚úì      Polish Defense                                    \n",
      "94   1631     449       A85        A85           ‚úì      Dutch Defense: Queen's Knight Variation           \n",
      "95   2326     2066      A00        A00           ‚úì      Barnes Opening                                    \n",
      "96   1493     619       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "97   2381     2127      A40        A40           ‚úì      Englund Gambit Declined: Reversed French          \n",
      "98   1523     114       A04        A04           ‚úì      Zukertort Opening: Dutch Variation                \n",
      "99   3907     1476      C65        C65           ‚úì      Ruy Lopez: Berlin Defense, Beverwijk Variation    \n",
      "100  3198     2103      C56        C56           ‚úì      Italian Game: Scotch Gambit, Max Lange Attack...  \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Verification Results:\n",
      "   ‚Ä¢ Total samples: 100\n",
      "   ‚Ä¢ Matches: 100/100 (100.0%)\n",
      "   ‚Ä¢ Mismatches: 0\n",
      "\n",
      "üéâ Perfect! All ECO codes reconstructed correctly!\n",
      "   ‚Ä¢ ID remapping preserved all ECO code mappings\n",
      "   ‚úì Retrieved 90 opening names from database\n",
      "\n",
      "#    Player   Opening   ECO (DB)   Reconstructed Match  Opening Name                                      \n",
      "====================================================================================================\n",
      "1    2108     34        A00        A00           ‚úì      Lasker Simul Special                              \n",
      "2    3085     704       B21        B21           ‚úì      Sicilian Defense: McDonnell Attack                \n",
      "3    1881     606       B08        B08           ‚úì      Pirc Defense: Classical Variation                 \n",
      "4    1521     533       B01        B01           ‚úì      Scandinavian Defense: Panov Transfer              \n",
      "5    1675     2116      B01        B01           ‚úì      Scandinavian Defense: Valencian Variation         \n",
      "6    2596     1243      C41        C41           ‚úì      Philidor Defense                                  \n",
      "7    1915     1680      D20        D20           ‚úì      Queen's Gambit Accepted                           \n",
      "8    3177     606       B08        B08           ‚úì      Pirc Defense: Classical Variation                 \n",
      "9    3351     549       B02        B02           ‚úì      Alekhine Defense: Scandinavian Variation, Ges...  \n",
      "10   1016     1508      C71        C71           ‚úì      Ruy Lopez: Morphy Defense, Modern Steinitz De...  \n",
      "11   283      488       B00        B00           ‚úì      Nimzowitsch Defense: Mikenas Variation            \n",
      "12   3220     447       A84        A84           ‚úì      Dutch Defense: Rubinstein Variation               \n",
      "13   1956     1667      D15        D15           ‚úì      Slav Defense: Three Knights Variation             \n",
      "14   1868     2170      A08        A08           ‚úì      Zukertort Opening: Reversed Gr√ºnfeld              \n",
      "15   348      129       A04        A04           ‚úì      Zukertort Opening: Vos Gambit                     \n",
      "16   504      530       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "17   1965     1089      C25        C25           ‚úì      Vienna Game: Anderssen Defense                    \n",
      "18   733      103       A02        A02           ‚úì      Bird Opening: Schlechter Gambit                   \n",
      "19   614      642       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Short V...  \n",
      "20   2573     548       B02        B02           ‚úì      Alekhine Defense: Scandinavian Variation          \n",
      "21   1778     1089      C25        C25           ‚úì      Vienna Game: Anderssen Defense                    \n",
      "22   2543     637       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation              \n",
      "23   3773     743       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Fianchetto     \n",
      "24   3654     2086      D37        D37           ‚úì      Queen's Gambit Declined: Three Knights, Vienn...  \n",
      "25   3816     2295      A40        A40           ‚úì      Englund Gambit: Felbecker Gambit                  \n",
      "26   2937     829       B45        B45           ‚úì      Sicilian Defense: Taimanov Variation, Normal ...  \n",
      "27   3848     263       A40        A40           ‚úì      Englund Gambit                                    \n",
      "28   1528     825       B44        B44           ‚úì      Sicilian Defense: Taimanov Variation              \n",
      "29   2030     169       A10        A10           ‚úì      English Opening: Jaenisch Gambit                  \n",
      "30   97       1138      C31        C31           ‚úì      King's Gambit Declined: Falkbeer Countergambit    \n",
      "31   304      841       B52        B52           ‚úì      Sicilian Defense: Canal Attack, Main Line         \n",
      "32   2323     820       B41        B41           ‚úì      Sicilian Defense: Kan Variation, Mar√≥czy Bind...  \n",
      "33   969      810       B40        B40           ‚úì      Sicilian Defense: Kramnik Variation               \n",
      "34   546      742       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Dragon         \n",
      "35   1054     633       B10        B10           ‚úì      Caro-Kann Defense: Two Knights Attack             \n",
      "36   3578     464       B00        B00           ‚úì      Duras Gambit                                      \n",
      "37   920      1449      C60        C60           ‚úì      Ruy Lopez: Cozio Defense                          \n",
      "38   1599     619       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "39   3669     2154      C50        C50           ‚úì      Italian Game: Blackburne-Kostiƒá Gambit            \n",
      "40   3938     1225      C40        C40           ‚úì      Elephant Gambit: Paulsen Countergambit            \n",
      "41   3784     1374      C50        C50           ‚úì      Italian Game: Anti-Fried Liver Defense            \n",
      "42   0        464       B00        B00           ‚úì      Duras Gambit                                      \n",
      "43   1840     863       B70        B70           ‚úì      Sicilian Defense: Dragon Variation                \n",
      "44   827      606       B08        B08           ‚úì      Pirc Defense: Classical Variation                 \n",
      "45   2615     1605      D02        D02           ‚úì      Queen's Pawn Game: Krause Variation               \n",
      "46   445      274       A40        A40           ‚úì      Englund Gambit Declined: Reversed Krebs           \n",
      "47   2113     1667      D15        D15           ‚úì      Slav Defense: Three Knights Variation             \n",
      "48   1000     803       B40        B40           ‚úì      Sicilian Defense: Four Knights Variation          \n",
      "49   2886     1505      C70        C70           ‚úì      Ruy Lopez: Morphy Defense, Norwegian Variation    \n",
      "50   480      1628      D06        D06           ‚úì      Queen's Gambit Declined: Marshall Defense         \n",
      "51   2023     1603      D02        D02           ‚úì      Queen's Pawn Game: Anti-Torre                     \n",
      "52   1021     1813      D55        D55           ‚úì      Queen's Gambit Declined: Modern Variation, No...  \n",
      "53   2521     1273      C42        C42           ‚úì      Russian Game: Cozio Attack                        \n",
      "54   3518     1069      C22        C22           ‚úì      Center Game: Normal Variation                     \n",
      "55   3696     1758      D37        D37           ‚úì      Queen's Gambit Declined: Harrwitz Attack, Ort...  \n",
      "56   3346     481       B00        B00           ‚úì      Nimzowitsch Defense: Kennedy Variation, Links...  \n",
      "57   2611     602       B07        B07           ‚úì      Pirc Defense: Byrne Variation                     \n",
      "58   3038     1320      C44        C44           ‚úì      Scotch Game: Lolli Variation                      \n",
      "59   299      1453      C60        C60           ‚úì      Ruy Lopez: N√ºrnberg Variation                     \n",
      "60   3841     2109      B02        B02           ‚úì      Alekhine Defense: Two Pawns Attack, Tate Vari...  \n",
      "61   3448     421       A80        A80           ‚úì      Dutch Defense                                     \n",
      "62   3373     1244      C41        C41           ‚úì      Philidor Defense: Bird Gambit                     \n",
      "63   3130     354       A50        A50           ‚úì      Indian Defense: Normal Variation                  \n",
      "64   3199     1607      D02        D02           ‚úì      Queen's Pawn Game: London System                  \n",
      "65   2283     545       B02        B02           ‚úì      Alekhine Defense: Mar√≥czy Variation               \n",
      "66   3112     532       B01        B01           ‚úì      Scandinavian Defense: Modern Variation, Gipsl...  \n",
      "67   600      583       B06        B06           ‚úì      Modern Defense: Standard Line                     \n",
      "68   2048     1750      D35        D35           ‚úì      Queen's Gambit Declined: Exchange Variation, ...  \n",
      "69   2136     530       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "70   216      502       B00        B00           ‚úì      Owen Defense: Smith Gambit                        \n",
      "71   3888     1600      D01        D01           ‚úì      Richter-Veresov Attack                            \n",
      "72   2507     1358      C47        C47           ‚úì      Four Knights Game: Scotch Variation Accepted      \n",
      "73   3379     338       A46        A46           ‚úì      Indian Defense: Spielmann-Indian                  \n",
      "74   925      1469      C64        C64           ‚úì      Ruy Lopez: Classical Variation                    \n",
      "75   204      471       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "76   3373     1668      D15        D15           ‚úì      Slav Defense: Two Knights Attack                  \n",
      "77   1763     1348      C46        C46           ‚úì      Three Knights Opening                             \n",
      "78   462      481       B00        B00           ‚úì      Nimzowitsch Defense: Kennedy Variation, Links...  \n",
      "79   529      1246      C41        C41           ‚úì      Philidor Defense: Exchange Variation              \n",
      "80   1612     825       B44        B44           ‚úì      Sicilian Defense: Taimanov Variation              \n",
      "81   3183     1243      C41        C41           ‚úì      Philidor Defense                                  \n",
      "82   2514     119       A04        A04           ‚úì      Zukertort Opening: Pirc Invitation                \n",
      "83   1095     2039      E90        E90           ‚úì      King's Indian Defense: Normal Variation, Rare...  \n",
      "84   3252     1342      C45        C45           ‚úì      Scotch Game: Modern Defense                       \n",
      "85   3663     261       A40        A40           ‚úì      English Defense                                   \n",
      "86   1973     1500      C69        C69           ‚úì      Ruy Lopez: Exchange Variation, Normal Variation   \n",
      "87   1916     531       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "88   2031     80        A01        A01           ‚úì      Nimzo-Larsen Attack                               \n",
      "89   2174     694       B20        B20           ‚úì      Sicilian Defense: Snyder Variation                \n",
      "90   2499     2288      C42        C42           ‚úì      Petrov's Defense: Three Knights Game              \n",
      "91   2798     951       C02        C02           ‚úì      French Defense: Advance Variation, Nimzowitsc...  \n",
      "92   154      963       C05        C05           ‚úì      French Defense: Tarrasch Variation, Closed Va...  \n",
      "93   207      284       A40        A40           ‚úì      Polish Defense                                    \n",
      "94   1631     449       A85        A85           ‚úì      Dutch Defense: Queen's Knight Variation           \n",
      "95   2326     2066      A00        A00           ‚úì      Barnes Opening                                    \n",
      "96   1493     619       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "97   2381     2127      A40        A40           ‚úì      Englund Gambit Declined: Reversed French          \n",
      "98   1523     114       A04        A04           ‚úì      Zukertort Opening: Dutch Variation                \n",
      "99   3907     1476      C65        C65           ‚úì      Ruy Lopez: Berlin Defense, Beverwijk Variation    \n",
      "100  3198     2103      C56        C56           ‚úì      Italian Game: Scotch Gambit, Max Lange Attack...  \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Verification Results:\n",
      "   ‚Ä¢ Total samples: 100\n",
      "   ‚Ä¢ Matches: 100/100 (100.0%)\n",
      "   ‚Ä¢ Mismatches: 0\n",
      "\n",
      "üéâ Perfect! All ECO codes reconstructed correctly!\n",
      "   ‚Ä¢ ID remapping preserved all ECO code mappings\n"
     ]
    }
   ],
   "source": [
    "# Verification: Sample 100 player-opening pairs with reconstructed ECO codes and opening names\n",
    "# Doing this to make sure that our ECO encoding/decoding is correct\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Sample 100 random player-opening pairs from training data\n",
    "sample_size = 100\n",
    "sample_data = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "\n",
    "print(f\"\\nSampling {len(sample_data)} player-opening pairs for verification...\\n\")\n",
    "\n",
    "# Get unique opening IDs from sample (these are NEW remapped IDs)\n",
    "new_opening_ids = sample_data['opening_id'].unique()\n",
    "\n",
    "# Convert NEW opening IDs back to OLD database IDs for query\n",
    "if opening_remapped:\n",
    "    old_opening_ids = [opening_idx_to_id[int(new_id)] for new_id in new_opening_ids]\n",
    "    print(f\"   ‚Ä¢ Converting {len(new_opening_ids)} NEW opening IDs to OLD database IDs for query\")\n",
    "    print(f\"   ‚Ä¢ Example: NEW ID {new_opening_ids[0]} ‚Üí OLD ID {old_opening_ids[0]}\")\n",
    "else:\n",
    "    old_opening_ids = new_opening_ids\n",
    "    print(f\"   ‚Ä¢ No remapping was done - using opening IDs directly\")\n",
    "\n",
    "opening_ids_str = ','.join(map(str, old_opening_ids))\n",
    "\n",
    "# Query database for opening names using OLD IDs\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    opening_query = f\"\"\"\n",
    "        SELECT id, name, eco\n",
    "        FROM opening\n",
    "        WHERE id IN ({opening_ids_str})\n",
    "    \"\"\"\n",
    "    opening_names = pd.DataFrame(con.execute(opening_query).df()).set_index('id')\n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "print(f\"   ‚úì Retrieved {len(opening_names)} opening names from database\\n\")\n",
    "\n",
    "# Create reverse mappings for ECO decoding\n",
    "eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "\n",
    "# Build verification table\n",
    "print(f\"{'#':<4} {'Player':<8} {'Opening':<9} {'ECO (DB)':<10} {'Reconstructed':<13} {'Match':<6} {'Opening Name':<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "matches = 0\n",
    "for i, (idx, row) in enumerate(sample_data.iterrows(), 1):\n",
    "    player_id = int(row['player_id'])\n",
    "    new_opening_id = int(row['opening_id'])\n",
    "    \n",
    "    # Convert NEW opening ID to OLD database ID for lookup\n",
    "    if opening_remapped:\n",
    "        old_opening_id = opening_idx_to_id[new_opening_id]\n",
    "    else:\n",
    "        old_opening_id = new_opening_id\n",
    "    \n",
    "    # Lookup opening side info using NEW ID\n",
    "    opening_info = opening_side_info.loc[new_opening_id]\n",
    "    \n",
    "    # Reconstruct ECO from encoded categorical values\n",
    "    eco_letter_encoded = opening_info['eco_letter_cat']\n",
    "    eco_number_encoded = opening_info['eco_number_cat']\n",
    "    \n",
    "    eco_letter_decoded = eco_int_to_letter[eco_letter_encoded]\n",
    "    eco_number_decoded = eco_int_to_number[eco_number_encoded]\n",
    "    \n",
    "    reconstructed_eco = f\"{eco_letter_decoded}{eco_number_decoded}\"\n",
    "    \n",
    "    # Get original ECO from database using OLD ID\n",
    "    db_eco = opening_names.loc[old_opening_id, 'eco']\n",
    "    opening_name = opening_names.loc[old_opening_id, 'name']\n",
    "    \n",
    "    # Check if they match\n",
    "    match = \"‚úì\" if reconstructed_eco == db_eco else \"‚úó\"\n",
    "    if reconstructed_eco == db_eco:\n",
    "        matches += 1\n",
    "    \n",
    "    # Truncate opening name if too long\n",
    "    if len(opening_name) > 48:\n",
    "        opening_name = opening_name[:45] + \"...\"\n",
    "    \n",
    "    # Display using NEW opening ID (what's in the data now)\n",
    "    print(f\"{i:<4} {player_id:<8} {new_opening_id:<9} {db_eco:<10} {reconstructed_eco:<13} {match:<6} {opening_name:<50}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n‚úÖ Verification Results:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(sample_data)}\")\n",
    "print(f\"   ‚Ä¢ Matches: {matches}/{len(sample_data)} ({100*matches/len(sample_data):.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Mismatches: {len(sample_data) - matches}\")\n",
    "\n",
    "if matches == len(sample_data):\n",
    "    print(f\"\\nüéâ Perfect! All ECO codes reconstructed correctly!\")\n",
    "    print(f\"   ‚Ä¢ ID remapping preserved all ECO code mappings\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Some ECO codes did not match. Investigate mismatches above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf4878",
   "metadata": {},
   "source": [
    "## 5. Data Verification and Examination\n",
    "We're almost there. Let's examine our data structures to check for any obvious flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f76afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train \n",
      "         player_id  opening_id  confidence\n",
      "111085       1759        2095    0.180328\n",
      "234318       3746         244    0.206349\n",
      "34446         549         521    0.206349\n",
      "135082       2146        1726    0.193548\n",
      "51914         827         303    0.695122\n",
      "============================================================\n",
      "X_val \n",
      "         player_id  opening_id  confidence\n",
      "217251       3464         782    0.612403\n",
      "64558        1017        2116    0.645390\n",
      "91780        1442         382    0.180328\n",
      "15806         256        1755    0.404762\n",
      "112209       1777        1312    0.681529\n",
      "============================================================\n",
      "X_test \n",
      "         player_id  opening_id  confidence\n",
      "76909        1206        1277    0.590164\n",
      "163010       2595          20    0.367089\n",
      "240818       3859        1136    0.382716\n",
      "94882        1495         517    0.180328\n",
      "174063       2769         939    0.275362\n",
      "============================================================\n",
      "y_train \n",
      " 111085    0.439227\n",
      "234318    0.552550\n",
      "34446     0.524440\n",
      "135082    0.584154\n",
      "51914     0.467852\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_val \n",
      " 217251    0.533467\n",
      "64558     0.527132\n",
      "91780     0.451252\n",
      "15806     0.536301\n",
      "112209    0.604767\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_test \n",
      " 76909     0.470092\n",
      "163010    0.495383\n",
      "240818    0.545484\n",
      "94882     0.482696\n",
      "174063    0.543261\n",
      "Name: score, dtype: float64\n",
      "player_side_info \n",
      "            rating_z\n",
      "player_id          \n",
      "0         -0.082603\n",
      "1          0.600709\n",
      "2         -1.553734\n",
      "3         -0.970908\n",
      "4          0.572573\n",
      "============================================================\n",
      "opening_side_info \n",
      "             eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "142                      0               6\n",
      "302                      0              43\n",
      "464                      1               0\n",
      "473                      1               0\n",
      "500                      1               0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"X_train \\n\", X_train.head())\n",
    "print(\"=\"*60)\n",
    "print(\"X_val \\n\", X_val.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"X_test \\n\", X_test.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_train \\n\", y_train.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_val \\n\", y_val.head())\n",
    "print(\"=\" * 60) \n",
    "print(\"y_test \\n\", y_test.head())\n",
    "\n",
    "# Now side information\n",
    "print(\"player_side_info \\n\", player_side_info.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"opening_side_info \\n\", opening_side_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52936cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANED SIDE INFORMATION TABLES\n",
      "============================================================\n",
      "\n",
      "üìä player_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (3994, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Index: player_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "           rating_z\n",
      "player_id          \n",
      "0         -0.082603\n",
      "1          0.600709\n",
      "2         -1.553734\n",
      "3         -0.970908\n",
      "4          0.572573\n",
      "\n",
      "üìä opening_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (2324, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Index: opening_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "142                      0               6\n",
      "302                      0              43\n",
      "464                      1               0\n",
      "473                      1               0\n",
      "500                      1               0\n",
      "\n",
      "‚úÖ Both side info tables contain ONLY the necessary model inputs:\n",
      "   ‚Ä¢ player_side_info: rating_z (normalized rating)\n",
      "   ‚Ä¢ opening_side_info: eco_letter_cat, eco_number_cat (categorical encodings)\n",
      "   ‚Ä¢ No unnecessary columns (names, titles, raw strings, etc.)\n"
     ]
    }
   ],
   "source": [
    "# Final verification: Display cleaned side info tables\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANED SIDE INFORMATION TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä player_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: player_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(player_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\nüìä opening_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: opening_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(opening_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\n‚úÖ Both side info tables contain ONLY the necessary model inputs:\")\n",
    "print(f\"   ‚Ä¢ player_side_info: rating_z (normalized rating)\")\n",
    "print(f\"   ‚Ä¢ opening_side_info: eco_letter_cat, eco_number_cat (categorical encodings)\")\n",
    "print(f\"   ‚Ä¢ No unnecessary columns (names, titles, raw strings, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19883cba",
   "metadata": {},
   "source": [
    "## Step 5: Convert to PyTorch Tensors\n",
    "\n",
    "**What are tensors?**\n",
    "Tensors are PyTorch's version of arrays - just multi-dimensional data structures optimized for deep learning. Think of them like fancy NumPy arrays that can run on GPUs.\n",
    "\n",
    "**What we need to convert:**\n",
    "\n",
    "1. **Main features** (X_train, X_val, X_test):\n",
    "   - `player_id` ‚Üí long tensor (integer IDs)\n",
    "   - `opening_id` ‚Üí long tensor (integer IDs)\n",
    "   - `confidence` ‚Üí float tensor (weights for loss function)\n",
    "\n",
    "2. **Targets** (y_train, y_val, y_test):\n",
    "   - `score` ‚Üí float tensor (what we're predicting)\n",
    "\n",
    "3. **Player side info** (player_side_info):\n",
    "   - `rating_z` ‚Üí float tensor (normalized ratings)\n",
    "   - Indexed by player_id for fast lookup\n",
    "\n",
    "4. **Opening side info** (opening_side_info):\n",
    "   - `eco_letter_cat` ‚Üí long tensor (categorical)\n",
    "   - `eco_number_cat` ‚Üí long tensor (categorical)\n",
    "   - Indexed by opening_id for fast lookup\n",
    "\n",
    "**Why these data types?**\n",
    "- `long` (int64): For IDs and categorical features that will be embedded\n",
    "- `float` (float32): For continuous values like scores, confidence, and normalized ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "350be514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4c/gt84bby56mqdnzwsg72zvd680000gn/T/ipykernel_12274/1901790236.py\", line 4, in <module>\n",
      "    import torch\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch (if not already installed)\n",
    "import sys\n",
    "import subprocess\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda01b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 5: CONVERT TO PYTORCH TENSORS\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ PyTorch version: 2.2.2\n",
      "   ‚Ä¢ CUDA available: False\n",
      "   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\n",
      "\n",
      "0Ô∏è‚É£  Index alignment sanity checks...\n",
      "   Checking player_side_info index alignment...\n",
      "   ‚úì player_side_info index is sorted\n",
      "   ‚Ä¢ Index range: [0, 3993]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "\n",
      "   Checking opening_side_info index alignment...\n",
      "   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\n",
      "   ‚Ä¢ Index range: [0, 2323]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "\n",
      "   Checking if indices are contiguous 0-based...\n",
      "   ‚Ä¢ player_side_info contiguous 0-based: True\n",
      "   ‚Ä¢ opening_side_info contiguous 0-based: False\n",
      "   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\n",
      "\n",
      "1Ô∏è‚É£  Converting main features (X_train, X_val, X_test)...\n",
      "   Train tensors:\n",
      "   ‚Ä¢ player_ids_train: torch.Size([185026]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_train: torch.Size([185026]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_train: torch.Size([185026]), dtype=torch.float32\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([37006]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([37006]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([37006]), dtype=torch.float32\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([24671]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([24671]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([24671]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   ‚Ä¢ scores_train: torch.Size([185026]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([37006]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([24671]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1361, 0.9091]\n",
      "   ‚Ä¢ Val: [0.1906, 0.7326]\n",
      "   ‚Ä¢ Test: [0.2128, 0.8018]\n",
      "\n",
      "3Ô∏è‚É£  Converting player side information...\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([3994]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([3994]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.3456, 4.1740]\n",
      "   ‚úì All 3994 unique players in splits have side information\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2324]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2324]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2324]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "   ‚úì All 2324 unique openings in splits have side information\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "\n",
      "   Dataset sizes:\n",
      "   ‚Ä¢ Train: 185,026 samples\n",
      "   ‚Ä¢ Val: 37,006 samples\n",
      "   ‚Ä¢ Test: 24,671 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 3,994\n",
      "   ‚Ä¢ Openings: 2,324\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 3994 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2324 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 5.70 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Available tensors for training:\n",
      "\n",
      "   Main features (train/val/test):\n",
      "   ‚Ä¢ player_ids_train, player_ids_val, player_ids_test\n",
      "   ‚Ä¢ opening_ids_train, opening_ids_val, opening_ids_test\n",
      "   ‚Ä¢ confidence_train, confidence_val, confidence_test\n",
      "\n",
      "   Targets (train/val/test):\n",
      "   ‚Ä¢ scores_train, scores_val, scores_test\n",
      "\n",
      "   Side information:\n",
      "   ‚Ä¢ player_ratings_tensor (indexed by player_ids_in_side_info)\n",
      "   ‚Ä¢ opening_eco_letter_tensor (indexed by opening_ids_in_side_info)\n",
      "   ‚Ä¢ opening_eco_number_tensor (indexed by opening_ids_in_side_info)\n",
      "\n",
      "üí° Ready for model training!\n",
      "   These tensors can be directly fed into PyTorch DataLoaders and models.\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([37006]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([37006]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([37006]), dtype=torch.float32\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([24671]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([24671]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([24671]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   ‚Ä¢ scores_train: torch.Size([185026]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([37006]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([24671]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1361, 0.9091]\n",
      "   ‚Ä¢ Val: [0.1906, 0.7326]\n",
      "   ‚Ä¢ Test: [0.2128, 0.8018]\n",
      "\n",
      "3Ô∏è‚É£  Converting player side information...\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([3994]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([3994]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.3456, 4.1740]\n",
      "   ‚úì All 3994 unique players in splits have side information\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2324]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2324]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2324]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "   ‚úì All 2324 unique openings in splits have side information\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "\n",
      "   Dataset sizes:\n",
      "   ‚Ä¢ Train: 185,026 samples\n",
      "   ‚Ä¢ Val: 37,006 samples\n",
      "   ‚Ä¢ Test: 24,671 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 3,994\n",
      "   ‚Ä¢ Openings: 2,324\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 3994 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2324 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 5.70 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Available tensors for training:\n",
      "\n",
      "   Main features (train/val/test):\n",
      "   ‚Ä¢ player_ids_train, player_ids_val, player_ids_test\n",
      "   ‚Ä¢ opening_ids_train, opening_ids_val, opening_ids_test\n",
      "   ‚Ä¢ confidence_train, confidence_val, confidence_test\n",
      "\n",
      "   Targets (train/val/test):\n",
      "   ‚Ä¢ scores_train, scores_val, scores_test\n",
      "\n",
      "   Side information:\n",
      "   ‚Ä¢ player_ratings_tensor (indexed by player_ids_in_side_info)\n",
      "   ‚Ä¢ opening_eco_letter_tensor (indexed by opening_ids_in_side_info)\n",
      "   ‚Ä¢ opening_eco_number_tensor (indexed by opening_ids_in_side_info)\n",
      "\n",
      "üí° Ready for model training!\n",
      "   These tensors can be directly fed into PyTorch DataLoaders and models.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Convert all data to PyTorch tensors\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 5: CONVERT TO PYTORCH TENSORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   ‚Ä¢ PyTorch version: {torch.__version__}\")\n",
    "print(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\")\n",
    "\n",
    "# 0. Index alignment sanity checks\n",
    "print(f\"\\n0Ô∏è‚É£  Index alignment sanity checks...\")\n",
    "\n",
    "# Check player_side_info is properly indexed\n",
    "print(f\"   Checking player_side_info index alignment...\")\n",
    "player_ids_sorted = sorted(player_side_info.index.values)\n",
    "if player_ids_sorted != list(player_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  player_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì player_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{player_side_info.index.min()}, {player_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {player_side_info.index.dtype}\")\n",
    "\n",
    "# Check opening_side_info is properly indexed\n",
    "print(f\"\\n   Checking opening_side_info index alignment...\")\n",
    "opening_ids_sorted = sorted(opening_side_info.index.values)\n",
    "if opening_ids_sorted != list(opening_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì opening_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{opening_side_info.index.min()}, {opening_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {opening_side_info.index.dtype}\")\n",
    "\n",
    "# CRITICAL: Check if indices are contiguous 0-based\n",
    "# If opening_side_info.index = [0, 1, 2, ..., N-1], then we can use opening_id as direct array index\n",
    "# If not (e.g., [10, 15, 23, ...]), we'll need a mapping dictionary\n",
    "print(f\"\\n   Checking if indices are contiguous 0-based...\")\n",
    "player_contiguous = (player_side_info.index == range(len(player_side_info))).all()\n",
    "opening_contiguous = (opening_side_info.index == range(len(opening_side_info))).all()\n",
    "\n",
    "print(f\"   ‚Ä¢ player_side_info contiguous 0-based: {player_contiguous}\")\n",
    "print(f\"   ‚Ä¢ opening_side_info contiguous 0-based: {opening_contiguous}\")\n",
    "\n",
    "if not player_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Player IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "if not opening_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "\n",
    "# 1. Convert main features (train/val/test)\n",
    "print(f\"\\n1Ô∏è‚É£  Converting main features (X_train, X_val, X_test)...\")\n",
    "\n",
    "# Train set\n",
    "player_ids_train = torch.tensor(X_train['player_id'].values, dtype=torch.long)\n",
    "opening_ids_train = torch.tensor(X_train['opening_id'].values, dtype=torch.long)\n",
    "confidence_train = torch.tensor(X_train['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   Train tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_train: {player_ids_train.shape}, dtype={player_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_train: {opening_ids_train.shape}, dtype={opening_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_train: {confidence_train.shape}, dtype={confidence_train.dtype}\")\n",
    "\n",
    "# Validation set\n",
    "player_ids_val = torch.tensor(X_val['player_id'].values, dtype=torch.long)\n",
    "opening_ids_val = torch.tensor(X_val['opening_id'].values, dtype=torch.long)\n",
    "confidence_val = torch.tensor(X_val['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Validation tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_val: {player_ids_val.shape}, dtype={player_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_val: {opening_ids_val.shape}, dtype={opening_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_val: {confidence_val.shape}, dtype={confidence_val.dtype}\")\n",
    "\n",
    "# Test set\n",
    "player_ids_test = torch.tensor(X_test['player_id'].values, dtype=torch.long)\n",
    "opening_ids_test = torch.tensor(X_test['opening_id'].values, dtype=torch.long)\n",
    "confidence_test = torch.tensor(X_test['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Test tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_test: {player_ids_test.shape}, dtype={player_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_test: {opening_ids_test.shape}, dtype={opening_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_test: {confidence_test.shape}, dtype={confidence_test.dtype}\")\n",
    "\n",
    "# 2. Convert targets (scores)\n",
    "print(f\"\\n2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\")\n",
    "\n",
    "scores_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "scores_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "scores_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ scores_train: {scores_train.shape}, dtype={scores_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_val: {scores_val.shape}, dtype={scores_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_test: {scores_test.shape}, dtype={scores_test.dtype}\")\n",
    "\n",
    "print(f\"\\n   Score ranges (sanity check):\")\n",
    "print(f\"   ‚Ä¢ Train: [{scores_train.min():.4f}, {scores_train.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Val: [{scores_val.min():.4f}, {scores_val.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Test: [{scores_test.min():.4f}, {scores_test.max():.4f}]\")\n",
    "\n",
    "# 3. Convert player side information\n",
    "print(f\"\\n3Ô∏è‚É£  Converting player side information...\")\n",
    "\n",
    "# Create tensor of all player ratings (indexed by player_id)\n",
    "# Since player_side_info is indexed by player_id, we need to ensure coverage\n",
    "player_ratings_tensor = torch.tensor(player_side_info['rating_z'].values, dtype=torch.float32)\n",
    "player_ids_in_side_info = torch.tensor(player_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ player_ratings_tensor: {player_ratings_tensor.shape}, dtype={player_ratings_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ player_ids_in_side_info: {player_ids_in_side_info.shape}, dtype={player_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings_tensor.min():.4f}, {player_ratings_tensor.max():.4f}]\")\n",
    "\n",
    "# Verify all player IDs in train/val/test are covered\n",
    "all_player_ids = torch.cat([player_ids_train, player_ids_val, player_ids_test]).unique()\n",
    "missing_players = set(all_player_ids.tolist()) - set(player_ids_in_side_info.tolist())\n",
    "if len(missing_players) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in splits missing from side_info!\")\n",
    "else:\n",
    "    print(f\"   ‚úì All {len(all_player_ids)} unique players in splits have side information\")\n",
    "\n",
    "# 4. Convert opening side information\n",
    "print(f\"\\n4Ô∏è‚É£  Converting opening side information...\")\n",
    "\n",
    "# Verify column names exist (using eco_letter_cat and eco_number_cat consistently)\n",
    "if 'eco_letter_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_letter_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "if 'eco_number_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_number_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "\n",
    "# Create tensors for opening ECO features (indexed by opening_id)\n",
    "opening_eco_letter_tensor = torch.tensor(opening_side_info['eco_letter_cat'].values, dtype=torch.long)\n",
    "opening_eco_number_tensor = torch.tensor(opening_side_info['eco_number_cat'].values, dtype=torch.long)\n",
    "opening_ids_in_side_info = torch.tensor(opening_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ opening_eco_letter_tensor: {opening_eco_letter_tensor.shape}, dtype={opening_eco_letter_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_eco_number_tensor: {opening_eco_number_tensor.shape}, dtype={opening_eco_number_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_in_side_info: {opening_ids_in_side_info.shape}, dtype={opening_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ ECO letter range: [{opening_eco_letter_tensor.min()}, {opening_eco_letter_tensor.max()}]\")\n",
    "print(f\"   ‚Ä¢ ECO number range: [{opening_eco_number_tensor.min()}, {opening_eco_number_tensor.max()}]\")\n",
    "\n",
    "# Verify all opening IDs in train/val/test are covered\n",
    "all_opening_ids = torch.cat([opening_ids_train, opening_ids_val, opening_ids_test]).unique()\n",
    "missing_openings = set(all_opening_ids.tolist()) - set(opening_ids_in_side_info.tolist())\n",
    "if len(missing_openings) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits missing from side_info!\")\n",
    "else:\n",
    "    print(f\"   ‚úì All {len(all_opening_ids)} unique openings in splits have side information\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(f\"\\n5Ô∏è‚É£  Summary statistics:\")\n",
    "print(f\"\\n   Dataset sizes:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(scores_train):,} samples\")\n",
    "print(f\"   ‚Ä¢ Val: {len(scores_val):,} samples\")\n",
    "print(f\"   ‚Ä¢ Test: {len(scores_test):,} samples\")\n",
    "\n",
    "print(f\"\\n   Unique entities:\")\n",
    "print(f\"   ‚Ä¢ Players: {len(player_ids_in_side_info):,}\")\n",
    "print(f\"   ‚Ä¢ Openings: {len(opening_ids_in_side_info):,}\")\n",
    "\n",
    "print(f\"\\n   Vocabulary sizes (for embedding layers):\")\n",
    "print(f\"   ‚Ä¢ num_players: {player_ids_in_side_info.max() + 1} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_openings: {opening_ids_in_side_info.max() + 1} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_letters: {opening_eco_letter_tensor.max() + 1} (max eco_letter_cat + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_numbers: {opening_eco_number_tensor.max() + 1} (max eco_number_cat + 1)\")\n",
    "\n",
    "# 6. Memory usage (approximate - using simple calculation)\n",
    "print(f\"\\n6Ô∏è‚É£  Approximate memory usage:\")\n",
    "# More efficient calculation: element_size * nelement for each tensor\n",
    "# Using list comprehension with helper function for cleaner code\n",
    "def tensor_memory_mb(t):\n",
    "    \"\"\"Calculate tensor memory in MB\"\"\"\n",
    "    return (t.element_size() * t.nelement()) / (1024 * 1024)\n",
    "\n",
    "tensors = [\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train,\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val,\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test,\n",
    "    player_ratings_tensor, opening_eco_letter_tensor, opening_eco_number_tensor\n",
    "]\n",
    "total_memory_mb = sum(tensor_memory_mb(t) for t in tensors)\n",
    "print(f\"   ‚Ä¢ Total tensor memory: {total_memory_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TENSOR CONVERSION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Available tensors for training:\")\n",
    "print(f\"\\n   Main features (train/val/test):\")\n",
    "print(f\"   ‚Ä¢ player_ids_train, player_ids_val, player_ids_test\")\n",
    "print(f\"   ‚Ä¢ opening_ids_train, opening_ids_val, opening_ids_test\")\n",
    "print(f\"   ‚Ä¢ confidence_train, confidence_val, confidence_test\")\n",
    "\n",
    "print(f\"\\n   Targets (train/val/test):\")\n",
    "print(f\"   ‚Ä¢ scores_train, scores_val, scores_test\")\n",
    "\n",
    "print(f\"\\n   Side information:\")\n",
    "print(f\"   ‚Ä¢ player_ratings_tensor (indexed by player_ids_in_side_info)\")\n",
    "print(f\"   ‚Ä¢ opening_eco_letter_tensor (indexed by opening_ids_in_side_info)\")\n",
    "print(f\"   ‚Ä¢ opening_eco_number_tensor (indexed by opening_ids_in_side_info)\")\n",
    "\n",
    "print(f\"\\nüí° Ready for model training!\")\n",
    "print(f\"   These tensors can be directly fed into PyTorch DataLoaders and models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fc308",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup\n",
    "\n",
    "Define all constants, loss functions, optimizer, and helper functions for training.\n",
    "\n",
    "**What we need to set up:**\n",
    "\n",
    "1. **Constants and Hyperparameters:**\n",
    "   - `NUM_FACTORS`: Dimensionality of latent factor embeddings (e.g., 50)\n",
    "   - `LEARNING_RATE`: Step size for SGD optimizer (e.g., 0.01)\n",
    "   - `BATCH_SIZE`: Number of samples per training batch (e.g., 1024)\n",
    "   - `N_EPOCHS`: Number of full passes through training data (e.g., 10)\n",
    "   - Random seeds for reproducibility\n",
    "\n",
    "2. **Loss Functions:**\n",
    "   - MSE (Mean Squared Error): Main training loss\n",
    "   - RMSE (Root Mean Squared Error): Evaluation metric\n",
    "   - Confidence-weighted loss: Down-weight uncertain predictions\n",
    "\n",
    "3. **Optimizer:**\n",
    "   - SGD (Stochastic Gradient Descent) with momentum\n",
    "\n",
    "4. **Helper Functions:**\n",
    "   - `train_one_epoch()`: Train for one epoch\n",
    "   - `evaluate_model()`: Evaluate on validation/test set\n",
    "   - `calculate_rmse()`: Compute RMSE metric\n",
    "   - `save_checkpoint()`: Save model state\n",
    "\n",
    "5. **Logging:**\n",
    "   - Progress bars with ETA\n",
    "   - Loss tracking per epoch\n",
    "   - Model checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54d13c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\n",
      "============================================================\n",
      "\n",
      "üìã Hyperparameters:\n",
      "   ‚Ä¢ NUM_FACTORS: 50\n",
      "   ‚Ä¢ LEARNING_RATE: 0.01\n",
      "   ‚Ä¢ MOMENTUM: 0.9\n",
      "   ‚Ä¢ BATCH_SIZE: 1024\n",
      "   ‚Ä¢ N_EPOCHS: 20\n",
      "   ‚Ä¢ WEIGHT_DECAY: 0.0\n",
      "\n",
      "üé≤ Random Seed:\n",
      "   ‚Ä¢ RANDOM_SEED: 42\n",
      "\n",
      "üíª Device Configuration:\n",
      "   ‚Ä¢ DEVICE: cpu\n",
      "\n",
      "üíæ Model Saving:\n",
      "   ‚Ä¢ Save directory: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints\n",
      "\n",
      "üîí Setting random seeds for reproducibility...\n",
      "   ‚úì Python random seed set to 42\n",
      "   ‚úì NumPy random seed set to 42\n",
      "   ‚úì PyTorch CPU random seed set to 42\n",
      "\n",
      "üìä Vocabulary Sizes (for embedding layers):\n",
      "   ‚Ä¢ NUM_PLAYERS: 3,994 (max player_id + 1)\n",
      "   ‚Ä¢ NUM_OPENINGS: 2,324 (max opening_id + 1)\n",
      "   ‚Ä¢ NUM_ECO_LETTERS: 5 (ECO letter categories)\n",
      "   ‚Ä¢ NUM_ECO_NUMBERS: 100 (ECO number categories)\n",
      "\n",
      "üìà Dataset Statistics:\n",
      "   ‚Ä¢ Train samples: 185,026\n",
      "   ‚Ä¢ Validation samples: 37,006\n",
      "   ‚Ä¢ Test samples: 24,671\n",
      "   ‚Ä¢ Total samples: 246,703\n",
      "\n",
      "   ‚Ä¢ Batches per epoch: 180\n",
      "   ‚Ä¢ Training iterations (total): 3,600\n",
      "\n",
      "============================================================\n",
      "‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\n",
      "============================================================\n",
      "\n",
      "üí° To modify hyperparameters:\n",
      "   ‚Ä¢ Edit the values at the top of this cell\n",
      "   ‚Ä¢ Rerun this cell to apply changes\n",
      "   ‚Ä¢ All subsequent cells will use the updated values\n"
     ]
    }
   ],
   "source": [
    "# Step 6a: Define Hyperparameters and Constants\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HYPERPARAMETERS (Easy to modify here)\n",
    "# ========================================\n",
    "\n",
    "# Model architecture\n",
    "NUM_FACTORS = 50  # Dimensionality of latent factor embeddings\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.01  # SGD learning rate\n",
    "MOMENTUM = 0.9  # SGD momentum\n",
    "BATCH_SIZE = 1024  # Mini-batch size\n",
    "N_EPOCHS = 20  # Number of training epochs\n",
    "\n",
    "# Regularization (if needed)\n",
    "WEIGHT_DECAY = 0.0  # L2 regularization (0 = no regularization)\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model saving\n",
    "MODEL_SAVE_DIR = Path.cwd() / \"model_checkpoints\"\n",
    "MODEL_SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìã Hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ NUM_FACTORS: {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ LEARNING_RATE: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ MOMENTUM: {MOMENTUM}\")\n",
    "print(f\"   ‚Ä¢ BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ N_EPOCHS: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ WEIGHT_DECAY: {WEIGHT_DECAY}\")\n",
    "\n",
    "print(f\"\\nüé≤ Random Seed:\")\n",
    "print(f\"   ‚Ä¢ RANDOM_SEED: {RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\nüíª Device Configuration:\")\n",
    "print(f\"   ‚Ä¢ DEVICE: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ‚Ä¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüíæ Model Saving:\")\n",
    "print(f\"   ‚Ä¢ Save directory: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# ========================================\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüîí Setting random seeds for reproducibility...\")\n",
    "\n",
    "# Python random\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì Python random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì NumPy random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# PyTorch random\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì PyTorch CPU random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU\n",
    "    print(f\"   ‚úì PyTorch GPU random seed set to {RANDOM_SEED}\")\n",
    "    \n",
    "    # Additional CUDA reproducibility settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"   ‚úì CUDA deterministic mode enabled\")\n",
    "\n",
    "# ========================================\n",
    "# CALCULATE VOCABULARY SIZES\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Sizes (for embedding layers):\")\n",
    "\n",
    "# Calculate from our tensors\n",
    "NUM_PLAYERS = int(player_ids_in_side_info.max()) + 1\n",
    "NUM_OPENINGS = int(opening_ids_in_side_info.max()) + 1\n",
    "NUM_ECO_LETTERS = int(opening_eco_letter_tensor.max()) + 1\n",
    "NUM_ECO_NUMBERS = int(opening_eco_number_tensor.max()) + 1\n",
    "\n",
    "print(f\"   ‚Ä¢ NUM_PLAYERS: {NUM_PLAYERS:,} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_OPENINGS: {NUM_OPENINGS:,} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_LETTERS: {NUM_ECO_LETTERS} (ECO letter categories)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_NUMBERS: {NUM_ECO_NUMBERS} (ECO number categories)\")\n",
    "\n",
    "# ========================================\n",
    "# DATASET STATISTICS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Train samples: {len(scores_train):,}\")\n",
    "print(f\"   ‚Ä¢ Validation samples: {len(scores_val):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(scores_test):,}\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(scores_train) + len(scores_val) + len(scores_test):,}\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Batches per epoch: {len(scores_train) // BATCH_SIZE:,}\")\n",
    "print(f\"   ‚Ä¢ Training iterations (total): {(len(scores_train) // BATCH_SIZE) * N_EPOCHS:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° To modify hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ Edit the values at the top of this cell\")\n",
    "print(f\"   ‚Ä¢ Rerun this cell to apply changes\")\n",
    "print(f\"   ‚Ä¢ All subsequent cells will use the updated values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a93e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6B: DEFINE LOSS FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Loss functions defined:\n",
      "   ‚Ä¢ mse_loss(): Mean Squared Error (with optional confidence weighting)\n",
      "   ‚Ä¢ rmse_loss(): Root Mean Squared Error\n",
      "   ‚Ä¢ calculate_rmse(): RMSE evaluation metric\n",
      "\n",
      "üß™ Testing loss functions with dummy data...\n",
      "\n",
      "   Dummy data:\n",
      "   ‚Ä¢ Predictions: [0.5, 0.6000000238418579, 0.699999988079071, 0.800000011920929]\n",
      "   ‚Ä¢ Targets: [0.550000011920929, 0.6200000047683716, 0.6800000071525574, 0.75]\n",
      "   ‚Ä¢ Confidence: [0.800000011920929, 0.8999999761581421, 0.699999988079071, 1.0]\n",
      "\n",
      "   MSE (unweighted): 0.001450\n",
      "   MSE (weighted): 0.001512\n",
      "\n",
      "   RMSE (unweighted): 0.038079\n",
      "   RMSE (weighted): 0.038881\n",
      "\n",
      "   calculate_rmse() result: 0.038881\n",
      "   ‚úì Returns Python float (not tensor): <class 'float'>\n",
      "\n",
      "============================================================\n",
      "‚úÖ LOSS FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Training will use:\n",
      "   ‚Ä¢ Loss function: mse_loss() with confidence weighting\n",
      "   ‚Ä¢ Evaluation metric: calculate_rmse()\n",
      "   ‚Ä¢ Confidence weights: From 'confidence' column in data\n",
      "   ‚Ä¢ Weighting strategy: confidence = num_games / (num_games + K)\n",
      "   ‚Ä¢ Effect: High-game-count entries have larger loss impact\n"
     ]
    }
   ],
   "source": [
    "# Step 6b: Define Loss Functions\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6B: DEFINE LOSS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "def mse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss with optional confidence weighting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        Higher confidence = larger loss contribution\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Compute squared error\n",
    "    squared_error = (predictions - targets) ** 2\n",
    "    \n",
    "    # Apply confidence weighting if provided\n",
    "    if confidence_weights is not None:\n",
    "        weighted_squared_error = squared_error * confidence_weights\n",
    "        # Average over all samples (sum of weighted errors / sum of weights)\n",
    "        loss = weighted_squared_error.sum() / confidence_weights.sum()\n",
    "    else:\n",
    "        # Standard MSE (average over all samples)\n",
    "        loss = squared_error.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def rmse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error loss.\n",
    "    \n",
    "    This is the square root of MSE and is our primary evaluation metric.\n",
    "    RMSE is in the same units as the target (score), making it interpretable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar RMSE value\n",
    "    \"\"\"\n",
    "    mse = mse_loss(predictions, targets, confidence_weights)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def calculate_rmse(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Calculate RMSE metric (convenience function for evaluation).\n",
    "    \n",
    "    This is the same as rmse_loss but with clearer naming for evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        RMSE value (Python float, not tensor)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Don't compute gradients for evaluation\n",
    "        rmse = rmse_loss(predictions, targets, confidence_weights)\n",
    "    return rmse.item()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ Loss functions defined:\")\n",
    "print(f\"   ‚Ä¢ mse_loss(): Mean Squared Error (with optional confidence weighting)\")\n",
    "print(f\"   ‚Ä¢ rmse_loss(): Root Mean Squared Error\")\n",
    "print(f\"   ‚Ä¢ calculate_rmse(): RMSE evaluation metric\")\n",
    "\n",
    "print(f\"\\nüß™ Testing loss functions with dummy data...\")\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_predictions = torch.tensor([0.5, 0.6, 0.7, 0.8], dtype=torch.float32)\n",
    "dummy_targets = torch.tensor([0.55, 0.62, 0.68, 0.75], dtype=torch.float32)\n",
    "dummy_confidence = torch.tensor([0.8, 0.9, 0.7, 1.0], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Dummy data:\")\n",
    "print(f\"   ‚Ä¢ Predictions: {dummy_predictions.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Targets: {dummy_targets.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Confidence: {dummy_confidence.tolist()}\")\n",
    "\n",
    "# Test MSE without confidence weighting\n",
    "mse_unweighted = mse_loss(dummy_predictions, dummy_targets)\n",
    "print(f\"\\n   MSE (unweighted): {mse_unweighted.item():.6f}\")\n",
    "\n",
    "# Test MSE with confidence weighting\n",
    "mse_weighted = mse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"   MSE (weighted): {mse_weighted.item():.6f}\")\n",
    "\n",
    "# Test RMSE\n",
    "rmse_unweighted = rmse_loss(dummy_predictions, dummy_targets)\n",
    "rmse_weighted = rmse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   RMSE (unweighted): {rmse_unweighted.item():.6f}\")\n",
    "print(f\"   RMSE (weighted): {rmse_weighted.item():.6f}\")\n",
    "\n",
    "# Test calculate_rmse\n",
    "rmse_eval = calculate_rmse(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   calculate_rmse() result: {rmse_eval:.6f}\")\n",
    "print(f\"   ‚úì Returns Python float (not tensor): {type(rmse_eval)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LOSS FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Training will use:\")\n",
    "print(f\"   ‚Ä¢ Loss function: mse_loss() with confidence weighting\")\n",
    "print(f\"   ‚Ä¢ Evaluation metric: calculate_rmse()\")\n",
    "print(f\"   ‚Ä¢ Confidence weights: From 'confidence' column in data\")\n",
    "print(f\"   ‚Ä¢ Weighting strategy: confidence = num_games / (num_games + K)\")\n",
    "print(f\"   ‚Ä¢ Effect: High-game-count entries have larger loss impact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35e00b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Creating PyTorch Datasets...\n",
      "   ‚úì Train dataset: 185,026 samples\n",
      "   ‚úì Validation dataset: 37,006 samples\n",
      "   ‚úì Test dataset: 24,671 samples\n",
      "\n",
      "2Ô∏è‚É£  Testing dataset access...\n",
      "   Sample from train_dataset[0]:\n",
      "   ‚Ä¢ player_id: 1759\n",
      "   ‚Ä¢ opening_id: 2095\n",
      "   ‚Ä¢ confidence: 0.1803\n",
      "   ‚Ä¢ score: 0.4392\n",
      "\n",
      "3Ô∏è‚É£  Creating PyTorch DataLoaders...\n",
      "   ‚úì Train loader: 181 batches of size 1024\n",
      "   ‚úì Validation loader: 37 batches of size 1024\n",
      "   ‚úì Test loader: 25 batches of size 1024\n",
      "\n",
      "4Ô∏è‚É£  Testing DataLoader batch access...\n",
      "   First batch from train_loader:\n",
      "   ‚Ä¢ player_id shape: torch.Size([1024])\n",
      "   ‚Ä¢ opening_id shape: torch.Size([1024])\n",
      "   ‚Ä¢ confidence shape: torch.Size([1024])\n",
      "   ‚Ä¢ score shape: torch.Size([1024])\n",
      "\n",
      "   Sample values from first batch (first 5):\n",
      "   [0] player=1918, opening=1665, conf=0.2537, score=0.5759\n",
      "   [1] player=2695, opening=267, conf=0.4505, score=0.4763\n",
      "   [2] player=960, opening=530, conf=0.4949, score=0.5556\n",
      "   [3] player=3398, opening=359, conf=0.6528, score=0.6126\n",
      "   [4] player=568, opening=1085, conf=0.2857, score=0.5649\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATASET AND DATALOADER COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Available objects:\n",
      "   ‚Ä¢ train_dataset, val_dataset, test_dataset\n",
      "   ‚Ä¢ train_loader, val_loader, test_loader\n",
      "\n",
      "üí° DataLoader features:\n",
      "   ‚Ä¢ Automatic batching: 1024 samples per batch\n",
      "   ‚Ä¢ Shuffling: Training data shuffled each epoch\n",
      "   ‚Ä¢ Pin memory: Disabled (speeds up GPU transfer)\n",
      "   ‚Ä¢ Ready for training loop!\n"
     ]
    }
   ],
   "source": [
    "# Step 6c: Create PyTorch Dataset and DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# CUSTOM DATASET CLASS\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for chess player-opening interactions.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - player_id: Integer ID for embedding lookup\n",
    "    - opening_id: Integer ID for embedding lookup\n",
    "    - confidence: Confidence weight for loss function\n",
    "    - score: Target value (win rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, player_ids, opening_ids, confidence, scores):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (long tensor)\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (long tensor)\n",
    "        confidence : torch.Tensor\n",
    "            Confidence weights (float tensor)\n",
    "        scores : torch.Tensor\n",
    "            Target scores (float tensor)\n",
    "        \"\"\"\n",
    "        self.player_ids = player_ids\n",
    "        self.opening_ids = opening_ids\n",
    "        self.confidence = confidence\n",
    "        self.scores = scores\n",
    "        \n",
    "        # Validate shapes\n",
    "        assert len(player_ids) == len(opening_ids) == len(confidence) == len(scores), \\\n",
    "            \"All tensors must have the same length\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.player_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with keys: 'player_id', 'opening_id', 'confidence', 'score'\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'player_id': self.player_ids[idx],\n",
    "            'opening_id': self.opening_ids[idx],\n",
    "            'confidence': self.confidence[idx],\n",
    "            'score': self.scores[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATASETS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Creating PyTorch Datasets...\")\n",
    "\n",
    "train_dataset = ChessOpeningDataset(\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train\n",
    ")\n",
    "val_dataset = ChessOpeningDataset(\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val\n",
    ")\n",
    "test_dataset = ChessOpeningDataset(\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"   ‚úì Validation dataset: {len(val_dataset):,} samples\")\n",
    "print(f\"   ‚úì Test dataset: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Test dataset access\n",
    "print(f\"\\n2Ô∏è‚É£  Testing dataset access...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"   Sample from train_dataset[0]:\")\n",
    "print(f\"   ‚Ä¢ player_id: {sample['player_id']}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {sample['opening_id']}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "print(f\"   ‚Ä¢ score: {sample['score']:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATALOADERS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Creating PyTorch DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle training data each epoch\n",
    "    num_workers=0,  # Number of subprocesses for data loading (0 = main process)\n",
    "    pin_memory=True if torch.cuda.is_available() else False  # Speed up GPU transfer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle test data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Train loader: {len(train_loader):,} batches of size {BATCH_SIZE}\")\n",
    "print(f\"   ‚úì Validation loader: {len(val_loader):,} batches of size {BATCH_SIZE}\")\n",
    "print(f\"   ‚úì Test loader: {len(test_loader):,} batches of size {BATCH_SIZE}\")\n",
    "\n",
    "# Test dataloader\n",
    "print(f\"\\n4Ô∏è‚É£  Testing DataLoader batch access...\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"   First batch from train_loader:\")\n",
    "print(f\"   ‚Ä¢ player_id shape: {batch['player_id'].shape}\")\n",
    "print(f\"   ‚Ä¢ opening_id shape: {batch['opening_id'].shape}\")\n",
    "print(f\"   ‚Ä¢ confidence shape: {batch['confidence'].shape}\")\n",
    "print(f\"   ‚Ä¢ score shape: {batch['score'].shape}\")\n",
    "\n",
    "print(f\"\\n   Sample values from first batch (first 5):\")\n",
    "for i in range(min(5, batch['player_id'].shape[0])):\n",
    "    print(f\"   [{i}] player={batch['player_id'][i]}, opening={batch['opening_id'][i]}, \"\n",
    "          f\"conf={batch['confidence'][i]:.4f}, score={batch['score'][i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATASET AND DATALOADER COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Available objects:\")\n",
    "print(f\"   ‚Ä¢ train_dataset, val_dataset, test_dataset\")\n",
    "print(f\"   ‚Ä¢ train_loader, val_loader, test_loader\")\n",
    "\n",
    "print(f\"\\nüí° DataLoader features:\")\n",
    "print(f\"   ‚Ä¢ Automatic batching: {BATCH_SIZE} samples per batch\")\n",
    "print(f\"   ‚Ä¢ Shuffling: Training data shuffled each epoch\")\n",
    "print(f\"   ‚Ä¢ Pin memory: {'Enabled' if torch.cuda.is_available() else 'Disabled'} (speeds up GPU transfer)\")\n",
    "print(f\"   ‚Ä¢ Ready for training loop!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e5fbb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6D: DEFINE HELPER FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Helper functions defined:\n",
      "   ‚Ä¢ save_checkpoint(): Save model and optimizer state\n",
      "   ‚Ä¢ load_checkpoint(): Load model and optimizer state\n",
      "   ‚Ä¢ format_time(): Format seconds as readable time\n",
      "   ‚Ä¢ print_progress(): Print training progress with ETA\n",
      "\n",
      "üß™ Testing helper functions...\n",
      "\n",
      "   format_time() tests:\n",
      "   ‚Ä¢ 30 seconds ‚Üí 30.0s\n",
      "   ‚Ä¢ 90 seconds ‚Üí 1m 30s\n",
      "   ‚Ä¢ 300 seconds ‚Üí 5m 0s\n",
      "   ‚Ä¢ 3661 seconds ‚Üí 1h 1m 1s\n",
      "   ‚Ä¢ 7384 seconds ‚Üí 2h 3m 4s\n",
      "\n",
      "   print_progress() test (simulating batch progress):\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° These functions will be used during training for:\n",
      "   ‚Ä¢ Progress tracking with ETA estimation\n",
      "   ‚Ä¢ Saving checkpoints after each epoch\n",
      "   ‚Ä¢ Loading checkpoints for resuming training\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° These functions will be used during training for:\n",
      "   ‚Ä¢ Progress tracking with ETA estimation\n",
      "   ‚Ä¢ Saving checkpoints after each epoch\n",
      "   ‚Ä¢ Loading checkpoints for resuming training\n"
     ]
    }
   ],
   "source": [
    "# Step 6d: Define Helper Functions for Training\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6D: DEFINE HELPER FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Save Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to save\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer state to save\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    train_loss : float\n",
    "        Training loss for this epoch\n",
    "    val_loss : float\n",
    "        Validation loss for this epoch\n",
    "    filepath : Path or str\n",
    "        Where to save the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        \n",
    "        # Save hyperparameters for reproducibility\n",
    "        'hyperparameters': {\n",
    "            'num_factors': NUM_FACTORS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'momentum': MOMENTUM,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'num_players': NUM_PLAYERS,\n",
    "            'num_openings': NUM_OPENINGS,\n",
    "            'num_eco_letters': NUM_ECO_LETTERS,\n",
    "            'num_eco_numbers': NUM_ECO_NUMBERS,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"   üíæ Checkpoint saved to: {filepath}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Load Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to load weights into\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to load state into\n",
    "    filepath : Path or str\n",
    "        Where to load the checkpoint from\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        The epoch number from the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"   üìÇ Checkpoint loaded from: {filepath}\")\n",
    "    print(f\"   ‚Ä¢ Epoch: {epoch}\")\n",
    "    print(f\"   ‚Ä¢ Train loss: {checkpoint['train_loss']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Val loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    return epoch\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Format Time\n",
    "# ========================================\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"\n",
    "    Format seconds as human-readable time string.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seconds : float\n",
    "        Number of seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted time string (e.g., \"1h 23m 45s\")\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Print Progress\n",
    "# ========================================\n",
    "\n",
    "def print_progress(epoch, batch_idx, total_batches, loss, elapsed_time):\n",
    "    \"\"\"\n",
    "    Print training progress with ETA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    batch_idx : int\n",
    "        Current batch index (0-based)\n",
    "    total_batches : int\n",
    "        Total number of batches per epoch\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    elapsed_time : float\n",
    "        Elapsed time in seconds since epoch start\n",
    "    \"\"\"\n",
    "    # Calculate progress\n",
    "    progress_pct = 100.0 * (batch_idx + 1) / total_batches\n",
    "    \n",
    "    # Estimate time remaining\n",
    "    time_per_batch = elapsed_time / (batch_idx + 1)\n",
    "    remaining_batches = total_batches - (batch_idx + 1)\n",
    "    eta_seconds = time_per_batch * remaining_batches\n",
    "    \n",
    "    # Print progress bar\n",
    "    bar_length = 40\n",
    "    filled_length = int(bar_length * (batch_idx + 1) / total_batches)\n",
    "    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)\n",
    "    \n",
    "    print(f\"   Epoch {epoch} [{bar}] {progress_pct:>6.2f}% | \"\n",
    "          f\"Loss: {loss:.6f} | \"\n",
    "          f\"ETA: {format_time(eta_seconds)}\", end='\\r')\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST HELPER FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ Helper functions defined:\")\n",
    "print(f\"   ‚Ä¢ save_checkpoint(): Save model and optimizer state\")\n",
    "print(f\"   ‚Ä¢ load_checkpoint(): Load model and optimizer state\")\n",
    "print(f\"   ‚Ä¢ format_time(): Format seconds as readable time\")\n",
    "print(f\"   ‚Ä¢ print_progress(): Print training progress with ETA\")\n",
    "\n",
    "print(f\"\\nüß™ Testing helper functions...\")\n",
    "\n",
    "# Test format_time\n",
    "print(f\"\\n   format_time() tests:\")\n",
    "test_times = [30, 90, 300, 3661, 7384]\n",
    "for t in test_times:\n",
    "    print(f\"   ‚Ä¢ {t} seconds ‚Üí {format_time(t)}\")\n",
    "\n",
    "# Test print_progress (will overwrite line)\n",
    "print(f\"\\n   print_progress() test (simulating batch progress):\")\n",
    "for i in range(0, 101, 20):\n",
    "    total = 100\n",
    "    loss = 0.05 - i * 0.0003\n",
    "    elapsed = i * 0.5\n",
    "    print_progress(epoch=1, batch_idx=i, total_batches=total, loss=loss, elapsed_time=elapsed)\n",
    "    time.sleep(0.1)  # Brief pause to show animation\n",
    "print()  # New line after progress bar\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HELPER FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° These functions will be used during training for:\")\n",
    "print(f\"   ‚Ä¢ Progress tracking with ETA estimation\")\n",
    "print(f\"   ‚Ä¢ Saving checkpoints after each epoch\")\n",
    "print(f\"   ‚Ä¢ Loading checkpoints for resuming training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cab0cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Training and evaluation functions defined:\n",
      "   ‚Ä¢ train_one_epoch(): Train model for one epoch\n",
      "   ‚Ä¢ evaluate_model(): Evaluate model on a dataset\n",
      "\n",
      "üí° Function features:\n",
      "   ‚Ä¢ Automatic device handling (CPU/CUDA)\n",
      "   ‚Ä¢ Progress tracking with ETA\n",
      "   ‚Ä¢ Confidence-weighted loss\n",
      "   ‚Ä¢ Batch processing via DataLoader\n",
      "   ‚Ä¢ Train/eval mode switching\n",
      "\n",
      "============================================================\n",
      "‚úÖ TRAINING AND EVALUATION FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìã Training workflow:\n",
      "   1. Initialize model and optimizer\n",
      "   2. For each epoch:\n",
      "      a. Call train_one_epoch()\n",
      "      b. Call evaluate_model() on validation set\n",
      "      c. Save checkpoint with save_checkpoint()\n",
      "      d. Log metrics\n",
      "   3. After training, evaluate on test set\n",
      "\n",
      "üöÄ Ready to define the model architecture (next cell)!\n"
     ]
    }
   ],
   "source": [
    "# Step 6e: Define Training and Evaluation Functions\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# TRAINING FUNCTION: Train One Epoch\n",
    "# ========================================\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, device, epoch_num):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to train\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for updating model parameters\n",
    "    device : torch.device\n",
    "        Device to train on (CPU or CUDA)\n",
    "    epoch_num : int\n",
    "        Current epoch number (for logging)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (average_loss, elapsed_time)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move batch to device\n",
    "        player_ids = batch['player_id'].to(device)\n",
    "        opening_ids = batch['opening_id'].to(device)\n",
    "        confidence = batch['confidence'].to(device)\n",
    "        targets = batch['score'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(player_ids, opening_ids)\n",
    "        \n",
    "        # Compute loss with confidence weighting\n",
    "        loss = mse_loss(predictions, targets, confidence)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 10 batches or on last batch\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print_progress(epoch_num, batch_idx, num_batches, loss.item(), elapsed)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    elapsed_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Clear progress line and print summary\n",
    "    print()  # New line after progress bar\n",
    "    \n",
    "    return avg_loss, elapsed_time\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# EVALUATION FUNCTION: Evaluate Model\n",
    "# ========================================\n",
    "\n",
    "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to evaluate\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for evaluation data\n",
    "    device : torch.device\n",
    "        Device to evaluate on (CPU or CUDA)\n",
    "    dataset_name : str\n",
    "        Name of dataset for logging (e.g., \"Validation\", \"Test\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (mse, rmse)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_mse = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in data_loader:\n",
    "            # Move batch to device\n",
    "            player_ids = batch['player_id'].to(device)\n",
    "            opening_ids = batch['opening_id'].to(device)\n",
    "            confidence = batch['confidence'].to(device)\n",
    "            targets = batch['score'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(player_ids, opening_ids)\n",
    "            \n",
    "            # Compute MSE with confidence weighting\n",
    "            batch_mse = mse_loss(predictions, targets, confidence)\n",
    "            \n",
    "            # Track weighted sum of MSE\n",
    "            total_mse += batch_mse.item() * len(targets)\n",
    "            total_samples += len(targets)\n",
    "    \n",
    "    # Calculate average MSE and RMSE\n",
    "    avg_mse = total_mse / total_samples\n",
    "    avg_rmse = avg_mse ** 0.5\n",
    "    \n",
    "    return avg_mse, avg_rmse\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST FUNCTIONS (with dummy model)\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ Training and evaluation functions defined:\")\n",
    "print(f\"   ‚Ä¢ train_one_epoch(): Train model for one epoch\")\n",
    "print(f\"   ‚Ä¢ evaluate_model(): Evaluate model on a dataset\")\n",
    "\n",
    "print(f\"\\nüí° Function features:\")\n",
    "print(f\"   ‚Ä¢ Automatic device handling (CPU/CUDA)\")\n",
    "print(f\"   ‚Ä¢ Progress tracking with ETA\")\n",
    "print(f\"   ‚Ä¢ Confidence-weighted loss\")\n",
    "print(f\"   ‚Ä¢ Batch processing via DataLoader\")\n",
    "print(f\"   ‚Ä¢ Train/eval mode switching\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TRAINING AND EVALUATION FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã Training workflow:\")\n",
    "print(f\"   1. Initialize model and optimizer\")\n",
    "print(f\"   2. For each epoch:\")\n",
    "print(f\"      a. Call train_one_epoch()\")\n",
    "print(f\"      b. Call evaluate_model() on validation set\")\n",
    "print(f\"      c. Save checkpoint with save_checkpoint()\")\n",
    "print(f\"      d. Log metrics\")\n",
    "print(f\"   3. After training, evaluate on test set\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready to define the model architecture (next cell)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50c5eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6F: DEFINE MODEL ARCHITECTURE\n",
      "============================================================\n",
      "\n",
      "‚úÖ ChessOpeningRecommender model class defined\n",
      "\n",
      "üìä Model Architecture:\n",
      "   Player Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(3994, 50)\n",
      "      ‚Ä¢ Biases: Embedding(3994, 1)\n",
      "      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\n",
      "      ‚Ä¢ Combiner: Linear(51, 50)\n",
      "\n",
      "   Opening Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(2324, 50)\n",
      "      ‚Ä¢ Biases: Embedding(2324, 1)\n",
      "      ‚Ä¢ ECO letter embedding: Embedding(5, 4)\n",
      "      ‚Ä¢ ECO number embedding: Embedding(100, 4)\n",
      "      ‚Ä¢ Combiner: Linear(58, 50)\n",
      "\n",
      "   Prediction:\n",
      "      ‚Ä¢ Dot product of player and opening representations\n",
      "      ‚Ä¢ + player bias + opening bias + global bias\n",
      "      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\n",
      "\n",
      "üí° Model Statistics:\n",
      "   ‚Ä¢ Total trainable parameters: 328,189\n",
      "   ‚Ä¢ Player parameters: 206,294\n",
      "   ‚Ä¢ Opening parameters: 121,894\n",
      "   ‚Ä¢ Global parameters: 1\n",
      "\n",
      "============================================================\n",
      "‚úÖ MODEL ARCHITECTURE COMPLETE\n",
      "============================================================\n",
      "\n",
      "üöÄ Ready for Step 7: Training Loop!\n"
     ]
    }
   ],
   "source": [
    "# Step 6f: Define Model Architecture\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6F: DEFINE MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Chess Opening Recommender Model\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningRecommender(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for chess opening recommendations.\n",
    "    \n",
    "    The model learns latent factors for players and openings, incorporating\n",
    "    side information:\n",
    "    - Player ratings (normalized)\n",
    "    - Opening ECO codes (letter and number as categorical features)\n",
    "    \n",
    "    Architecture:\n",
    "    - Player embedding: learnable latent factors\n",
    "    - Opening embedding: learnable latent factors\n",
    "    - Player rating: fixed side information (no embedding)\n",
    "    - ECO letter/number: categorical embeddings\n",
    "    \n",
    "    Prediction: dot product of player and opening representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_players, num_openings, num_factors,\n",
    "                 player_ratings, opening_eco_letters, opening_eco_numbers,\n",
    "                 num_eco_letters, num_eco_numbers, eco_embed_dim=4):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_players : int\n",
    "            Total number of unique players\n",
    "        num_openings : int\n",
    "            Total number of unique openings\n",
    "        num_factors : int\n",
    "            Dimensionality of latent factor embeddings\n",
    "        player_ratings : torch.Tensor\n",
    "            Z-score normalized ratings for all players (shape: [num_players])\n",
    "        opening_eco_letters : torch.Tensor\n",
    "            ECO letter categories for all openings (shape: [num_openings])\n",
    "        opening_eco_numbers : torch.Tensor\n",
    "            ECO number categories for all openings (shape: [num_openings])\n",
    "        num_eco_letters : int\n",
    "            Number of unique ECO letter categories\n",
    "        num_eco_numbers : int\n",
    "            Number of unique ECO number categories\n",
    "        eco_embed_dim : int\n",
    "            Dimensionality of ECO categorical embeddings (default: 4)\n",
    "        \"\"\"\n",
    "        super(ChessOpeningRecommender, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_players = num_players\n",
    "        self.num_openings = num_openings\n",
    "        self.num_factors = num_factors\n",
    "        self.eco_embed_dim = eco_embed_dim\n",
    "        \n",
    "        # ========================================\n",
    "        # Player Components\n",
    "        # ========================================\n",
    "        \n",
    "        # Player latent factors (learnable)\n",
    "        self.player_factors = torch.nn.Embedding(num_players, num_factors)\n",
    "        \n",
    "        # Player biases (learnable)\n",
    "        self.player_biases = torch.nn.Embedding(num_players, 1)\n",
    "        \n",
    "        # Player ratings (fixed side information - registered as buffer, not parameter)\n",
    "        self.register_buffer('player_ratings', player_ratings)\n",
    "        \n",
    "        # ========================================\n",
    "        # Opening Components\n",
    "        # ========================================\n",
    "        \n",
    "        # Opening latent factors (learnable)\n",
    "        self.opening_factors = torch.nn.Embedding(num_openings, num_factors)\n",
    "        \n",
    "        # Opening biases (learnable)\n",
    "        self.opening_biases = torch.nn.Embedding(num_openings, 1)\n",
    "        \n",
    "        # ECO letter and number (fixed side information - registered as buffers)\n",
    "        self.register_buffer('opening_eco_letters', opening_eco_letters)\n",
    "        self.register_buffer('opening_eco_numbers', opening_eco_numbers)\n",
    "        \n",
    "        # ECO embeddings (learnable)\n",
    "        self.eco_letter_embedding = torch.nn.Embedding(num_eco_letters, eco_embed_dim)\n",
    "        self.eco_number_embedding = torch.nn.Embedding(num_eco_numbers, eco_embed_dim)\n",
    "        \n",
    "        # ========================================\n",
    "        # Combination Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Combine player latent factors with rating\n",
    "        # Input: [num_factors + 1] ‚Üí Output: [num_factors]\n",
    "        self.player_combiner = torch.nn.Linear(num_factors + 1, num_factors)\n",
    "        \n",
    "        # Combine opening latent factors with ECO embeddings\n",
    "        # Input: [num_factors + 2*eco_embed_dim] ‚Üí Output: [num_factors]\n",
    "        self.opening_combiner = torch.nn.Linear(num_factors + 2 * eco_embed_dim, num_factors)\n",
    "        \n",
    "        # ========================================\n",
    "        # Global Bias\n",
    "        # ========================================\n",
    "        \n",
    "        # Global bias term (learnable scalar)\n",
    "        self.global_bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # ========================================\n",
    "        # Initialize Weights\n",
    "        # ========================================\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        torch.nn.init.normal_(self.player_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.opening_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_letter_embedding.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_number_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        torch.nn.init.zeros_(self.player_biases.weight)\n",
    "        torch.nn.init.zeros_(self.opening_biases.weight)\n",
    "        \n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        torch.nn.init.xavier_uniform_(self.player_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.player_combiner.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.opening_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.opening_combiner.bias)\n",
    "    \n",
    "    def forward(self, player_ids, opening_ids):\n",
    "        \"\"\"\n",
    "        Forward pass: predict player-opening scores.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (shape: [batch_size])\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (shape: [batch_size])\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Predicted scores (shape: [batch_size])\n",
    "        \"\"\"\n",
    "        # ========================================\n",
    "        # Get Player Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get player latent factors [batch_size, num_factors]\n",
    "        player_embed = self.player_factors(player_ids)\n",
    "        \n",
    "        # Get player ratings [batch_size, 1]\n",
    "        player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "        \n",
    "        # Concatenate player factors with rating [batch_size, num_factors + 1]\n",
    "        player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "        \n",
    "        # Combine into final player representation [batch_size, num_factors]\n",
    "        player_repr = self.player_combiner(player_concat)\n",
    "        \n",
    "        # Get player bias [batch_size]\n",
    "        player_bias = self.player_biases(player_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Get Opening Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get opening latent factors [batch_size, num_factors]\n",
    "        opening_embed = self.opening_factors(opening_ids)\n",
    "        \n",
    "        # Get ECO embeddings\n",
    "        eco_letters = self.opening_eco_letters[opening_ids]  # [batch_size]\n",
    "        eco_numbers = self.opening_eco_numbers[opening_ids]  # [batch_size]\n",
    "        \n",
    "        eco_letter_embed = self.eco_letter_embedding(eco_letters)  # [batch_size, eco_embed_dim]\n",
    "        eco_number_embed = self.eco_number_embedding(eco_numbers)  # [batch_size, eco_embed_dim]\n",
    "        \n",
    "        # Concatenate opening factors with ECO embeddings\n",
    "        # [batch_size, num_factors + 2*eco_embed_dim]\n",
    "        opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "        \n",
    "        # Combine into final opening representation [batch_size, num_factors]\n",
    "        opening_repr = self.opening_combiner(opening_concat)\n",
    "        \n",
    "        # Get opening bias [batch_size]\n",
    "        opening_bias = self.opening_biases(opening_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Compute Prediction\n",
    "        # ========================================\n",
    "        \n",
    "        # Dot product of player and opening representations [batch_size]\n",
    "        interaction = (player_repr * opening_repr).sum(dim=1)\n",
    "        \n",
    "        # Add biases and global bias\n",
    "        prediction = interaction + player_bias + opening_bias + self.global_bias\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] range (since scores are win rates)\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def get_player_embedding(self, player_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific player.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_id : int\n",
    "            Player ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Player representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            player_ids = torch.tensor([player_id], device=self.player_factors.weight.device)\n",
    "            player_embed = self.player_factors(player_ids)\n",
    "            player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "            player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "            player_repr = self.player_combiner(player_concat)\n",
    "            return player_repr.squeeze()\n",
    "    \n",
    "    def get_opening_embedding(self, opening_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific opening.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        opening_id : int\n",
    "            Opening ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Opening representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            opening_ids = torch.tensor([opening_id], device=self.opening_factors.weight.device)\n",
    "            opening_embed = self.opening_factors(opening_ids)\n",
    "            \n",
    "            eco_letters = self.opening_eco_letters[opening_ids]\n",
    "            eco_numbers = self.opening_eco_numbers[opening_ids]\n",
    "            \n",
    "            eco_letter_embed = self.eco_letter_embedding(eco_letters)\n",
    "            eco_number_embed = self.eco_number_embedding(eco_numbers)\n",
    "            \n",
    "            opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "            opening_repr = self.opening_combiner(opening_concat)\n",
    "            return opening_repr.squeeze()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model Summary\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ ChessOpeningRecommender model class defined\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Player Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_PLAYERS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_PLAYERS}, 1)\")\n",
    "print(f\"      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 1}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Opening Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_OPENINGS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_OPENINGS}, 1)\")\n",
    "print(f\"      ‚Ä¢ ECO letter embedding: Embedding({NUM_ECO_LETTERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ ECO number embedding: Embedding({NUM_ECO_NUMBERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 8}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Prediction:\")\n",
    "print(f\"      ‚Ä¢ Dot product of player and opening representations\")\n",
    "print(f\"      ‚Ä¢ + player bias + opening bias + global bias\")\n",
    "print(f\"      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\")\n",
    "\n",
    "# Calculate total parameters\n",
    "player_factor_params = NUM_PLAYERS * NUM_FACTORS\n",
    "player_bias_params = NUM_PLAYERS\n",
    "player_combiner_params = (NUM_FACTORS + 1) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "opening_factor_params = NUM_OPENINGS * NUM_FACTORS\n",
    "opening_bias_params = NUM_OPENINGS\n",
    "eco_letter_params = NUM_ECO_LETTERS * 4\n",
    "eco_number_params = NUM_ECO_NUMBERS * 4\n",
    "opening_combiner_params = (NUM_FACTORS + 8) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "global_bias_params = 1\n",
    "\n",
    "total_params = (player_factor_params + player_bias_params + player_combiner_params +\n",
    "                opening_factor_params + opening_bias_params + \n",
    "                eco_letter_params + eco_number_params + opening_combiner_params +\n",
    "                global_bias_params)\n",
    "\n",
    "print(f\"\\nüí° Model Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total trainable parameters: {total_params:,}\")\n",
    "print(f\"   ‚Ä¢ Player parameters: {player_factor_params + player_bias_params + player_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Opening parameters: {opening_factor_params + opening_bias_params + eco_letter_params + eco_number_params + opening_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Global parameters: {global_bias_params}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ MODEL ARCHITECTURE COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Step 7: Training Loop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc2c6d",
   "metadata": {},
   "source": [
    "## Step 7: Training Loop\n",
    "\n",
    "Now we'll implement the main training loop that:\n",
    "- Initializes the model with player and opening embeddings\n",
    "- Trains for multiple epochs using mini-batch SGD\n",
    "- Evaluates on validation set after each epoch\n",
    "- Saves checkpoints after each epoch\n",
    "- Logs MSE/RMSE metrics throughout training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da549756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 7: TRAINING LOOP\n",
      "============================================================\n",
      "\n",
      "üì¶ Initializing model...\n",
      "‚úÖ Model initialized on cpu\n",
      "   ‚Ä¢ Player embeddings: 3994 √ó 50\n",
      "   ‚Ä¢ Opening embeddings: 2324 √ó 50\n",
      "   ‚Ä¢ Player ratings: 3994 (z-score normalized)\n",
      "   ‚Ä¢ ECO letters: 5 categories\n",
      "   ‚Ä¢ ECO numbers: 100 categories\n",
      "\n",
      "üîß Initializing optimizer...\n",
      "‚úÖ SGD optimizer initialized:\n",
      "   ‚Ä¢ Learning rate: 0.01\n",
      "   ‚Ä¢ Momentum: 0.9\n",
      "   ‚Ä¢ Weight decay: 0.0\n",
      "\n",
      "============================================================\n",
      "üöÄ STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Training configuration:\n",
      "   ‚Ä¢ Epochs: 20\n",
      "   ‚Ä¢ Batch size: 1024\n",
      "   ‚Ä¢ Training samples: 185,026\n",
      "   ‚Ä¢ Validation samples: 37,006\n",
      "   ‚Ä¢ Batches per epoch: 181\n",
      "\n",
      "============================================================\n",
      "TRAINING PROGRESS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/20\n",
      "============================================================\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.002000 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.002000 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 1 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001848\n",
      "   ‚Ä¢ Time: 3.51s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001748\n",
      "   ‚Ä¢ RMSE: 0.041806\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_001.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 2/20\n",
      "============================================================\n",
      "   Epoch 2 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001794 | ETA: 3.1s\n",
      "============================================================\n",
      "EPOCH 1 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001848\n",
      "   ‚Ä¢ Time: 3.51s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001748\n",
      "   ‚Ä¢ RMSE: 0.041806\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_001.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 2/20\n",
      "============================================================\n",
      "   Epoch 2 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001840 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 2 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001840 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 2 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001793\n",
      "   ‚Ä¢ Time: 3.03s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001743\n",
      "   ‚Ä¢ RMSE: 0.041744\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_002.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 3/20\n",
      "============================================================\n",
      "   Epoch 3 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001808 | ETA: 2.7s\n",
      "============================================================\n",
      "EPOCH 2 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001793\n",
      "   ‚Ä¢ Time: 3.03s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001743\n",
      "   ‚Ä¢ RMSE: 0.041744\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_002.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 3/20\n",
      "============================================================\n",
      "   Epoch 3 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001798 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 3 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001798 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 3 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001788\n",
      "   ‚Ä¢ Time: 2.97s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001738\n",
      "   ‚Ä¢ RMSE: 0.041693\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_003.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 4/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 3 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001788\n",
      "   ‚Ä¢ Time: 2.97s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001738\n",
      "   ‚Ä¢ RMSE: 0.041693\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_003.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 4/20\n",
      "============================================================\n",
      "   Epoch 4 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.002113 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 4 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.002113 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 4 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001784\n",
      "   ‚Ä¢ Time: 2.95s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001734\n",
      "   ‚Ä¢ RMSE: 0.041642\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_004.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 5/20\n",
      "============================================================\n",
      "   Epoch 5 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.002038 | ETA: 2.3s\n",
      "============================================================\n",
      "EPOCH 4 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001784\n",
      "   ‚Ä¢ Time: 2.95s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001734\n",
      "   ‚Ä¢ RMSE: 0.041642\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_004.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 5/20\n",
      "============================================================\n",
      "   Epoch 5 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001664 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 5 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001664 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 5 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001779\n",
      "   ‚Ä¢ Time: 2.81s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001730\n",
      "   ‚Ä¢ RMSE: 0.041592\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_005.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 6/20\n",
      "============================================================\n",
      "   Epoch 6 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001655 | ETA: 2.4s\n",
      "============================================================\n",
      "EPOCH 5 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001779\n",
      "   ‚Ä¢ Time: 2.81s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001730\n",
      "   ‚Ä¢ RMSE: 0.041592\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_005.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 6/20\n",
      "============================================================\n",
      "   Epoch 6 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001724 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 6 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001724 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 6 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001775\n",
      "   ‚Ä¢ Time: 2.84s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001726\n",
      "   ‚Ä¢ RMSE: 0.041543\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_006.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 7/20\n",
      "============================================================\n",
      "   Epoch 7 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001948 | ETA: 2.5s\n",
      "============================================================\n",
      "EPOCH 6 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001775\n",
      "   ‚Ä¢ Time: 2.84s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001726\n",
      "   ‚Ä¢ RMSE: 0.041543\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_006.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 7/20\n",
      "============================================================\n",
      "   Epoch 7 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001593 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 7 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001593 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 7 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001771\n",
      "   ‚Ä¢ Time: 2.93s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001722\n",
      "   ‚Ä¢ RMSE: 0.041496\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_007.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 8/20\n",
      "============================================================\n",
      "   Epoch 8 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001873 | ETA: 2.3s\n",
      "============================================================\n",
      "EPOCH 7 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001771\n",
      "   ‚Ä¢ Time: 2.93s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001722\n",
      "   ‚Ä¢ RMSE: 0.041496\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_007.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 8/20\n",
      "============================================================\n",
      "   Epoch 8 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001742 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 8 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001742 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 8 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001767\n",
      "   ‚Ä¢ Time: 2.93s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001718\n",
      "   ‚Ä¢ RMSE: 0.041450\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_008.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 9/20\n",
      "============================================================\n",
      "   Epoch 9 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001805 | ETA: 2.3s\n",
      "============================================================\n",
      "EPOCH 8 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001767\n",
      "   ‚Ä¢ Time: 2.93s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001718\n",
      "   ‚Ä¢ RMSE: 0.041450\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_008.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 9/20\n",
      "============================================================\n",
      "   Epoch 9 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001729 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 9 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001729 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 9 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001764\n",
      "   ‚Ä¢ Time: 2.83s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001714\n",
      "   ‚Ä¢ RMSE: 0.041404\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_009.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 10/20\n",
      "============================================================\n",
      "   Epoch 10 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001902 | ETA: 2.4s\n",
      "============================================================\n",
      "EPOCH 9 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001764\n",
      "   ‚Ä¢ Time: 2.83s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001714\n",
      "   ‚Ä¢ RMSE: 0.041404\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_009.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 10/20\n",
      "============================================================\n",
      "   Epoch 10 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001898 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 10 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001898 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 10 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001760\n",
      "   ‚Ä¢ Time: 2.78s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001711\n",
      "   ‚Ä¢ RMSE: 0.041360\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_010.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 11/20\n",
      "============================================================\n",
      "   Epoch 11 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001707 | ETA: 2.4s\n",
      "============================================================\n",
      "EPOCH 10 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001760\n",
      "   ‚Ä¢ Time: 2.78s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001711\n",
      "   ‚Ä¢ RMSE: 0.041360\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_010.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 11/20\n",
      "============================================================\n",
      "   Epoch 11 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001727 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 11 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001727 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 11 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001756\n",
      "   ‚Ä¢ Time: 2.82s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001707\n",
      "   ‚Ä¢ RMSE: 0.041317\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_011.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 12/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 11 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001756\n",
      "   ‚Ä¢ Time: 2.82s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001707\n",
      "   ‚Ä¢ RMSE: 0.041317\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_011.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 12/20\n",
      "============================================================\n",
      "   Epoch 12 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001767 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 12 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001767 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 12 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001752\n",
      "   ‚Ä¢ Time: 3.00s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001704\n",
      "   ‚Ä¢ RMSE: 0.041275\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_012.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 13/20\n",
      "============================================================\n",
      "   Epoch 13 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001756 | ETA: 2.6s\n",
      "============================================================\n",
      "EPOCH 12 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001752\n",
      "   ‚Ä¢ Time: 3.00s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001704\n",
      "   ‚Ä¢ RMSE: 0.041275\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_012.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 13/20\n",
      "============================================================\n",
      "   Epoch 13 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001799 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 13 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001799 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 13 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001748\n",
      "   ‚Ä¢ Time: 3.02s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001700\n",
      "   ‚Ä¢ RMSE: 0.041233\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_013.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 14/20\n",
      "============================================================\n",
      "   Epoch 14 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001800 | ETA: 2.3s\n",
      "============================================================\n",
      "EPOCH 13 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001748\n",
      "   ‚Ä¢ Time: 3.02s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001700\n",
      "   ‚Ä¢ RMSE: 0.041233\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_013.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 14/20\n",
      "============================================================\n",
      "   Epoch 14 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001655 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 14 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001655 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 14 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001745\n",
      "   ‚Ä¢ Time: 2.96s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001697\n",
      "   ‚Ä¢ RMSE: 0.041192\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_014.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 15/20\n",
      "============================================================\n",
      "   Epoch 15 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001641 | ETA: 2.4s\n",
      "============================================================\n",
      "EPOCH 14 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001745\n",
      "   ‚Ä¢ Time: 2.96s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001697\n",
      "   ‚Ä¢ RMSE: 0.041192\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_014.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 15/20\n",
      "============================================================\n",
      "   Epoch 15 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001770 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 15 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001770 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 15 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001742\n",
      "   ‚Ä¢ Time: 3.23s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001693\n",
      "   ‚Ä¢ RMSE: 0.041152\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_015.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 16/20\n",
      "============================================================\n",
      "   Epoch 16 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001644 | ETA: 3.0s\n",
      "============================================================\n",
      "EPOCH 15 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001742\n",
      "   ‚Ä¢ Time: 3.23s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001693\n",
      "   ‚Ä¢ RMSE: 0.041152\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_015.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 16/20\n",
      "============================================================\n",
      "   Epoch 16 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001690 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 16 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001690 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 16 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001738\n",
      "   ‚Ä¢ Time: 3.12s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001690\n",
      "   ‚Ä¢ RMSE: 0.041113\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_016.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 17/20\n",
      "============================================================\n",
      "   Epoch 17 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001758 | ETA: 3.0s\n",
      "============================================================\n",
      "EPOCH 16 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001738\n",
      "   ‚Ä¢ Time: 3.12s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001690\n",
      "   ‚Ä¢ RMSE: 0.041113\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_016.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 17/20\n",
      "============================================================\n",
      "   Epoch 17 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001888 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 17 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001888 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 17 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001735\n",
      "   ‚Ä¢ Time: 3.13s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001687\n",
      "   ‚Ä¢ RMSE: 0.041074\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_017.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 18/20\n",
      "============================================================\n",
      "   Epoch 18 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001754 | ETA: 2.6s\n",
      "============================================================\n",
      "EPOCH 17 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001735\n",
      "   ‚Ä¢ Time: 3.13s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001687\n",
      "   ‚Ä¢ RMSE: 0.041074\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_017.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 18/20\n",
      "============================================================\n",
      "   Epoch 18 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.002030 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 18 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.002030 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 18 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001732\n",
      "   ‚Ä¢ Time: 3.36s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001684\n",
      "   ‚Ä¢ RMSE: 0.041036\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_018.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 19/20\n",
      "============================================================\n",
      "   Epoch 19 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001792 | ETA: 2.5s\n",
      "============================================================\n",
      "EPOCH 18 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001732\n",
      "   ‚Ä¢ Time: 3.36s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001684\n",
      "   ‚Ä¢ RMSE: 0.041036\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_018.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 19/20\n",
      "============================================================\n",
      "   Epoch 19 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.002024 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 19 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.002024 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 19 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001729\n",
      "   ‚Ä¢ Time: 2.99s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001681\n",
      "   ‚Ä¢ RMSE: 0.040999\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_019.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 20/20\n",
      "============================================================\n",
      "   Epoch 20 [‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   5.52% | Loss: 0.001859 | ETA: 2.6s\n",
      "============================================================\n",
      "EPOCH 19 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001729\n",
      "   ‚Ä¢ Time: 2.99s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001681\n",
      "   ‚Ä¢ RMSE: 0.040999\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_019.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 20/20\n",
      "============================================================\n",
      "   Epoch 20 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001723 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 20 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001723 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 20 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001725\n",
      "   ‚Ä¢ Time: 3.02s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001678\n",
      "   ‚Ä¢ RMSE: 0.040962\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_020.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Training Summary:\n",
      "   ‚Ä¢ Total epochs: 20\n",
      "   ‚Ä¢ Best epoch: 20\n",
      "   ‚Ä¢ Best validation RMSE: 0.040962\n",
      "   ‚Ä¢ Final validation RMSE: 0.040962\n",
      "\n",
      "‚è±Ô∏è  Training Time:\n",
      "   ‚Ä¢ Total: 60.25s (1.00m)\n",
      "   ‚Ä¢ Average per epoch: 3.01s\n",
      "\n",
      "üìà Training History:\n",
      " epoch  train_loss  train_time  val_mse  val_rmse  learning_rate\n",
      "     1    0.001848    3.514738 0.001748  0.041806           0.01\n",
      "     2    0.001793    3.033746 0.001743  0.041744           0.01\n",
      "     3    0.001788    2.969576 0.001738  0.041693           0.01\n",
      "     4    0.001784    2.949562 0.001734  0.041642           0.01\n",
      "     5    0.001779    2.813230 0.001730  0.041592           0.01\n",
      "     6    0.001775    2.838481 0.001726  0.041543           0.01\n",
      "     7    0.001771    2.934857 0.001722  0.041496           0.01\n",
      "     8    0.001767    2.926379 0.001718  0.041450           0.01\n",
      "     9    0.001764    2.833801 0.001714  0.041404           0.01\n",
      "    10    0.001760    2.783923 0.001711  0.041360           0.01\n",
      "    11    0.001756    2.824422 0.001707  0.041317           0.01\n",
      "    12    0.001752    2.999167 0.001704  0.041275           0.01\n",
      "    13    0.001748    3.023629 0.001700  0.041233           0.01\n",
      "    14    0.001745    2.956601 0.001697  0.041192           0.01\n",
      "    15    0.001742    3.232713 0.001693  0.041152           0.01\n",
      "    16    0.001738    3.121931 0.001690  0.041113           0.01\n",
      "    17    0.001735    3.128080 0.001687  0.041074           0.01\n",
      "    18    0.001732    3.359241 0.001684  0.041036           0.01\n",
      "    19    0.001729    2.986165 0.001681  0.040999           0.01\n",
      "    20    0.001725    3.022620 0.001678  0.040962           0.01\n",
      "\n",
      "============================================================\n",
      "üéØ Next Step: Evaluate on test set (Step 8)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 20 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001725\n",
      "   ‚Ä¢ Time: 3.02s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001678\n",
      "   ‚Ä¢ RMSE: 0.040962\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_020.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Training Summary:\n",
      "   ‚Ä¢ Total epochs: 20\n",
      "   ‚Ä¢ Best epoch: 20\n",
      "   ‚Ä¢ Best validation RMSE: 0.040962\n",
      "   ‚Ä¢ Final validation RMSE: 0.040962\n",
      "\n",
      "‚è±Ô∏è  Training Time:\n",
      "   ‚Ä¢ Total: 60.25s (1.00m)\n",
      "   ‚Ä¢ Average per epoch: 3.01s\n",
      "\n",
      "üìà Training History:\n",
      " epoch  train_loss  train_time  val_mse  val_rmse  learning_rate\n",
      "     1    0.001848    3.514738 0.001748  0.041806           0.01\n",
      "     2    0.001793    3.033746 0.001743  0.041744           0.01\n",
      "     3    0.001788    2.969576 0.001738  0.041693           0.01\n",
      "     4    0.001784    2.949562 0.001734  0.041642           0.01\n",
      "     5    0.001779    2.813230 0.001730  0.041592           0.01\n",
      "     6    0.001775    2.838481 0.001726  0.041543           0.01\n",
      "     7    0.001771    2.934857 0.001722  0.041496           0.01\n",
      "     8    0.001767    2.926379 0.001718  0.041450           0.01\n",
      "     9    0.001764    2.833801 0.001714  0.041404           0.01\n",
      "    10    0.001760    2.783923 0.001711  0.041360           0.01\n",
      "    11    0.001756    2.824422 0.001707  0.041317           0.01\n",
      "    12    0.001752    2.999167 0.001704  0.041275           0.01\n",
      "    13    0.001748    3.023629 0.001700  0.041233           0.01\n",
      "    14    0.001745    2.956601 0.001697  0.041192           0.01\n",
      "    15    0.001742    3.232713 0.001693  0.041152           0.01\n",
      "    16    0.001738    3.121931 0.001690  0.041113           0.01\n",
      "    17    0.001735    3.128080 0.001687  0.041074           0.01\n",
      "    18    0.001732    3.359241 0.001684  0.041036           0.01\n",
      "    19    0.001729    2.986165 0.001681  0.040999           0.01\n",
      "    20    0.001725    3.022620 0.001678  0.040962           0.01\n",
      "\n",
      "============================================================\n",
      "üéØ Next Step: Evaluate on test set (Step 8)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Training Loop\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 7: TRAINING LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Initialize Model\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüì¶ Initializing model...\")\n",
    "model = ChessOpeningRecommender(\n",
    "    num_players=NUM_PLAYERS,\n",
    "    num_openings=NUM_OPENINGS,\n",
    "    num_factors=NUM_FACTORS,\n",
    "    player_ratings=player_ratings_tensor,\n",
    "    opening_eco_letters=opening_eco_letter_tensor,\n",
    "    opening_eco_numbers=opening_eco_number_tensor,\n",
    "    num_eco_letters=NUM_ECO_LETTERS,\n",
    "    num_eco_numbers=NUM_ECO_NUMBERS\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"‚úÖ Model initialized on {DEVICE}\")\n",
    "print(f\"   ‚Ä¢ Player embeddings: {NUM_PLAYERS} √ó {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ Opening embeddings: {NUM_OPENINGS} √ó {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ Player ratings: {NUM_PLAYERS} (z-score normalized)\")\n",
    "print(f\"   ‚Ä¢ ECO letters: {NUM_ECO_LETTERS} categories\")\n",
    "print(f\"   ‚Ä¢ ECO numbers: {NUM_ECO_NUMBERS} categories\")\n",
    "\n",
    "# ========================================\n",
    "# Initialize Optimizer\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüîß Initializing optimizer...\")\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ SGD optimizer initialized:\")\n",
    "print(f\"   ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ Momentum: {MOMENTUM}\")\n",
    "print(f\"   ‚Ä¢ Weight decay: {WEIGHT_DECAY}\")\n",
    "\n",
    "# ========================================\n",
    "# Training Loop\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"   ‚Ä¢ Epochs: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   ‚Ä¢ Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   ‚Ä¢ Batches per epoch: {len(train_loader):,}\")\n",
    "\n",
    "# Track best validation RMSE for early stopping info\n",
    "best_val_rmse = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "# Training history\n",
    "training_history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_time': [],\n",
    "    'val_mse': [],\n",
    "    'val_rmse': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING PROGRESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch}/{N_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_time = train_one_epoch(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=DEVICE,\n",
    "        epoch_num=epoch\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(f\"\\nüìä Evaluating on validation set...\")\n",
    "    val_mse, val_rmse = evaluate_model(\n",
    "        model=model,\n",
    "        data_loader=val_loader,\n",
    "        device=DEVICE,\n",
    "        dataset_name=\"Validation\"\n",
    "    )\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch} SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training:\")\n",
    "    print(f\"   ‚Ä¢ Average loss: {train_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Time: {train_time:.2f}s\")\n",
    "    print(f\"Validation:\")\n",
    "    print(f\"   ‚Ä¢ MSE: {val_mse:.6f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE: {val_rmse:.6f}\")\n",
    "    \n",
    "    # Track best model\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        print(f\"   üåü New best validation RMSE!\")\n",
    "    \n",
    "    # Store history\n",
    "    training_history['epoch'].append(epoch)\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['train_time'].append(train_time)\n",
    "    training_history['val_mse'].append(val_mse)\n",
    "    training_history['val_rmse'].append(val_rmse)\n",
    "    training_history['learning_rate'].append(LEARNING_RATE)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_filename = f\"checkpoint_epoch_{epoch:03d}.pt\"\n",
    "    checkpoint_path = MODEL_SAVE_DIR / checkpoint_filename\n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        train_loss=train_loss,\n",
    "        val_loss=val_mse,\n",
    "        filepath=checkpoint_path\n",
    "    )\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ========================================\n",
    "# Training Complete\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"   ‚Ä¢ Total epochs: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ Best epoch: {best_epoch}\")\n",
    "print(f\"   ‚Ä¢ Best validation RMSE: {best_val_rmse:.6f}\")\n",
    "print(f\"   ‚Ä¢ Final validation RMSE: {training_history['val_rmse'][-1]:.6f}\")\n",
    "\n",
    "# Calculate total training time\n",
    "total_training_time = sum(training_history['train_time'])\n",
    "print(f\"\\n‚è±Ô∏è  Training Time:\")\n",
    "print(f\"   ‚Ä¢ Total: {total_training_time:.2f}s ({total_training_time/60:.2f}m)\")\n",
    "print(f\"   ‚Ä¢ Average per epoch: {total_training_time/N_EPOCHS:.2f}s\")\n",
    "\n",
    "# Convert history to DataFrame for easy viewing\n",
    "import pandas as pd\n",
    "history_df = pd.DataFrame(training_history)\n",
    "print(f\"\\nüìà Training History:\")\n",
    "print(history_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Next Step: Evaluate on test set (Step 8)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc479f",
   "metadata": {},
   "source": [
    "## Step 8: Evaluation on Test Set\n",
    "\n",
    "Now that training is complete, we'll evaluate the final model on the held-out test set to get an unbiased estimate of model performance. We'll also:\n",
    "- Calculate MSE and RMSE metrics\n",
    "- Analyze prediction accuracy across different rating ranges\n",
    "- Examine predicted vs actual scores\n",
    "- Identify best and worst predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d8f0315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 8: EVALUATION ON TEST SET\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating model on test set...\n",
      "\n",
      "============================================================\n",
      "TEST SET RESULTS\n",
      "============================================================\n",
      "   ‚Ä¢ MSE: 0.001708\n",
      "   ‚Ä¢ RMSE: 0.041334\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "Final Training Loss:   0.001725\n",
      "Final Validation RMSE: 0.040962\n",
      "Test RMSE:             0.041334\n",
      "\n",
      "‚úÖ Test and validation RMSE are similar (within 10%)\n",
      "   Model generalizes appropriately.\n",
      "\n",
      "============================================================\n",
      "GENERATING PREDICTIONS FOR ANALYSIS\n",
      "============================================================\n",
      "‚úÖ Generated 24,671 predictions\n",
      "\n",
      "============================================================\n",
      "PREDICTION STATISTICS\n",
      "============================================================\n",
      "\n",
      "Prediction Range:\n",
      "   ‚Ä¢ Min: 0.4929\n",
      "   ‚Ä¢ Max: 0.5436\n",
      "   ‚Ä¢ Mean: 0.5145\n",
      "   ‚Ä¢ Std: 0.0041\n",
      "\n",
      "Actual Score Range:\n",
      "   ‚Ä¢ Min: 0.2128\n",
      "   ‚Ä¢ Max: 0.8018\n",
      "   ‚Ä¢ Mean: 0.5125\n",
      "   ‚Ä¢ Std: 0.0421\n",
      "\n",
      "Prediction Errors:\n",
      "   ‚Ä¢ Mean Error: 0.002038\n",
      "   ‚Ä¢ Mean Absolute Error: 0.032250\n",
      "   ‚Ä¢ Median Absolute Error: 0.026525\n",
      "   ‚Ä¢ Max Error: 0.302513\n",
      "\n",
      "============================================================\n",
      "ERROR DISTRIBUTION\n",
      "============================================================\n",
      "\n",
      "Predictions within error threshold:\n",
      "   ‚Ä¢ ¬±0.01: 5,080 (20.6%)\n",
      "   ‚Ä¢ ¬±0.02: 9,638 (39.1%)\n",
      "   ‚Ä¢ ¬±0.05: 19,444 (78.8%)\n",
      "   ‚Ä¢ ¬±0.10: 24,191 (98.1%)\n",
      "   ‚Ä¢ ¬±0.15: 24,619 (99.8%)\n",
      "\n",
      "============================================================\n",
      "BEST AND WORST PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "üéØ Top 10 Best Predictions (lowest error):\n",
      " Player ID   Opening ID   Actual  Predicted    Error\n",
      "------------------------------------------------------------\n",
      "      1404          530   0.5176     0.5176   0.0000\n",
      "       757         1137   0.5179     0.5178  -0.0000\n",
      "      3249          376   0.5151     0.5151  -0.0000\n",
      "       693          142   0.5107     0.5106  -0.0000\n",
      "      3070          500   0.5161     0.5161  -0.0000\n",
      "      3512          689   0.5142     0.5142   0.0000\n",
      "      2741          447   0.5137     0.5137   0.0000\n",
      "      1722          278   0.5141     0.5141  -0.0000\n",
      "      2110           12   0.5060     0.5060  -0.0000\n",
      "        34          216   0.5144     0.5144  -0.0000\n",
      "\n",
      "‚ùå Top 10 Worst Predictions (highest error):\n",
      " Player ID   Opening ID   Actual  Predicted    Error\n",
      "------------------------------------------------------------\n",
      "      3701         1143   0.2128     0.5153   0.3025\n",
      "       289         1545   0.8018     0.5108  -0.2909\n",
      "      1432         2180   0.2776     0.5147   0.2372\n",
      "      2936         2195   0.7349     0.5159  -0.2190\n",
      "      2519         1944   0.7368     0.5209  -0.2160\n",
      "      3821         1096   0.7316     0.5162  -0.2154\n",
      "       772         1970   0.3000     0.5151   0.2151\n",
      "      3293         2082   0.3002     0.5132   0.2130\n",
      "       518           97   0.7273     0.5143  -0.2129\n",
      "        11         1193   0.7212     0.5154  -0.2058\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE BY SCORE RANGE\n",
      "============================================================\n",
      "\n",
      "               Range    Count   Mean Error      MAE     RMSE\n",
      "--------------------------------------------------------------------\n",
      "       Low (0.0-0.3)        2     0.269850 0.269850 0.271820\n",
      " Below Avg (0.3-0.4)      115     0.133821 0.133821 0.135590\n",
      "   Average (0.4-0.5)    9,246     0.041063 0.041064 0.046273\n",
      " Above Avg (0.5-0.6)   14,784    -0.019617 0.023318 0.030786\n",
      "      High (0.6-1.0)      524    -0.105531 0.105531 0.108442\n",
      "\n",
      "============================================================\n",
      "‚úÖ EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Final Test Set Metrics:\n",
      "   ‚Ä¢ Test MSE: 0.001708\n",
      "   ‚Ä¢ Test RMSE: 0.041334\n",
      "   ‚Ä¢ Mean Absolute Error: 0.032250\n",
      "   ‚Ä¢ Predictions within ¬±0.05: 78.8%\n",
      "\n",
      "üí° Interpretation:\n",
      "   ‚Ä¢ RMSE of 0.0413 means average prediction error is ~0.0413\n",
      "   ‚Ä¢ For a player with true win rate of 0.50, model typically predicts within ¬±0.0413\n",
      "   ‚Ä¢ This is equivalent to ¬±4.13 percentage points\n",
      "\n",
      "============================================================\n",
      "üéØ Next: Save model and create inference pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Evaluation on Test Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 8: EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Evaluate on Test Set\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüìä Evaluating model on test set...\")\n",
    "test_mse, test_rmse = evaluate_model(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    device=DEVICE,\n",
    "    dataset_name=\"Test\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   ‚Ä¢ MSE: {test_mse:.6f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {test_rmse:.6f}\")\n",
    "\n",
    "# ========================================\n",
    "# Compare with Training and Validation\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_train_loss = training_history['train_loss'][-1]\n",
    "final_val_rmse = training_history['val_rmse'][-1]\n",
    "\n",
    "print(f\"\\nFinal Training Loss:   {final_train_loss:.6f}\")\n",
    "print(f\"Final Validation RMSE: {final_val_rmse:.6f}\")\n",
    "print(f\"Test RMSE:             {test_rmse:.6f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "if test_rmse > final_val_rmse * 1.1:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Test RMSE is {(test_rmse/final_val_rmse - 1)*100:.1f}% higher than validation RMSE\")\n",
    "    print(f\"   This suggests possible overfitting.\")\n",
    "elif test_rmse < final_val_rmse * 0.9:\n",
    "    print(f\"\\n‚úÖ Test RMSE is {(1 - test_rmse/final_val_rmse)*100:.1f}% lower than validation RMSE\")\n",
    "    print(f\"   Model generalizes well!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Test and validation RMSE are similar (within 10%)\")\n",
    "    print(f\"   Model generalizes appropriately.\")\n",
    "\n",
    "# ========================================\n",
    "# Get Predictions for Analysis\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING PREDICTIONS FOR ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "all_player_ids = []\n",
    "all_opening_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        player_ids = batch['player_id'].to(DEVICE)\n",
    "        opening_ids = batch['opening_id'].to(DEVICE)\n",
    "        targets = batch['score'].to(DEVICE)\n",
    "        \n",
    "        predictions = model(player_ids, opening_ids)\n",
    "        \n",
    "        all_predictions.extend(predictions.detach().cpu().tolist())\n",
    "        all_actuals.extend(targets.detach().cpu().tolist())\n",
    "        all_player_ids.extend(player_ids.detach().cpu().tolist())\n",
    "        all_opening_ids.extend(opening_ids.detach().cpu().tolist())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_actuals = np.array(all_actuals)\n",
    "all_player_ids = np.array(all_player_ids)\n",
    "all_opening_ids = np.array(all_opening_ids)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(all_predictions):,} predictions\")\n",
    "\n",
    "# ========================================\n",
    "# Prediction Statistics\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "errors = all_predictions - all_actuals\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "print(f\"\\nPrediction Range:\")\n",
    "print(f\"   ‚Ä¢ Min: {all_predictions.min():.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {all_predictions.max():.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {all_predictions.mean():.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {all_predictions.std():.4f}\")\n",
    "\n",
    "print(f\"\\nActual Score Range:\")\n",
    "print(f\"   ‚Ä¢ Min: {all_actuals.min():.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {all_actuals.max():.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {all_actuals.mean():.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {all_actuals.std():.4f}\")\n",
    "\n",
    "print(f\"\\nPrediction Errors:\")\n",
    "print(f\"   ‚Ä¢ Mean Error: {errors.mean():.6f}\")\n",
    "print(f\"   ‚Ä¢ Mean Absolute Error: {abs_errors.mean():.6f}\")\n",
    "print(f\"   ‚Ä¢ Median Absolute Error: {np.median(abs_errors):.6f}\")\n",
    "print(f\"   ‚Ä¢ Max Error: {abs_errors.max():.6f}\")\n",
    "\n",
    "# ========================================\n",
    "# Error Distribution\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ERROR DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count predictions within various error thresholds\n",
    "thresholds = [0.01, 0.02, 0.05, 0.10, 0.15]\n",
    "print(f\"\\nPredictions within error threshold:\")\n",
    "for threshold in thresholds:\n",
    "    count = np.sum(abs_errors <= threshold)\n",
    "    pct = 100.0 * count / len(abs_errors)\n",
    "    print(f\"   ‚Ä¢ ¬±{threshold:.2f}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# ========================================\n",
    "# Best and Worst Predictions\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST AND WORST PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sort by absolute error\n",
    "sorted_indices = np.argsort(abs_errors)\n",
    "\n",
    "print(f\"\\nüéØ Top 10 Best Predictions (lowest error):\")\n",
    "print(f\"{'Player ID':>10} {'Opening ID':>12} {'Actual':>8} {'Predicted':>10} {'Error':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(min(10, len(sorted_indices))):\n",
    "    idx = sorted_indices[i]\n",
    "    print(f\"{all_player_ids[idx]:>10.0f} {all_opening_ids[idx]:>12.0f} \"\n",
    "          f\"{all_actuals[idx]:>8.4f} {all_predictions[idx]:>10.4f} \"\n",
    "          f\"{errors[idx]:>8.4f}\")\n",
    "\n",
    "print(f\"\\n‚ùå Top 10 Worst Predictions (highest error):\")\n",
    "print(f\"{'Player ID':>10} {'Opening ID':>12} {'Actual':>8} {'Predicted':>10} {'Error':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(min(10, len(sorted_indices))):\n",
    "    idx = sorted_indices[-(i+1)]\n",
    "    print(f\"{all_player_ids[idx]:>10.0f} {all_opening_ids[idx]:>12.0f} \"\n",
    "          f\"{all_actuals[idx]:>8.4f} {all_predictions[idx]:>10.4f} \"\n",
    "          f\"{errors[idx]:>8.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# Analyze by Score Range\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE BY SCORE RANGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "score_ranges = [\n",
    "    (0.0, 0.3, \"Low (0.0-0.3)\"),\n",
    "    (0.3, 0.4, \"Below Avg (0.3-0.4)\"),\n",
    "    (0.4, 0.5, \"Average (0.4-0.5)\"),\n",
    "    (0.5, 0.6, \"Above Avg (0.5-0.6)\"),\n",
    "    (0.6, 1.0, \"High (0.6-1.0)\")\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Range':>20} {'Count':>8} {'Mean Error':>12} {'MAE':>8} {'RMSE':>8}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "for low, high, label in score_ranges:\n",
    "    mask = (all_actuals >= low) & (all_actuals < high)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    range_errors = errors[mask]\n",
    "    range_abs_errors = abs_errors[mask]\n",
    "    range_rmse = np.sqrt(np.mean(range_errors ** 2))\n",
    "    \n",
    "    print(f\"{label:>20} {mask.sum():>8,} {range_errors.mean():>12.6f} \"\n",
    "          f\"{range_abs_errors.mean():>8.6f} {range_rmse:>8.6f}\")\n",
    "\n",
    "# ========================================\n",
    "# Summary\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Final Test Set Metrics:\")\n",
    "print(f\"   ‚Ä¢ Test MSE: {test_mse:.6f}\")\n",
    "print(f\"   ‚Ä¢ Test RMSE: {test_rmse:.6f}\")\n",
    "print(f\"   ‚Ä¢ Mean Absolute Error: {abs_errors.mean():.6f}\")\n",
    "print(f\"   ‚Ä¢ Predictions within ¬±0.05: {100.0 * np.sum(abs_errors <= 0.05) / len(abs_errors):.1f}%\")\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   ‚Ä¢ RMSE of {test_rmse:.4f} means average prediction error is ~{test_rmse:.4f}\")\n",
    "print(f\"   ‚Ä¢ For a player with true win rate of 0.50, model typically predicts within ¬±{test_rmse:.4f}\")\n",
    "print(f\"   ‚Ä¢ This is equivalent to ¬±{test_rmse*100:.2f} percentage points\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Next: Save model and create inference pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
