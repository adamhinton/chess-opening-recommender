{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e163a7",
   "metadata": {},
   "source": [
    "# Notebook 26 ‚Äî Opening Recommender Model: Training Pipeline\n",
    "\n",
    "### 0. Overview and Goals\n",
    "\n",
    "This notebook defines the full pipeline for training the chess opening recommender model.  \n",
    "The objective is to predict **player‚Äìopening performance scores** ((wins + (0.5 * draws) / num games)) for openings a player hasn‚Äôt yet played, based on their results in the openings they *have* played.  \n",
    "\n",
    "The model will use **matrix factorization** with **stochastic gradient descent (SGD)** to learn latent factors representing player and opening characteristics.  \n",
    "All computations will be implemented in **PyTorch**, with data loaded from my local **DuckDB** database.\n",
    "\n",
    "**High-level specs:**\n",
    "- Use only *White* openings initially (we‚Äôll extend to Black later).  \n",
    "- Data source: processed player‚Äìopening stats from local DuckDB.  \n",
    "- Predict: normalized ‚Äúscore‚Äù = win rate ((wins + 0.5 x draws) / total games).  \n",
    "- Filter: only include entries with ‚â• `MIN_GAMES_THRESHOLD` (default = 50).  \n",
    "- Ignore: rating differences, time controls, and other metadata.  \n",
    "- Model parameters (to be defined in appropriate places for easy editing):  \n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_PLAYERS_TO_PROCESS`  \n",
    "- Logging and checkpoints throughout for reproducibility.  \n",
    "- All random operations seeded for deterministic runs.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Extraction\n",
    "- Connect to local DuckDB\n",
    "- Pull all processed player‚Äìopening statistics from\n",
    "- Verify schema consistency:  \n",
    "  - Required columns: `player_id`, `opening_id`, `eco`, `num_games`, `wins`, `draws`, `losses`.  \n",
    "- Include a row-count sanity check.\n",
    "- Only players with ratings above 1200\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Sanitization & Normalization\n",
    "- Optionally normalize scores if needed for MF convergence.  \n",
    "- Drop players with no qualifying openings and openings with no qualifying players.  \n",
    "  - I believe there shouldn't be any but we'll double check.\n",
    "- Resequence player_id and opening_id to be sequential integers - right now there are gaps because of entries we deleted from the DB \n",
    "- Check for sparsity consistency (no implicit zeros yet).  \n",
    "- Note that this data has already been split in to white and black games further up the pipeline\n",
    "\n",
    "### Data Quality\n",
    "- Drop entries with fewer than `MIN_GAMES_THRESHOLD` games\n",
    "- Handle any duplicate `(player_id, opening_id)` combinations\n",
    "- Remove players with no qualifying openings\n",
    "- Remove openings with no qualifying players\n",
    "- Verify no null values remain\n",
    "\n",
    "### ECO Codes\n",
    "- Keep ECO codes for later categorical encoding (Step 4)\n",
    "- ECO will be used as opening side information (similar to rating for players)\n",
    "\n",
    "### Confidence Weighting\n",
    "- Use `MIN_GAMES_THRESHOLD = 10` to keep more data\n",
    "- Add a **confidence weight** column: `confidence = num_games / (num_games + K)` where K ‚âà 50\n",
    "- This weight will be used in the loss function to down-weight uncertain predictions\n",
    "- High-game-count entries ‚Üí high confidence ‚Üí larger loss impact\n",
    "- Low-game-count entries ‚Üí low confidence ‚Üí smaller loss impact\n",
    "\n",
    "### Player Rating (Side Information)\n",
    "- **Player ratings are side information** - they describe player characteristics, not individual player-opening interactions\n",
    "- Ratings will be stored separately and joined to player embeddings during training\n",
    "- We'll **normalize ratings** (likely z-score normalization) to avoid scaling issues with the embedding layer\n",
    "- Rating normalization will be done once after extraction, not per-row\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Splits\n",
    "- Split into train/test/val sets.  \n",
    "- Ensure every player and every opening appears at least once in the training data.  \n",
    "- Strategy:  \n",
    "  - Sample unique players and openings to guarantee coverage in train.  \n",
    "  - Remaining data ‚Üí stratified random split into train/test.  \n",
    "  - Deduplicate and merge unique IDs back into train if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Enumerate Categorical Variables\n",
    "- Enumerate `eco` (if included) as an integer categorical variable.  \n",
    "- Confirm all columns are numeric and compatible with PyTorch tensors.  \n",
    "- Verify no missing or out-of-range IDs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Data Structure\n",
    "- Each row: one `(player_id, opening_id, score)` record.\n",
    "- Include other fields- eco, num games etc\n",
    "- Convert DataFrame to PyTorch tensors (`torch.long` for IDs, `torch.float` for scores).  \n",
    "- Log dataset shapes and sparsity metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Training Setup\n",
    "Define constants:\n",
    "- `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_FACTORS`  \n",
    "- Loss functions: MSE and RMSE  \n",
    "- Activation: sigmoid or none (depending on score normalization)  \n",
    "- Optimizer: SGD  \n",
    "- Figure out if there's anything else we need to design or specify\n",
    "\n",
    "Implement helper functions:\n",
    "- `train_one_epoch()`\n",
    "- `evaluate_model()`\n",
    "- `calculate_rmse()`\n",
    "- `save_checkpoint()`  \n",
    "\n",
    "Ensure detailed logging, ETA reporting, and reproducible random seeds.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Training Loop\n",
    "- Initialize player and opening embeddings.  \n",
    "- Iterate through epochs with mini-batch SGD (`BATCH_SIZE = 1024`).  \n",
    "- Compute and log MSE/RMSE per epoch.  \n",
    "- Save model checkpoints locally after each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Evaluation\n",
    "- Evaluate on test set.  \n",
    "- Report MSE, RMSE, and visual diagnostics (predicted vs actual score).  \n",
    "- Inspect a few player and opening latent factors for sanity.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Cross-Validation & Hyperparameter Tuning\n",
    "- Define ranges for:  \n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`  \n",
    "- Perform small-scale grid or random search for best configuration.  \n",
    "- Compare validation RMSE across runs.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Next Steps\n",
    "- Extend model to include Black openings.  \n",
    "- Experiment with hybrid inputs (player rating, ECO grouping).  \n",
    "- Consider implicit feedback handling (unplayed openings as zeros).  \n",
    "- Integrate trained model into API for recommendation output.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**  \n",
    "- Every random seed and parameter definition will be explicit.  \n",
    "- Every major step includes row-count, schema, and type validation.  \n",
    "- Model artifacts and logs will be saved locally for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d823",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "Connect to DuckDB and extract all player-opening statistics.\n",
    "Verify schema and perform sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcea569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA EXTRACTION\n",
      "============================================================\n",
      "\n",
      "üìÅ Database: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "üìÅ Database exists: True\n",
      "üé® Color filter: White\n",
      "üîí Minimum holdout players: 1,000\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path.cwd() / 'utils'))\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = Path.cwd().parent / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "COLOR_FILTER = 'w'  # 'w' for white, 'b' for black\n",
    "MIN_HOLDOUT_PLAYERS = 1000  # Minimum number of players to reserve for fold-in verification. These will not be used at all in this notebook for training, test/val or anything else.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: DATA EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìÅ Database: {DB_PATH}\")\n",
    "print(f\"üìÅ Database exists: {DB_PATH.exists()}\")\n",
    "print(f\"üé® Color filter: {'White' if COLOR_FILTER == 'w' else 'Black'}\")\n",
    "print(f\"üîí Minimum holdout players: {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9435033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£  Extracting player-opening statistics (color: 'w')...\n",
      "   ‚Ä¢ Minimum rating filter: 1200\n",
      "\n",
      "2Ô∏è‚É£  Selecting holdout players for fold-in verification...\n",
      "   ‚Ä¢ Holdout size: 1,000 players minimum\n",
      "   ‚Ä¢ Total eligible players: 15,000\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 14,000\n",
      "   ‚Ä¢ Holdout percentage: 6.7%\n",
      "\n",
      "3Ô∏è‚É£  Extracting training data (excluding holdout players)...\n",
      "   ‚Ä¢ Total eligible players: 15,000\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 14,000\n",
      "   ‚Ä¢ Holdout percentage: 6.7%\n",
      "\n",
      "3Ô∏è‚É£  Extracting training data (excluding holdout players)...\n",
      "   ‚úì Extracted 3,561,849 rows\n",
      "\n",
      "   üíæ Saved holdout_players_df with 1,000 player IDs\n",
      "   ‚Ä¢ These players are COMPLETELY UNSEEN by the training process\n",
      "   ‚Ä¢ Use them later for fold-in verification\n",
      "\n",
      "4Ô∏è‚É£  Verifying schema...\n",
      "   ‚úì All required columns present: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "5Ô∏è‚É£  Checking data types...\n",
      "   ‚Ä¢ player_id: int32\n",
      "   ‚Ä¢ opening_id: int32\n",
      "   ‚Ä¢ num_games: int32\n",
      "   ‚Ä¢ score: float64\n",
      "   ‚Ä¢ eco: object\n",
      "\n",
      "6Ô∏è‚É£  Data statistics...\n",
      "   ‚Ä¢ Total rows: 3,561,849\n",
      "   ‚Ä¢ Unique players: 14,000\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "   ‚Ä¢ Total games (sum): 67,967,224\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 3\n",
      "   ‚Ä¢ Max: 49989\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 11953\n",
      "   ‚Ä¢ Mean: 19.1\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5009\n",
      "   ‚úì Extracted 3,561,849 rows\n",
      "\n",
      "   üíæ Saved holdout_players_df with 1,000 player IDs\n",
      "   ‚Ä¢ These players are COMPLETELY UNSEEN by the training process\n",
      "   ‚Ä¢ Use them later for fold-in verification\n",
      "\n",
      "4Ô∏è‚É£  Verifying schema...\n",
      "   ‚úì All required columns present: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "5Ô∏è‚É£  Checking data types...\n",
      "   ‚Ä¢ player_id: int32\n",
      "   ‚Ä¢ opening_id: int32\n",
      "   ‚Ä¢ num_games: int32\n",
      "   ‚Ä¢ score: float64\n",
      "   ‚Ä¢ eco: object\n",
      "\n",
      "6Ô∏è‚É£  Data statistics...\n",
      "   ‚Ä¢ Total rows: 3,561,849\n",
      "   ‚Ä¢ Unique players: 14,000\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "   ‚Ä¢ Total games (sum): 67,967,224\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 3\n",
      "   ‚Ä¢ Max: 49989\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 11953\n",
      "   ‚Ä¢ Mean: 19.1\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5009\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0          3         144          3  0.666667  A02\n",
      "1          3         163          3  0.666667  A03\n",
      "2          3         207          1  0.000000  A05\n",
      "3          3         218          4  0.500000  A06\n",
      "4          3         222          1  0.000000  A07\n",
      "5          3         376         11  0.409091  A40\n",
      "6          3         382          7  0.857143  A40\n",
      "7          3         383          1  1.000000  A40\n",
      "8          3         385          2  0.000000  A40\n",
      "9          3         387          4  0.750000  A40\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (3561849, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: 1,000 players held out for fold-in verification\n",
      "   ‚Ä¢ Access via: holdout_players_df\n",
      "   ‚Ä¢ These players will NOT appear in any training, validation, or test splits\n",
      "\n",
      "‚úì Database connection closed\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0          3         144          3  0.666667  A02\n",
      "1          3         163          3  0.666667  A03\n",
      "2          3         207          1  0.000000  A05\n",
      "3          3         218          4  0.500000  A06\n",
      "4          3         222          1  0.000000  A07\n",
      "5          3         376         11  0.409091  A40\n",
      "6          3         382          7  0.857143  A40\n",
      "7          3         383          1  1.000000  A40\n",
      "8          3         385          2  0.000000  A40\n",
      "9          3         387          4  0.750000  A40\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (3561849, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: 1,000 players held out for fold-in verification\n",
      "   ‚Ä¢ Access via: holdout_players_df\n",
      "   ‚Ä¢ These players will NOT appear in any training, validation, or test splits\n",
      "\n",
      "‚úì Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# Connect to DuckDB and extract player-opening statistics\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "MAX_PLAYERS = 15_000 # for testing, will increase later\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player-opening statistics (color: '{COLOR_FILTER}')...\")\n",
    "    \n",
    "    # Extract stats with calculated score and num_games\n",
    "    # Filter by color, minimum rating, and calculate score in the database\n",
    "    MIN_RATING = 1200\n",
    "    print(f\"   ‚Ä¢ Minimum rating filter: {MIN_RATING}\")\n",
    "    \n",
    "    # First, get all eligible players and randomly select holdout set\n",
    "    print(f\"\\n2Ô∏è‚É£  Selecting holdout players for fold-in verification...\")\n",
    "    print(f\"   ‚Ä¢ Holdout size: {MIN_HOLDOUT_PLAYERS:,} players minimum\")\n",
    "    \n",
    "    # Get all players with sufficient data\n",
    "    player_query = f\"\"\"\n",
    "        SELECT DISTINCT p.id as player_id\n",
    "        FROM player p\n",
    "        JOIN player_opening_stats pos ON p.id = pos.player_id\n",
    "        WHERE p.rating >= {MIN_RATING}\n",
    "        AND pos.color = '{COLOR_FILTER}'\n",
    "        LIMIT {MAX_PLAYERS}\n",
    "    \"\"\"\n",
    "    \n",
    "    all_eligible_players = pd.DataFrame(con.execute(player_query).df())\n",
    "    total_eligible = len(all_eligible_players)\n",
    "    print(f\"   ‚Ä¢ Total eligible players: {total_eligible:,}\")\n",
    "    \n",
    "    if total_eligible < MIN_HOLDOUT_PLAYERS:\n",
    "        raise ValueError(f\"Not enough eligible players ({total_eligible:,}) to create holdout set of {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "    \n",
    "    # Randomly sample holdout players (deterministic with seed)\n",
    "    import numpy as np\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    holdout_player_ids = np.random.choice(\n",
    "        all_eligible_players['player_id'].values,\n",
    "        size=MIN_HOLDOUT_PLAYERS,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    training_player_ids = set(all_eligible_players['player_id'].values) - set(holdout_player_ids)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Holdout players selected: {len(holdout_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Training players available: {len(training_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Holdout percentage: {100 * len(holdout_player_ids) / total_eligible:.1f}%\")\n",
    "    \n",
    "    # Convert training player IDs to SQL-friendly string\n",
    "    training_player_ids_str = ','.join(map(str, training_player_ids))\n",
    "    \n",
    "    # Extract data ONLY for training players\n",
    "    print(f\"\\n3Ô∏è‚É£  Extracting training data (excluding holdout players)...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            pos.player_id,\n",
    "            pos.opening_id,\n",
    "            pos.num_wins + pos.num_draws + pos.num_losses as num_games,\n",
    "            (pos.num_wins + (pos.num_draws * 0.5)) / \n",
    "                NULLIF(pos.num_wins + pos.num_draws + pos.num_losses, 0) as score,\n",
    "            o.eco\n",
    "        FROM player_opening_stats pos\n",
    "        JOIN opening o ON pos.opening_id = o.id\n",
    "        JOIN player p ON pos.player_id = p.id\n",
    "        WHERE pos.color = '{COLOR_FILTER}'\n",
    "        AND p.rating >= {MIN_RATING}\n",
    "        AND pos.player_id IN ({training_player_ids_str})\n",
    "        ORDER BY pos.player_id, pos.opening_id\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_data = pd.DataFrame(con.execute(query).df())\n",
    "    \n",
    "    print(f\"   ‚úì Extracted {len(raw_data):,} rows\")\n",
    "    \n",
    "    # Also save holdout player IDs for later use\n",
    "    holdout_players_df = pd.DataFrame({'player_id': holdout_player_ids})\n",
    "    print(f\"\\n   üíæ Saved holdout_players_df with {len(holdout_players_df):,} player IDs\")\n",
    "    print(f\"   ‚Ä¢ These players are COMPLETELY UNSEEN by the training process\")\n",
    "    print(f\"   ‚Ä¢ Use them later for fold-in verification\")\n",
    "    \n",
    "    # Schema verification\n",
    "    print(\"\\n4Ô∏è‚É£  Verifying schema...\")\n",
    "    required_columns = ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
    "    \n",
    "    for col in required_columns:\n",
    "        if col not in raw_data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    print(f\"   ‚úì All required columns present: {required_columns}\")\n",
    "    \n",
    "    # Data types verification\n",
    "    print(\"\\n5Ô∏è‚É£  Checking data types...\")\n",
    "    print(f\"   ‚Ä¢ player_id: {raw_data['player_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ opening_id: {raw_data['opening_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ num_games: {raw_data['num_games'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ score: {raw_data['score'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ eco: {raw_data['eco'].dtype}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n6Ô∏è‚É£  Data statistics...\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {len(raw_data):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Total games (sum): {raw_data['num_games'].sum():,}\")\n",
    "    \n",
    "    # Player ID range\n",
    "    print(f\"\\n   Player ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['player_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['player_id'].max()}\")\n",
    "    \n",
    "    # Opening ID range\n",
    "    print(f\"\\n   Opening ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['opening_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['opening_id'].max()}\")\n",
    "    \n",
    "    # Games per entry statistics\n",
    "    print(f\"\\n   Games per entry:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['num_games'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['num_games'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['num_games'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['num_games'].median():.0f}\")\n",
    "    \n",
    "    # Score statistics\n",
    "    print(f\"\\n   Score distribution:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['score'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['score'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['score'].mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['score'].median():.4f}\")\n",
    "    \n",
    "    # Check for null values\n",
    "    print(\"\\n7Ô∏è‚É£  Checking for null values...\")\n",
    "    null_counts = raw_data.isnull().sum()\n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"   ‚úì No null values found\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(\"\\n8Ô∏è‚É£  Sample of extracted data (first 10 rows):\")\n",
    "    print(raw_data.head(10).to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nData shape: {raw_data.shape}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  IMPORTANT: {len(holdout_player_ids):,} players held out for fold-in verification\")\n",
    "    print(f\"   ‚Ä¢ Access via: holdout_players_df\")\n",
    "    print(f\"   ‚Ä¢ These players will NOT appear in any training, validation, or test splits\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"\\n‚úì Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad110b85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "519d8f2f",
   "metadata": {},
   "source": [
    "## Step 2: Data Sanitization & Normalization\n",
    "\n",
    "Filter low-quality data, handle duplicates, and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74f0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: DATA SANITIZATION & NORMALIZATION\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ MIN_GAMES_THRESHOLD: 10\n",
      "\n",
      "üìä Starting data shape: (3561849, 5)\n",
      "   ‚Ä¢ Rows: 3,561,849\n",
      "   ‚Ä¢ Unique players: 14,000\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Before: 3,561,849 rows\n",
      "   ‚Ä¢ After: 888,341 rows\n",
      "   ‚Ä¢ Filtered out: 2,673,508 rows (75.1%)\n",
      "\n",
      "2Ô∏è‚É£  Checking for duplicate (player_id, opening_id) combinations...\n",
      "   ‚úì No duplicates found\n",
      "\n",
      "3Ô∏è‚É£  Removing players with no qualifying openings...\n",
      "   ‚Ä¢ Players before: 13,990\n",
      "   ‚Ä¢ Players after: 13,990\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "4Ô∏è‚É£  Removing openings with no qualifying players...\n",
      "   ‚Ä¢ Openings before: 2,560\n",
      "   ‚Ä¢ Openings after: 2,560\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "5Ô∏è‚É£  Verifying no null values...\n",
      "   ‚úì No duplicates found\n",
      "\n",
      "3Ô∏è‚É£  Removing players with no qualifying openings...\n",
      "   ‚Ä¢ Players before: 13,990\n",
      "   ‚Ä¢ Players after: 13,990\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "4Ô∏è‚É£  Removing openings with no qualifying players...\n",
      "   ‚Ä¢ Openings before: 2,560\n",
      "   ‚Ä¢ Openings after: 2,560\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "5Ô∏è‚É£  Verifying no null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Final data statistics:\n",
      "   ‚Ä¢ Total rows: 888,341\n",
      "   ‚Ä¢ Unique players: 13,990\n",
      "   ‚Ä¢ Unique openings: 2,560\n",
      "   ‚Ä¢ Total games: 60,976,204\n",
      "   ‚Ä¢ Avg games per entry: 68.6\n",
      "   ‚Ä¢ Avg openings per player: 63.5\n",
      "   ‚Ä¢ Avg players per opening: 347.0\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5122\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5110\n",
      "   ‚Ä¢ Std: 0.1087\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "        player_id  opening_id  num_games     score  eco\n",
      "680597      36625        3204         66  0.522727  B01\n",
      "198094      10647        1642         15  0.666667  C30\n",
      "56059        3187        1538         24  0.375000  C21\n",
      "916            40        2066         18  0.666667  C50\n",
      "366161      19434         937         13  0.576923  B12\n",
      "577510      30959        1986        200  0.520000  C45\n",
      "685119      36818        1153         88  0.659091  B34\n",
      "572642      30722         445         20  0.400000  A43\n",
      "418286      22309         935         29  0.603448  B12\n",
      "399054      21360        2231         31  0.596774  C64\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (888341, 5)\n",
      "Data reduction: 75.1%\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Final data statistics:\n",
      "   ‚Ä¢ Total rows: 888,341\n",
      "   ‚Ä¢ Unique players: 13,990\n",
      "   ‚Ä¢ Unique openings: 2,560\n",
      "   ‚Ä¢ Total games: 60,976,204\n",
      "   ‚Ä¢ Avg games per entry: 68.6\n",
      "   ‚Ä¢ Avg openings per player: 63.5\n",
      "   ‚Ä¢ Avg players per opening: 347.0\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5122\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5110\n",
      "   ‚Ä¢ Std: 0.1087\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "        player_id  opening_id  num_games     score  eco\n",
      "680597      36625        3204         66  0.522727  B01\n",
      "198094      10647        1642         15  0.666667  C30\n",
      "56059        3187        1538         24  0.375000  C21\n",
      "916            40        2066         18  0.666667  C50\n",
      "366161      19434         937         13  0.576923  B12\n",
      "577510      30959        1986        200  0.520000  C45\n",
      "685119      36818        1153         88  0.659091  B34\n",
      "572642      30722         445         20  0.400000  A43\n",
      "418286      22309         935         29  0.603448  B12\n",
      "399054      21360        2231         31  0.596774  C64\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (888341, 5)\n",
      "Data reduction: 75.1%\n"
     ]
    }
   ],
   "source": [
    "# 2a. Filter low-quality data, handle duplicates, and prepare for training.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "MIN_GAMES_THRESHOLD = 10\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: DATA SANITIZATION & NORMALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   ‚Ä¢ MIN_GAMES_THRESHOLD: {MIN_GAMES_THRESHOLD}\")\n",
    "\n",
    "# Start with raw_data from Step 1\n",
    "print(f\"\\nüìä Starting data shape: {raw_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Rows: {len(raw_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "\n",
    "# 1. Filter by minimum games threshold\n",
    "print(f\"\\n1Ô∏è‚É£  Filtering entries with < {MIN_GAMES_THRESHOLD} games...\")\n",
    "before_filter = len(raw_data)\n",
    "clean_data = raw_data.query(f'num_games >= {MIN_GAMES_THRESHOLD}').copy()\n",
    "num_rows_after_filter = len(clean_data)\n",
    "num_rows_filtered_out = before_filter - num_rows_after_filter\n",
    "\n",
    "print(f\"   ‚Ä¢ Before: {before_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ After: {num_rows_after_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ Filtered out: {num_rows_filtered_out:,} rows ({100*num_rows_filtered_out/before_filter:.1f}%)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "print(f\"\\n2Ô∏è‚É£  Checking for duplicate (player_id, opening_id) combinations...\")\n",
    "num_duplicates = clean_data.duplicated(subset=['player_id', 'opening_id']).sum()\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {num_duplicates} duplicate entries\")\n",
    "    dup_mask = clean_data.duplicated(subset=['player_id', 'opening_id'], keep=False)\n",
    "    print(\"\\n   Sample of duplicates:\")\n",
    "    print(clean_data[dup_mask].head(10).to_string())\n",
    "    \n",
    "    # Keep only first occurrence of any duplicate player-opening pair\n",
    "    print(\"\\n   Removing duplicates (keeping first occurrence)...\")\n",
    "    clean_data = pd.DataFrame.drop_duplicates(clean_data, subset=['player_id', 'opening_id'], keep='first')\n",
    "    print(f\"   ‚úì After deduplication: {len(clean_data):,} rows\")\n",
    "else:\n",
    "    print(f\"   ‚úì No duplicates found\")\n",
    "\n",
    "# 3. Remove players with no qualifying openings\n",
    "print(f\"\\n3Ô∏è‚É£  Removing players with no qualifying openings...\") # Note that a few players only play stuff like the Van't Kruijs which we've excluded, so a small numer of players will be excluded here\n",
    "players_before = clean_data['player_id'].nunique()\n",
    "\n",
    "# Count openings per player\n",
    "num_openings_per_player = pd.DataFrame(clean_data.groupby('player_id').size(), columns=['count'])\n",
    "players_with_data = num_openings_per_player[num_openings_per_player['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter\n",
    "clean_data = clean_data[clean_data['player_id'].isin(players_with_data)]\n",
    "players_after = clean_data['player_id'].nunique()\n",
    "\n",
    "print(f\"   ‚Ä¢ Players before: {players_before:,}\")\n",
    "print(f\"   ‚Ä¢ Players after: {players_after:,}\")\n",
    "print(f\"   ‚Ä¢ Removed: {players_before - players_after}\")\n",
    "\n",
    "# 4. Remove openings with no qualifying players\n",
    "print(f\"\\n4Ô∏è‚É£  Removing openings with no qualifying players...\")\n",
    "num_openings_before = clean_data['opening_id'].nunique()\n",
    "\n",
    "# Use pd.DataFrame.groupby() to count players per opening\n",
    "num_players_per_opening = pd.DataFrame(clean_data.groupby('opening_id').size(), columns=['count'])\n",
    "openings_with_data = num_players_per_opening[num_players_per_opening['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter using pd.DataFrame.isin()\n",
    "clean_data = clean_data[clean_data['opening_id'].isin(openings_with_data)]\n",
    "openings_after = clean_data['opening_id'].nunique()\n",
    "\n",
    "print(f\"   ‚Ä¢ Openings before: {num_openings_before:,}\")\n",
    "print(f\"   ‚Ä¢ Openings after: {openings_after:,}\")\n",
    "print(f\"   ‚Ä¢ Removed: {num_openings_before - openings_after}\")\n",
    "\n",
    "# 5. Verify no null values using pd.isna()\n",
    "print(f\"\\n5Ô∏è‚É£  Verifying no null values...\")\n",
    "null_counts = pd.DataFrame.isna(clean_data).sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"   ‚úì No null values found\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "    for col, count in null_counts[null_counts > 0].items():\n",
    "        print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    # Drop rows with nulls using pd.DataFrame.dropna()\n",
    "    clean_data = pd.DataFrame.dropna(clean_data)\n",
    "    print(f\"   ‚úì Dropped null rows. New shape: {clean_data.shape}\")\n",
    "\n",
    "# TODO: Add confidence weighting column\n",
    "# TODO: Extract and normalize player ratings (side information)\n",
    "\n",
    "# Reset index using pd.DataFrame.reset_index()\n",
    "clean_data = pd.DataFrame.reset_index(clean_data, drop=True)\n",
    "\n",
    "# Final statistics using pd functions\n",
    "print(f\"\\n6Ô∏è‚É£  Final data statistics:\")\n",
    "print(f\"   ‚Ä¢ Total rows: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {pd.Series.nunique(clean_data['player_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {pd.Series.nunique(clean_data['opening_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Total games: {pd.Series.sum(clean_data['num_games']):,}\")\n",
    "print(f\"   ‚Ä¢ Avg games per entry: {pd.Series.mean(clean_data['num_games']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg openings per player: {len(clean_data) / pd.Series.nunique(clean_data['player_id']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg players per opening: {len(clean_data) / pd.Series.nunique(clean_data['opening_id']):.1f}\")\n",
    "\n",
    "# Score distribution using pd functions\n",
    "print(f\"\\n   Score statistics:\")\n",
    "print(f\"   ‚Ä¢ Min: {pd.Series.min(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {pd.Series.quantile(clean_data['score'], 0.25):.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {pd.Series.median(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {pd.Series.quantile(clean_data['score'], 0.75):.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {pd.Series.max(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {pd.Series.mean(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {pd.Series.std(clean_data['score']):.4f}\")\n",
    "\n",
    "# Sample of cleaned data using pd.DataFrame.sample()\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of cleaned data (10 random rows):\")\n",
    "print(pd.DataFrame.sample(clean_data, min(10, len(clean_data)), random_state=42).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SANITIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCleaned data shape: {clean_data.shape}\")\n",
    "print(f\"Data reduction: {100 * (1 - len(clean_data)/len(raw_data)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e671b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ K_PLAYER (shrinkage constant): 50\n",
      "   ‚Ä¢ Method: Two-level empirical Bayes shrinkage\n",
      "   ‚Ä¢ Level 1: Calculate opening-specific means\n",
      "   ‚Ä¢ Level 2: Shrink player scores toward opening means\n",
      "\n",
      "üìä Global statistics:\n",
      "   ‚Ä¢ Global mean score: 0.5110\n",
      "   ‚Ä¢ Total entries: 888,341\n",
      "   ‚Ä¢ Unique openings: 2,560\n",
      "\n",
      "1Ô∏è‚É£  LEVEL 1: Calculating opening-specific means...\n",
      "   ‚úì Calculated means for 2,560 openings\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1748\n",
      "   ‚Ä¢ 25th percentile: 0.4943\n",
      "   ‚Ä¢ Median: 0.5163\n",
      "   ‚Ä¢ 75th percentile: 0.5400\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0560\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 1606\n",
      "   ‚Ä¢ Players per opening (median): 55\n",
      "   ‚Ä¢ Total games range: [10, 1764556]\n",
      "   ‚Ä¢ Players range: [1, 12801]\n",
      "\n",
      "2Ô∏è‚É£  LEVEL 2: Shrinking player scores toward opening means...\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 888,341 entries\n",
      "\n",
      "3Ô∏è‚É£  Calculating confidence weights...\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Formula: confidence = num_games / (num_games + 50)\n",
      "\n",
      "üìä Global statistics:\n",
      "   ‚Ä¢ Global mean score: 0.5110\n",
      "   ‚Ä¢ Total entries: 888,341\n",
      "   ‚Ä¢ Unique openings: 2,560\n",
      "\n",
      "1Ô∏è‚É£  LEVEL 1: Calculating opening-specific means...\n",
      "   ‚úì Calculated means for 2,560 openings\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1748\n",
      "   ‚Ä¢ 25th percentile: 0.4943\n",
      "   ‚Ä¢ Median: 0.5163\n",
      "   ‚Ä¢ 75th percentile: 0.5400\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0560\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 1606\n",
      "   ‚Ä¢ Players per opening (median): 55\n",
      "   ‚Ä¢ Total games range: [10, 1764556]\n",
      "   ‚Ä¢ Players range: [1, 12801]\n",
      "\n",
      "2Ô∏è‚É£  LEVEL 2: Shrinking player scores toward opening means...\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 888,341 entries\n",
      "\n",
      "3Ô∏è‚É£  Calculating confidence weights...\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Formula: confidence = num_games / (num_games + 50)\n",
      "   ‚Ä¢ Range: [0.1667, 0.9958]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000946\n",
      "   ‚Ä¢ Std adjustment: 0.077011\n",
      "   ‚Ä¢ Max adjustment: 0.457772\n",
      "   ‚Ä¢ Min adjustment: -0.459661\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003670\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001619\n",
      "   ‚Ä¢ 75th percentile (n=64 games): avg adjustment = -0.000363\n",
      "   ‚Ä¢ >75th percentile (n>64 games): avg adjustment = -0.001385\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1005\n",
      "   ‚Ä¢ 25th percentile: 0.4847\n",
      "   ‚Ä¢ Median: 0.5116\n",
      "   ‚Ä¢ 75th percentile: 0.5387\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0416\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   ‚Ä¢ Range: [0.1667, 0.9958]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000946\n",
      "   ‚Ä¢ Std adjustment: 0.077011\n",
      "   ‚Ä¢ Max adjustment: 0.457772\n",
      "   ‚Ä¢ Min adjustment: -0.459661\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003670\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001619\n",
      "   ‚Ä¢ 75th percentile (n=64 games): avg adjustment = -0.000363\n",
      "   ‚Ä¢ >75th percentile (n>64 games): avg adjustment = -0.001385\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1005\n",
      "   ‚Ä¢ 25th percentile: 0.4847\n",
      "   ‚Ä¢ Median: 0.5116\n",
      "   ‚Ä¢ 75th percentile: 0.5387\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0416\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   Player  7773 | Opening 1509 | Games:  11 | Opening mean: 0.5623 | Original: 0.5455 ‚Üí Adjusted: 0.5593 | Diff: +0.0138 | Confidence: 0.180\n",
      "   Player 25725 | Opening  991 | Games:  18 | Opening mean: 0.5356 | Original: 0.6667 ‚Üí Adjusted: 0.5703 | Diff: -0.0964 | Confidence: 0.265\n",
      "   Player 36523 | Opening  930 | Games:  20 | Opening mean: 0.5171 | Original: 0.6500 ‚Üí Adjusted: 0.5551 | Diff: -0.0949 | Confidence: 0.286\n",
      "   Player 43643 | Opening  935 | Games:  13 | Opening mean: 0.5007 | Original: 0.2308 ‚Üí Adjusted: 0.4450 | Diff: +0.2142 | Confidence: 0.206\n",
      "   Player 13052 | Opening  507 | Games:  10 | Opening mean: 0.5295 | Original: 0.7000 ‚Üí Adjusted: 0.5579 | Diff: -0.1421 | Confidence: 0.167\n",
      "   Player  2120 | Opening 1665 | Games:  13 | Opening mean: 0.5176 | Original: 0.3077 ‚Üí Adjusted: 0.4743 | Diff: +0.1666 | Confidence: 0.206\n",
      "   Player  7320 | Opening 1229 | Games:  11 | Opening mean: 0.4995 | Original: 0.5000 ‚Üí Adjusted: 0.4996 | Diff: -0.0004 | Confidence: 0.180\n",
      "   Player   785 | Opening 1156 | Games:  12 | Opening mean: 0.5023 | Original: 0.4167 ‚Üí Adjusted: 0.4857 | Diff: +0.0690 | Confidence: 0.194\n",
      "   Player 34867 | Opening  838 | Games:  19 | Opening mean: 0.5102 | Original: 0.6842 ‚Üí Adjusted: 0.5581 | Diff: -0.1261 | Confidence: 0.275\n",
      "   Player  5504 | Opening 1167 | Games:  11 | Opening mean: 0.4948 | Original: 0.6364 ‚Üí Adjusted: 0.5203 | Diff: -0.1161 | Confidence: 0.180\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 30426 | Opening  737 | Games:  74 | Opening mean: 0.5088 | Original: 0.4595 ‚Üí Adjusted: 0.4793 | Diff: +0.0199 | Confidence: 0.597\n",
      "   Player 44449 | Opening 1001 | Games:  91 | Opening mean: 0.4882 | Original: 0.4341 ‚Üí Adjusted: 0.4532 | Diff: +0.0192 | Confidence: 0.645\n",
      "   Player 13989 | Opening 3180 | Games:  82 | Opening mean: 0.4899 | Original: 0.4756 ‚Üí Adjusted: 0.4810 | Diff: +0.0054 | Confidence: 0.621\n",
      "   Player  4452 | Opening  772 | Games:  54 | Opening mean: 0.5168 | Original: 0.6111 ‚Üí Adjusted: 0.5658 | Diff: -0.0453 | Confidence: 0.519\n",
      "   Player  1756 | Opening 1859 | Games:  69 | Opening mean: 0.5258 | Original: 0.5072 ‚Üí Adjusted: 0.5151 | Diff: +0.0078 | Confidence: 0.580\n",
      "   Player 14534 | Opening 2679 | Games:  58 | Opening mean: 0.5233 | Original: 0.4569 ‚Üí Adjusted: 0.4876 | Diff: +0.0307 | Confidence: 0.537\n",
      "   Player  7405 | Opening 1356 | Games:  88 | Opening mean: 0.5067 | Original: 0.4545 ‚Üí Adjusted: 0.4734 | Diff: +0.0189 | Confidence: 0.638\n",
      "   Player 22579 | Opening  737 | Games:  62 | Opening mean: 0.5088 | Original: 0.4274 ‚Üí Adjusted: 0.4637 | Diff: +0.0363 | Confidence: 0.554\n",
      "   Player  8017 | Opening 1147 | Games:  83 | Opening mean: 0.5030 | Original: 0.5301 ‚Üí Adjusted: 0.5199 | Diff: -0.0102 | Confidence: 0.624\n",
      "   Player 32429 | Opening 1958 | Games:  78 | Opening mean: 0.5391 | Original: 0.5897 ‚Üí Adjusted: 0.5700 | Diff: -0.0198 | Confidence: 0.609\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player  2557 | Opening 2052 | Games: 595 | Opening mean: 0.5386 | Original: 0.5479 ‚Üí Adjusted: 0.5472 | Diff: -0.0007 | Confidence: 0.922\n",
      "   Player 27756 | Opening 1665 | Games: 282 | Opening mean: 0.5176 | Original: 0.4610 ‚Üí Adjusted: 0.4695 | Diff: +0.0085 | Confidence: 0.849\n",
      "   Player 38078 | Opening  191 | Games: 415 | Opening mean: 0.5086 | Original: 0.4855 ‚Üí Adjusted: 0.4880 | Diff: +0.0025 | Confidence: 0.892\n",
      "   Player  9151 | Opening 1967 | Games: 212 | Opening mean: 0.5467 | Original: 0.6014 ‚Üí Adjusted: 0.5910 | Diff: -0.0104 | Confidence: 0.809\n",
      "   Player   998 | Opening   39 | Games: 1756 | Opening mean: 0.5056 | Original: 0.5120 ‚Üí Adjusted: 0.5118 | Diff: -0.0002 | Confidence: 0.972\n",
      "   Player 39936 | Opening  737 | Games: 216 | Opening mean: 0.5088 | Original: 0.5069 ‚Üí Adjusted: 0.5073 | Diff: +0.0003 | Confidence: 0.812\n",
      "   Player 22633 | Opening 2155 | Games: 316 | Opening mean: 0.5204 | Original: 0.4557 ‚Üí Adjusted: 0.4645 | Diff: +0.0088 | Confidence: 0.863\n",
      "   Player 10046 | Opening 1575 | Games: 414 | Opening mean: 0.5126 | Original: 0.5531 ‚Üí Adjusted: 0.5488 | Diff: -0.0044 | Confidence: 0.892\n",
      "   Player  7754 | Opening 2852 | Games: 367 | Opening mean: 0.5225 | Original: 0.5313 ‚Üí Adjusted: 0.5303 | Diff: -0.0011 | Confidence: 0.880\n",
      "   Player 32566 | Opening 3212 | Games: 211 | Opening mean: 0.5186 | Original: 0.4953 ‚Üí Adjusted: 0.4997 | Diff: +0.0045 | Confidence: 0.808\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Player  7773 | Opening 1509 | Games:  11 | Opening mean: 0.5623 | Original: 0.5455 ‚Üí Adjusted: 0.5593 | Diff: +0.0138 | Confidence: 0.180\n",
      "   Player 25725 | Opening  991 | Games:  18 | Opening mean: 0.5356 | Original: 0.6667 ‚Üí Adjusted: 0.5703 | Diff: -0.0964 | Confidence: 0.265\n",
      "   Player 36523 | Opening  930 | Games:  20 | Opening mean: 0.5171 | Original: 0.6500 ‚Üí Adjusted: 0.5551 | Diff: -0.0949 | Confidence: 0.286\n",
      "   Player 43643 | Opening  935 | Games:  13 | Opening mean: 0.5007 | Original: 0.2308 ‚Üí Adjusted: 0.4450 | Diff: +0.2142 | Confidence: 0.206\n",
      "   Player 13052 | Opening  507 | Games:  10 | Opening mean: 0.5295 | Original: 0.7000 ‚Üí Adjusted: 0.5579 | Diff: -0.1421 | Confidence: 0.167\n",
      "   Player  2120 | Opening 1665 | Games:  13 | Opening mean: 0.5176 | Original: 0.3077 ‚Üí Adjusted: 0.4743 | Diff: +0.1666 | Confidence: 0.206\n",
      "   Player  7320 | Opening 1229 | Games:  11 | Opening mean: 0.4995 | Original: 0.5000 ‚Üí Adjusted: 0.4996 | Diff: -0.0004 | Confidence: 0.180\n",
      "   Player   785 | Opening 1156 | Games:  12 | Opening mean: 0.5023 | Original: 0.4167 ‚Üí Adjusted: 0.4857 | Diff: +0.0690 | Confidence: 0.194\n",
      "   Player 34867 | Opening  838 | Games:  19 | Opening mean: 0.5102 | Original: 0.6842 ‚Üí Adjusted: 0.5581 | Diff: -0.1261 | Confidence: 0.275\n",
      "   Player  5504 | Opening 1167 | Games:  11 | Opening mean: 0.4948 | Original: 0.6364 ‚Üí Adjusted: 0.5203 | Diff: -0.1161 | Confidence: 0.180\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 30426 | Opening  737 | Games:  74 | Opening mean: 0.5088 | Original: 0.4595 ‚Üí Adjusted: 0.4793 | Diff: +0.0199 | Confidence: 0.597\n",
      "   Player 44449 | Opening 1001 | Games:  91 | Opening mean: 0.4882 | Original: 0.4341 ‚Üí Adjusted: 0.4532 | Diff: +0.0192 | Confidence: 0.645\n",
      "   Player 13989 | Opening 3180 | Games:  82 | Opening mean: 0.4899 | Original: 0.4756 ‚Üí Adjusted: 0.4810 | Diff: +0.0054 | Confidence: 0.621\n",
      "   Player  4452 | Opening  772 | Games:  54 | Opening mean: 0.5168 | Original: 0.6111 ‚Üí Adjusted: 0.5658 | Diff: -0.0453 | Confidence: 0.519\n",
      "   Player  1756 | Opening 1859 | Games:  69 | Opening mean: 0.5258 | Original: 0.5072 ‚Üí Adjusted: 0.5151 | Diff: +0.0078 | Confidence: 0.580\n",
      "   Player 14534 | Opening 2679 | Games:  58 | Opening mean: 0.5233 | Original: 0.4569 ‚Üí Adjusted: 0.4876 | Diff: +0.0307 | Confidence: 0.537\n",
      "   Player  7405 | Opening 1356 | Games:  88 | Opening mean: 0.5067 | Original: 0.4545 ‚Üí Adjusted: 0.4734 | Diff: +0.0189 | Confidence: 0.638\n",
      "   Player 22579 | Opening  737 | Games:  62 | Opening mean: 0.5088 | Original: 0.4274 ‚Üí Adjusted: 0.4637 | Diff: +0.0363 | Confidence: 0.554\n",
      "   Player  8017 | Opening 1147 | Games:  83 | Opening mean: 0.5030 | Original: 0.5301 ‚Üí Adjusted: 0.5199 | Diff: -0.0102 | Confidence: 0.624\n",
      "   Player 32429 | Opening 1958 | Games:  78 | Opening mean: 0.5391 | Original: 0.5897 ‚Üí Adjusted: 0.5700 | Diff: -0.0198 | Confidence: 0.609\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player  2557 | Opening 2052 | Games: 595 | Opening mean: 0.5386 | Original: 0.5479 ‚Üí Adjusted: 0.5472 | Diff: -0.0007 | Confidence: 0.922\n",
      "   Player 27756 | Opening 1665 | Games: 282 | Opening mean: 0.5176 | Original: 0.4610 ‚Üí Adjusted: 0.4695 | Diff: +0.0085 | Confidence: 0.849\n",
      "   Player 38078 | Opening  191 | Games: 415 | Opening mean: 0.5086 | Original: 0.4855 ‚Üí Adjusted: 0.4880 | Diff: +0.0025 | Confidence: 0.892\n",
      "   Player  9151 | Opening 1967 | Games: 212 | Opening mean: 0.5467 | Original: 0.6014 ‚Üí Adjusted: 0.5910 | Diff: -0.0104 | Confidence: 0.809\n",
      "   Player   998 | Opening   39 | Games: 1756 | Opening mean: 0.5056 | Original: 0.5120 ‚Üí Adjusted: 0.5118 | Diff: -0.0002 | Confidence: 0.972\n",
      "   Player 39936 | Opening  737 | Games: 216 | Opening mean: 0.5088 | Original: 0.5069 ‚Üí Adjusted: 0.5073 | Diff: +0.0003 | Confidence: 0.812\n",
      "   Player 22633 | Opening 2155 | Games: 316 | Opening mean: 0.5204 | Original: 0.4557 ‚Üí Adjusted: 0.4645 | Diff: +0.0088 | Confidence: 0.863\n",
      "   Player 10046 | Opening 1575 | Games: 414 | Opening mean: 0.5126 | Original: 0.5531 ‚Üí Adjusted: 0.5488 | Diff: -0.0044 | Confidence: 0.892\n",
      "   Player  7754 | Opening 2852 | Games: 367 | Opening mean: 0.5225 | Original: 0.5313 ‚Üí Adjusted: 0.5303 | Diff: -0.0011 | Confidence: 0.880\n",
      "   Player 32566 | Opening 3212 | Games: 211 | Opening mean: 0.5186 | Original: 0.4953 ‚Üí Adjusted: 0.4997 | Diff: +0.0045 | Confidence: 0.808\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4890 vs global) | 1 player entries\n",
      "   Opening  732 (B00): mean = 0.8500 (+0.3390 vs global) | 1 player entries\n",
      "   Opening 1795 (C39): mean = 0.8333 (+0.3223 vs global) | 1 player entries\n",
      "   Opening 2570 (D20): mean = 0.8175 (+0.3064 vs global) | 3 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 1793 (C39): mean = 0.1748 (-0.3362 vs global) | 4 player entries\n",
      "   Opening 2099 (C52): mean = 0.2500 (-0.2610 vs global) | 1 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4890 vs global) | 1 player entries\n",
      "   Opening  732 (B00): mean = 0.8500 (+0.3390 vs global) | 1 player entries\n",
      "   Opening 1795 (C39): mean = 0.8333 (+0.3223 vs global) | 1 player entries\n",
      "   Opening 2570 (D20): mean = 0.8175 (+0.3064 vs global) | 3 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 1793 (C39): mean = 0.1748 (-0.3362 vs global) | 4 player entries\n",
      "   Opening 2099 (C52): mean = 0.2500 (-0.2610 vs global) | 1 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 43868 | Opening 1697 (C33) | Games: 10 | Opening mean: 0.7561 | Original: 0.8500 ‚Üí 0.7717\n",
      "      If we'd shrunk to global mean: 0.5675 (would lose +0.2042 of deserved credit)\n",
      "   Player 22858 | Opening 3292 (C54) | Games: 10 | Opening mean: 0.7314 | Original: 0.7000 ‚Üí 0.7262\n",
      "      If we'd shrunk to global mean: 0.5425 (would lose +0.1836 of deserved credit)\n",
      "   Player 28647 | Opening 2215 (C62) | Games: 13 | Opening mean: 0.7308 | Original: 0.7308 ‚Üí 0.7308\n",
      "      If we'd shrunk to global mean: 0.5564 (would lose +0.1744 of deserved credit)\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player 24719 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3516 | Original: 0.4000 ‚Üí 0.3596\n",
      "      If we'd shrunk to global mean: 0.4925 (would unfairly boost by +0.1329)\n",
      "   Player 38669 | Opening 1793 (C39) | Games: 14 | Opening mean: 0.1748 | Original: 0.0714 ‚Üí 0.1522\n",
      "      If we'd shrunk to global mean: 0.4149 (would unfairly boost by +0.2627)\n",
      "   Player 23052 | Opening 1779 (C37) | Games: 19 | Opening mean: 0.3516 | Original: 0.3684 ‚Üí 0.3562\n",
      "      If we'd shrunk to global mean: 0.4718 (would unfairly boost by +0.1156)\n",
      "\n",
      "9Ô∏è‚É£  Cleaning up...\n",
      "   ‚úì Removed temporary columns\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (888341, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 43868 | Opening 1697 (C33) | Games: 10 | Opening mean: 0.7561 | Original: 0.8500 ‚Üí 0.7717\n",
      "      If we'd shrunk to global mean: 0.5675 (would lose +0.2042 of deserved credit)\n",
      "   Player 22858 | Opening 3292 (C54) | Games: 10 | Opening mean: 0.7314 | Original: 0.7000 ‚Üí 0.7262\n",
      "      If we'd shrunk to global mean: 0.5425 (would lose +0.1836 of deserved credit)\n",
      "   Player 28647 | Opening 2215 (C62) | Games: 13 | Opening mean: 0.7308 | Original: 0.7308 ‚Üí 0.7308\n",
      "      If we'd shrunk to global mean: 0.5564 (would lose +0.1744 of deserved credit)\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player 24719 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3516 | Original: 0.4000 ‚Üí 0.3596\n",
      "      If we'd shrunk to global mean: 0.4925 (would unfairly boost by +0.1329)\n",
      "   Player 38669 | Opening 1793 (C39) | Games: 14 | Opening mean: 0.1748 | Original: 0.0714 ‚Üí 0.1522\n",
      "      If we'd shrunk to global mean: 0.4149 (would unfairly boost by +0.2627)\n",
      "   Player 23052 | Opening 1779 (C37) | Games: 19 | Opening mean: 0.3516 | Original: 0.3684 ‚Üí 0.3562\n",
      "      If we'd shrunk to global mean: 0.4718 (would unfairly boost by +0.1156)\n",
      "\n",
      "9Ô∏è‚É£  Cleaning up...\n",
      "   ‚úì Removed temporary columns\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (888341, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n"
     ]
    }
   ],
   "source": [
    "# 2b. Apply hierarchical Bayesian shrinkage to adjust scores based on sample size confidence\n",
    "\n",
    "# Check if confidence already exists - if so, skip this processing\n",
    "if 'confidence' in clean_data.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'confidence' column already exists in data\")\n",
    "    print(\"   This indicates hierarchical Bayesian processing has already been applied.\")\n",
    "    print(f\"\\nCurrent data shape: {clean_data.shape}\")\n",
    "    print(f\"Confidence range: [{clean_data['confidence'].min():.4f}, {clean_data['confidence'].max():.4f}]\")\n",
    "else:\n",
    "    # Define the processing function\n",
    "    # This is a long function, I recommend you fold it down in your editor\n",
    "    def apply_hierarchical_bayesian_shrinkage(data, k_player=50):\n",
    "        \"\"\"\n",
    "        Apply two-level hierarchical Bayesian shrinkage to adjust scores.\n",
    "        \n",
    "        A lot of our player-opening entries have a small number of games played, because openings are so specific.\n",
    "        This introduces sample size issues.\n",
    "        \n",
    "        We use TWO-LEVEL shrinkage:\n",
    "        Level 1: Calculate opening-specific means (these are our \"ground truth\" for each opening)\n",
    "        Level 2: Shrink individual player-opening scores toward their opening's mean\n",
    "        This is better than shrinking toward global mean because different openings have different baseline win rates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Clean data with columns: player_id, opening_id, score, num_games, eco\n",
    "        k_player : int\n",
    "            Shrinkage constant for player-opening scores (default: 50)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Data with adjusted scores and new 'confidence' column\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "        print(f\"   ‚Ä¢ K_PLAYER (shrinkage constant): {k_player}\")\n",
    "        print(f\"   ‚Ä¢ Method: Two-level empirical Bayes shrinkage\")\n",
    "        print(f\"   ‚Ä¢ Level 1: Calculate opening-specific means\")\n",
    "        print(f\"   ‚Ä¢ Level 2: Shrink player scores toward opening means\")\n",
    "        \n",
    "        # Calculate global mean score for comparison\n",
    "        global_mean_score = data[\"score\"].mean()\n",
    "        print(f\"\\nüìä Global statistics:\")\n",
    "        print(f\"   ‚Ä¢ Global mean score: {global_mean_score:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Total entries: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique openings: {data['opening_id'].nunique():,}\")\n",
    "        \n",
    "        # Store original scores for comparison\n",
    "        data = data.copy()  # Best practice: work on a copy\n",
    "        data[\"score_original\"] = data[\"score\"].copy()\n",
    "        \n",
    "        # LEVEL 1: Calculate opening-specific means and statistics\n",
    "        print(f\"\\n1Ô∏è‚É£  LEVEL 1: Calculating opening-specific means...\")\n",
    "        \n",
    "        opening_stats = (\n",
    "            data.groupby(\"opening_id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"score\": \"mean\",\n",
    "                    \"num_games\": \"sum\",\n",
    "                    \"player_id\": \"count\",  # Number of players who played this opening\n",
    "                }\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"score\": \"opening_mean\",\n",
    "                    \"num_games\": \"opening_total_games\",\n",
    "                    \"player_id\": \"opening_num_players\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úì Calculated means for {len(opening_stats):,} openings\")\n",
    "        \n",
    "        # Opening mean statistics\n",
    "        print(f\"\\n   Opening mean score distribution:\")\n",
    "        print(f\"   ‚Ä¢ Min: {opening_stats['opening_mean'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {opening_stats['opening_mean'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {opening_stats['opening_mean'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {opening_stats['opening_mean'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {opening_stats['opening_mean'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {opening_stats['opening_mean'].std():.4f}\")\n",
    "        \n",
    "        # Show distribution of opening sizes\n",
    "        print(f\"\\n   Opening sample size distribution:\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games per opening (median): {opening_stats['opening_total_games'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players per opening (median): {opening_stats['opening_num_players'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games range: [{opening_stats['opening_total_games'].min():.0f}, {opening_stats['opening_total_games'].max():.0f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players range: [{opening_stats['opening_num_players'].min():.0f}, {opening_stats['opening_num_players'].max():.0f}]\"\n",
    "        )\n",
    "        \n",
    "        # Merge opening means back into main dataframe\n",
    "        data = data.merge(\n",
    "            opening_stats[[\"opening_mean\"]], left_on=\"opening_id\", right_index=True, how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # LEVEL 2: Shrink player-opening scores toward opening-specific means\n",
    "        print(f\"\\n2Ô∏è‚É£  LEVEL 2: Shrinking player scores toward opening means...\")\n",
    "        print(\n",
    "            f\"   Formula: adjusted_score = (num_games √ó player_score + {k_player} √ó opening_mean) / (num_games + {k_player})\"\n",
    "        )\n",
    "        \n",
    "        numerator = (data[\"num_games\"] * data[\"score_original\"]) + (\n",
    "            k_player * data[\"opening_mean\"]\n",
    "        )\n",
    "        denominator = data[\"num_games\"] + k_player\n",
    "        data[\"score\"] = numerator / denominator\n",
    "        \n",
    "        print(f\"   ‚úì Scores adjusted for {len(data):,} entries\")\n",
    "        \n",
    "        # Calculate confidence weights (will be used in loss function later)\n",
    "        print(f\"\\n3Ô∏è‚É£  Calculating confidence weights...\")\n",
    "        data[\"confidence\"] = data[\"num_games\"] / (\n",
    "            data[\"num_games\"] + k_player\n",
    "        )\n",
    "        print(f\"   ‚úì Confidence weights calculated\")\n",
    "        print(f\"   ‚Ä¢ Formula: confidence = num_games / (num_games + {k_player})\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Range: [{data['confidence'].min():.4f}, {data['confidence'].max():.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # Statistics on the adjustment\n",
    "        score_diff = data[\"score\"] - data[\"score_original\"]\n",
    "        print(f\"\\n4Ô∏è‚É£  Adjustment statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean adjustment: {score_diff.mean():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Std adjustment: {score_diff.std():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Max adjustment: {score_diff.max():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Min adjustment: {score_diff.min():.6f}\")\n",
    "        \n",
    "        # Show distribution of adjustments\n",
    "        print(f\"\\n   Adjustment by num_games quartiles:\")\n",
    "        quartiles = data[\"num_games\"].quantile([0.25, 0.5, 0.75])\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 25th percentile (n={quartiles[0.25]:.0f} games): avg adjustment = {score_diff[data['num_games'] <= quartiles[0.25]].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 50th percentile (n={quartiles[0.5]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.25]) & (data['num_games'] <= quartiles[0.5])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 75th percentile (n={quartiles[0.75]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.5]) & (data['num_games'] <= quartiles[0.75])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ >75th percentile (n>{quartiles[0.75]:.0f} games): avg adjustment = {score_diff[data['num_games'] > quartiles[0.75]].mean():.6f}\"\n",
    "        )\n",
    "        \n",
    "        # New score distribution after adjustment\n",
    "        print(f\"\\n5Ô∏è‚É£  Adjusted score statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {data['score'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {data['score'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {data['score'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {data['score'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {data['score'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {data['score'].mean():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {data['score'].std():.4f}\")\n",
    "        \n",
    "        # Detailed sample showing the effect across different game counts\n",
    "        print(f\"\\n6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\")\n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        low_game_sample = data[\n",
    "            (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in low_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Medium-game entries (50-100 games) - MODERATE shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        med_game_sample = data[\n",
    "            (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in med_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   High-game entries (200+ games) - LOW shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        high_game_sample = data[data[\"num_games\"] >= 200].sample(\n",
    "            min(10, len(data[data[\"num_games\"] >= 200])), random_state=42\n",
    "        )\n",
    "        for idx, row in high_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        # Show extreme cases - comparing to both opening mean AND global mean\n",
    "        print(f\"\\n7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\")\n",
    "        \n",
    "        # Find entries where opening mean differs significantly from global mean\n",
    "        data[\"opening_deviation_from_global\"] = (\n",
    "            data[\"opening_mean\"] - global_mean_score\n",
    "        ).abs()\n",
    "        \n",
    "        print(f\"\\n   Openings with HIGHEST win rates (strong for White):\")\n",
    "        strong_openings = data.nlargest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in strong_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"(+{deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   Openings with LOWEST win rates (weak for White):\")\n",
    "        weak_openings = data.nsmallest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in weak_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"({deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        # Show specific examples where hierarchical shrinkage made a difference\n",
    "        print(f\"\\n8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\")\n",
    "        \n",
    "        # Find entries with strong openings where player did well\n",
    "        strong_opening_ids = data.nlargest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        strong_examples = data[\n",
    "            (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] > 0.6)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] > 0.6)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"\\n   Strong opening + good player performance (shrunk toward HIGH opening mean):\"\n",
    "        )\n",
    "        for idx, row in strong_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would lose {difference:+.4f} of deserved credit)\"\n",
    "            )\n",
    "        \n",
    "        # Find entries with weak openings where player did poorly\n",
    "        weak_opening_ids = data.nsmallest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        weak_examples = data[\n",
    "            (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] < 0.45)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] < 0.45)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Weak opening + poor player performance (shrunk toward LOW opening mean):\")\n",
    "        for idx, row in weak_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would unfairly boost by {-difference:+.4f})\"\n",
    "            )\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        print(f\"\\n9Ô∏è‚É£  Cleaning up...\")\n",
    "        data = data.drop(\n",
    "            columns=[\"score_original\", \"opening_mean\", \"opening_deviation_from_global\"]\n",
    "        )\n",
    "        print(f\"   ‚úì Removed temporary columns\")\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nFinal data shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        print(f\"\\nNew columns added:\")\n",
    "        print(f\"   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\")\n",
    "        print(f\"   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\")\n",
    "        print(f\"\\nKey improvement over simple shrinkage:\")\n",
    "        print(f\"   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\")\n",
    "        print(f\"   ‚Ä¢ Preserves opening difficulty differences\")\n",
    "        print(f\"   ‚Ä¢ More accurate for both strong and weak openings\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Configuration for Bayesian shrinkage\n",
    "    K_PLAYER = 50  # Shrinkage constant for player-opening scores\n",
    "    \n",
    "    # Call the function\n",
    "    clean_data = apply_hierarchical_bayesian_shrinkage(clean_data, k_player=K_PLAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73319d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        player_id  opening_id  num_games     score  eco  confidence\n",
      "772595      41398         505        223  0.525595  A46     0.81685\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2C: PLAYER RATING STATISTICS\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Extracting player ratings from database...\n",
      "   ‚úì Retrieved ratings for 13,990 players\n",
      "   ‚úì Database connection closed\n",
      "\n",
      "2Ô∏è‚É£  Merging ratings with clean_data...\n",
      "   ‚úì Merged successfully\n",
      "   ‚úì All entries have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 13,990\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1772.43\n",
      "   ‚Ä¢ Median: 1772\n",
      "   ‚Ä¢ Std Dev: 248.34\n",
      "\n",
      "4Ô∏è‚É£  Quartile statistics:\n",
      "   ‚Ä¢ 25th percentile: 1593\n",
      "   ‚Ä¢ 50th percentile (median): 1772\n",
      "   ‚Ä¢ 75th percentile: 1944\n",
      "\n",
      "5Ô∏è‚É£  Detailed percentile distribution (5% increments):\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1368    ‚ñà‚ñà‚ñà‚ñà\n",
      "      10%          1443    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1503    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1548    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1593    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1631    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1669    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1705    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1737    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1772    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1805    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1838    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1872    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1907    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1944    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1988    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2030    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2092    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2187    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400         958      6.85%      ‚ñà‚ñà\n",
      "   1400-1600       2,646     18.91%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800       3,951     28.24%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000       3,811     27.24%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       1,980     14.15%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400         530      3.79%      ‚ñà\n",
      "   2400-2600         104      0.74%      \n",
      "   2600-3000          10      0.07%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 351\n",
      "   ‚Ä¢ 10th-90th percentile range: 649\n",
      "   ‚úì Retrieved ratings for 13,990 players\n",
      "   ‚úì Database connection closed\n",
      "\n",
      "2Ô∏è‚É£  Merging ratings with clean_data...\n",
      "   ‚úì Merged successfully\n",
      "   ‚úì All entries have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 13,990\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1772.43\n",
      "   ‚Ä¢ Median: 1772\n",
      "   ‚Ä¢ Std Dev: 248.34\n",
      "\n",
      "4Ô∏è‚É£  Quartile statistics:\n",
      "   ‚Ä¢ 25th percentile: 1593\n",
      "   ‚Ä¢ 50th percentile (median): 1772\n",
      "   ‚Ä¢ 75th percentile: 1944\n",
      "\n",
      "5Ô∏è‚É£  Detailed percentile distribution (5% increments):\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1368    ‚ñà‚ñà‚ñà‚ñà\n",
      "      10%          1443    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1503    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1548    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1593    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1631    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1669    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1705    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1737    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1772    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1805    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1838    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1872    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1907    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1944    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1988    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2030    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2092    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2187    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400         958      6.85%      ‚ñà‚ñà\n",
      "   1400-1600       2,646     18.91%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800       3,951     28.24%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000       3,811     27.24%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       1,980     14.15%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400         530      3.79%      ‚ñà\n",
      "   2400-2600         104      0.74%      \n",
      "   2600-3000          10      0.07%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 351\n",
      "   ‚Ä¢ 10th-90th percentile range: 649\n",
      "\n",
      "8Ô∏è‚É£  Distribution shape:\n",
      "   ‚Ä¢ Skewness: 0.1545 (right-skewed)\n",
      "   ‚Ä¢ Kurtosis: -0.2711 (light-tailed)\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1443):\n",
      "      Player 18992: Sergey_Kamsky - Rating: 1443\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1593):\n",
      "      Player 10403: Jose_19_68 - Rating: 1593\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1772):\n",
      "      Player 11929: LeRickRiant - Rating: 1772\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1944):\n",
      "      Player 9684: JBirns8 - Rating: 1944\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2092):\n",
      "      Player 24984: bilbao - Rating: 2092\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Key takeaways:\n",
      "   ‚Ä¢ Total players: 13,990\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1772 ¬± 248\n",
      "   ‚Ä¢ Median: 1772\n",
      "\n",
      "   Next steps: Normalize ratings for model input\n",
      "\n",
      "8Ô∏è‚É£  Distribution shape:\n",
      "   ‚Ä¢ Skewness: 0.1545 (right-skewed)\n",
      "   ‚Ä¢ Kurtosis: -0.2711 (light-tailed)\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1443):\n",
      "      Player 18992: Sergey_Kamsky - Rating: 1443\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1593):\n",
      "      Player 10403: Jose_19_68 - Rating: 1593\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1772):\n",
      "      Player 11929: LeRickRiant - Rating: 1772\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1944):\n",
      "      Player 9684: JBirns8 - Rating: 1944\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2092):\n",
      "      Player 24984: bilbao - Rating: 2092\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Key takeaways:\n",
      "   ‚Ä¢ Total players: 13,990\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1772 ¬± 248\n",
      "   ‚Ä¢ Median: 1772\n",
      "\n",
      "   Next steps: Normalize ratings for model input\n"
     ]
    }
   ],
   "source": [
    "# 2c. Gather player rating statistics (no mutation, just exploration)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2C: PLAYER RATING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to database and extract player ratings\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player ratings from database...\")\n",
    "    \n",
    "    # Get unique player IDs from our clean_data\n",
    "    unique_player_ids = clean_data['player_id'].unique()\n",
    "    player_ids_str = ','.join(map(str, unique_player_ids))\n",
    "    \n",
    "    # Query to get player ratings\n",
    "    rating_query = f\"\"\"\n",
    "        SELECT \n",
    "            id as player_id,\n",
    "            name,\n",
    "            title,\n",
    "            rating\n",
    "        FROM player\n",
    "        WHERE id IN ({player_ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    player_ratings = pd.DataFrame(con.execute(rating_query).df())\n",
    "    print(f\"   ‚úì Retrieved ratings for {len(player_ratings):,} players\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"   ‚úì Database connection closed\")\n",
    "\n",
    "# Merge ratings into clean_data for analysis\n",
    "print(f\"\\n2Ô∏è‚É£  Merging ratings with clean_data...\")\n",
    "clean_data_with_ratings = clean_data.merge(player_ratings[['player_id', 'rating']], on='player_id', how='left')\n",
    "print(f\"   ‚úì Merged successfully\")\n",
    "\n",
    "# Check for missing ratings\n",
    "num_missing_ratings = clean_data_with_ratings['rating'].isna().sum()\n",
    "if num_missing_ratings > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  {num_missing_ratings:,} entries ({100*num_missing_ratings/len(clean_data_with_ratings):.2f}%) have missing ratings\")\n",
    "else:\n",
    "    print(f\"   ‚úì All entries have ratings\")\n",
    "\n",
    "# Basic rating statistics\n",
    "print(f\"\\n3Ô∏è‚É£  Basic rating statistics:\")\n",
    "print(f\"   ‚Ä¢ Count: {player_ratings['rating'].notna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Missing: {player_ratings['rating'].isna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Min: {player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Max: {player_ratings['rating'].max():.0f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {player_ratings['rating'].mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"   ‚Ä¢ Std Dev: {player_ratings['rating'].std():.2f}\")\n",
    "\n",
    "# Quartile statistics\n",
    "print(f\"\\n4Ô∏è‚É£  Quartile statistics:\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {player_ratings['rating'].quantile(0.25):.0f}\")\n",
    "print(f\"   ‚Ä¢ 50th percentile (median): {player_ratings['rating'].quantile(0.50):.0f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {player_ratings['rating'].quantile(0.75):.0f}\")\n",
    "\n",
    "# Granular percentile statistics (5% increments)\n",
    "print(f\"\\n5Ô∏è‚É£  Detailed percentile distribution (5% increments):\")\n",
    "percentiles = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,\n",
    "               0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "print(f\"\\n   {'Percentile':<12} {'Rating':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*12} {'-'*10} {'-'*40}\")\n",
    "\n",
    "for p in percentiles:\n",
    "    rating_value = player_ratings['rating'].quantile(p)\n",
    "    # Create a simple bar visualization\n",
    "    bar_length = int((rating_value - player_ratings['rating'].min()) / \n",
    "                     (player_ratings['rating'].max() - player_ratings['rating'].min()) * 40)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {p*100:>5.0f}%       {rating_value:>7.0f}    {bar}\")\n",
    "\n",
    "# Rating ranges and counts\n",
    "print(f\"\\n6Ô∏è‚É£  Rating distribution by range:\")\n",
    "rating_ranges = [\n",
    "    (0, 1000), (1000, 1200), (1200, 1400), (1400, 1600), \n",
    "    (1600, 1800), (1800, 2000), (2000, 2200), (2200, 2400), \n",
    "    (2400, 2600), (2600, 3000)\n",
    "]\n",
    "\n",
    "print(f\"\\n   {'Range':<15} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*15} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "\n",
    "for low, high in rating_ranges:\n",
    "    count = len(player_ratings[(player_ratings['rating'] >= low) & (player_ratings['rating'] < high)])\n",
    "    pct = 100 * count / len(player_ratings)\n",
    "    bar_length = int(pct * 0.4)  # Scale for visualization\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {low:>4}-{high:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Interquartile range\n",
    "iqr = player_ratings['rating'].quantile(0.75) - player_ratings['rating'].quantile(0.25)\n",
    "print(f\"\\n7Ô∏è‚É£  Spread statistics:\")\n",
    "print(f\"   ‚Ä¢ Range: {player_ratings['rating'].max() - player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Interquartile Range (IQR): {iqr:.0f}\")\n",
    "print(f\"   ‚Ä¢ 10th-90th percentile range: {player_ratings['rating'].quantile(0.90) - player_ratings['rating'].quantile(0.10):.0f}\")\n",
    "\n",
    "# Skewness and kurtosis if available\n",
    "try:\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    skewness = skew(player_ratings['rating'].dropna())\n",
    "    kurt = kurtosis(player_ratings['rating'].dropna())\n",
    "    print(f\"\\n8Ô∏è‚É£  Distribution shape:\")\n",
    "    print(f\"   ‚Ä¢ Skewness: {skewness:.4f} {'(right-skewed)' if skewness > 0 else '(left-skewed)' if skewness < 0 else '(symmetric)'}\")\n",
    "    print(f\"   ‚Ä¢ Kurtosis: {kurt:.4f} {'(heavy-tailed)' if kurt > 0 else '(light-tailed)' if kurt < 0 else '(normal)'}\")\n",
    "except ImportError:\n",
    "    print(f\"\\n8Ô∏è‚É£  Distribution shape:\")\n",
    "    print(f\"   ‚Ä¢ scipy not available for skewness/kurtosis calculation\")\n",
    "\n",
    "# Sample of players at different rating levels\n",
    "print(f\"\\n9Ô∏è‚É£  Sample players at different rating levels:\")\n",
    "sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "for p in sample_percentiles:\n",
    "    rating_threshold = player_ratings['rating'].quantile(p)\n",
    "    # Get a player near this rating\n",
    "    sample_player = player_ratings.iloc[(player_ratings['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "    print(f\"\\n   ~{p*100:.0f}th percentile (rating ‚âà {rating_threshold:.0f}):\")\n",
    "    for idx, row in sample_player.iterrows():\n",
    "        # print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f} {f'({row['title']})' if pd.notna(row['title']) else ''}\")\n",
    "        title_str = f\" ({row['title']})\" if pd.notna(row['title']) else \"\"\n",
    "        print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f}{title_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ RATING STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nKey takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total players: {len(player_ratings):,}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings['rating'].min():.0f}, {player_ratings['rating'].max():.0f}]\")\n",
    "print(f\"   ‚Ä¢ Mean ¬± std: {player_ratings['rating'].mean():.0f} ¬± {player_ratings['rating'].std():.0f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"\\n   Next steps: Normalize ratings for model input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ab6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Normalization strategy: Z-score\n",
      "   ‚Ä¢ Formula: (rating - mean) / std\n",
      "   ‚Ä¢ Purpose: Scale ratings for use as side information in MF model\n",
      "   ‚Ä¢ Storage: SEPARATE lookup table, NOT merged into clean_data\n",
      "   ‚Ä¢ Usage: Model will lookup player_id ‚Üí rating_z during training\n",
      "\n",
      "1Ô∏è‚É£  Normalization parameters (calculated from 13,990 players):\n",
      "   ‚Ä¢ Mean: 1772.43\n",
      "   ‚Ä¢ Std Dev: 248.34\n",
      "\n",
      "2Ô∏è‚É£  Normalized rating statistics:\n",
      "   ‚Ä¢ Min: -2.3050\n",
      "   ‚Ä¢ Max: 4.2304\n",
      "   ‚Ä¢ Mean: 0.000000 (should be ~0)\n",
      "   ‚Ä¢ Std: 1.000000 (should be ~1)\n",
      "   ‚Ä¢ Range: [-2.31, 4.23]\n",
      "\n",
      "3Ô∏è‚É£  Sample normalized ratings across skill levels:\n",
      "   ~10th percentile: Player 5516 | Rating: 1443 ‚Üí Z-score: -1.327\n",
      "   ~25th percentile: Player 2972 | Rating: 1593 ‚Üí Z-score: -0.723\n",
      "   ~50th percentile: Player 3421 | Rating: 1772 ‚Üí Z-score: -0.002\n",
      "   ~75th percentile: Player 2752 | Rating: 1944 ‚Üí Z-score:  0.691\n",
      "   ~90th percentile: Player 7235 | Rating: 2092 ‚Üí Z-score:  1.287\n",
      "\n",
      "4Ô∏è‚É£  Interpretation guide:\n",
      "   ‚Ä¢ rating_z ‚âà -2.3: 1200 player (minimum)\n",
      "   ‚Ä¢ rating_z ‚âà -0.7: 1593 player (25th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà  0.0: 1772 player (mean)\n",
      "   ‚Ä¢ rating_z ‚âà 0.7: 1944 player (75th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà 4.2: 2823 player (maximum)\n",
      "\n",
      "5Ô∏è‚É£  Side information table structure:\n",
      "   ‚Ä¢ Shape: (13990, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'rating', 'rating_z']\n",
      "   ‚Ä¢ Indexing: Setting player_id as index for O(1) lookups\n",
      "\n",
      "6Ô∏è‚É£  Sample entries from side information table:\n",
      "   Player 10137 | Rating: 1719 ‚Üí Z-score: -0.215\n",
      "   Player 10962 | Rating: 1626 ‚Üí Z-score: -0.590\n",
      "   Player 42021 | Rating: 1949 ‚Üí Z-score:  0.711\n",
      "   Player 13347 | Rating: 1919 ‚Üí Z-score:  0.590\n",
      "   Player 45984 | Rating: 1839 ‚Üí Z-score:  0.268\n",
      "   Player 35330 | Rating: 2169 ‚Üí Z-score:  1.597\n",
      "   Player 29822 | Rating: 1813 ‚Üí Z-score:  0.163\n",
      "   Player 25231 | Rating: 1903 ‚Üí Z-score:  0.526\n",
      "   Player 26465 | Rating: 1664 ‚Üí Z-score: -0.437\n",
      "   Player 19042 | Rating: 1878 ‚Üí Z-score:  0.425\n",
      "\n",
      "7Ô∏è‚É£  Removing unnecessary columns...\n",
      "   ‚úì Dropped 'rating' column (only keeping 'rating_z')\n",
      "   ‚Ä¢ Final columns: ['rating_z']\n",
      "\n",
      "8Ô∏è‚É£  Verifying all clean_data players have ratings:\n",
      "   ‚úì All 13,990 players in clean_data have side information\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING NORMALIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Created: player_side_info\n",
      "   ‚Ä¢ Shape: (13990, 1)\n",
      "   ‚Ä¢ Index: player_id\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ clean_data: 888,341 rows (player-opening interactions)\n",
      "   ‚Ä¢ player_side_info: 13,990 rows (one per player)\n",
      "   ‚Ä¢ Rating storage: ONE value per player (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\n",
      "   RATING_MEAN = 1772.43\n",
      "   RATING_STD = 248.34\n",
      "\n",
      "   You'll need them to normalize ratings for new users at inference time.\n"
     ]
    }
   ],
   "source": [
    "# 2d. Normalize player ratings using z-score normalization (for use as side information in MF model)\n",
    "\n",
    "# Check if we've already created the player_side_info table\n",
    "if 'player_side_info' in globals() and 'rating_z' in player_side_info.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2D: RATING NORMALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'player_side_info' table already exists\")\n",
    "    print(\"   This indicates rating normalization has already been applied.\")\n",
    "    print(f\"\\nPlayer side info shape: {player_side_info.shape}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing normalized rating statistics:\")\n",
    "    print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "    print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing normalized ratings:\")\n",
    "    sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "    for idx, row in sample_data.iterrows():\n",
    "        print(f\"   Player {idx:>5} | {row['name']:<20} | \"\n",
    "              f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "else:\n",
    "    def normalize_player_ratings(player_ratings_df):\n",
    "        \"\"\"\n",
    "        Apply z-score normalization to player ratings for use as side information.\n",
    "        \n",
    "        This creates a SEPARATE table of player-level features, NOT merged into clean_data.\n",
    "        Rating is side information - it describes the player, not the player-opening interaction.\n",
    "        \n",
    "        During training, the model will LOOK UP each player's rating_z from this table.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ratings_df : pd.DataFrame\n",
    "            Player ratings with columns: player_id, name, title, rating\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (player_side_info DataFrame, RATING_MEAN, RATING_STD)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Normalization strategy: Z-score\")\n",
    "        print(f\"   ‚Ä¢ Formula: (rating - mean) / std\")\n",
    "        print(f\"   ‚Ä¢ Purpose: Scale ratings for use as side information in MF model\")\n",
    "        print(f\"   ‚Ä¢ Storage: SEPARATE lookup table, NOT merged into clean_data\")\n",
    "        print(f\"   ‚Ä¢ Usage: Model will lookup player_id ‚Üí rating_z during training\")\n",
    "        \n",
    "        # Calculate normalization parameters\n",
    "        RATING_MEAN = player_ratings_df['rating'].mean()\n",
    "        RATING_STD = player_ratings_df['rating'].std()\n",
    "        \n",
    "        print(f\"\\n1Ô∏è‚É£  Normalization parameters (calculated from {len(player_ratings_df):,} players):\")\n",
    "        print(f\"   ‚Ä¢ Mean: {RATING_MEAN:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Std Dev: {RATING_STD:.2f}\")\n",
    "        \n",
    "        # Create side information table - only keep player_id and rating for now\n",
    "        player_side_info = player_ratings_df[['player_id', 'rating']].copy()\n",
    "        player_side_info['rating_z'] = (player_side_info['rating'] - RATING_MEAN) / RATING_STD\n",
    "        \n",
    "        print(f\"\\n2Ô∏è‚É£  Normalized rating statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "        print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "        print(f\"   ‚Ä¢ Range: [{player_side_info['rating_z'].min():.2f}, {player_side_info['rating_z'].max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\n3Ô∏è‚É£  Sample normalized ratings across skill levels:\")\n",
    "        sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "        for p in sample_percentiles:\n",
    "            rating_threshold = player_side_info['rating'].quantile(p)\n",
    "            sample_player = player_side_info.iloc[(player_side_info['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "            for idx, row in sample_player.iterrows():\n",
    "                print(f\"   ~{p*100:.0f}th percentile: Player {idx} | \"\n",
    "                      f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n4Ô∏è‚É£  Interpretation guide:\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(1200 - RATING_MEAN)/RATING_STD:.1f}: 1200 player (minimum)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.25) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.25):.0f} player (25th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà  0.0: {RATING_MEAN:.0f} player (mean)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.75) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.75):.0f} player (75th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].max() - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].max():.0f} player (maximum)\")\n",
    "        \n",
    "        print(f\"\\n5Ô∏è‚É£  Side information table structure:\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        print(f\"   ‚Ä¢ Indexing: Setting player_id as index for O(1) lookups\")\n",
    "        \n",
    "        # Set player_id as index for fast lookups\n",
    "        player_side_info = player_side_info.set_index('player_id')\n",
    "        \n",
    "        print(f\"\\n6Ô∏è‚É£  Sample entries from side information table:\")\n",
    "        sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "        for idx, row in sample_data.iterrows():\n",
    "            print(f\"   Player {idx:>5} | Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n7Ô∏è‚É£  Removing unnecessary columns...\")\n",
    "        # Drop rating column - we only need rating_z for the model\n",
    "        player_side_info = player_side_info.drop(columns=['rating'])\n",
    "        print(f\"   ‚úì Dropped 'rating' column (only keeping 'rating_z')\")\n",
    "        print(f\"   ‚Ä¢ Final columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\n8Ô∏è‚É£  Verifying all clean_data players have ratings:\")\n",
    "        # This is important - make sure every player in clean_data has a rating\n",
    "        missing_players = set(clean_data['player_id'].unique()) - set(player_side_info.index)\n",
    "        if len(missing_players) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in clean_data are missing from side_info!\")\n",
    "            print(f\"   Missing player IDs: {sorted(list(missing_players))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(player_side_info):,} players in clean_data have side information\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ RATING NORMALIZATION COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nCreated: player_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: player_id\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ clean_data: {clean_data.shape[0]:,} rows (player-opening interactions)\")\n",
    "        print(f\"   ‚Ä¢ player_side_info: {len(player_side_info):,} rows (one per player)\")\n",
    "        print(f\"   ‚Ä¢ Rating storage: ONE value per player (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\")\n",
    "        print(f\"   RATING_MEAN = {RATING_MEAN:.2f}\")\n",
    "        print(f\"   RATING_STD = {RATING_STD:.2f}\")\n",
    "        print(f\"\\n   You'll need them to normalize ratings for new users at inference time.\")\n",
    "        \n",
    "        return player_side_info, RATING_MEAN, RATING_STD\n",
    "    \n",
    "    # Call the function\n",
    "    player_side_info, RATING_MEAN, RATING_STD = normalize_player_ratings(player_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14eba9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        player_id  opening_id  num_games     score  eco  confidence\n",
      "487329      26071        2065         25  0.439814  C50    0.333333\n",
      "669708      36043        3181         17  0.485487  B07    0.253731\n",
      "266003      14128         639         31  0.494622  A84    0.382716\n",
      "762628      40853        1190         22  0.458638  B43    0.305556\n",
      "352308      18756         744         12  0.511143  B00    0.193548\n",
      "505130      27039        1997         17  0.448246  C45    0.253731\n",
      "451934      24185        3179         17  0.535806  C25    0.253731\n",
      "656379      35338         812         10  0.531584  B03    0.166667\n",
      "445475      23782        2032         33  0.573376  C48    0.397590\n",
      "833372      44792         737        193  0.526493  B00    0.794239\n",
      "162468       8812        1138         27  0.491085  B32    0.350649\n",
      "835082      44902         509         11  0.550738  A48    0.180328\n",
      "761837      40804        1988         16  0.497570  C45    0.242424\n",
      "822200      44071         772         90  0.527435  B01    0.642857\n",
      "797881      42792        2200         23  0.499107  C60    0.315068\n",
      "578647      31042         799         29  0.445306  B02    0.367089\n",
      "652155      35074        2188         16  0.539882  C58    0.242424\n",
      "321894      17094         476         12  0.509446  A45    0.193548\n",
      "241630      12880         447         77  0.434508  A43    0.606299\n",
      "717259      38515        1226         22  0.474834  B54    0.305556\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac119ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           rating_z\n",
      "player_id          \n",
      "6530      -0.122537\n",
      "28608     -0.086297\n",
      "48550     -0.042003\n",
      "22292      1.568692\n",
      "43745     -0.633933\n",
      "40548      0.574088\n",
      "14351      0.775425\n",
      "9272       0.831799\n",
      "23041      0.356644\n",
      "34588      0.723077\n"
     ]
    }
   ],
   "source": [
    "print(player_side_info.sample(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30280db",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test/Val splits\n",
    "\n",
    "Here, I split my data and drop columns that are no longer needed. We're very close to being able to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf3849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: TRAIN/VALIDATION/TEST SPLIT\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ Train: 75%\n",
      "   ‚Ä¢ Validation: 15%\n",
      "   ‚Ä¢ Test: 10%\n",
      "   ‚Ä¢ Random seed: 42 (for reproducibility)\n",
      "\n",
      "1Ô∏è‚É£  Preparing data for split...\n",
      "   ‚Ä¢ Features (X): (888341, 4)\n",
      "   ‚Ä¢ Target (y): (888341,)\n",
      "   ‚Ä¢ Feature columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Cleaning player side information...\n",
      "   ‚Ä¢ Original player_side_info shape: (13990, 1)\n",
      "   ‚Ä¢ Cleaned player_side_info shape: (13990, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "3Ô∏è‚É£  Splitting data (optimized approach)...\n",
      "   ‚Ä¢ Train: 666,255 samples (75.0%)\n",
      "   ‚Ä¢ Validation: 133,251 samples (15.0%)\n",
      "   ‚Ä¢ Test: 88,835 samples (10.0%)\n",
      "\n",
      "4Ô∏è‚É£  Verification:\n",
      "   ‚Ä¢ Total samples: 888,341 (should equal 888,341)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "5Ô∏è‚É£  Computing coverage statistics (cached)...\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 13,977 unique players\n",
      "   ‚Ä¢ Val: 13,759 unique players\n",
      "   ‚Ä¢ Test: 13,556 unique players\n",
      "   ‚Ä¢ Total unique: 13,990 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,510 unique openings\n",
      "   ‚Ä¢ Val: 2,162 unique openings\n",
      "   ‚Ä¢ Test: 2,057 unique openings\n",
      "   ‚Ä¢ Total unique: 2,560 openings\n",
      "\n",
      "6Ô∏è‚É£  Cold start analysis (vectorized)...\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 6 (0.0%)\n",
      "   ‚Ä¢ Openings not in train: 32 (1.5%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 8 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 22 (1.1%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0416\n",
      "   ‚Ä¢ Min: 0.1005\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0415\n",
      "   ‚Ä¢ Min: 0.1800\n",
      "   ‚Ä¢ Max: 0.8333\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0416\n",
      "   ‚Ä¢ Min: 0.2500\n",
      "   ‚Ä¢ Max: 0.7759\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "   ‚Ä¢ Train: 666,255 samples (75.0%)\n",
      "   ‚Ä¢ Validation: 133,251 samples (15.0%)\n",
      "   ‚Ä¢ Test: 88,835 samples (10.0%)\n",
      "\n",
      "4Ô∏è‚É£  Verification:\n",
      "   ‚Ä¢ Total samples: 888,341 (should equal 888,341)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "5Ô∏è‚É£  Computing coverage statistics (cached)...\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 13,977 unique players\n",
      "   ‚Ä¢ Val: 13,759 unique players\n",
      "   ‚Ä¢ Test: 13,556 unique players\n",
      "   ‚Ä¢ Total unique: 13,990 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,510 unique openings\n",
      "   ‚Ä¢ Val: 2,162 unique openings\n",
      "   ‚Ä¢ Test: 2,057 unique openings\n",
      "   ‚Ä¢ Total unique: 2,560 openings\n",
      "\n",
      "6Ô∏è‚É£  Cold start analysis (vectorized)...\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 6 (0.0%)\n",
      "   ‚Ä¢ Openings not in train: 32 (1.5%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 8 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 22 (1.1%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0416\n",
      "   ‚Ä¢ Min: 0.1005\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0415\n",
      "   ‚Ä¢ Min: 0.1800\n",
      "   ‚Ä¢ Max: 0.8333\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0416\n",
      "   ‚Ä¢ Min: 0.2500\n",
      "   ‚Ä¢ Max: 0.7759\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.4117\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.4121\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.4123\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 666,255 samples (75%)\n",
      "   ‚Ä¢ Validation data: 133,251 samples (15%)\n",
      "   ‚Ä¢ Test data: 88,835 samples (10%)\n",
      "   ‚Ä¢ Player side info: 13,990 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n",
      "\n",
      "üì¶ Available datasets:\n",
      "   ‚Ä¢ X_train, y_train - Training features and targets\n",
      "   ‚Ä¢ X_val, y_val - Validation features and targets\n",
      "   ‚Ä¢ X_test, y_test - Test features and targets\n",
      "   ‚Ä¢ player_side_info_clean - Player ratings (indexed by player_id)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as categorical features\n",
      "   ‚Ä¢ Convert to PyTorch tensors\n",
      "   ‚Ä¢ Build matrix factorization model with side information\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.4117\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.4121\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.4123\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 666,255 samples (75%)\n",
      "   ‚Ä¢ Validation data: 133,251 samples (15%)\n",
      "   ‚Ä¢ Test data: 88,835 samples (10%)\n",
      "   ‚Ä¢ Player side info: 13,990 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n",
      "\n",
      "üì¶ Available datasets:\n",
      "   ‚Ä¢ X_train, y_train - Training features and targets\n",
      "   ‚Ä¢ X_val, y_val - Validation features and targets\n",
      "   ‚Ä¢ X_test, y_test - Test features and targets\n",
      "   ‚Ä¢ player_side_info_clean - Player ratings (indexed by player_id)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as categorical features\n",
      "   ‚Ä¢ Convert to PyTorch tensors\n",
      "   ‚Ä¢ Build matrix factorization model with side information\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train/Validation/Test Split (75/15/10) - OPTIMIZED\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: TRAIN/VALIDATION/TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   ‚Ä¢ Train: 75%\")\n",
    "print(f\"   ‚Ä¢ Validation: 15%\")\n",
    "print(f\"   ‚Ä¢ Test: 10%\")\n",
    "print(f\"   ‚Ä¢ Random seed: 42 (for reproducibility)\")\n",
    "\n",
    "# Prepare the data\n",
    "print(f\"\\n1Ô∏è‚É£  Preparing data for split...\")\n",
    "\n",
    "# Drop num_games from clean_data - we don't need it for training\n",
    "# Keep: player_id, opening_id, score, eco, confidence\n",
    "X = clean_data[[\"player_id\", \"opening_id\", \"eco\", \"confidence\"]].copy()\n",
    "y = clean_data[\"score\"].copy()\n",
    "\n",
    "print(f\"   ‚Ä¢ Features (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Target (y): {y.shape}\")\n",
    "print(f\"   ‚Ä¢ Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Clean up player_side_info - only keep rating_z\n",
    "print(f\"\\n2Ô∏è‚É£  Cleaning player side information...\")\n",
    "player_side_info_clean = player_side_info[[\"rating_z\"]].copy()\n",
    "print(f\"   ‚Ä¢ Original player_side_info shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Cleaned player_side_info shape: {player_side_info_clean.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info_clean.columns)}\")\n",
    "\n",
    "# OPTIMIZED: Use index-based splitting to avoid DataFrame copies\n",
    "print(f\"\\n3Ô∏è‚É£  Splitting data (optimized approach)...\")\n",
    "idx = np.arange(len(X))\n",
    "\n",
    "# First split: separate out test set (10%)\n",
    "idx_temp, idx_test = train_test_split(idx, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: split remaining into train (75%) and val (15%)\n",
    "# 15% of original = 15/90 ‚âà 0.1667 of temp\n",
    "idx_train, idx_val = train_test_split(idx_temp, test_size=15/90, random_state=42, shuffle=True)\n",
    "\n",
    "# Create splits using iloc (view, not copy)\n",
    "X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
    "X_test, y_test = X.iloc[idx_test], y.iloc[idx_test]\n",
    "\n",
    "print(f\"   ‚Ä¢ Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Validation: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split\n",
    "print(f\"\\n4Ô∏è‚É£  Verification:\")\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"   ‚Ä¢ Total samples: {total:,} (should equal {len(X):,})\")\n",
    "print(f\"   ‚Ä¢ Train %: {len(X_train)/total*100:.2f}% (target: 75%)\")\n",
    "print(f\"   ‚Ä¢ Val %: {len(X_val)/total*100:.2f}% (target: 15%)\")\n",
    "print(f\"   ‚Ä¢ Test %: {len(X_test)/total*100:.2f}% (target: 10%)\")\n",
    "\n",
    "# OPTIMIZED: Pre-compute unique arrays once\n",
    "print(f\"\\n5Ô∏è‚É£  Computing coverage statistics (cached)...\")\n",
    "players_train = X_train[\"player_id\"].unique()\n",
    "players_val = X_val[\"player_id\"].unique()\n",
    "players_test = X_test[\"player_id\"].unique()\n",
    "\n",
    "openings_train = X_train[\"opening_id\"].unique()\n",
    "openings_val = X_val[\"opening_id\"].unique()\n",
    "openings_test = X_test[\"opening_id\"].unique()\n",
    "\n",
    "print(f\"\\n   Players:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(players_train):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Val: {len(players_val):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Test: {len(players_test):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['player_id'].nunique():,} players\")\n",
    "\n",
    "print(f\"\\n   Openings:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(openings_train):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Val: {len(openings_val):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Test: {len(openings_test):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['opening_id'].nunique():,} openings\")\n",
    "\n",
    "# OPTIMIZED: Use NumPy setdiff1d for cold-start analysis (C-speed)\n",
    "print(f\"\\n6Ô∏è‚É£  Cold start analysis (vectorized)...\")\n",
    "\n",
    "val_cold_players = np.setdiff1d(players_val, players_train, assume_unique=True)\n",
    "val_cold_openings = np.setdiff1d(openings_val, openings_train, assume_unique=True)\n",
    "\n",
    "test_cold_players = np.setdiff1d(players_test, players_train, assume_unique=True)\n",
    "test_cold_openings = np.setdiff1d(openings_test, openings_train, assume_unique=True)\n",
    "\n",
    "print(f\"\\n   Validation set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(val_cold_players):,} ({len(val_cold_players)/len(players_val)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(val_cold_openings):,} ({len(val_cold_openings)/len(openings_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(test_cold_players):,} ({len(test_cold_players)/len(players_test)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(test_cold_openings):,} ({len(test_cold_openings)/len(openings_test)*100:.1f}%)\")\n",
    "\n",
    "# OPTIMIZED: Compute stats in one pass using describe()\n",
    "print(f\"\\n7Ô∏è‚É£  Score distribution across splits:\")\n",
    "\n",
    "y_train_stats = y_train.describe()\n",
    "y_val_stats = y_val.describe()\n",
    "y_test_stats = y_test.describe()\n",
    "\n",
    "print(f\"\\n   Train:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_train_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_train_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_train_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_val_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_val_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_val_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_test_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_test_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_test_stats['max']:.4f}\")\n",
    "\n",
    "# OPTIMIZED: Compute confidence stats in one pass\n",
    "print(f\"\\n8Ô∏è‚É£  Confidence distribution across splits:\")\n",
    "\n",
    "conf_train_stats = X_train['confidence'].describe()\n",
    "conf_val_stats = X_val['confidence'].describe()\n",
    "conf_test_stats = X_test['confidence'].describe()\n",
    "\n",
    "print(f\"\\n   Train:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_train_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_val_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_test_stats['50%']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SPLIT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   ‚Ä¢ Training data: {len(X_train):,} samples (75%)\")\n",
    "print(f\"   ‚Ä¢ Validation data: {len(X_val):,} samples (15%)\")\n",
    "print(f\"   ‚Ä¢ Test data: {len(X_test):,} samples (10%)\")\n",
    "print(f\"   ‚Ä¢ Player side info: {len(player_side_info_clean):,} players\")\n",
    "print(f\"   ‚Ä¢ Side info columns: {list(player_side_info_clean.columns)}\")\n",
    "\n",
    "print(f\"\\nüì¶ Available datasets:\")\n",
    "print(f\"   ‚Ä¢ X_train, y_train - Training features and targets\")\n",
    "print(f\"   ‚Ä¢ X_val, y_val - Validation features and targets\")\n",
    "print(f\"   ‚Ä¢ X_test, y_test - Test features and targets\")\n",
    "print(f\"   ‚Ä¢ player_side_info_clean - Player ratings (indexed by player_id)\")\n",
    "\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"   ‚Ä¢ Enumerate ECO codes as categorical features\")\n",
    "print(f\"   ‚Ä¢ Convert to PyTorch tensors\")\n",
    "print(f\"   ‚Ä¢ Build matrix factorization model with side information\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781a382",
   "metadata": {},
   "source": [
    "## Step 3b: Remap Player and Opening IDs to Sequential Integers\n",
    "\n",
    "**Why remap IDs?**\n",
    "- Database IDs may have gaps (e.g., [1, 5, 10, 15, ...]) from deleted entries\n",
    "- Embedding layers need 0-based contiguous indices for efficiency\n",
    "- Remapping saves memory (no unused embedding slots)\n",
    "\n",
    "**Process:**\n",
    "1. Check if IDs are already sequential (0 or 1-based with no gaps)\n",
    "2. If not, create mappings: old_id ‚Üí new_sequential_id\n",
    "3. Remap all DataFrames and side info tables\n",
    "4. Verify mappings with spot checks\n",
    "\n",
    "This ensures embeddings use minimal memory and indices align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b4c409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Processing player IDs...\n",
      "\n",
      "============================================================\n",
      "Checking player IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique players: 13990\n",
      "   ‚Ä¢ ID range: [3, 49989]\n",
      "   ‚ö†Ô∏è  player IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 35997\n",
      "   ‚Ä¢ Wasted embedding slots without remapping: 35997\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      player_id 3 ‚Üí 0\n",
      "      player_id 6 ‚Üí 1\n",
      "      player_id 10 ‚Üí 2\n",
      "      player_id 15 ‚Üí 3\n",
      "      player_id 16 ‚Üí 4\n",
      "      player_id 49959 ‚Üí 13985\n",
      "      player_id 49975 ‚Üí 13986\n",
      "      player_id 49976 ‚Üí 13987\n",
      "      player_id 49987 ‚Üí 13988\n",
      "      player_id 49989 ‚Üí 13989\n",
      "\n",
      "   Remapping 5 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "\n",
      "============================================================\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2560\n",
      "   ‚Ä¢ ID range: [2, 3589]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1028\n",
      "   ‚Ä¢ Wasted embedding slots without remapping: 1028\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 5 ‚Üí 1\n",
      "      opening_id 6 ‚Üí 2\n",
      "      opening_id 9 ‚Üí 3\n",
      "      opening_id 10 ‚Üí 4\n",
      "      opening_id 3564 ‚Üí 2555\n",
      "      opening_id 3566 ‚Üí 2556\n",
      "      opening_id 3572 ‚Üí 2557\n",
      "      opening_id 3575 ‚Üí 2558\n",
      "      opening_id 3589 ‚Üí 2559\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/4\n",
      "   ‚úì Remapped DataFrame 2/4\n",
      "   ‚úì Remapped DataFrame 3/4\n",
      "   ‚úì Remapped DataFrame 4/4\n",
      "\n",
      "   ‚úÖ Opening ID remapping complete!\n",
      "\n",
      "3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\n",
      "   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\n",
      "\n",
      "   Sampling 10 entries from X_train (before remapping was applied)...\n",
      "   ‚Ä¢ Sample indices: [0, 74028, 148056, 222084, 296112, 370140, 444168, 518196, 592224, 666254]\n",
      "   ‚Ä¢ These represent: first, evenly spaced middle rows, and last\n",
      "\n",
      "   Verification checks:\n",
      "   #    Row Idx    New Player   New Opening  Confidence   Status         \n",
      "   ---- ---------- ------------ ------------ ------------ ---------------\n",
      "   1    0          8732         2365         0.1803       ‚úì PASS         \n",
      "   2    74028      2961         580          0.4118       ‚úì PASS         \n",
      "   3    148056     3770         1001         0.2424       ‚úì PASS         \n",
      "   4    222084     1851         947          0.2857       ‚úì PASS         \n",
      "   5    296112     2941         537          0.2188       ‚úì PASS         \n",
      "   6    370140     13887        752          0.2063       ‚úì PASS         \n",
      "   7    444168     11655        1379         0.1667       ‚úì PASS         \n",
      "   8    518196     490          1178         0.7475       ‚úì PASS         \n",
      "   9    592224     10929        1402         0.5495       ‚úì PASS         \n",
      "   10   666254     11036        2303         0.2308       ‚úì PASS         \n",
      "\n",
      "   Reverse mapping verification (sample of 3 entries):\n",
      "   #    New‚ÜíOld Player                 New‚ÜíOld Opening               \n",
      "   ---- ------------------------------ ------------------------------\n",
      "   1    8732 ‚Üí 30258                   2365 ‚Üí 3247                   \n",
      "   6    13887 ‚Üí 49487                  752 ‚Üí 1017                    \n",
      "   10   11036 ‚Üí 38280                  2303 ‚Üí 3181                   \n",
      "\n",
      "   ‚úÖ All spot checks passed! ID mappings are correct.\n",
      "\n",
      "4Ô∏è‚É£  Summary:\n",
      "\n",
      "   Player IDs:\n",
      "   ‚Ä¢ Original range: [3, 49989]\n",
      "   ‚Ä¢ New range: [0, 13989]\n",
      "   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\n",
      "\n",
      "   Opening IDs:\n",
      "   ‚Ä¢ Original range: [2, 3589]\n",
      "   ‚Ä¢ New range: [0, 2559]\n",
      "   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\n",
      "\n",
      "   Updated DataFrames:\n",
      "   ‚Ä¢ X_train: (666255, 4)\n",
      "   ‚Ä¢ X_val: (133251, 4)\n",
      "   ‚Ä¢ X_test: (88835, 4)\n",
      "   ‚Ä¢ clean_data: (888341, 6)\n",
      "   ‚Ä¢ player_side_info: (13990, 1)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ID REMAPPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Important:\n",
      "   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\n",
      "   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\n",
      "   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\n",
      "   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\n",
      "\n",
      "============================================================\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2560\n",
      "   ‚Ä¢ ID range: [2, 3589]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1028\n",
      "   ‚Ä¢ Wasted embedding slots without remapping: 1028\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 5 ‚Üí 1\n",
      "      opening_id 6 ‚Üí 2\n",
      "      opening_id 9 ‚Üí 3\n",
      "      opening_id 10 ‚Üí 4\n",
      "      opening_id 3564 ‚Üí 2555\n",
      "      opening_id 3566 ‚Üí 2556\n",
      "      opening_id 3572 ‚Üí 2557\n",
      "      opening_id 3575 ‚Üí 2558\n",
      "      opening_id 3589 ‚Üí 2559\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/4\n",
      "   ‚úì Remapped DataFrame 2/4\n",
      "   ‚úì Remapped DataFrame 3/4\n",
      "   ‚úì Remapped DataFrame 4/4\n",
      "\n",
      "   ‚úÖ Opening ID remapping complete!\n",
      "\n",
      "3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\n",
      "   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\n",
      "\n",
      "   Sampling 10 entries from X_train (before remapping was applied)...\n",
      "   ‚Ä¢ Sample indices: [0, 74028, 148056, 222084, 296112, 370140, 444168, 518196, 592224, 666254]\n",
      "   ‚Ä¢ These represent: first, evenly spaced middle rows, and last\n",
      "\n",
      "   Verification checks:\n",
      "   #    Row Idx    New Player   New Opening  Confidence   Status         \n",
      "   ---- ---------- ------------ ------------ ------------ ---------------\n",
      "   1    0          8732         2365         0.1803       ‚úì PASS         \n",
      "   2    74028      2961         580          0.4118       ‚úì PASS         \n",
      "   3    148056     3770         1001         0.2424       ‚úì PASS         \n",
      "   4    222084     1851         947          0.2857       ‚úì PASS         \n",
      "   5    296112     2941         537          0.2188       ‚úì PASS         \n",
      "   6    370140     13887        752          0.2063       ‚úì PASS         \n",
      "   7    444168     11655        1379         0.1667       ‚úì PASS         \n",
      "   8    518196     490          1178         0.7475       ‚úì PASS         \n",
      "   9    592224     10929        1402         0.5495       ‚úì PASS         \n",
      "   10   666254     11036        2303         0.2308       ‚úì PASS         \n",
      "\n",
      "   Reverse mapping verification (sample of 3 entries):\n",
      "   #    New‚ÜíOld Player                 New‚ÜíOld Opening               \n",
      "   ---- ------------------------------ ------------------------------\n",
      "   1    8732 ‚Üí 30258                   2365 ‚Üí 3247                   \n",
      "   6    13887 ‚Üí 49487                  752 ‚Üí 1017                    \n",
      "   10   11036 ‚Üí 38280                  2303 ‚Üí 3181                   \n",
      "\n",
      "   ‚úÖ All spot checks passed! ID mappings are correct.\n",
      "\n",
      "4Ô∏è‚É£  Summary:\n",
      "\n",
      "   Player IDs:\n",
      "   ‚Ä¢ Original range: [3, 49989]\n",
      "   ‚Ä¢ New range: [0, 13989]\n",
      "   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\n",
      "\n",
      "   Opening IDs:\n",
      "   ‚Ä¢ Original range: [2, 3589]\n",
      "   ‚Ä¢ New range: [0, 2559]\n",
      "   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\n",
      "\n",
      "   Updated DataFrames:\n",
      "   ‚Ä¢ X_train: (666255, 4)\n",
      "   ‚Ä¢ X_val: (133251, 4)\n",
      "   ‚Ä¢ X_test: (88835, 4)\n",
      "   ‚Ä¢ clean_data: (888341, 6)\n",
      "   ‚Ä¢ player_side_info: (13990, 1)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ID REMAPPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Important:\n",
      "   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\n",
      "   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\n",
      "   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\n",
      "   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\n"
     ]
    }
   ],
   "source": [
    "# Step 3b: Remap player and opening IDs to 0-based sequential integers\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def check_and_remap_ids(df_list, id_column, entity_name):\n",
    "    \"\"\"\n",
    "    Check if IDs are sequential starting from 0, and remap if not.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of DataFrames\n",
    "        List of DataFrames containing the ID column to check/remap\n",
    "    id_column : str\n",
    "        Name of the ID column ('player_id' or 'opening_id')\n",
    "    entity_name : str\n",
    "        Name for logging ('player' or 'opening')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (df_list with remapped IDs, id_to_idx mapping dict, needs_remapping bool)\n",
    "    \"\"\"\n",
    "    # Get all unique IDs across all dataframes\n",
    "    all_ids = pd.concat([df[id_column] for df in df_list]).unique()\n",
    "    all_ids_sorted = sorted(all_ids)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Checking {entity_name} IDs...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ‚Ä¢ Total unique {entity_name}s: {len(all_ids_sorted)}\")\n",
    "    print(f\"   ‚Ä¢ ID range: [{all_ids_sorted[0]}, {all_ids_sorted[-1]}]\")\n",
    "    \n",
    "    # Check if IDs are already 0-based sequential (0, 1, 2, ...)\n",
    "    expected_sequential = list(range(len(all_ids_sorted)))\n",
    "    is_sequential = (all_ids_sorted == expected_sequential)\n",
    "    \n",
    "    if is_sequential:\n",
    "        print(f\"   ‚úì {entity_name} IDs are already 0-based sequential - no remapping needed!\")\n",
    "        return df_list, None, False\n",
    "    \n",
    "    # Check if IDs are 1-based sequential (1, 2, 3, ...)\n",
    "    expected_sequential_1based = list(range(1, len(all_ids_sorted) + 1))\n",
    "    is_sequential_1based = (all_ids_sorted == expected_sequential_1based)\n",
    "    \n",
    "    if is_sequential_1based:\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs are 1-based sequential - will remap to 0-based\")\n",
    "    else:\n",
    "        # Calculate gaps\n",
    "        num_gaps = (all_ids_sorted[-1] - all_ids_sorted[0] + 1) - len(all_ids_sorted)\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs have gaps - will remap to 0-based sequential\")\n",
    "        print(f\"   ‚Ä¢ Number of gaps: {num_gaps}\")\n",
    "        print(f\"   ‚Ä¢ Wasted embedding slots without remapping: {num_gaps}\")\n",
    "    \n",
    "    # Create mapping: old_id -> new_idx (0-based)\n",
    "    id_to_idx = {old_id: new_idx for new_idx, old_id in enumerate(all_ids_sorted)}\n",
    "    idx_to_id = {new_idx: old_id for old_id, new_idx in id_to_idx.items()}\n",
    "    \n",
    "    print(f\"\\n   Creating mapping...\")\n",
    "    print(f\"   ‚Ä¢ Example mappings:\")\n",
    "    sample_ids = all_ids_sorted[:5] + all_ids_sorted[-5:]\n",
    "    for old_id in sample_ids[:10]:  # Show first 5 and last 5\n",
    "        print(f\"      {entity_name}_id {old_id} ‚Üí {id_to_idx[old_id]}\")\n",
    "    \n",
    "    # Remap all DataFrames\n",
    "    print(f\"\\n   Remapping {len(df_list)} DataFrames...\")\n",
    "    remapped_dfs = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[id_column] = df_copy[id_column].map(id_to_idx)\n",
    "        remapped_dfs.append(df_copy)\n",
    "        print(f\"   ‚úì Remapped DataFrame {i+1}/{len(df_list)}\")\n",
    "    \n",
    "    return remapped_dfs, (id_to_idx, idx_to_id), True\n",
    "\n",
    "# 1. Remap player IDs\n",
    "print(f\"\\n1Ô∏è‚É£  Processing player IDs...\")\n",
    "player_dfs = [X_train, X_val, X_test, clean_data, player_side_info.reset_index()]\n",
    "remapped_player_dfs, player_mappings, player_remapped = check_and_remap_ids(\n",
    "    player_dfs, 'player_id', 'player'\n",
    ")\n",
    "\n",
    "if player_remapped:\n",
    "    X_train, X_val, X_test, clean_data, player_side_info_remapped = remapped_player_dfs\n",
    "    player_id_to_idx, player_idx_to_id = player_mappings\n",
    "    player_side_info = player_side_info_remapped.set_index('player_id')\n",
    "    print(f\"\\n   ‚úÖ Player ID remapping complete!\")\n",
    "else:\n",
    "    player_id_to_idx, player_idx_to_id = None, None\n",
    "\n",
    "# 2. Remap opening IDs\n",
    "print(f\"\\n2Ô∏è‚É£  Processing opening IDs...\")\n",
    "opening_dfs = [X_train, X_val, X_test, clean_data]\n",
    "remapped_opening_dfs, opening_mappings, opening_remapped = check_and_remap_ids(\n",
    "    opening_dfs, 'opening_id', 'opening'\n",
    ")\n",
    "\n",
    "if opening_remapped:\n",
    "    X_train, X_val, X_test, clean_data = remapped_opening_dfs\n",
    "    opening_id_to_idx, opening_idx_to_id = opening_mappings\n",
    "    print(f\"\\n   ‚úÖ Opening ID remapping complete!\")\n",
    "else:\n",
    "    opening_id_to_idx, opening_idx_to_id = None, None\n",
    "\n",
    "# 3. Spot checks to verify mappings\n",
    "print(f\"\\n3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\")\n",
    "print(f\"   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\")\n",
    "\n",
    "# Sample 10 entries: first, last, and 8 in between\n",
    "print(f\"\\n   Sampling 10 entries from X_train (before remapping was applied)...\")\n",
    "total_rows = len(X_train)\n",
    "# Get indices: first, last, and 8 evenly spaced in between\n",
    "sample_indices = [0]  # First row\n",
    "step = (total_rows - 1) // 9  # Divide remaining rows into 9 parts\n",
    "sample_indices.extend([min(i * step, total_rows - 1) for i in range(1, 9)])\n",
    "sample_indices.append(total_rows - 1)  # Last row\n",
    "\n",
    "print(f\"   ‚Ä¢ Sample indices: {sample_indices}\")\n",
    "print(f\"   ‚Ä¢ These represent: first, evenly spaced middle rows, and last\")\n",
    "\n",
    "# Store samples with their NEW (remapped) IDs\n",
    "samples = []\n",
    "for idx in sample_indices:\n",
    "    row = X_train.iloc[idx]\n",
    "    samples.append({\n",
    "        'index': idx,\n",
    "        'new_player_id': row['player_id'],\n",
    "        'new_opening_id': row['opening_id'],\n",
    "        'confidence': row['confidence']\n",
    "    })\n",
    "\n",
    "print(f\"\\n   Verification checks:\")\n",
    "print(f\"   {'#':<4} {'Row Idx':<10} {'New Player':<12} {'New Opening':<12} {'Confidence':<12} {'Status':<15}\")\n",
    "print(f\"   {'-'*4} {'-'*10} {'-'*12} {'-'*12} {'-'*12} {'-'*15}\")\n",
    "\n",
    "all_checks_passed = True\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    new_player_id = sample['new_player_id']\n",
    "    new_opening_id = sample['new_opening_id']\n",
    "    confidence = sample['confidence']\n",
    "    idx = sample['index']\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: New player ID is valid (0-based sequential)\n",
    "    if player_remapped:\n",
    "        player_valid = 0 <= new_player_id < len(player_id_to_idx)\n",
    "        checks.append((\"player_id\", player_valid))\n",
    "    else:\n",
    "        checks.append((\"player_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 2: New opening ID is valid (0-based sequential)\n",
    "    if opening_remapped:\n",
    "        opening_valid = 0 <= new_opening_id < len(opening_id_to_idx)\n",
    "        checks.append((\"opening_id\", opening_valid))\n",
    "    else:\n",
    "        checks.append((\"opening_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 3: Player exists in player_side_info\n",
    "    player_exists = new_player_id in player_side_info.index\n",
    "    checks.append((\"in_side_info\", player_exists))\n",
    "    \n",
    "    # Check 4: Opening exists in clean_data\n",
    "    opening_exists = new_opening_id in clean_data['opening_id'].values\n",
    "    checks.append((\"in_clean\", opening_exists))\n",
    "    \n",
    "    # All checks must pass\n",
    "    all_pass = all(check[1] for check in checks)\n",
    "    status = \"‚úì PASS\" if all_pass else f\"‚úó FAIL ({','.join([c[0] for c in checks if not c[1]])})\"\n",
    "    \n",
    "    if not all_pass:\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(f\"   {i:<4} {idx:<10} {new_player_id:<12} {new_opening_id:<12} {confidence:<12.4f} {status:<15}\")\n",
    "\n",
    "# Additional verification: check if we can reverse map\n",
    "if player_remapped or opening_remapped:\n",
    "    print(f\"\\n   Reverse mapping verification (sample of 3 entries):\")\n",
    "    print(f\"   {'#':<4} {'New‚ÜíOld Player':<30} {'New‚ÜíOld Opening':<30}\")\n",
    "    print(f\"   {'-'*4} {'-'*30} {'-'*30}\")\n",
    "    \n",
    "    for i in [0, len(samples)//2, len(samples)-1]:  # First, middle, last\n",
    "        sample = samples[i]\n",
    "        \n",
    "        # Reverse map player\n",
    "        if player_remapped:\n",
    "            old_player = player_idx_to_id.get(sample['new_player_id'], 'NOT_FOUND')\n",
    "            player_str = f\"{sample['new_player_id']} ‚Üí {old_player}\"\n",
    "        else:\n",
    "            player_str = f\"{sample['new_player_id']} (unchanged)\"\n",
    "        \n",
    "        # Reverse map opening\n",
    "        if opening_remapped:\n",
    "            old_opening = opening_idx_to_id.get(sample['new_opening_id'], 'NOT_FOUND')\n",
    "            opening_str = f\"{sample['new_opening_id']} ‚Üí {old_opening}\"\n",
    "        else:\n",
    "            opening_str = f\"{sample['new_opening_id']} (unchanged)\"\n",
    "        \n",
    "        print(f\"   {i+1:<4} {player_str:<30} {opening_str:<30}\")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(f\"\\n   ‚úÖ All spot checks passed! ID mappings are correct.\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Some spot checks failed - investigate immediately!\")\n",
    "    raise ValueError(\"ID remapping verification failed!\")\n",
    "\n",
    "# 4. Summary\n",
    "print(f\"\\n4Ô∏è‚É£  Summary:\")\n",
    "print(f\"\\n   Player IDs:\")\n",
    "if player_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{player_idx_to_id[0]}, {player_idx_to_id[len(player_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(player_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "    \n",
    "print(f\"\\n   Opening IDs:\")\n",
    "if opening_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{opening_idx_to_id[0]}, {opening_idx_to_id[len(opening_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(opening_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "\n",
    "print(f\"\\n   Updated DataFrames:\")\n",
    "print(f\"   ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ X_val: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ clean_data: {clean_data.shape}\")\n",
    "print(f\"   ‚Ä¢ player_side_info: {player_side_info.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ID REMAPPING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Important:\")\n",
    "print(f\"   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\")\n",
    "print(f\"   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\")\n",
    "print(f\"   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\")\n",
    "print(f\"   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a580c0",
   "metadata": {},
   "source": [
    "## 4. Enumerate Categorical Variables\n",
    "\n",
    "I believe the only variable we need to enumerate here is `eco`. That's the broad categorization of a specific opening.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- One ECO code will have many openings\n",
    "- They're sorted by letter, then further by number. For instance, C21 and C44 are in the `C` family.\n",
    "- Maybe we make this side information?\n",
    "\n",
    "First, let's get some data on ECO codes to help us better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9504a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: ECO CODE STATISTICS\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Overall ECO statistics:\n",
      "   ‚Ä¢ Total unique ECO codes: 452\n",
      "   ‚Ä¢ Total entries: 888,341\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 1965.4\n",
      "   ‚Ä¢ Median entries per ECO: 249.5\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 64397\n",
      "   ‚Ä¢ Std: 5801.6\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "   ‚Ä¢ Total unique ECO codes: 452\n",
      "   ‚Ä¢ Total entries: 888,341\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 1965.4\n",
      "   ‚Ä¢ Median entries per ECO: 249.5\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 64397\n",
      "   ‚Ä¢ Std: 5801.6\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        135,555     15.26%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        339,661     38.24%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        318,997     35.91%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D         74,903      8.43%      ‚ñà‚ñà‚ñà\n",
      "   E         19,225      2.16%      \n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00     64,397      7.25%      ‚ñà‚ñà\n",
      "   2      B01     63,485      7.15%      ‚ñà‚ñà\n",
      "   3      C50     32,812      3.69%      ‚ñà\n",
      "   4      A40     32,412      3.65%      ‚ñà\n",
      "   5      C00     27,714      3.12%      \n",
      "   6      C40     26,477      2.98%      \n",
      "   7      B06     22,689      2.55%      \n",
      "   8      C42     21,231      2.39%      \n",
      "   9      C44     19,501      2.20%      \n",
      "   10     B10     18,000      2.03%      \n",
      "   11     C41     17,936      2.02%      \n",
      "   12     B40     17,396      1.96%      \n",
      "   13     B12     15,508      1.75%      \n",
      "   14     C02     15,432      1.74%      \n",
      "   15     A00     13,190      1.48%      \n",
      "   16     B21     12,490      1.41%      \n",
      "   17     D00     11,830      1.33%      \n",
      "   18     B32     11,351      1.28%      \n",
      "   19     B02     10,108      1.14%      \n",
      "   20     A43     10,068      1.13%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      D48          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   2      C99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   3      E45          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   4      A75          3    ‚ñà‚ñà‚ñà\n",
      "   5      D98          3    ‚ñà‚ñà‚ñà\n",
      "   6      B58          2    ‚ñà‚ñà\n",
      "   7      D84          2    ‚ñà‚ñà\n",
      "   8      D62          2    ‚ñà‚ñà\n",
      "   9      C94          2    ‚ñà‚ñà\n",
      "   10     D68          2    ‚ñà‚ñà\n",
      "   11     C76          2    ‚ñà‚ñà\n",
      "   12     E78          1    ‚ñà\n",
      "   13     D29          1    ‚ñà\n",
      "   14     A72          1    ‚ñà\n",
      "   15     E79          1    ‚ñà\n",
      "   16     E26          1    ‚ñà\n",
      "   17     D49          1    ‚ñà\n",
      "   18     A63          1    ‚ñà\n",
      "   19     B69          1    ‚ñà\n",
      "   20     E31          1    ‚ñà\n",
      "\n",
      "6Ô∏è‚É£  ECO code format analysis:\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        135,555     15.26%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        339,661     38.24%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        318,997     35.91%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D         74,903      8.43%      ‚ñà‚ñà‚ñà\n",
      "   E         19,225      2.16%      \n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00     64,397      7.25%      ‚ñà‚ñà\n",
      "   2      B01     63,485      7.15%      ‚ñà‚ñà\n",
      "   3      C50     32,812      3.69%      ‚ñà\n",
      "   4      A40     32,412      3.65%      ‚ñà\n",
      "   5      C00     27,714      3.12%      \n",
      "   6      C40     26,477      2.98%      \n",
      "   7      B06     22,689      2.55%      \n",
      "   8      C42     21,231      2.39%      \n",
      "   9      C44     19,501      2.20%      \n",
      "   10     B10     18,000      2.03%      \n",
      "   11     C41     17,936      2.02%      \n",
      "   12     B40     17,396      1.96%      \n",
      "   13     B12     15,508      1.75%      \n",
      "   14     C02     15,432      1.74%      \n",
      "   15     A00     13,190      1.48%      \n",
      "   16     B21     12,490      1.41%      \n",
      "   17     D00     11,830      1.33%      \n",
      "   18     B32     11,351      1.28%      \n",
      "   19     B02     10,108      1.14%      \n",
      "   20     A43     10,068      1.13%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      D48          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   2      C99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   3      E45          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   4      A75          3    ‚ñà‚ñà‚ñà\n",
      "   5      D98          3    ‚ñà‚ñà‚ñà\n",
      "   6      B58          2    ‚ñà‚ñà\n",
      "   7      D84          2    ‚ñà‚ñà\n",
      "   8      D62          2    ‚ñà‚ñà\n",
      "   9      C94          2    ‚ñà‚ñà\n",
      "   10     D68          2    ‚ñà‚ñà\n",
      "   11     C76          2    ‚ñà‚ñà\n",
      "   12     E78          1    ‚ñà\n",
      "   13     D29          1    ‚ñà\n",
      "   14     A72          1    ‚ñà\n",
      "   15     E79          1    ‚ñà\n",
      "   16     E26          1    ‚ñà\n",
      "   17     D49          1    ‚ñà\n",
      "   18     A63          1    ‚ñà\n",
      "   19     B69          1    ‚ñà\n",
      "   20     E31          1    ‚ñà\n",
      "\n",
      "6Ô∏è‚É£  ECO code format analysis:\n",
      "   ‚Ä¢ ECO code lengths:\n",
      "      3 characters: 888,341 (100.00%)\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   A06, A61, A87, A90, B17, B22, B44, C10, C12, C47, C78, C96, D40, D50, D55, D66, D77, D95, E04, E09\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   ‚Ä¢ ECO code lengths:\n",
      "      3 characters: 888,341 (100.00%)\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   A06, A61, A87, A90, B17, B22, B44, C10, C12, C47, C78, C96, D40, D50, D55, D66, D77, D95, E04, E09\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   ‚Ä¢ Unique ECO codes: 448\n",
      "   ‚Ä¢ Total entries: 666,255\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 427\n",
      "   ‚Ä¢ Total entries: 133,251\n",
      "   ‚Ä¢ Unique ECO codes: 448\n",
      "   ‚Ä¢ Total entries: 666,255\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 427\n",
      "   ‚Ä¢ Total entries: 133,251\n",
      "   ‚Ä¢ ECO codes not in train: 2\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 422\n",
      "   ‚Ä¢ Total entries: 88,835\n",
      "   ‚Ä¢ ECO codes not in train: 2\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "   ‚Ä¢ ECO codes not in train: 2\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 422\n",
      "   ‚Ä¢ Total entries: 88,835\n",
      "   ‚Ä¢ ECO codes not in train: 2\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   D29    0.6765             1\n",
      "   B85    0.6127             5\n",
      "   C94    0.6116             2\n",
      "   A72    0.6000             1\n",
      "   E23    0.5979            13\n",
      "   E75    0.5976            12\n",
      "   C14    0.5840            74\n",
      "   D98    0.5809             3\n",
      "   B71    0.5797            88\n",
      "   D84    0.5794             2\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C38    0.4557           505\n",
      "   C82    0.4507            15\n",
      "   D83    0.4459            30\n",
      "   D56    0.4438             4\n",
      "   C99    0.4352             4\n",
      "   E25    0.4315             4\n",
      "   B58    0.4073             2\n",
      "   E78    0.3636             1\n",
      "   D62    0.3569             2\n",
      "   E26    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   A97    0.0189       0.1373             4\n",
      "   E45    0.0112       0.1059             4\n",
      "   D97    0.0095       0.0976            31\n",
      "   B74    0.0080       0.0895            26\n",
      "   B62    0.0068       0.0825             8\n",
      "   C57    0.0062       0.0785         3,840\n",
      "   C39    0.0052       0.0722           471\n",
      "   C37    0.0048       0.0690         1,829\n",
      "   B55    0.0044       0.0665             5\n",
      "   C52    0.0043       0.0657           677\n",
      "\n",
      "1Ô∏è‚É£1Ô∏è‚É£  Openings per ECO code:\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   D29    0.6765             1\n",
      "   B85    0.6127             5\n",
      "   C94    0.6116             2\n",
      "   A72    0.6000             1\n",
      "   E23    0.5979            13\n",
      "   E75    0.5976            12\n",
      "   C14    0.5840            74\n",
      "   D98    0.5809             3\n",
      "   B71    0.5797            88\n",
      "   D84    0.5794             2\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C38    0.4557           505\n",
      "   C82    0.4507            15\n",
      "   D83    0.4459            30\n",
      "   D56    0.4438             4\n",
      "   C99    0.4352             4\n",
      "   E25    0.4315             4\n",
      "   B58    0.4073             2\n",
      "   E78    0.3636             1\n",
      "   D62    0.3569             2\n",
      "   E26    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   A97    0.0189       0.1373             4\n",
      "   E45    0.0112       0.1059             4\n",
      "   D97    0.0095       0.0976            31\n",
      "   B74    0.0080       0.0895            26\n",
      "   B62    0.0068       0.0825             8\n",
      "   C57    0.0062       0.0785         3,840\n",
      "   C39    0.0052       0.0722           471\n",
      "   C37    0.0048       0.0690         1,829\n",
      "   B55    0.0044       0.0665             5\n",
      "   C52    0.0043       0.0657           677\n",
      "\n",
      "1Ô∏è‚É£1Ô∏è‚É£  Openings per ECO code:\n",
      "   ‚Ä¢ Mean openings per ECO: 6.6\n",
      "   ‚Ä¢ Median openings per ECO: 3.0\n",
      "   ‚Ä¢ Max openings per ECO: 107\n",
      "   ‚Ä¢ Min openings per ECO: 1\n",
      "\n",
      "   Top 10 ECO codes by number of openings:\n",
      "\n",
      "   ECO    # Openings  \n",
      "   ------ ------------\n",
      "   A00           107\n",
      "   B00            87\n",
      "   D00            86\n",
      "   C42            59\n",
      "   A40            55\n",
      "   C44            45\n",
      "   B01            45\n",
      "   C00            42\n",
      "   B06            40\n",
      "   C33            38\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 452\n",
      "   ‚Ä¢ Most common family: B (339,661 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (64,397 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as integers for categorical encoding\n",
      "   ‚Ä¢ Consider ECO as opening-level side information (similar to player ratings)\n",
      "   ‚Ä¢ Verify all ECO codes in validation/test exist in training set\n",
      "   ‚Ä¢ Mean openings per ECO: 6.6\n",
      "   ‚Ä¢ Median openings per ECO: 3.0\n",
      "   ‚Ä¢ Max openings per ECO: 107\n",
      "   ‚Ä¢ Min openings per ECO: 1\n",
      "\n",
      "   Top 10 ECO codes by number of openings:\n",
      "\n",
      "   ECO    # Openings  \n",
      "   ------ ------------\n",
      "   A00           107\n",
      "   B00            87\n",
      "   D00            86\n",
      "   C42            59\n",
      "   A40            55\n",
      "   C44            45\n",
      "   B01            45\n",
      "   C00            42\n",
      "   B06            40\n",
      "   C33            38\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 452\n",
      "   ‚Ä¢ Most common family: B (339,661 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (64,397 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as integers for categorical encoding\n",
      "   ‚Ä¢ Consider ECO as opening-level side information (similar to player ratings)\n",
      "   ‚Ä¢ Verify all ECO codes in validation/test exist in training set\n"
     ]
    }
   ],
   "source": [
    "# Step 4: ECO Code Statistics (no mutations, just exploration)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: ECO CODE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic ECO statistics across all data\n",
    "print(f\"\\n1Ô∏è‚É£  Overall ECO statistics:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Missing ECO values: {clean_data['eco'].isna().sum()}\")\n",
    "\n",
    "# ECO value counts\n",
    "eco_counts = clean_data['eco'].value_counts().sort_index()\n",
    "print(f\"\\n2Ô∏è‚É£  Distribution of entries per ECO code:\")\n",
    "print(f\"   ‚Ä¢ Mean entries per ECO: {eco_counts.mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Median entries per ECO: {eco_counts.median():.1f}\")\n",
    "print(f\"   ‚Ä¢ Min entries: {eco_counts.min()}\")\n",
    "print(f\"   ‚Ä¢ Max entries: {eco_counts.max()}\")\n",
    "print(f\"   ‚Ä¢ Std: {eco_counts.std():.1f}\")\n",
    "\n",
    "# ECO by first letter (family)\n",
    "print(f\"\\n3Ô∏è‚É£  ECO families (by first letter):\")\n",
    "eco_families = clean_data['eco'].str[0].value_counts().sort_index()\n",
    "print(f\"\\n   {'Family':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "for family, count in eco_families.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.4)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {family:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Top 20 most common ECO codes\n",
    "print(f\"\\n4Ô∏è‚É£  Top 20 most common ECO codes:\")\n",
    "top_eco = clean_data['eco'].value_counts().head(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*12} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(top_eco.items(), 1):\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.3)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Bottom 20 least common ECO codes\n",
    "print(f\"\\n5Ô∏è‚É£  Bottom 20 least common ECO codes:\")\n",
    "bottom_eco = clean_data['eco'].value_counts().tail(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(bottom_eco.items(), 1):\n",
    "    bar_length = min(count, 30)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {bar}\")\n",
    "\n",
    "# ECO code format analysis\n",
    "print(f\"\\n6Ô∏è‚É£  ECO code format analysis:\")\n",
    "eco_lengths = clean_data['eco'].str.len().value_counts().sort_index()\n",
    "print(f\"   ‚Ä¢ ECO code lengths:\")\n",
    "for length, count in eco_lengths.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    print(f\"      {length} characters: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Check for any unusual ECO codes\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of ECO codes:\")\n",
    "sample_eco = clean_data['eco'].drop_duplicates().sample(min(20, clean_data['eco'].nunique()), random_state=42).sort_values()\n",
    "print(f\"   {', '.join(sample_eco.values)}\")\n",
    "\n",
    "# ECO statistics by split\n",
    "print(f\"\\n8Ô∏è‚É£  ECO distribution across splits:\")\n",
    "print(f\"\\n   Train split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_train['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\n   Validation split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_val['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_val):,}\")\n",
    "val_new_eco = set(X_val['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(val_new_eco)}\")\n",
    "\n",
    "print(f\"\\n   Test split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_test['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_test):,}\")\n",
    "test_new_eco = set(X_test['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(test_new_eco)}\")\n",
    "\n",
    "# Average score by ECO code (top 10 and bottom 10)\n",
    "print(f\"\\n9Ô∏è‚É£  Average score by ECO code:\")\n",
    "eco_scores = clean_data.groupby('eco')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\n   Top 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.head(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(f\"\\n   Bottom 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.tail(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# ECO codes with high variance in scores\n",
    "print(f\"\\nüîü  ECO codes with highest score variance (may indicate difficulty):\")\n",
    "eco_variance = clean_data.groupby('eco')['score'].agg(['var', 'std', 'count']).sort_values('var', ascending=False).head(10)\n",
    "print(f\"\\n   {'ECO':<6} {'Variance':<12} {'Std Dev':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_variance.iterrows():\n",
    "    print(f\"   {eco:<6} {row['var']:<12.4f} {row['std']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# Number of openings per ECO code\n",
    "print(f\"\\n1Ô∏è‚É£1Ô∏è‚É£  Openings per ECO code:\")\n",
    "# Connect to database to get opening counts\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    eco_opening_query = \"\"\"\n",
    "        SELECT eco, COUNT(DISTINCT id) as num_openings\n",
    "        FROM opening\n",
    "        GROUP BY eco\n",
    "        ORDER BY num_openings DESC\n",
    "    \"\"\"\n",
    "    eco_opening_counts = pd.DataFrame(con.execute(eco_opening_query).df())\n",
    "    \n",
    "    # Filter to only ECO codes in our data\n",
    "    eco_opening_counts = eco_opening_counts[eco_opening_counts['eco'].isin(clean_data['eco'].unique())]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Mean openings per ECO: {eco_opening_counts['num_openings'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median openings per ECO: {eco_opening_counts['num_openings'].median():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Max openings per ECO: {eco_opening_counts['num_openings'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Min openings per ECO: {eco_opening_counts['num_openings'].min()}\")\n",
    "    \n",
    "    print(f\"\\n   Top 10 ECO codes by number of openings:\")\n",
    "    print(f\"\\n   {'ECO':<6} {'# Openings':<12}\")\n",
    "    print(f\"   {'-'*6} {'-'*12}\")\n",
    "    for idx, row in eco_opening_counts.head(10).iterrows():\n",
    "        print(f\"   {row['eco']:<6} {int(row['num_openings']):>10}\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ECO CODE STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Key takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Most common family: {eco_families.idxmax()} ({eco_families.max():,} entries)\")\n",
    "print(f\"   ‚Ä¢ Most common ECO: {top_eco.index[0]} ({top_eco.iloc[0]:,} entries)\")\n",
    "print(f\"   ‚Ä¢ ECO codes appear in all splits (good for training)\")\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"   ‚Ä¢ Enumerate ECO codes as integers for categorical encoding\")\n",
    "print(f\"   ‚Ä¢ Consider ECO as opening-level side information (similar to player ratings)\")\n",
    "print(f\"   ‚Ä¢ Verify all ECO codes in validation/test exist in training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbad18",
   "metadata": {},
   "source": [
    "## 4b. Create ECO Side Information\n",
    "\n",
    "**Why ECO is Side Information:**\n",
    "- ECO codes describe **opening characteristics**, not individual player-opening interactions\n",
    "- Similar to how player ratings describe players, ECO describes openings\n",
    "- Each opening has ONE ECO code (not per player-opening pair)\n",
    "\n",
    "**Implementation Strategy:**\n",
    "- Split ECO codes into two categorical features:\n",
    "  - `eco_letter`: A, B, C, D, or E ‚Üí encoded as integers 0-4\n",
    "  - `eco_number`: The numeric part (e.g., \"21\" from \"C21\") ‚Üí encoded as sequential integers\n",
    "- Store in a separate `opening_side_info` lookup table (indexed by opening_id)\n",
    "- Remove `eco` from train/test/val DataFrames (it's not interaction data)\n",
    "- During training, model will lookup opening_id ‚Üí (eco_letter, eco_number)\n",
    "\n",
    "**Why Split ECO into Letter and Number:**\n",
    "- ECO families (A-E) represent fundamentally different opening types:\n",
    "  - **A**: Flank openings (English, R√©ti, Bird's, etc.)\n",
    "  - **B**: Semi-Open games (Sicilian, French, Caro-Kann, etc.)\n",
    "  - **C**: Open games (King's pawn openings, Spanish, Italian, etc.)\n",
    "  - **D**: Closed games (Queen's Gambit variations)\n",
    "  - **E**: Indian defenses (King's Indian, Nimzo-Indian, etc.)\n",
    "- Numbers within each family represent variations (C20-C29, C30-C39, etc.)\n",
    "- Model can learn separate embeddings for family vs variation\n",
    "\n",
    "**Categorical Encoding:**\n",
    "- Both features will be treated as categorical (not ordinal)\n",
    "- Higher numbers don't mean \"better\" openings\n",
    "- Model will learn embedding vectors for each category\n",
    "- This allows the model to capture non-linear relationships between ECO codes and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64124475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4B: CREATE ECO SIDE INFORMATION\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Strategy:\n",
      "   ‚Ä¢ Extract unique opening_id ‚Üí eco mappings from clean_data\n",
      "   ‚Ä¢ Split ECO codes: 'C21' ‚Üí letter='C', number='21'\n",
      "   ‚Ä¢ Encode as sequential integers (categorical, not ordinal)\n",
      "   ‚Ä¢ Store in opening_side_info lookup table\n",
      "   ‚Ä¢ Remove 'eco' column from X_train, X_val, X_test\n",
      "\n",
      "1Ô∏è‚É£  Extracting unique opening-ECO mappings...\n",
      "   ‚úì Extracted 2,560 unique openings\n",
      "   ‚úì Extracted 2,560 unique openings\n",
      "   ‚úì Verified: Each opening has exactly one ECO code (good!)\n",
      "\n",
      "2Ô∏è‚É£  Splitting ECO codes into letter and number components...\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['A' 'B' 'C' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "\n",
      "3Ô∏è‚É£  Encoding ECO letters as categorical integers...\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (543 openings)\n",
      "      'B' ‚Üí 1 (560 openings)\n",
      "      'C' ‚Üí 2 (836 openings)\n",
      "      'D' ‚Üí 3 (402 openings)\n",
      "      'E' ‚Üí 4 (219 openings)\n",
      "\n",
      "4Ô∏è‚É£  Encoding ECO numbers as categorical integers...\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 270 openings\n",
      "      '40' (‚Üí 40):  93 openings\n",
      "      '20' (‚Üí 20):  68 openings\n",
      "      '01' (‚Üí  1):  68 openings\n",
      "      '02' (‚Üí  2):  66 openings\n",
      "      '45' (‚Üí 45):  60 openings\n",
      "      '42' (‚Üí 42):  59 openings\n",
      "      '21' (‚Üí 21):  58 openings\n",
      "      '10' (‚Üí 10):  57 openings\n",
      "      '15' (‚Üí 15):  56 openings\n",
      "\n",
      "5Ô∏è‚É£  Creating opening_side_info lookup table...\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2560, 2)\n",
      "      ‚Ä¢ Index: opening_id\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "      ‚Ä¢ eco_letter_cat: Categorical encoding of ECO letter (A-E ‚Üí 0-4)\n",
      "      ‚Ä¢ eco_number_cat: Categorical encoding of ECO number\n",
      "\n",
      "6Ô∏è‚É£  Verifying coverage of train/val/test openings...\n",
      "   ‚úì All 2,560 openings in train/val/test have ECO side information\n",
      "\n",
      "7Ô∏è‚É£  Removing 'eco' column from train/val/test DataFrames...\n",
      "   ‚Ä¢ X_train before: (666255, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (133251, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (88835, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "   After removing 'eco':\n",
      "   ‚Ä¢ X_train: (666255, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_val: (133251, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_test: (88835, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   82           0               0               A00               \n",
      "   2155         4               42              E42               \n",
      "   376          0               50              A50               \n",
      "   1602         2               63              C63               \n",
      "   2326         4               15              E15               \n",
      "   214          0               15              A15               \n",
      "   742          1               20              B20               \n",
      "   2298         2               54              C54               \n",
      "   737          1               20              B20               \n",
      "   707          1               13              B13               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            543     21.21%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            560     21.88%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            836     32.66%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            402     15.70%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            219      8.55%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO SIDE INFORMATION CREATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2560, 2)\n",
      "   ‚Ä¢ Index: opening_id (for O(1) lookups)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 666,255 rows, 3 features\n",
      "   ‚Ä¢ X_val: 133,251 rows, 3 features\n",
      "   ‚Ä¢ X_test: 88,835 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,560 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "üí° Model usage:\n",
      "   During training, for each (player_id, opening_id) pair:\n",
      "   1. Lookup opening_id ‚Üí opening_side_info[opening_id]\n",
      "   2. Get eco_letter_cat and eco_number_cat (already encoded as integers)\n",
      "   3. Feed into categorical embedding layers\n",
      "   4. Combine with opening latent factors\n",
      "\n",
      "üßπ Final cleanup:\n",
      "   ‚Ä¢ Kept only encoded categorical columns: eco_letter_cat, eco_number_cat\n",
      "   ‚Ä¢ Removed raw ECO strings (eco, eco_letter_str, eco_number_str)\n",
      "   ‚Ä¢ Column names clearly indicate categorical encoding (_cat suffix)\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n",
      "   ‚úì Verified: Each opening has exactly one ECO code (good!)\n",
      "\n",
      "2Ô∏è‚É£  Splitting ECO codes into letter and number components...\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['A' 'B' 'C' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "\n",
      "3Ô∏è‚É£  Encoding ECO letters as categorical integers...\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (543 openings)\n",
      "      'B' ‚Üí 1 (560 openings)\n",
      "      'C' ‚Üí 2 (836 openings)\n",
      "      'D' ‚Üí 3 (402 openings)\n",
      "      'E' ‚Üí 4 (219 openings)\n",
      "\n",
      "4Ô∏è‚É£  Encoding ECO numbers as categorical integers...\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 270 openings\n",
      "      '40' (‚Üí 40):  93 openings\n",
      "      '20' (‚Üí 20):  68 openings\n",
      "      '01' (‚Üí  1):  68 openings\n",
      "      '02' (‚Üí  2):  66 openings\n",
      "      '45' (‚Üí 45):  60 openings\n",
      "      '42' (‚Üí 42):  59 openings\n",
      "      '21' (‚Üí 21):  58 openings\n",
      "      '10' (‚Üí 10):  57 openings\n",
      "      '15' (‚Üí 15):  56 openings\n",
      "\n",
      "5Ô∏è‚É£  Creating opening_side_info lookup table...\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2560, 2)\n",
      "      ‚Ä¢ Index: opening_id\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "      ‚Ä¢ eco_letter_cat: Categorical encoding of ECO letter (A-E ‚Üí 0-4)\n",
      "      ‚Ä¢ eco_number_cat: Categorical encoding of ECO number\n",
      "\n",
      "6Ô∏è‚É£  Verifying coverage of train/val/test openings...\n",
      "   ‚úì All 2,560 openings in train/val/test have ECO side information\n",
      "\n",
      "7Ô∏è‚É£  Removing 'eco' column from train/val/test DataFrames...\n",
      "   ‚Ä¢ X_train before: (666255, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (133251, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (88835, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "   After removing 'eco':\n",
      "   ‚Ä¢ X_train: (666255, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_val: (133251, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_test: (88835, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   82           0               0               A00               \n",
      "   2155         4               42              E42               \n",
      "   376          0               50              A50               \n",
      "   1602         2               63              C63               \n",
      "   2326         4               15              E15               \n",
      "   214          0               15              A15               \n",
      "   742          1               20              B20               \n",
      "   2298         2               54              C54               \n",
      "   737          1               20              B20               \n",
      "   707          1               13              B13               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            543     21.21%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            560     21.88%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            836     32.66%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            402     15.70%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            219      8.55%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO SIDE INFORMATION CREATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2560, 2)\n",
      "   ‚Ä¢ Index: opening_id (for O(1) lookups)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 666,255 rows, 3 features\n",
      "   ‚Ä¢ X_val: 133,251 rows, 3 features\n",
      "   ‚Ä¢ X_test: 88,835 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,560 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "üí° Model usage:\n",
      "   During training, for each (player_id, opening_id) pair:\n",
      "   1. Lookup opening_id ‚Üí opening_side_info[opening_id]\n",
      "   2. Get eco_letter_cat and eco_number_cat (already encoded as integers)\n",
      "   3. Feed into categorical embedding layers\n",
      "   4. Combine with opening latent factors\n",
      "\n",
      "üßπ Final cleanup:\n",
      "   ‚Ä¢ Kept only encoded categorical columns: eco_letter_cat, eco_number_cat\n",
      "   ‚Ä¢ Removed raw ECO strings (eco, eco_letter_str, eco_number_str)\n",
      "   ‚Ä¢ Column names clearly indicate categorical encoding (_cat suffix)\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n"
     ]
    }
   ],
   "source": [
    "# 4b. Create ECO side information and remove ECO from train/test/val DataFrames\n",
    "\n",
    "# Check if ECO processing has already been done\n",
    "if 'eco' not in X_train.columns and 'opening_side_info' in globals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 4B: ECO SIDE INFORMATION CREATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì ECO column already removed from train/test/val data\")\n",
    "    print(\"‚úì 'opening_side_info' table already exists\")\n",
    "    print(f\"\\nOpening side info shape: {opening_side_info.shape}\")\n",
    "    print(f\"Columns: {list(opening_side_info.columns)}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing ECO encoding statistics:\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_letter values: {opening_side_info['eco_letter'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_number values: {opening_side_info['eco_number'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ eco_letter range: [{opening_side_info['eco_letter'].min()}, {opening_side_info['eco_letter'].max()}]\")\n",
    "    print(f\"   ‚Ä¢ eco_number range: [{opening_side_info['eco_number'].min()}, {opening_side_info['eco_number'].max()}]\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing ECO encoding:\")\n",
    "    sample_data = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "    for idx, row in sample_data.iterrows():\n",
    "        print(f\"   Opening {idx:>4} | ECO: {row['eco']:>3} ‚Üí Letter: {row['eco_letter']} ({row['eco_letter_str']}), Number: {row['eco_number']:>2} ({row['eco_number_str']:>2})\")\n",
    "else:\n",
    "    def create_eco_side_information(clean_data_df, X_train_df, X_val_df, X_test_df):\n",
    "        \"\"\"\n",
    "        Create ECO side information table and remove ECO from train/test/val DataFrames.\n",
    "        \n",
    "        ECO codes are opening-level features, not player-opening interaction features.\n",
    "        We split each ECO code (e.g., \"C21\") into:\n",
    "        - eco_letter: The letter part (A, B, C, D, or E)\n",
    "        - eco_number: The numeric part (e.g., 21)\n",
    "        \n",
    "        Both are encoded as sequential integers for use as categorical features in embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        clean_data_df : pd.DataFrame\n",
    "            Full cleaned data with opening_id and eco columns\n",
    "        X_train_df, X_val_df, X_test_df : pd.DataFrame\n",
    "            Train/val/test feature DataFrames (will be modified to remove 'eco')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 4B: CREATE ECO SIDE INFORMATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Strategy:\")\n",
    "        print(f\"   ‚Ä¢ Extract unique opening_id ‚Üí eco mappings from clean_data\")\n",
    "        print(f\"   ‚Ä¢ Split ECO codes: 'C21' ‚Üí letter='C', number='21'\")\n",
    "        print(f\"   ‚Ä¢ Encode as sequential integers (categorical, not ordinal)\")\n",
    "        print(f\"   ‚Ä¢ Store in opening_side_info lookup table\")\n",
    "        print(f\"   ‚Ä¢ Remove 'eco' column from X_train, X_val, X_test\")\n",
    "        \n",
    "        # Extract unique opening ‚Üí ECO mappings\n",
    "        print(f\"\\n1Ô∏è‚É£  Extracting unique opening-ECO mappings...\")\n",
    "        opening_eco_map = clean_data_df[['opening_id', 'eco']].drop_duplicates().set_index('opening_id')\n",
    "        print(f\"   ‚úì Extracted {len(opening_eco_map):,} unique openings\")\n",
    "        \n",
    "        # Verify one-to-one mapping\n",
    "        eco_per_opening = clean_data_df.groupby('opening_id')['eco'].nunique()\n",
    "        if (eco_per_opening > 1).any():\n",
    "            problematic = eco_per_opening[eco_per_opening > 1]\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(problematic)} openings have multiple ECO codes!\")\n",
    "            print(f\"   Problematic opening IDs: {problematic.index.tolist()[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì Verified: Each opening has exactly one ECO code (good!)\")\n",
    "        \n",
    "        # Split ECO into letter and number components\n",
    "        print(f\"\\n2Ô∏è‚É£  Splitting ECO codes into letter and number components...\")\n",
    "        opening_eco_map['eco_letter_str'] = opening_eco_map['eco'].str[0]  # First character (A-E)\n",
    "        opening_eco_map['eco_number_str'] = opening_eco_map['eco'].str[1:]  # Remaining characters (numeric)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted letter and number components\")\n",
    "        print(f\"   ‚Ä¢ Unique letters: {opening_eco_map['eco_letter_str'].unique()}\")\n",
    "        print(f\"   ‚Ä¢ Unique numbers: {opening_eco_map['eco_number_str'].nunique()}\")\n",
    "        \n",
    "        # Create encoding mappings for eco_letter (A-E ‚Üí 0-4)\n",
    "        print(f\"\\n3Ô∏è‚É£  Encoding ECO letters as categorical integers...\")\n",
    "        unique_letters = sorted(opening_eco_map['eco_letter_str'].unique())\n",
    "        eco_letter_to_int = {letter: idx for idx, letter in enumerate(unique_letters)}\n",
    "        eco_int_to_letter = {idx: letter for letter, idx in eco_letter_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_letter'] = opening_eco_map['eco_letter_str'].map(eco_letter_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Letter encoding created:\")\n",
    "        for letter, idx in sorted(eco_letter_to_int.items()):\n",
    "            count = (opening_eco_map['eco_letter_str'] == letter).sum()\n",
    "            print(f\"      '{letter}' ‚Üí {idx} ({count:,} openings)\")\n",
    "        \n",
    "        # Create encoding mappings for eco_number (00-99 ‚Üí sequential integers)\n",
    "        print(f\"\\n4Ô∏è‚É£  Encoding ECO numbers as categorical integers...\")\n",
    "        unique_numbers = sorted(opening_eco_map['eco_number_str'].unique())\n",
    "        eco_number_to_int = {num: idx for idx, num in enumerate(unique_numbers)}\n",
    "        eco_int_to_number = {idx: num for num, idx in eco_number_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_number'] = opening_eco_map['eco_number_str'].map(eco_number_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Number encoding created:\")\n",
    "        print(f\"      {len(unique_numbers)} unique numbers mapped to [0, {len(unique_numbers)-1}]\")\n",
    "        print(f\"      Range: '{unique_numbers[0]}' ‚Üí 0, ..., '{unique_numbers[-1]}' ‚Üí {len(unique_numbers)-1}\")\n",
    "        \n",
    "        # Show distribution of numbers\n",
    "        print(f\"\\n   Distribution of ECO numbers (top 10):\")\n",
    "        number_counts = opening_eco_map['eco_number_str'].value_counts().head(10)\n",
    "        for num, count in number_counts.items():\n",
    "            encoded = eco_number_to_int[num]\n",
    "            print(f\"      '{num}' (‚Üí {encoded:>2}): {count:>3} openings\")\n",
    "        \n",
    "        # Create final opening_side_info table\n",
    "        print(f\"\\n5Ô∏è‚É£  Creating opening_side_info lookup table...\")\n",
    "        # Only keep the encoded categorical columns and rename them for clarity\n",
    "        opening_side_info = opening_eco_map[['eco_letter', 'eco_number']].copy()\n",
    "        opening_side_info = opening_side_info.rename(columns={\n",
    "            'eco_letter': 'eco_letter_cat',  # _cat suffix indicates categorical encoding\n",
    "            'eco_number': 'eco_number_cat'\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úì Created opening_side_info\")\n",
    "        print(f\"      ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"      ‚Ä¢ Index: opening_id\")\n",
    "        print(f\"      ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        print(f\"      ‚Ä¢ eco_letter_cat: Categorical encoding of ECO letter (A-E ‚Üí 0-4)\")\n",
    "        print(f\"      ‚Ä¢ eco_number_cat: Categorical encoding of ECO number\")\n",
    "        \n",
    "        # Verify all openings in train/val/test have ECO info\n",
    "        print(f\"\\n6Ô∏è‚É£  Verifying coverage of train/val/test openings...\")\n",
    "        all_openings = set(X_train_df['opening_id'].unique()) | \\\n",
    "                       set(X_val_df['opening_id'].unique()) | \\\n",
    "                       set(X_test_df['opening_id'].unique())\n",
    "        \n",
    "        missing_openings = all_openings - set(opening_side_info.index)\n",
    "        if len(missing_openings) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits are missing ECO info!\")\n",
    "            print(f\"   Missing opening IDs: {sorted(list(missing_openings))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(all_openings):,} openings in train/val/test have ECO side information\")\n",
    "        \n",
    "        # Remove ECO from train/val/test DataFrames\n",
    "        print(f\"\\n7Ô∏è‚É£  Removing 'eco' column from train/val/test DataFrames...\")\n",
    "        print(f\"   ‚Ä¢ X_train before: {X_train_df.shape}, columns: {list(X_train_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val before: {X_val_df.shape}, columns: {list(X_val_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test before: {X_test_df.shape}, columns: {list(X_test_df.columns)}\")\n",
    "        \n",
    "        X_train_clean = X_train_df.drop(columns=['eco'])\n",
    "        X_val_clean = X_val_df.drop(columns=['eco'])\n",
    "        X_test_clean = X_test_df.drop(columns=['eco'])\n",
    "        \n",
    "        print(f\"\\n   After removing 'eco':\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape}, columns: {list(X_train_clean.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape}, columns: {list(X_val_clean.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape}, columns: {list(X_test_clean.columns)}\")\n",
    "        \n",
    "        # Sample data showing the transformation\n",
    "        print(f\"\\n8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\")\n",
    "        sample_openings = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "        \n",
    "        # For display purposes, reconstruct original values from the categorical encodings\n",
    "        print(f\"\\n   {'Opening ID':<12} {'Letter (Cat)':<15} {'Number (Cat)':<15} {'Reconstructed ECO':<18}\")\n",
    "        print(f\"   {'-'*12} {'-'*15} {'-'*15} {'-'*18}\")\n",
    "        for idx, row in sample_openings.iterrows():\n",
    "            letter_str = eco_int_to_letter[row['eco_letter_cat']]\n",
    "            number_str = eco_int_to_number[row['eco_number_cat']]\n",
    "            reconstructed = f\"{letter_str}{number_str}\"\n",
    "            print(f\"   {idx:<12} {row['eco_letter_cat']:<15} {row['eco_number_cat']:<15} {reconstructed:<18}\")\n",
    "        \n",
    "        # Show ECO family distribution\n",
    "        print(f\"\\n9Ô∏è‚É£  ECO family distribution in opening_side_info:\")\n",
    "        letter_dist = opening_side_info['eco_letter_cat'].value_counts().sort_index()\n",
    "        print(f\"\\n   {'Encoded':<8} {'Letter':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "        print(f\"   {'-'*8} {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "        for encoded_val, count in letter_dist.items():\n",
    "            letter = eco_int_to_letter[encoded_val]\n",
    "            pct = 100 * count / len(opening_side_info)\n",
    "            bar_length = int(pct * 0.4)\n",
    "            bar = '‚ñà' * bar_length\n",
    "            print(f\"   {encoded_val:<8} {letter:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ ECO SIDE INFORMATION CREATION COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nCreated: opening_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: opening_id (for O(1) lookups)\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape[0]:,} rows, {X_train_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape[0]:,} rows, {X_val_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape[0]:,} rows, {X_test_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ opening_side_info: {len(opening_side_info):,} openings (one per opening)\")\n",
    "        print(f\"   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\")\n",
    "        print(f\"   ‚Ä¢ eco_letter_to_int: {eco_letter_to_int}\")\n",
    "        print(f\"   ‚Ä¢ eco_number_to_int: (dict with {len(eco_number_to_int)} entries)\")\n",
    "        print(f\"\\n   You'll need them to encode ECO codes for new openings at inference time.\")\n",
    "        \n",
    "        print(f\"\\nüí° Model usage:\")\n",
    "        print(f\"   During training, for each (player_id, opening_id) pair:\")\n",
    "        print(f\"   1. Lookup opening_id ‚Üí opening_side_info[opening_id]\")\n",
    "        print(f\"   2. Get eco_letter_cat and eco_number_cat (already encoded as integers)\")\n",
    "        print(f\"   3. Feed into categorical embedding layers\")\n",
    "        print(f\"   4. Combine with opening latent factors\")\n",
    "        \n",
    "        print(f\"\\nüßπ Final cleanup:\")\n",
    "        print(f\"   ‚Ä¢ Kept only encoded categorical columns: eco_letter_cat, eco_number_cat\")\n",
    "        print(f\"   ‚Ä¢ Removed raw ECO strings (eco, eco_letter_str, eco_number_str)\")\n",
    "        print(f\"   ‚Ä¢ Column names clearly indicate categorical encoding (_cat suffix)\")\n",
    "        \n",
    "        return opening_side_info, eco_letter_to_int, eco_number_to_int, X_train_clean, X_val_clean, X_test_clean\n",
    "    \n",
    "    # Call the function\n",
    "    opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test = create_eco_side_information(\n",
    "        clean_data, X_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Create reverse mappings for decoding (needed for verification and debugging)\n",
    "    eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "    eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "    print(f\"\\n‚úì Created reverse mappings for ECO decoding:\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_letter: {len(eco_int_to_letter)} entries\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_number: {len(eco_int_to_number)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df4b4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICATION: FINAL DATA STRUCTURE\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\n",
      "\n",
      "   X_train:\n",
      "   ‚Ä¢ Shape: (666255, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ Sample:\n",
      "        player_id  opening_id  confidence\n",
      "563051       8732        2365    0.180328\n",
      "823471      12760         583    0.647887\n",
      "788779      12225         100    0.264706\n",
      "\n",
      "   X_val:\n",
      "   ‚Ä¢ Shape: (133251, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "   X_test:\n",
      "   ‚Ä¢ Shape: (88835, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Side Information Tables:\n",
      "\n",
      "   player_side_info (indexed by player_id):\n",
      "   ‚Ä¢ Shape: (13990, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Sample:\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.533821\n",
      "1         -0.335954\n",
      "2          0.191548\n",
      "\n",
      "   opening_side_info (indexed by opening_id):\n",
      "   ‚Ä¢ Shape: (2560, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Sample:\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "277                      0              40\n",
      "316                      0              43\n",
      "322                      0              43\n",
      "\n",
      "3Ô∏è‚É£  Encoding Mappings (for inference):\n",
      "\n",
      "   eco_letter_map:\n",
      "      'A' ‚Üí 0\n",
      "      'B' ‚Üí 1\n",
      "      'C' ‚Üí 2\n",
      "      'D' ‚Üí 3\n",
      "      'E' ‚Üí 4\n",
      "\n",
      "   eco_number_map (first 10):\n",
      "      '00' ‚Üí 0\n",
      "      '01' ‚Üí 1\n",
      "      '02' ‚Üí 2\n",
      "      '03' ‚Üí 3\n",
      "      '04' ‚Üí 4\n",
      "      '05' ‚Üí 5\n",
      "      '06' ‚Üí 6\n",
      "      '07' ‚Üí 7\n",
      "      '08' ‚Üí 8\n",
      "      '09' ‚Üí 9\n",
      "      ... (100 total)\n",
      "\n",
      "4Ô∏è‚É£  Example: Lookup flow for a random train sample:\n",
      "\n",
      "   Sample interaction:\n",
      "   ‚Ä¢ player_id: 2123.0\n",
      "   ‚Ä¢ opening_id: 2306.0\n",
      "   ‚Ä¢ confidence: 0.5798\n",
      "\n",
      "   Player side info lookup:\n",
      "   ‚Ä¢ rating_z: 1.0049\n",
      "\n",
      "   Opening side info lookup:\n",
      "   ‚Ä¢ eco_letter_cat: 2 (letter: C)\n",
      "   ‚Ä¢ eco_number_cat: 56 (number: 56)\n",
      "\n",
      "============================================================\n",
      "‚úÖ VERIFICATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Ready for PyTorch tensor conversion:\n",
      "   ‚Ä¢ Features: player_id, opening_id, confidence\n",
      "   ‚Ä¢ Target: score (in y_train, y_val, y_test)\n",
      "   ‚Ä¢ Player side info: rating_z\n",
      "   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\n",
      "\n",
      "   All ECO and rating data is now properly separated as side information!\n",
      "\n",
      "   Side info tables are clean - only contain necessary model inputs!\n"
     ]
    }
   ],
   "source": [
    "# Verify final data structure after ECO processing\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION: FINAL DATA STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\")\n",
    "print(f\"\\n   X_train:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_train.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(X_train.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   X_val:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_val.columns)}\")\n",
    "\n",
    "print(f\"\\n   X_test:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_test.columns)}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Side Information Tables:\")\n",
    "\n",
    "print(f\"\\n   player_side_info (indexed by player_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(player_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   opening_side_info (indexed by opening_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(opening_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Encoding Mappings (for inference):\")\n",
    "print(f\"\\n   eco_letter_map:\")\n",
    "for k, v in sorted(eco_letter_map.items()):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "\n",
    "print(f\"\\n   eco_number_map (first 10):\")\n",
    "for i, (k, v) in enumerate(sorted(eco_number_map.items())[:10]):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "print(f\"      ... ({len(eco_number_map)} total)\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Example: Lookup flow for a random train sample:\")\n",
    "sample = X_train.sample(1, random_state=42).iloc[0]\n",
    "player_id = sample['player_id']\n",
    "opening_id = sample['opening_id']\n",
    "\n",
    "print(f\"\\n   Sample interaction:\")\n",
    "print(f\"   ‚Ä¢ player_id: {player_id}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {opening_id}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Player side info lookup:\")\n",
    "player_info = player_side_info.loc[player_id]\n",
    "print(f\"   ‚Ä¢ rating_z: {player_info['rating_z']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Opening side info lookup:\")\n",
    "opening_info = opening_side_info.loc[opening_id]\n",
    "print(f\"   ‚Ä¢ eco_letter_cat: {opening_info['eco_letter_cat']} (letter: {eco_int_to_letter[opening_info['eco_letter_cat']]})\")\n",
    "print(f\"   ‚Ä¢ eco_number_cat: {opening_info['eco_number_cat']} (number: {eco_int_to_number[opening_info['eco_number_cat']]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Ready for PyTorch tensor conversion:\")\n",
    "print(f\"   ‚Ä¢ Features: player_id, opening_id, confidence\")\n",
    "print(f\"   ‚Ä¢ Target: score (in y_train, y_val, y_test)\")\n",
    "print(f\"   ‚Ä¢ Player side info: rating_z\")\n",
    "print(f\"   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\")\n",
    "print(f\"\\n   All ECO and rating data is now properly separated as side information!\")\n",
    "print(f\"\\n   Side info tables are clean - only contain necessary model inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "017515d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\n",
      "====================================================================================================\n",
      "\n",
      "Sampling 100 player-opening pairs for verification...\n",
      "\n",
      "   ‚Ä¢ Converting 85 NEW opening IDs to OLD database IDs for query\n",
      "   ‚Ä¢ Example: NEW ID 2306 ‚Üí OLD ID 3184\n",
      "   ‚úì Retrieved 85 opening names from database\n",
      "\n",
      "#    Player   Opening   ECO (DB)   Reconstructed Match  Opening Name                                      \n",
      "====================================================================================================\n",
      "1    2123     2306      C56        C56           ‚úì      Italian Game: Two Knights Defense, Max Lange ...  \n",
      "2    2155     1023      C02        C02           ‚úì      French Defense: Advance Variation, Euwe Varia...  \n",
      "3    10550    780       B22        B22           ‚úì      Sicilian Defense: Alapin Variation                \n",
      "4    369      1067      C11        C11           ‚úì      French Defense: Classical Variation, Steinitz...  \n",
      "5    388      2360      C34        C34           ‚úì      King's Gambit Accepted: King's Knight's Gambit    \n",
      "6    7512     1588      C60        C60           ‚úì      Ruy Lopez: Cozio Defense                          \n",
      "7    6315     1404      C43        C43           ‚úì      Bishop's Opening: Urusov Gambit, Keidansky Ga...  \n",
      "8    1280     1493      C48        C48           ‚úì      Four Knights Game: Spanish Variation              \n",
      "9    898      1002      C00        C00           ‚úì      French Defense: La Bourdonnais Variation          \n",
      "10   5951     1169      C23        C23           ‚úì      Bishop's Opening: Philidor Variation              \n",
      "11   13349    1031      C02        C02           ‚úì      French Defense: Advance Variation, Nimzowitsc...  \n",
      "12   7729     802       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Pterodactyl    \n",
      "13   10697    1588      C60        C60           ‚úì      Ruy Lopez: Cozio Defense                          \n",
      "14   5076     617       B06        B06           ‚úì      Modern Defense                                    \n",
      "15   2811     1032      C02        C02           ‚úì      French Defense: Advance Variation, Paulsen At...  \n",
      "16   10057    1817      D10        D10           ‚úì      Slav Defense: Exchange Variation                  \n",
      "17   11567    1816      D10        D10           ‚úì      Slav Defense: Diemer Gambit                       \n",
      "18   8524     1163      C23        C23           ‚úì      Bishop's Opening: Khan Gambit                     \n",
      "19   5956     2363      C50        C50           ‚úì      Italian Game: Blackburne-Kostiƒá Gambit            \n",
      "20   12622    564       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "21   11131    868       B40        B40           ‚úì      Sicilian Defense: French Variation                \n",
      "22   1827     318       A43        A43           ‚úì      Benoni Defense: Benoni-Indian Defense             \n",
      "23   3914     1782      D05        D05           ‚úì      Queen's Pawn Game: Colle System                   \n",
      "24   12760    1438      C44        C44           ‚úì      Scotch Game: Haxo Gambit                          \n",
      "25   13671    759       B21        B21           ‚úì      Sicilian Defense: McDonnell Attack                \n",
      "26   12518    682       B10        B10           ‚úì      Caro-Kann Defense: Two Knights Attack             \n",
      "27   13151    1617      C65        C65           ‚úì      Ruy Lopez: Berlin Defense, Anderssen Variation    \n",
      "28   10554    1219      C30        C30           ‚úì      King's Gambit Declined: Classical Variation       \n",
      "29   2416     2356      A53        A53           ‚úì      Old Indian Defense: Czech Variation, with Nc3     \n",
      "30   7087     1360      C41        C41           ‚úì      Philidor Defense: Hanham Variation                \n",
      "31   13654    126       A04        A04           ‚úì      Zukertort Opening: Kingside Fianchetto            \n",
      "32   13474    947       B76        B76           ‚úì      Sicilian Defense: Dragon Variation, Yugoslav ...  \n",
      "33   2589     617       B06        B06           ‚úì      Modern Defense                                    \n",
      "34   2344     295       A40        A40           ‚úì      Mikenas Defense                                   \n",
      "35   1650     1471      C46        C46           ‚úì      Three Knights Opening                             \n",
      "36   5284     2302      C50        C50           ‚úì      Italian Game: Giuoco Piano                        \n",
      "37   1845     533       B00        B00           ‚úì      Owen Defense                                      \n",
      "38   13264    845       B32        B32           ‚úì      Sicilian Defense: Open                            \n",
      "39   6262     788       B23        B23           ‚úì      Sicilian Defense: Closed                          \n",
      "40   1349     213       A15        A15           ‚úì      English Opening: Anglo-Indian Defense, Scandi...  \n",
      "41   7444     543       B00        B00           ‚úì      St. George Defense                                \n",
      "42   3121     2336      C00        C00           ‚úì      French Defense: Franco-Sicilian Defense           \n",
      "43   6097     501       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "44   8259     1160      C23        C23           ‚úì      Bishop's Opening: Boi Variation                   \n",
      "45   4161     1002      C00        C00           ‚úì      French Defense: La Bourdonnais Variation          \n",
      "46   1041     1510      C50        C50           ‚úì      Italian Game: Giuoco Pianissimo, Normal           \n",
      "47   5278     287       A40        A40           ‚úì      Englund Gambit Complex: Stockholm Variation       \n",
      "48   928      716       B15        B15           ‚úì      Caro-Kann Defense: Main Line                      \n",
      "49   3572     1182      C25        C25           ‚úì      Vienna Game: Max Lange Defense                    \n",
      "50   610      1674      C79        C79           ‚úì      Ruy Lopez: Morphy Defense, Steinitz Deferred      \n",
      "51   2830     564       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "52   12362    382       A51        A51           ‚úì      Indian Defense: Budapest Defense, Fajarowicz ...  \n",
      "53   10765    363       A46        A46           ‚úì      Torre Attack                                      \n",
      "54   614      892       B45        B45           ‚úì      Sicilian Defense: Taimanov Variation, Normal ...  \n",
      "55   1905     2299      C56        C56           ‚úì      Italian Game: Scotch Gambit, Janowski Defense     \n",
      "56   8461     346       A45        A45           ‚úì      Trompowsky Attack                                 \n",
      "57   9578     1142      C21        C21           ‚úì      Center Game: Kieseritzky Variation                \n",
      "58   11631    1543      C53        C53           ‚úì      Italian Game: Classical Variation                 \n",
      "59   11022    537       B00        B00           ‚úì      Pirc Defense                                      \n",
      "60   4038     830       B30        B30           ‚úì      Sicilian Defense: Nyezhmetdinov-Rossolimo Attack  \n",
      "61   8594     870       B40        B40           ‚úì      Sicilian Defense: French Variation, Open          \n",
      "62   5718     1355      C41        C41           ‚úì      Philidor Defense                                  \n",
      "63   201      2000      D55        D55           ‚úì      Queen's Gambit Declined: Neo-Orthodox Variati...  \n",
      "64   9997     688       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Botvinn...  \n",
      "65   3485     617       B06        B06           ‚úì      Modern Defense                                    \n",
      "66   2615     533       B00        B00           ‚úì      Owen Defense                                      \n",
      "67   7600     1743      D00        D00           ‚úì      Blackmar-Diemer Gambit: Zeller Defense            \n",
      "68   3813     1759      D00        D00           ‚úì      Queen's Pawn Game: Steinitz Countergambit         \n",
      "69   9218     1447      C44        C44           ‚úì      Scotch Game: Scotch Gambit, Kingside Variation    \n",
      "70   11031    605       B03        B03           ‚úì      Alekhine Defense: O'Sullivan Gambit               \n",
      "71   9970     135       A04        A04           ‚úì      Zukertort Opening: Slav Invitation                \n",
      "72   261      1282      C34        C34           ‚úì      King's Gambit Accepted: King's Knight Gambit      \n",
      "73   7441     559       B01        B01           ‚úì      Scandinavian Defense: Main Line                   \n",
      "74   7017     1445      C44        C44           ‚úì      Scotch Game: Scotch Gambit, Dubois R√©ti Defense   \n",
      "75   10476    501       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "76   11856    801       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Fianchetto     \n",
      "77   11234    738       B20        B20           ‚úì      Sicilian Defense: Bowdler Attack                  \n",
      "78   4376     1401      C42        C42           ‚úì      Russian Game: Stafford Gambit                     \n",
      "79   2639     1355      C41        C41           ‚úì      Philidor Defense                                  \n",
      "80   481      2316      B02        B02           ‚úì      Alekhine Defense: Two Pawns Attack, Tate Vari...  \n",
      "81   11155    363       A46        A46           ‚úì      Torre Attack                                      \n",
      "82   5658     1751      D00        D00           ‚úì      Queen's Pawn Game: Chigorin Variation, Irish ...  \n",
      "83   10131    66        A00        A00           ‚úì      Van Geet Opening                                  \n",
      "84   6957     1032      C02        C02           ‚úì      French Defense: Advance Variation, Paulsen At...  \n",
      "85   1034     1476      C47        C47           ‚úì      Four Knights Game                                 \n",
      "86   3103     503       B00        B00           ‚úì      Nimzowitsch Defense: Declined Variation           \n",
      "87   10652    319       A43        A43           ‚úì      Benoni Defense: Benoni-Indian Defense, Kingsi...  \n",
      "88   8549     1022      C02        C02           ‚úì      French Defense: Advance Variation                 \n",
      "89   1219     225       A20        A20           ‚úì      English Opening: King's English Variation         \n",
      "90   12512    1698      C89        C89           ‚úì      Ruy Lopez: Marshall Attack                        \n",
      "91   2464     1229      C30        C30           ‚úì      King's Gambit Declined: Petrov's Defense          \n",
      "92   13241    1442      C44        C44           ‚úì      Scotch Game: Scotch Gambit                        \n",
      "93   11250    2363      C50        C50           ‚úì      Italian Game: Blackburne-Kostiƒá Gambit            \n",
      "94   5737     1511      C50        C50           ‚úì      Italian Game: Hungarian Defense                   \n",
      "95   13513    617       B06        B06           ‚úì      Modern Defense                                    \n",
      "96   1396     543       B00        B00           ‚úì      St. George Defense                                \n",
      "97   11420    830       B30        B30           ‚úì      Sicilian Defense: Nyezhmetdinov-Rossolimo Attack  \n",
      "98   6315     1168      C23        C23           ‚úì      Bishop's Opening: Philidor Counterattack          \n",
      "99   10871    321       A43        A43           ‚úì      Benoni Defense: French Benoni                     \n",
      "100  12510    1782      D05        D05           ‚úì      Queen's Pawn Game: Colle System                   \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Verification Results:\n",
      "   ‚Ä¢ Total samples: 100\n",
      "   ‚Ä¢ Matches: 100/100 (100.0%)\n",
      "   ‚Ä¢ Mismatches: 0\n",
      "\n",
      "üéâ Perfect! All ECO codes reconstructed correctly!\n",
      "   ‚Ä¢ ID remapping preserved all ECO code mappings\n",
      "\n",
      "Sampling 100 player-opening pairs for verification...\n",
      "\n",
      "   ‚Ä¢ Converting 85 NEW opening IDs to OLD database IDs for query\n",
      "   ‚Ä¢ Example: NEW ID 2306 ‚Üí OLD ID 3184\n",
      "   ‚úì Retrieved 85 opening names from database\n",
      "\n",
      "#    Player   Opening   ECO (DB)   Reconstructed Match  Opening Name                                      \n",
      "====================================================================================================\n",
      "1    2123     2306      C56        C56           ‚úì      Italian Game: Two Knights Defense, Max Lange ...  \n",
      "2    2155     1023      C02        C02           ‚úì      French Defense: Advance Variation, Euwe Varia...  \n",
      "3    10550    780       B22        B22           ‚úì      Sicilian Defense: Alapin Variation                \n",
      "4    369      1067      C11        C11           ‚úì      French Defense: Classical Variation, Steinitz...  \n",
      "5    388      2360      C34        C34           ‚úì      King's Gambit Accepted: King's Knight's Gambit    \n",
      "6    7512     1588      C60        C60           ‚úì      Ruy Lopez: Cozio Defense                          \n",
      "7    6315     1404      C43        C43           ‚úì      Bishop's Opening: Urusov Gambit, Keidansky Ga...  \n",
      "8    1280     1493      C48        C48           ‚úì      Four Knights Game: Spanish Variation              \n",
      "9    898      1002      C00        C00           ‚úì      French Defense: La Bourdonnais Variation          \n",
      "10   5951     1169      C23        C23           ‚úì      Bishop's Opening: Philidor Variation              \n",
      "11   13349    1031      C02        C02           ‚úì      French Defense: Advance Variation, Nimzowitsc...  \n",
      "12   7729     802       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Pterodactyl    \n",
      "13   10697    1588      C60        C60           ‚úì      Ruy Lopez: Cozio Defense                          \n",
      "14   5076     617       B06        B06           ‚úì      Modern Defense                                    \n",
      "15   2811     1032      C02        C02           ‚úì      French Defense: Advance Variation, Paulsen At...  \n",
      "16   10057    1817      D10        D10           ‚úì      Slav Defense: Exchange Variation                  \n",
      "17   11567    1816      D10        D10           ‚úì      Slav Defense: Diemer Gambit                       \n",
      "18   8524     1163      C23        C23           ‚úì      Bishop's Opening: Khan Gambit                     \n",
      "19   5956     2363      C50        C50           ‚úì      Italian Game: Blackburne-Kostiƒá Gambit            \n",
      "20   12622    564       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "21   11131    868       B40        B40           ‚úì      Sicilian Defense: French Variation                \n",
      "22   1827     318       A43        A43           ‚úì      Benoni Defense: Benoni-Indian Defense             \n",
      "23   3914     1782      D05        D05           ‚úì      Queen's Pawn Game: Colle System                   \n",
      "24   12760    1438      C44        C44           ‚úì      Scotch Game: Haxo Gambit                          \n",
      "25   13671    759       B21        B21           ‚úì      Sicilian Defense: McDonnell Attack                \n",
      "26   12518    682       B10        B10           ‚úì      Caro-Kann Defense: Two Knights Attack             \n",
      "27   13151    1617      C65        C65           ‚úì      Ruy Lopez: Berlin Defense, Anderssen Variation    \n",
      "28   10554    1219      C30        C30           ‚úì      King's Gambit Declined: Classical Variation       \n",
      "29   2416     2356      A53        A53           ‚úì      Old Indian Defense: Czech Variation, with Nc3     \n",
      "30   7087     1360      C41        C41           ‚úì      Philidor Defense: Hanham Variation                \n",
      "31   13654    126       A04        A04           ‚úì      Zukertort Opening: Kingside Fianchetto            \n",
      "32   13474    947       B76        B76           ‚úì      Sicilian Defense: Dragon Variation, Yugoslav ...  \n",
      "33   2589     617       B06        B06           ‚úì      Modern Defense                                    \n",
      "34   2344     295       A40        A40           ‚úì      Mikenas Defense                                   \n",
      "35   1650     1471      C46        C46           ‚úì      Three Knights Opening                             \n",
      "36   5284     2302      C50        C50           ‚úì      Italian Game: Giuoco Piano                        \n",
      "37   1845     533       B00        B00           ‚úì      Owen Defense                                      \n",
      "38   13264    845       B32        B32           ‚úì      Sicilian Defense: Open                            \n",
      "39   6262     788       B23        B23           ‚úì      Sicilian Defense: Closed                          \n",
      "40   1349     213       A15        A15           ‚úì      English Opening: Anglo-Indian Defense, Scandi...  \n",
      "41   7444     543       B00        B00           ‚úì      St. George Defense                                \n",
      "42   3121     2336      C00        C00           ‚úì      French Defense: Franco-Sicilian Defense           \n",
      "43   6097     501       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "44   8259     1160      C23        C23           ‚úì      Bishop's Opening: Boi Variation                   \n",
      "45   4161     1002      C00        C00           ‚úì      French Defense: La Bourdonnais Variation          \n",
      "46   1041     1510      C50        C50           ‚úì      Italian Game: Giuoco Pianissimo, Normal           \n",
      "47   5278     287       A40        A40           ‚úì      Englund Gambit Complex: Stockholm Variation       \n",
      "48   928      716       B15        B15           ‚úì      Caro-Kann Defense: Main Line                      \n",
      "49   3572     1182      C25        C25           ‚úì      Vienna Game: Max Lange Defense                    \n",
      "50   610      1674      C79        C79           ‚úì      Ruy Lopez: Morphy Defense, Steinitz Deferred      \n",
      "51   2830     564       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "52   12362    382       A51        A51           ‚úì      Indian Defense: Budapest Defense, Fajarowicz ...  \n",
      "53   10765    363       A46        A46           ‚úì      Torre Attack                                      \n",
      "54   614      892       B45        B45           ‚úì      Sicilian Defense: Taimanov Variation, Normal ...  \n",
      "55   1905     2299      C56        C56           ‚úì      Italian Game: Scotch Gambit, Janowski Defense     \n",
      "56   8461     346       A45        A45           ‚úì      Trompowsky Attack                                 \n",
      "57   9578     1142      C21        C21           ‚úì      Center Game: Kieseritzky Variation                \n",
      "58   11631    1543      C53        C53           ‚úì      Italian Game: Classical Variation                 \n",
      "59   11022    537       B00        B00           ‚úì      Pirc Defense                                      \n",
      "60   4038     830       B30        B30           ‚úì      Sicilian Defense: Nyezhmetdinov-Rossolimo Attack  \n",
      "61   8594     870       B40        B40           ‚úì      Sicilian Defense: French Variation, Open          \n",
      "62   5718     1355      C41        C41           ‚úì      Philidor Defense                                  \n",
      "63   201      2000      D55        D55           ‚úì      Queen's Gambit Declined: Neo-Orthodox Variati...  \n",
      "64   9997     688       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Botvinn...  \n",
      "65   3485     617       B06        B06           ‚úì      Modern Defense                                    \n",
      "66   2615     533       B00        B00           ‚úì      Owen Defense                                      \n",
      "67   7600     1743      D00        D00           ‚úì      Blackmar-Diemer Gambit: Zeller Defense            \n",
      "68   3813     1759      D00        D00           ‚úì      Queen's Pawn Game: Steinitz Countergambit         \n",
      "69   9218     1447      C44        C44           ‚úì      Scotch Game: Scotch Gambit, Kingside Variation    \n",
      "70   11031    605       B03        B03           ‚úì      Alekhine Defense: O'Sullivan Gambit               \n",
      "71   9970     135       A04        A04           ‚úì      Zukertort Opening: Slav Invitation                \n",
      "72   261      1282      C34        C34           ‚úì      King's Gambit Accepted: King's Knight Gambit      \n",
      "73   7441     559       B01        B01           ‚úì      Scandinavian Defense: Main Line                   \n",
      "74   7017     1445      C44        C44           ‚úì      Scotch Game: Scotch Gambit, Dubois R√©ti Defense   \n",
      "75   10476    501       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "76   11856    801       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Fianchetto     \n",
      "77   11234    738       B20        B20           ‚úì      Sicilian Defense: Bowdler Attack                  \n",
      "78   4376     1401      C42        C42           ‚úì      Russian Game: Stafford Gambit                     \n",
      "79   2639     1355      C41        C41           ‚úì      Philidor Defense                                  \n",
      "80   481      2316      B02        B02           ‚úì      Alekhine Defense: Two Pawns Attack, Tate Vari...  \n",
      "81   11155    363       A46        A46           ‚úì      Torre Attack                                      \n",
      "82   5658     1751      D00        D00           ‚úì      Queen's Pawn Game: Chigorin Variation, Irish ...  \n",
      "83   10131    66        A00        A00           ‚úì      Van Geet Opening                                  \n",
      "84   6957     1032      C02        C02           ‚úì      French Defense: Advance Variation, Paulsen At...  \n",
      "85   1034     1476      C47        C47           ‚úì      Four Knights Game                                 \n",
      "86   3103     503       B00        B00           ‚úì      Nimzowitsch Defense: Declined Variation           \n",
      "87   10652    319       A43        A43           ‚úì      Benoni Defense: Benoni-Indian Defense, Kingsi...  \n",
      "88   8549     1022      C02        C02           ‚úì      French Defense: Advance Variation                 \n",
      "89   1219     225       A20        A20           ‚úì      English Opening: King's English Variation         \n",
      "90   12512    1698      C89        C89           ‚úì      Ruy Lopez: Marshall Attack                        \n",
      "91   2464     1229      C30        C30           ‚úì      King's Gambit Declined: Petrov's Defense          \n",
      "92   13241    1442      C44        C44           ‚úì      Scotch Game: Scotch Gambit                        \n",
      "93   11250    2363      C50        C50           ‚úì      Italian Game: Blackburne-Kostiƒá Gambit            \n",
      "94   5737     1511      C50        C50           ‚úì      Italian Game: Hungarian Defense                   \n",
      "95   13513    617       B06        B06           ‚úì      Modern Defense                                    \n",
      "96   1396     543       B00        B00           ‚úì      St. George Defense                                \n",
      "97   11420    830       B30        B30           ‚úì      Sicilian Defense: Nyezhmetdinov-Rossolimo Attack  \n",
      "98   6315     1168      C23        C23           ‚úì      Bishop's Opening: Philidor Counterattack          \n",
      "99   10871    321       A43        A43           ‚úì      Benoni Defense: French Benoni                     \n",
      "100  12510    1782      D05        D05           ‚úì      Queen's Pawn Game: Colle System                   \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Verification Results:\n",
      "   ‚Ä¢ Total samples: 100\n",
      "   ‚Ä¢ Matches: 100/100 (100.0%)\n",
      "   ‚Ä¢ Mismatches: 0\n",
      "\n",
      "üéâ Perfect! All ECO codes reconstructed correctly!\n",
      "   ‚Ä¢ ID remapping preserved all ECO code mappings\n"
     ]
    }
   ],
   "source": [
    "# Verification: Sample 100 player-opening pairs with reconstructed ECO codes and opening names\n",
    "# Doing this to make sure that our ECO encoding/decoding is correct\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Sample 100 random player-opening pairs from training data\n",
    "sample_size = 100\n",
    "sample_data = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "\n",
    "print(f\"\\nSampling {len(sample_data)} player-opening pairs for verification...\\n\")\n",
    "\n",
    "# Get unique opening IDs from sample (these are NEW remapped IDs)\n",
    "new_opening_ids = sample_data['opening_id'].unique()\n",
    "\n",
    "# Convert NEW opening IDs back to OLD database IDs for query\n",
    "if opening_remapped:\n",
    "    old_opening_ids = [opening_idx_to_id[int(new_id)] for new_id in new_opening_ids]\n",
    "    print(f\"   ‚Ä¢ Converting {len(new_opening_ids)} NEW opening IDs to OLD database IDs for query\")\n",
    "    print(f\"   ‚Ä¢ Example: NEW ID {new_opening_ids[0]} ‚Üí OLD ID {old_opening_ids[0]}\")\n",
    "else:\n",
    "    old_opening_ids = new_opening_ids\n",
    "    print(f\"   ‚Ä¢ No remapping was done - using opening IDs directly\")\n",
    "\n",
    "opening_ids_str = ','.join(map(str, old_opening_ids))\n",
    "\n",
    "# Query database for opening names using OLD IDs\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    opening_query = f\"\"\"\n",
    "        SELECT id, name, eco\n",
    "        FROM opening\n",
    "        WHERE id IN ({opening_ids_str})\n",
    "    \"\"\"\n",
    "    opening_names = pd.DataFrame(con.execute(opening_query).df()).set_index('id')\n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "print(f\"   ‚úì Retrieved {len(opening_names)} opening names from database\\n\")\n",
    "\n",
    "# Create reverse mappings for ECO decoding\n",
    "eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "\n",
    "# Build verification table\n",
    "print(f\"{'#':<4} {'Player':<8} {'Opening':<9} {'ECO (DB)':<10} {'Reconstructed':<13} {'Match':<6} {'Opening Name':<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "matches = 0\n",
    "for i, (idx, row) in enumerate(sample_data.iterrows(), 1):\n",
    "    player_id = int(row['player_id'])\n",
    "    new_opening_id = int(row['opening_id'])\n",
    "    \n",
    "    # Convert NEW opening ID to OLD database ID for lookup\n",
    "    if opening_remapped:\n",
    "        old_opening_id = opening_idx_to_id[new_opening_id]\n",
    "    else:\n",
    "        old_opening_id = new_opening_id\n",
    "    \n",
    "    # Lookup opening side info using NEW ID\n",
    "    opening_info = opening_side_info.loc[new_opening_id]\n",
    "    \n",
    "    # Reconstruct ECO from encoded categorical values\n",
    "    eco_letter_encoded = opening_info['eco_letter_cat']\n",
    "    eco_number_encoded = opening_info['eco_number_cat']\n",
    "    \n",
    "    eco_letter_decoded = eco_int_to_letter[eco_letter_encoded]\n",
    "    eco_number_decoded = eco_int_to_number[eco_number_encoded]\n",
    "    \n",
    "    reconstructed_eco = f\"{eco_letter_decoded}{eco_number_decoded}\"\n",
    "    \n",
    "    # Get original ECO from database using OLD ID\n",
    "    db_eco = opening_names.loc[old_opening_id, 'eco']\n",
    "    opening_name = opening_names.loc[old_opening_id, 'name']\n",
    "    \n",
    "    # Check if they match\n",
    "    match = \"‚úì\" if reconstructed_eco == db_eco else \"‚úó\"\n",
    "    if reconstructed_eco == db_eco:\n",
    "        matches += 1\n",
    "    \n",
    "    # Truncate opening name if too long\n",
    "    if len(opening_name) > 48:\n",
    "        opening_name = opening_name[:45] + \"...\"\n",
    "    \n",
    "    # Display using NEW opening ID (what's in the data now)\n",
    "    print(f\"{i:<4} {player_id:<8} {new_opening_id:<9} {db_eco:<10} {reconstructed_eco:<13} {match:<6} {opening_name:<50}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n‚úÖ Verification Results:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(sample_data)}\")\n",
    "print(f\"   ‚Ä¢ Matches: {matches}/{len(sample_data)} ({100*matches/len(sample_data):.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Mismatches: {len(sample_data) - matches}\")\n",
    "\n",
    "if matches == len(sample_data):\n",
    "    print(f\"\\nüéâ Perfect! All ECO codes reconstructed correctly!\")\n",
    "    print(f\"   ‚Ä¢ ID remapping preserved all ECO code mappings\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Some ECO codes did not match. Investigate mismatches above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf4878",
   "metadata": {},
   "source": [
    "## 5. Data Verification and Examination\n",
    "We're almost there. Let's examine our data structures to check for any obvious flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f76afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train \n",
      "         player_id  opening_id  confidence\n",
      "563051       8732        2365    0.180328\n",
      "823471      12760         583    0.647887\n",
      "788779      12225         100    0.264706\n",
      "838786      13030         283    0.305556\n",
      "713107      11033        1430    0.193548\n",
      "============================================================\n",
      "X_val \n",
      "         player_id  opening_id  confidence\n",
      "256019       3932        1448    0.600000\n",
      "702545      10879         529    0.242424\n",
      "624026       9655        2334    0.390244\n",
      "248808       3821         501    0.275362\n",
      "874096      13662         870    0.180328\n",
      "============================================================\n",
      "X_test \n",
      "         player_id  opening_id  confidence\n",
      "680597      10549        2324    0.568966\n",
      "198094       3046        1218    0.230769\n",
      "56059         870        1147    0.324324\n",
      "916            12        1516    0.264706\n",
      "366161       5662         688    0.206349\n",
      "============================================================\n",
      "y_train \n",
      " 563051    0.589439\n",
      "823471    0.585190\n",
      "788779    0.535126\n",
      "838786    0.419645\n",
      "713107    0.579959\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_val \n",
      " 256019    0.582749\n",
      "702545    0.484586\n",
      "624026    0.455366\n",
      "248808    0.512911\n",
      "874096    0.459978\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_test \n",
      " 680597    0.520637\n",
      "198094    0.574178\n",
      "56059     0.487700\n",
      "916       0.563867\n",
      "366161    0.496357\n",
      "Name: score, dtype: float64\n",
      "player_side_info \n",
      "            rating_z\n",
      "player_id          \n",
      "0          0.533821\n",
      "1         -0.335954\n",
      "2          0.191548\n",
      "3         -1.769473\n",
      "4         -0.807083\n",
      "============================================================\n",
      "opening_side_info \n",
      "             eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "277                      0              40\n",
      "316                      0              43\n",
      "322                      0              43\n",
      "338                      0              45\n",
      "503                      1               0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"X_train \\n\", X_train.head())\n",
    "print(\"=\"*60)\n",
    "print(\"X_val \\n\", X_val.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"X_test \\n\", X_test.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_train \\n\", y_train.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_val \\n\", y_val.head())\n",
    "print(\"=\" * 60) \n",
    "print(\"y_test \\n\", y_test.head())\n",
    "\n",
    "# Now side information\n",
    "print(\"player_side_info \\n\", player_side_info.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"opening_side_info \\n\", opening_side_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52936cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANED SIDE INFORMATION TABLES\n",
      "============================================================\n",
      "\n",
      "üìä player_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (13990, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Index: player_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.533821\n",
      "1         -0.335954\n",
      "2          0.191548\n",
      "3         -1.769473\n",
      "4         -0.807083\n",
      "\n",
      "üìä opening_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (2560, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Index: opening_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "277                      0              40\n",
      "316                      0              43\n",
      "322                      0              43\n",
      "338                      0              45\n",
      "503                      1               0\n",
      "\n",
      "‚úÖ Both side info tables contain ONLY the necessary model inputs:\n",
      "   ‚Ä¢ player_side_info: rating_z (normalized rating)\n",
      "   ‚Ä¢ opening_side_info: eco_letter_cat, eco_number_cat (categorical encodings)\n",
      "   ‚Ä¢ No unnecessary columns (names, titles, raw strings, etc.)\n"
     ]
    }
   ],
   "source": [
    "# Final verification: Display cleaned side info tables\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANED SIDE INFORMATION TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä player_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: player_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(player_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\nüìä opening_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: opening_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(opening_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\n‚úÖ Both side info tables contain ONLY the necessary model inputs:\")\n",
    "print(f\"   ‚Ä¢ player_side_info: rating_z (normalized rating)\")\n",
    "print(f\"   ‚Ä¢ opening_side_info: eco_letter_cat, eco_number_cat (categorical encodings)\")\n",
    "print(f\"   ‚Ä¢ No unnecessary columns (names, titles, raw strings, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19883cba",
   "metadata": {},
   "source": [
    "## Step 5: Convert to PyTorch Tensors\n",
    "\n",
    "**What are tensors?**\n",
    "Tensors are PyTorch's version of arrays - just multi-dimensional data structures optimized for deep learning. Think of them like fancy NumPy arrays that can run on GPUs.\n",
    "\n",
    "**What we need to convert:**\n",
    "\n",
    "1. **Main features** (X_train, X_val, X_test):\n",
    "   - `player_id` ‚Üí long tensor (integer IDs)\n",
    "   - `opening_id` ‚Üí long tensor (integer IDs)\n",
    "   - `confidence` ‚Üí float tensor (weights for loss function)\n",
    "\n",
    "2. **Targets** (y_train, y_val, y_test):\n",
    "   - `score` ‚Üí float tensor (what we're predicting)\n",
    "\n",
    "3. **Player side info** (player_side_info):\n",
    "   - `rating_z` ‚Üí float tensor (normalized ratings)\n",
    "   - Indexed by player_id for fast lookup\n",
    "\n",
    "4. **Opening side info** (opening_side_info):\n",
    "   - `eco_letter_cat` ‚Üí long tensor (categorical)\n",
    "   - `eco_number_cat` ‚Üí long tensor (categorical)\n",
    "   - Indexed by opening_id for fast lookup\n",
    "\n",
    "**Why these data types?**\n",
    "- `long` (int64): For IDs and categorical features that will be embedded\n",
    "- `float` (float32): For continuous values like scores, confidence, and normalized ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "350be514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4c/gt84bby56mqdnzwsg72zvd680000gn/T/ipykernel_13331/1901790236.py\", line 4, in <module>\n",
      "    import torch\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch (if not already installed)\n",
    "import sys\n",
    "import subprocess\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda01b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 5: CONVERT TO PYTORCH TENSORS\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ PyTorch version: 2.2.2\n",
      "   ‚Ä¢ CUDA available: False\n",
      "   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\n",
      "\n",
      "0Ô∏è‚É£  Index alignment sanity checks...\n",
      "   Checking player_side_info index alignment...\n",
      "   ‚úì player_side_info index is sorted\n",
      "   ‚Ä¢ Index range: [0, 13989]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "\n",
      "   Checking opening_side_info index alignment...\n",
      "   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\n",
      "   ‚Ä¢ Index range: [0, 2559]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "\n",
      "   Checking if indices are contiguous 0-based...\n",
      "   ‚Ä¢ player_side_info contiguous 0-based: True\n",
      "   ‚Ä¢ opening_side_info contiguous 0-based: False\n",
      "   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\n",
      "\n",
      "1Ô∏è‚É£  Converting main features (X_train, X_val, X_test)...\n",
      "   Train tensors:\n",
      "   ‚Ä¢ player_ids_train: torch.Size([666255]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_train: torch.Size([666255]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_train: torch.Size([666255]), dtype=torch.float32\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([133251]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([133251]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([133251]), dtype=torch.float32\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([88835]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([88835]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([88835]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   Train tensors:\n",
      "   ‚Ä¢ player_ids_train: torch.Size([666255]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_train: torch.Size([666255]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_train: torch.Size([666255]), dtype=torch.float32\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([133251]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([133251]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([133251]), dtype=torch.float32\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([88835]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([88835]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([88835]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   ‚Ä¢ scores_train: torch.Size([666255]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([133251]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([88835]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1005, 1.0000]\n",
      "   ‚Ä¢ Val: [0.1800, 0.8333]\n",
      "   ‚Ä¢ Test: [0.2500, 0.7759]\n",
      "\n",
      "3Ô∏è‚É£  Converting player side information...\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([13990]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([13990]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.3050, 4.2304]\n",
      "   ‚úì All 13990 unique players in splits have side information\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2560]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2560]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2560]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "   ‚úì All 2560 unique openings in splits have side information\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "\n",
      "   Dataset sizes:\n",
      "   ‚Ä¢ Train: 666,255 samples\n",
      "   ‚Ä¢ Val: 133,251 samples\n",
      "   ‚Ä¢ Test: 88,835 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 13,990\n",
      "   ‚Ä¢ Openings: 2,560\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 13990 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2560 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 20.42 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Available tensors for training:\n",
      "\n",
      "   Main features (train/val/test):\n",
      "   ‚Ä¢ player_ids_train, player_ids_val, player_ids_test\n",
      "   ‚Ä¢ opening_ids_train, opening_ids_val, opening_ids_test\n",
      "   ‚Ä¢ confidence_train, confidence_val, confidence_test\n",
      "\n",
      "   Targets (train/val/test):\n",
      "   ‚Ä¢ scores_train, scores_val, scores_test\n",
      "\n",
      "   Side information:\n",
      "   ‚Ä¢ player_ratings_tensor (indexed by player_ids_in_side_info)\n",
      "   ‚Ä¢ opening_eco_letter_tensor (indexed by opening_ids_in_side_info)\n",
      "   ‚Ä¢ opening_eco_number_tensor (indexed by opening_ids_in_side_info)\n",
      "\n",
      "üí° Ready for model training!\n",
      "   These tensors can be directly fed into PyTorch DataLoaders and models.\n",
      "   ‚Ä¢ scores_train: torch.Size([666255]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([133251]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([88835]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1005, 1.0000]\n",
      "   ‚Ä¢ Val: [0.1800, 0.8333]\n",
      "   ‚Ä¢ Test: [0.2500, 0.7759]\n",
      "\n",
      "3Ô∏è‚É£  Converting player side information...\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([13990]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([13990]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.3050, 4.2304]\n",
      "   ‚úì All 13990 unique players in splits have side information\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2560]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2560]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2560]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "   ‚úì All 2560 unique openings in splits have side information\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "\n",
      "   Dataset sizes:\n",
      "   ‚Ä¢ Train: 666,255 samples\n",
      "   ‚Ä¢ Val: 133,251 samples\n",
      "   ‚Ä¢ Test: 88,835 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 13,990\n",
      "   ‚Ä¢ Openings: 2,560\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 13990 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2560 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 20.42 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Available tensors for training:\n",
      "\n",
      "   Main features (train/val/test):\n",
      "   ‚Ä¢ player_ids_train, player_ids_val, player_ids_test\n",
      "   ‚Ä¢ opening_ids_train, opening_ids_val, opening_ids_test\n",
      "   ‚Ä¢ confidence_train, confidence_val, confidence_test\n",
      "\n",
      "   Targets (train/val/test):\n",
      "   ‚Ä¢ scores_train, scores_val, scores_test\n",
      "\n",
      "   Side information:\n",
      "   ‚Ä¢ player_ratings_tensor (indexed by player_ids_in_side_info)\n",
      "   ‚Ä¢ opening_eco_letter_tensor (indexed by opening_ids_in_side_info)\n",
      "   ‚Ä¢ opening_eco_number_tensor (indexed by opening_ids_in_side_info)\n",
      "\n",
      "üí° Ready for model training!\n",
      "   These tensors can be directly fed into PyTorch DataLoaders and models.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Convert all data to PyTorch tensors\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 5: CONVERT TO PYTORCH TENSORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   ‚Ä¢ PyTorch version: {torch.__version__}\")\n",
    "print(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\")\n",
    "\n",
    "# 0. Index alignment sanity checks\n",
    "print(f\"\\n0Ô∏è‚É£  Index alignment sanity checks...\")\n",
    "\n",
    "# Check player_side_info is properly indexed\n",
    "print(f\"   Checking player_side_info index alignment...\")\n",
    "player_ids_sorted = sorted(player_side_info.index.values)\n",
    "if player_ids_sorted != list(player_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  player_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì player_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{player_side_info.index.min()}, {player_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {player_side_info.index.dtype}\")\n",
    "\n",
    "# Check opening_side_info is properly indexed\n",
    "print(f\"\\n   Checking opening_side_info index alignment...\")\n",
    "opening_ids_sorted = sorted(opening_side_info.index.values)\n",
    "if opening_ids_sorted != list(opening_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì opening_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{opening_side_info.index.min()}, {opening_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {opening_side_info.index.dtype}\")\n",
    "\n",
    "# CRITICAL: Check if indices are contiguous 0-based\n",
    "# If opening_side_info.index = [0, 1, 2, ..., N-1], then we can use opening_id as direct array index\n",
    "# If not (e.g., [10, 15, 23, ...]), we'll need a mapping dictionary\n",
    "print(f\"\\n   Checking if indices are contiguous 0-based...\")\n",
    "player_contiguous = (player_side_info.index == range(len(player_side_info))).all()\n",
    "opening_contiguous = (opening_side_info.index == range(len(opening_side_info))).all()\n",
    "\n",
    "print(f\"   ‚Ä¢ player_side_info contiguous 0-based: {player_contiguous}\")\n",
    "print(f\"   ‚Ä¢ opening_side_info contiguous 0-based: {opening_contiguous}\")\n",
    "\n",
    "if not player_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Player IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "if not opening_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "\n",
    "# 1. Convert main features (train/val/test)\n",
    "print(f\"\\n1Ô∏è‚É£  Converting main features (X_train, X_val, X_test)...\")\n",
    "\n",
    "# Train set\n",
    "player_ids_train = torch.tensor(X_train['player_id'].values, dtype=torch.long)\n",
    "opening_ids_train = torch.tensor(X_train['opening_id'].values, dtype=torch.long)\n",
    "confidence_train = torch.tensor(X_train['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   Train tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_train: {player_ids_train.shape}, dtype={player_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_train: {opening_ids_train.shape}, dtype={opening_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_train: {confidence_train.shape}, dtype={confidence_train.dtype}\")\n",
    "\n",
    "# Validation set\n",
    "player_ids_val = torch.tensor(X_val['player_id'].values, dtype=torch.long)\n",
    "opening_ids_val = torch.tensor(X_val['opening_id'].values, dtype=torch.long)\n",
    "confidence_val = torch.tensor(X_val['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Validation tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_val: {player_ids_val.shape}, dtype={player_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_val: {opening_ids_val.shape}, dtype={opening_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_val: {confidence_val.shape}, dtype={confidence_val.dtype}\")\n",
    "\n",
    "# Test set\n",
    "player_ids_test = torch.tensor(X_test['player_id'].values, dtype=torch.long)\n",
    "opening_ids_test = torch.tensor(X_test['opening_id'].values, dtype=torch.long)\n",
    "confidence_test = torch.tensor(X_test['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Test tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_test: {player_ids_test.shape}, dtype={player_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_test: {opening_ids_test.shape}, dtype={opening_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_test: {confidence_test.shape}, dtype={confidence_test.dtype}\")\n",
    "\n",
    "# 2. Convert targets (scores)\n",
    "print(f\"\\n2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\")\n",
    "\n",
    "scores_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "scores_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "scores_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ scores_train: {scores_train.shape}, dtype={scores_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_val: {scores_val.shape}, dtype={scores_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_test: {scores_test.shape}, dtype={scores_test.dtype}\")\n",
    "\n",
    "print(f\"\\n   Score ranges (sanity check):\")\n",
    "print(f\"   ‚Ä¢ Train: [{scores_train.min():.4f}, {scores_train.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Val: [{scores_val.min():.4f}, {scores_val.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Test: [{scores_test.min():.4f}, {scores_test.max():.4f}]\")\n",
    "\n",
    "# 3. Convert player side information\n",
    "print(f\"\\n3Ô∏è‚É£  Converting player side information...\")\n",
    "\n",
    "# Create tensor of all player ratings (indexed by player_id)\n",
    "# Since player_side_info is indexed by player_id, we need to ensure coverage\n",
    "player_ratings_tensor = torch.tensor(player_side_info['rating_z'].values, dtype=torch.float32)\n",
    "player_ids_in_side_info = torch.tensor(player_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ player_ratings_tensor: {player_ratings_tensor.shape}, dtype={player_ratings_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ player_ids_in_side_info: {player_ids_in_side_info.shape}, dtype={player_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings_tensor.min():.4f}, {player_ratings_tensor.max():.4f}]\")\n",
    "\n",
    "# Verify all player IDs in train/val/test are covered\n",
    "all_player_ids = torch.cat([player_ids_train, player_ids_val, player_ids_test]).unique()\n",
    "missing_players = set(all_player_ids.tolist()) - set(player_ids_in_side_info.tolist())\n",
    "if len(missing_players) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in splits missing from side_info!\")\n",
    "else:\n",
    "    print(f\"   ‚úì All {len(all_player_ids)} unique players in splits have side information\")\n",
    "\n",
    "# 4. Convert opening side information\n",
    "print(f\"\\n4Ô∏è‚É£  Converting opening side information...\")\n",
    "\n",
    "# Verify column names exist (using eco_letter_cat and eco_number_cat consistently)\n",
    "if 'eco_letter_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_letter_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "if 'eco_number_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_number_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "\n",
    "# Create tensors for opening ECO features (indexed by opening_id)\n",
    "opening_eco_letter_tensor = torch.tensor(opening_side_info['eco_letter_cat'].values, dtype=torch.long)\n",
    "opening_eco_number_tensor = torch.tensor(opening_side_info['eco_number_cat'].values, dtype=torch.long)\n",
    "opening_ids_in_side_info = torch.tensor(opening_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ opening_eco_letter_tensor: {opening_eco_letter_tensor.shape}, dtype={opening_eco_letter_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_eco_number_tensor: {opening_eco_number_tensor.shape}, dtype={opening_eco_number_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_in_side_info: {opening_ids_in_side_info.shape}, dtype={opening_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ ECO letter range: [{opening_eco_letter_tensor.min()}, {opening_eco_letter_tensor.max()}]\")\n",
    "print(f\"   ‚Ä¢ ECO number range: [{opening_eco_number_tensor.min()}, {opening_eco_number_tensor.max()}]\")\n",
    "\n",
    "# Verify all opening IDs in train/val/test are covered\n",
    "all_opening_ids = torch.cat([opening_ids_train, opening_ids_val, opening_ids_test]).unique()\n",
    "missing_openings = set(all_opening_ids.tolist()) - set(opening_ids_in_side_info.tolist())\n",
    "if len(missing_openings) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits missing from side_info!\")\n",
    "else:\n",
    "    print(f\"   ‚úì All {len(all_opening_ids)} unique openings in splits have side information\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(f\"\\n5Ô∏è‚É£  Summary statistics:\")\n",
    "print(f\"\\n   Dataset sizes:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(scores_train):,} samples\")\n",
    "print(f\"   ‚Ä¢ Val: {len(scores_val):,} samples\")\n",
    "print(f\"   ‚Ä¢ Test: {len(scores_test):,} samples\")\n",
    "\n",
    "print(f\"\\n   Unique entities:\")\n",
    "print(f\"   ‚Ä¢ Players: {len(player_ids_in_side_info):,}\")\n",
    "print(f\"   ‚Ä¢ Openings: {len(opening_ids_in_side_info):,}\")\n",
    "\n",
    "print(f\"\\n   Vocabulary sizes (for embedding layers):\")\n",
    "print(f\"   ‚Ä¢ num_players: {player_ids_in_side_info.max() + 1} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_openings: {opening_ids_in_side_info.max() + 1} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_letters: {opening_eco_letter_tensor.max() + 1} (max eco_letter_cat + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_numbers: {opening_eco_number_tensor.max() + 1} (max eco_number_cat + 1)\")\n",
    "\n",
    "# 6. Memory usage (approximate - using simple calculation)\n",
    "print(f\"\\n6Ô∏è‚É£  Approximate memory usage:\")\n",
    "# More efficient calculation: element_size * nelement for each tensor\n",
    "# Using list comprehension with helper function for cleaner code\n",
    "def tensor_memory_mb(t):\n",
    "    \"\"\"Calculate tensor memory in MB\"\"\"\n",
    "    return (t.element_size() * t.nelement()) / (1024 * 1024)\n",
    "\n",
    "tensors = [\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train,\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val,\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test,\n",
    "    player_ratings_tensor, opening_eco_letter_tensor, opening_eco_number_tensor\n",
    "]\n",
    "total_memory_mb = sum(tensor_memory_mb(t) for t in tensors)\n",
    "print(f\"   ‚Ä¢ Total tensor memory: {total_memory_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TENSOR CONVERSION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Available tensors for training:\")\n",
    "print(f\"\\n   Main features (train/val/test):\")\n",
    "print(f\"   ‚Ä¢ player_ids_train, player_ids_val, player_ids_test\")\n",
    "print(f\"   ‚Ä¢ opening_ids_train, opening_ids_val, opening_ids_test\")\n",
    "print(f\"   ‚Ä¢ confidence_train, confidence_val, confidence_test\")\n",
    "\n",
    "print(f\"\\n   Targets (train/val/test):\")\n",
    "print(f\"   ‚Ä¢ scores_train, scores_val, scores_test\")\n",
    "\n",
    "print(f\"\\n   Side information:\")\n",
    "print(f\"   ‚Ä¢ player_ratings_tensor (indexed by player_ids_in_side_info)\")\n",
    "print(f\"   ‚Ä¢ opening_eco_letter_tensor (indexed by opening_ids_in_side_info)\")\n",
    "print(f\"   ‚Ä¢ opening_eco_number_tensor (indexed by opening_ids_in_side_info)\")\n",
    "\n",
    "print(f\"\\nüí° Ready for model training!\")\n",
    "print(f\"   These tensors can be directly fed into PyTorch DataLoaders and models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fc308",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup\n",
    "\n",
    "Define all constants, loss functions, optimizer, and helper functions for training.\n",
    "\n",
    "**What we need to set up:**\n",
    "\n",
    "1. **Constants and Hyperparameters:**\n",
    "   - `NUM_FACTORS`: Dimensionality of latent factor embeddings (e.g., 50)\n",
    "   - `LEARNING_RATE`: Step size for SGD optimizer (e.g., 0.01)\n",
    "   - `BATCH_SIZE`: Number of samples per training batch (e.g., 1024)\n",
    "   - `N_EPOCHS`: Number of full passes through training data (e.g., 10)\n",
    "   - Random seeds for reproducibility\n",
    "\n",
    "2. **Loss Functions:**\n",
    "   - MSE (Mean Squared Error): Main training loss\n",
    "   - RMSE (Root Mean Squared Error): Evaluation metric\n",
    "   - Confidence-weighted loss: Down-weight uncertain predictions\n",
    "\n",
    "3. **Optimizer:**\n",
    "   - SGD (Stochastic Gradient Descent) with momentum\n",
    "\n",
    "4. **Helper Functions:**\n",
    "   - `train_one_epoch()`: Train for one epoch\n",
    "   - `evaluate_model()`: Evaluate on validation/test set\n",
    "   - `calculate_rmse()`: Compute RMSE metric\n",
    "   - `save_checkpoint()`: Save model state\n",
    "\n",
    "5. **Logging:**\n",
    "   - Progress bars with ETA\n",
    "   - Loss tracking per epoch\n",
    "   - Model checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d13c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\n",
      "============================================================\n",
      "\n",
      "üìã Hyperparameters:\n",
      "   ‚Ä¢ NUM_FACTORS: 50\n",
      "   ‚Ä¢ LEARNING_RATE: 0.01\n",
      "   ‚Ä¢ MOMENTUM: 0.9\n",
      "   ‚Ä¢ BATCH_SIZE: 1024\n",
      "   ‚Ä¢ N_EPOCHS: 20\n",
      "   ‚Ä¢ WEIGHT_DECAY: 0.0\n",
      "\n",
      "üé≤ Random Seed:\n",
      "   ‚Ä¢ RANDOM_SEED: 42\n",
      "\n",
      "üíª Device Configuration:\n",
      "   ‚Ä¢ DEVICE: cpu\n",
      "\n",
      "üíæ Model Saving:\n",
      "   ‚Ä¢ Save directory: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints\n",
      "\n",
      "üîí Setting random seeds for reproducibility...\n",
      "   ‚úì Python random seed set to 42\n",
      "   ‚úì NumPy random seed set to 42\n",
      "   ‚úì PyTorch CPU random seed set to 42\n",
      "\n",
      "üìä Vocabulary Sizes (for embedding layers):\n",
      "   ‚Ä¢ NUM_PLAYERS: 13,990 (max player_id + 1)\n",
      "   ‚Ä¢ NUM_OPENINGS: 2,560 (max opening_id + 1)\n",
      "   ‚Ä¢ NUM_ECO_LETTERS: 5 (ECO letter categories)\n",
      "   ‚Ä¢ NUM_ECO_NUMBERS: 100 (ECO number categories)\n",
      "\n",
      "üìà Dataset Statistics:\n",
      "   ‚Ä¢ Train samples: 666,255\n",
      "   ‚Ä¢ Validation samples: 133,251\n",
      "   ‚Ä¢ Test samples: 88,835\n",
      "   ‚Ä¢ Total samples: 888,341\n",
      "\n",
      "   ‚Ä¢ Batches per epoch: 650\n",
      "   ‚Ä¢ Training iterations (total): 13,000\n",
      "\n",
      "============================================================\n",
      "‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\n",
      "============================================================\n",
      "\n",
      "üí° To modify hyperparameters:\n",
      "   ‚Ä¢ Edit the values at the top of this cell\n",
      "   ‚Ä¢ Rerun this cell to apply changes\n",
      "   ‚Ä¢ All subsequent cells will use the updated values\n"
     ]
    }
   ],
   "source": [
    "# Step 6a: Define Hyperparameters and Constants\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HYPERPARAMETERS (Easy to modify here)\n",
    "# ========================================\n",
    "\n",
    "# Model architecture\n",
    "NUM_FACTORS = 50  # Dimensionality of latent factor embeddings\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.01  # SGD learning rate\n",
    "MOMENTUM = 0.9  # SGD momentum\n",
    "BATCH_SIZE = 1024  # Mini-batch size\n",
    "N_EPOCHS = 20  # Number of training epochs\n",
    "\n",
    "# Regularization (if needed)\n",
    "WEIGHT_DECAY = 0.0  # L2 regularization (0 = no regularization)\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# MODEL_SAVE_DIR = Path.cwd() / \"model_checkpoints\"\n",
    "MODEL_SAVE_DIR = Path.cwd().parent / \"data\" / \"models\" / \"model_checkpoints\"  # Goes to root/data/models\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"\\nüìã Hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ NUM_FACTORS: {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ LEARNING_RATE: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ MOMENTUM: {MOMENTUM}\")\n",
    "print(f\"   ‚Ä¢ BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ N_EPOCHS: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ WEIGHT_DECAY: {WEIGHT_DECAY}\")\n",
    "\n",
    "print(f\"\\nüé≤ Random Seed:\")\n",
    "print(f\"   ‚Ä¢ RANDOM_SEED: {RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\nüíª Device Configuration:\")\n",
    "print(f\"   ‚Ä¢ DEVICE: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ‚Ä¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüíæ Model Saving:\")\n",
    "print(f\"   ‚Ä¢ Save directory: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# ========================================\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüîí Setting random seeds for reproducibility...\")\n",
    "\n",
    "# Python random\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì Python random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì NumPy random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# PyTorch random\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì PyTorch CPU random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU\n",
    "    print(f\"   ‚úì PyTorch GPU random seed set to {RANDOM_SEED}\")\n",
    "    \n",
    "    # Additional CUDA reproducibility settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"   ‚úì CUDA deterministic mode enabled\")\n",
    "\n",
    "# ========================================\n",
    "# CALCULATE VOCABULARY SIZES\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Sizes (for embedding layers):\")\n",
    "\n",
    "# Calculate from our tensors\n",
    "NUM_PLAYERS = int(player_ids_in_side_info.max()) + 1\n",
    "NUM_OPENINGS = int(opening_ids_in_side_info.max()) + 1\n",
    "NUM_ECO_LETTERS = int(opening_eco_letter_tensor.max()) + 1\n",
    "NUM_ECO_NUMBERS = int(opening_eco_number_tensor.max()) + 1\n",
    "\n",
    "print(f\"   ‚Ä¢ NUM_PLAYERS: {NUM_PLAYERS:,} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_OPENINGS: {NUM_OPENINGS:,} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_LETTERS: {NUM_ECO_LETTERS} (ECO letter categories)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_NUMBERS: {NUM_ECO_NUMBERS} (ECO number categories)\")\n",
    "\n",
    "# ========================================\n",
    "# DATASET STATISTICS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Train samples: {len(scores_train):,}\")\n",
    "print(f\"   ‚Ä¢ Validation samples: {len(scores_val):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(scores_test):,}\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(scores_train) + len(scores_val) + len(scores_test):,}\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Batches per epoch: {len(scores_train) // BATCH_SIZE:,}\")\n",
    "print(f\"   ‚Ä¢ Training iterations (total): {(len(scores_train) // BATCH_SIZE) * N_EPOCHS:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° To modify hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ Edit the values at the top of this cell\")\n",
    "print(f\"   ‚Ä¢ Rerun this cell to apply changes\")\n",
    "print(f\"   ‚Ä¢ All subsequent cells will use the updated values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a93e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6B: DEFINE LOSS FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Loss functions defined:\n",
      "   ‚Ä¢ mse_loss(): Mean Squared Error (with optional confidence weighting)\n",
      "   ‚Ä¢ rmse_loss(): Root Mean Squared Error\n",
      "   ‚Ä¢ calculate_rmse(): RMSE evaluation metric\n",
      "\n",
      "üß™ Testing loss functions with dummy data...\n",
      "\n",
      "   Dummy data:\n",
      "   ‚Ä¢ Predictions: [0.5, 0.6000000238418579, 0.699999988079071, 0.800000011920929]\n",
      "   ‚Ä¢ Targets: [0.550000011920929, 0.6200000047683716, 0.6800000071525574, 0.75]\n",
      "   ‚Ä¢ Confidence: [0.800000011920929, 0.8999999761581421, 0.699999988079071, 1.0]\n",
      "\n",
      "   MSE (unweighted): 0.001450\n",
      "   MSE (weighted): 0.001512\n",
      "\n",
      "   RMSE (unweighted): 0.038079\n",
      "   RMSE (weighted): 0.038881\n",
      "\n",
      "   calculate_rmse() result: 0.038881\n",
      "   ‚úì Returns Python float (not tensor): <class 'float'>\n",
      "\n",
      "============================================================\n",
      "‚úÖ LOSS FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Training will use:\n",
      "   ‚Ä¢ Loss function: mse_loss() with confidence weighting\n",
      "   ‚Ä¢ Evaluation metric: calculate_rmse()\n",
      "   ‚Ä¢ Confidence weights: From 'confidence' column in data\n",
      "   ‚Ä¢ Weighting strategy: confidence = num_games / (num_games + K)\n",
      "   ‚Ä¢ Effect: High-game-count entries have larger loss impact\n"
     ]
    }
   ],
   "source": [
    "# Step 6b: Define Loss Functions\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6B: DEFINE LOSS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "def mse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss with optional confidence weighting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        Higher confidence = larger loss contribution\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Compute squared error\n",
    "    squared_error = (predictions - targets) ** 2\n",
    "    \n",
    "    # Apply confidence weighting if provided\n",
    "    if confidence_weights is not None:\n",
    "        weighted_squared_error = squared_error * confidence_weights\n",
    "        # Average over all samples (sum of weighted errors / sum of weights)\n",
    "        loss = weighted_squared_error.sum() / confidence_weights.sum()\n",
    "    else:\n",
    "        # Standard MSE (average over all samples)\n",
    "        loss = squared_error.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def rmse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error loss.\n",
    "    \n",
    "    This is the square root of MSE and is our primary evaluation metric.\n",
    "    RMSE is in the same units as the target (score), making it interpretable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar RMSE value\n",
    "    \"\"\"\n",
    "    mse = mse_loss(predictions, targets, confidence_weights)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def calculate_rmse(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Calculate RMSE metric (convenience function for evaluation).\n",
    "    \n",
    "    This is the same as rmse_loss but with clearer naming for evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        RMSE value (Python float, not tensor)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Don't compute gradients for evaluation\n",
    "        rmse = rmse_loss(predictions, targets, confidence_weights)\n",
    "    return rmse.item()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ Loss functions defined:\")\n",
    "print(f\"   ‚Ä¢ mse_loss(): Mean Squared Error (with optional confidence weighting)\")\n",
    "print(f\"   ‚Ä¢ rmse_loss(): Root Mean Squared Error\")\n",
    "print(f\"   ‚Ä¢ calculate_rmse(): RMSE evaluation metric\")\n",
    "\n",
    "print(f\"\\nüß™ Testing loss functions with dummy data...\")\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_predictions = torch.tensor([0.5, 0.6, 0.7, 0.8], dtype=torch.float32)\n",
    "dummy_targets = torch.tensor([0.55, 0.62, 0.68, 0.75], dtype=torch.float32)\n",
    "dummy_confidence = torch.tensor([0.8, 0.9, 0.7, 1.0], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Dummy data:\")\n",
    "print(f\"   ‚Ä¢ Predictions: {dummy_predictions.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Targets: {dummy_targets.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Confidence: {dummy_confidence.tolist()}\")\n",
    "\n",
    "# Test MSE without confidence weighting\n",
    "mse_unweighted = mse_loss(dummy_predictions, dummy_targets)\n",
    "print(f\"\\n   MSE (unweighted): {mse_unweighted.item():.6f}\")\n",
    "\n",
    "# Test MSE with confidence weighting\n",
    "mse_weighted = mse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"   MSE (weighted): {mse_weighted.item():.6f}\")\n",
    "\n",
    "# Test RMSE\n",
    "rmse_unweighted = rmse_loss(dummy_predictions, dummy_targets)\n",
    "rmse_weighted = rmse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   RMSE (unweighted): {rmse_unweighted.item():.6f}\")\n",
    "print(f\"   RMSE (weighted): {rmse_weighted.item():.6f}\")\n",
    "\n",
    "# Test calculate_rmse\n",
    "rmse_eval = calculate_rmse(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   calculate_rmse() result: {rmse_eval:.6f}\")\n",
    "print(f\"   ‚úì Returns Python float (not tensor): {type(rmse_eval)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LOSS FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Training will use:\")\n",
    "print(f\"   ‚Ä¢ Loss function: mse_loss() with confidence weighting\")\n",
    "print(f\"   ‚Ä¢ Evaluation metric: calculate_rmse()\")\n",
    "print(f\"   ‚Ä¢ Confidence weights: From 'confidence' column in data\")\n",
    "print(f\"   ‚Ä¢ Weighting strategy: confidence = num_games / (num_games + K)\")\n",
    "print(f\"   ‚Ä¢ Effect: High-game-count entries have larger loss impact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35e00b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Creating PyTorch Datasets...\n",
      "   ‚úì Train dataset: 666,255 samples\n",
      "   ‚úì Validation dataset: 133,251 samples\n",
      "   ‚úì Test dataset: 88,835 samples\n",
      "\n",
      "2Ô∏è‚É£  Testing dataset access...\n",
      "   Sample from train_dataset[0]:\n",
      "   ‚Ä¢ player_id: 8732\n",
      "   ‚Ä¢ opening_id: 2365\n",
      "   ‚Ä¢ confidence: 0.1803\n",
      "   ‚Ä¢ score: 0.5894\n",
      "\n",
      "3Ô∏è‚É£  Creating PyTorch DataLoaders...\n",
      "   ‚úì Train loader: 651 batches of size 1024\n",
      "   ‚úì Validation loader: 131 batches of size 1024\n",
      "   ‚úì Test loader: 87 batches of size 1024\n",
      "\n",
      "4Ô∏è‚É£  Testing DataLoader batch access...\n",
      "   First batch from train_loader:\n",
      "   ‚Ä¢ player_id shape: torch.Size([1024])\n",
      "   ‚Ä¢ opening_id shape: torch.Size([1024])\n",
      "   ‚Ä¢ confidence shape: torch.Size([1024])\n",
      "   ‚Ä¢ score shape: torch.Size([1024])\n",
      "\n",
      "   Sample values from first batch (first 5):\n",
      "   [0] player=2113, opening=561, conf=0.1803, score=0.4506\n",
      "   [1] player=10704, opening=1558, conf=0.4382, score=0.5620\n",
      "   [2] player=12995, opening=1493, conf=0.2647, score=0.5307\n",
      "   [3] player=6209, opening=1222, conf=0.1667, score=0.4590\n",
      "   [4] player=7372, opening=259, conf=0.3827, score=0.4893\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATASET AND DATALOADER COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Available objects:\n",
      "   ‚Ä¢ train_dataset, val_dataset, test_dataset\n",
      "   ‚Ä¢ train_loader, val_loader, test_loader\n",
      "\n",
      "üí° DataLoader features:\n",
      "   ‚Ä¢ Automatic batching: 1024 samples per batch\n",
      "   ‚Ä¢ Shuffling: Training data shuffled each epoch\n",
      "   ‚Ä¢ Pin memory: Disabled (speeds up GPU transfer)\n",
      "   ‚Ä¢ Ready for training loop!\n",
      "   [1] player=10704, opening=1558, conf=0.4382, score=0.5620\n",
      "   [2] player=12995, opening=1493, conf=0.2647, score=0.5307\n",
      "   [3] player=6209, opening=1222, conf=0.1667, score=0.4590\n",
      "   [4] player=7372, opening=259, conf=0.3827, score=0.4893\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATASET AND DATALOADER COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Available objects:\n",
      "   ‚Ä¢ train_dataset, val_dataset, test_dataset\n",
      "   ‚Ä¢ train_loader, val_loader, test_loader\n",
      "\n",
      "üí° DataLoader features:\n",
      "   ‚Ä¢ Automatic batching: 1024 samples per batch\n",
      "   ‚Ä¢ Shuffling: Training data shuffled each epoch\n",
      "   ‚Ä¢ Pin memory: Disabled (speeds up GPU transfer)\n",
      "   ‚Ä¢ Ready for training loop!\n"
     ]
    }
   ],
   "source": [
    "# Step 6c: Create PyTorch Dataset and DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# CUSTOM DATASET CLASS\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for chess player-opening interactions.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - player_id: Integer ID for embedding lookup\n",
    "    - opening_id: Integer ID for embedding lookup\n",
    "    - confidence: Confidence weight for loss function\n",
    "    - score: Target value (win rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, player_ids, opening_ids, confidence, scores):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (long tensor)\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (long tensor)\n",
    "        confidence : torch.Tensor\n",
    "            Confidence weights (float tensor)\n",
    "        scores : torch.Tensor\n",
    "            Target scores (float tensor)\n",
    "        \"\"\"\n",
    "        self.player_ids = player_ids\n",
    "        self.opening_ids = opening_ids\n",
    "        self.confidence = confidence\n",
    "        self.scores = scores\n",
    "        \n",
    "        # Validate shapes\n",
    "        assert len(player_ids) == len(opening_ids) == len(confidence) == len(scores), \\\n",
    "            \"All tensors must have the same length\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.player_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with keys: 'player_id', 'opening_id', 'confidence', 'score'\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'player_id': self.player_ids[idx],\n",
    "            'opening_id': self.opening_ids[idx],\n",
    "            'confidence': self.confidence[idx],\n",
    "            'score': self.scores[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATASETS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Creating PyTorch Datasets...\")\n",
    "\n",
    "train_dataset = ChessOpeningDataset(\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train\n",
    ")\n",
    "val_dataset = ChessOpeningDataset(\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val\n",
    ")\n",
    "test_dataset = ChessOpeningDataset(\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"   ‚úì Validation dataset: {len(val_dataset):,} samples\")\n",
    "print(f\"   ‚úì Test dataset: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Test dataset access\n",
    "print(f\"\\n2Ô∏è‚É£  Testing dataset access...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"   Sample from train_dataset[0]:\")\n",
    "print(f\"   ‚Ä¢ player_id: {sample['player_id']}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {sample['opening_id']}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "print(f\"   ‚Ä¢ score: {sample['score']:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATALOADERS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Creating PyTorch DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle training data each epoch\n",
    "    num_workers=0,  # Number of subprocesses for data loading (0 = main process)\n",
    "    pin_memory=True if torch.cuda.is_available() else False  # Speed up GPU transfer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle test data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Train loader: {len(train_loader):,} batches of size {BATCH_SIZE}\")\n",
    "print(f\"   ‚úì Validation loader: {len(val_loader):,} batches of size {BATCH_SIZE}\")\n",
    "print(f\"   ‚úì Test loader: {len(test_loader):,} batches of size {BATCH_SIZE}\")\n",
    "\n",
    "# Test dataloader\n",
    "print(f\"\\n4Ô∏è‚É£  Testing DataLoader batch access...\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"   First batch from train_loader:\")\n",
    "print(f\"   ‚Ä¢ player_id shape: {batch['player_id'].shape}\")\n",
    "print(f\"   ‚Ä¢ opening_id shape: {batch['opening_id'].shape}\")\n",
    "print(f\"   ‚Ä¢ confidence shape: {batch['confidence'].shape}\")\n",
    "print(f\"   ‚Ä¢ score shape: {batch['score'].shape}\")\n",
    "\n",
    "print(f\"\\n   Sample values from first batch (first 5):\")\n",
    "for i in range(min(5, batch['player_id'].shape[0])):\n",
    "    print(f\"   [{i}] player={batch['player_id'][i]}, opening={batch['opening_id'][i]}, \"\n",
    "          f\"conf={batch['confidence'][i]:.4f}, score={batch['score'][i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATASET AND DATALOADER COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Available objects:\")\n",
    "print(f\"   ‚Ä¢ train_dataset, val_dataset, test_dataset\")\n",
    "print(f\"   ‚Ä¢ train_loader, val_loader, test_loader\")\n",
    "\n",
    "print(f\"\\nüí° DataLoader features:\")\n",
    "print(f\"   ‚Ä¢ Automatic batching: {BATCH_SIZE} samples per batch\")\n",
    "print(f\"   ‚Ä¢ Shuffling: Training data shuffled each epoch\")\n",
    "print(f\"   ‚Ä¢ Pin memory: {'Enabled' if torch.cuda.is_available() else 'Disabled'} (speeds up GPU transfer)\")\n",
    "print(f\"   ‚Ä¢ Ready for training loop!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e5fbb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6D: DEFINE HELPER FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Helper functions defined:\n",
      "   ‚Ä¢ save_checkpoint(): Save model and optimizer state\n",
      "   ‚Ä¢ load_checkpoint(): Load model and optimizer state\n",
      "   ‚Ä¢ format_time(): Format seconds as readable time\n",
      "   ‚Ä¢ print_progress(): Print training progress with ETA\n",
      "\n",
      "üß™ Testing helper functions...\n",
      "\n",
      "   format_time() tests:\n",
      "   ‚Ä¢ 30 seconds ‚Üí 30.0s\n",
      "   ‚Ä¢ 90 seconds ‚Üí 1m 30s\n",
      "   ‚Ä¢ 300 seconds ‚Üí 5m 0s\n",
      "   ‚Ä¢ 3661 seconds ‚Üí 1h 1m 1s\n",
      "   ‚Ä¢ 7384 seconds ‚Üí 2h 3m 4s\n",
      "\n",
      "   print_progress() test (simulating batch progress):\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° These functions will be used during training for:\n",
      "   ‚Ä¢ Progress tracking with ETA estimation\n",
      "   ‚Ä¢ Saving checkpoints after each epoch\n",
      "   ‚Ä¢ Loading checkpoints for resuming training\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° These functions will be used during training for:\n",
      "   ‚Ä¢ Progress tracking with ETA estimation\n",
      "   ‚Ä¢ Saving checkpoints after each epoch\n",
      "   ‚Ä¢ Loading checkpoints for resuming training\n"
     ]
    }
   ],
   "source": [
    "# Step 6d: Define Helper Functions for Training\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6D: DEFINE HELPER FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Save Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to save\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer state to save\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    train_loss : float\n",
    "        Training loss for this epoch\n",
    "    val_loss : float\n",
    "        Validation loss for this epoch\n",
    "    filepath : Path or str\n",
    "        Where to save the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        \n",
    "        # Save hyperparameters for reproducibility\n",
    "        'hyperparameters': {\n",
    "            'num_factors': NUM_FACTORS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'momentum': MOMENTUM,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'num_players': NUM_PLAYERS,\n",
    "            'num_openings': NUM_OPENINGS,\n",
    "            'num_eco_letters': NUM_ECO_LETTERS,\n",
    "            'num_eco_numbers': NUM_ECO_NUMBERS,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"   üíæ Checkpoint saved to: {filepath}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Load Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to load weights into\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to load state into\n",
    "    filepath : Path or str\n",
    "        Where to load the checkpoint from\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        The epoch number from the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"   üìÇ Checkpoint loaded from: {filepath}\")\n",
    "    print(f\"   ‚Ä¢ Epoch: {epoch}\")\n",
    "    print(f\"   ‚Ä¢ Train loss: {checkpoint['train_loss']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Val loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    return epoch\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Format Time\n",
    "# ========================================\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"\n",
    "    Format seconds as human-readable time string.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seconds : float\n",
    "        Number of seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted time string (e.g., \"1h 23m 45s\")\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Print Progress\n",
    "# ========================================\n",
    "\n",
    "def print_progress(epoch, batch_idx, total_batches, loss, elapsed_time):\n",
    "    \"\"\"\n",
    "    Print training progress with ETA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    batch_idx : int\n",
    "        Current batch index (0-based)\n",
    "    total_batches : int\n",
    "        Total number of batches per epoch\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    elapsed_time : float\n",
    "        Elapsed time in seconds since epoch start\n",
    "    \"\"\"\n",
    "    # Calculate progress\n",
    "    progress_pct = 100.0 * (batch_idx + 1) / total_batches\n",
    "    \n",
    "    # Estimate time remaining\n",
    "    time_per_batch = elapsed_time / (batch_idx + 1)\n",
    "    remaining_batches = total_batches - (batch_idx + 1)\n",
    "    eta_seconds = time_per_batch * remaining_batches\n",
    "    \n",
    "    # Print progress bar\n",
    "    bar_length = 40\n",
    "    filled_length = int(bar_length * (batch_idx + 1) / total_batches)\n",
    "    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)\n",
    "    \n",
    "    print(f\"   Epoch {epoch} [{bar}] {progress_pct:>6.2f}% | \"\n",
    "          f\"Loss: {loss:.6f} | \"\n",
    "          f\"ETA: {format_time(eta_seconds)}\", end='\\r')\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST HELPER FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ Helper functions defined:\")\n",
    "print(f\"   ‚Ä¢ save_checkpoint(): Save model and optimizer state\")\n",
    "print(f\"   ‚Ä¢ load_checkpoint(): Load model and optimizer state\")\n",
    "print(f\"   ‚Ä¢ format_time(): Format seconds as readable time\")\n",
    "print(f\"   ‚Ä¢ print_progress(): Print training progress with ETA\")\n",
    "\n",
    "print(f\"\\nüß™ Testing helper functions...\")\n",
    "\n",
    "# Test format_time\n",
    "print(f\"\\n   format_time() tests:\")\n",
    "test_times = [30, 90, 300, 3661, 7384]\n",
    "for t in test_times:\n",
    "    print(f\"   ‚Ä¢ {t} seconds ‚Üí {format_time(t)}\")\n",
    "\n",
    "# Test print_progress (will overwrite line)\n",
    "print(f\"\\n   print_progress() test (simulating batch progress):\")\n",
    "for i in range(0, 101, 20):\n",
    "    total = 100\n",
    "    loss = 0.05 - i * 0.0003\n",
    "    elapsed = i * 0.5\n",
    "    print_progress(epoch=1, batch_idx=i, total_batches=total, loss=loss, elapsed_time=elapsed)\n",
    "    time.sleep(0.1)  # Brief pause to show animation\n",
    "print()  # New line after progress bar\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HELPER FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° These functions will be used during training for:\")\n",
    "print(f\"   ‚Ä¢ Progress tracking with ETA estimation\")\n",
    "print(f\"   ‚Ä¢ Saving checkpoints after each epoch\")\n",
    "print(f\"   ‚Ä¢ Loading checkpoints for resuming training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cab0cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Training and evaluation functions defined:\n",
      "   ‚Ä¢ train_one_epoch(): Train model for one epoch\n",
      "   ‚Ä¢ evaluate_model(): Evaluate model on a dataset\n",
      "\n",
      "üí° Function features:\n",
      "   ‚Ä¢ Automatic device handling (CPU/CUDA)\n",
      "   ‚Ä¢ Progress tracking with ETA\n",
      "   ‚Ä¢ Confidence-weighted loss\n",
      "   ‚Ä¢ Batch processing via DataLoader\n",
      "   ‚Ä¢ Train/eval mode switching\n",
      "\n",
      "============================================================\n",
      "‚úÖ TRAINING AND EVALUATION FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìã Training workflow:\n",
      "   1. Initialize model and optimizer\n",
      "   2. For each epoch:\n",
      "      a. Call train_one_epoch()\n",
      "      b. Call evaluate_model() on validation set\n",
      "      c. Save checkpoint with save_checkpoint()\n",
      "      d. Log metrics\n",
      "   3. After training, evaluate on test set\n",
      "\n",
      "üöÄ Ready to define the model architecture (next cell)!\n"
     ]
    }
   ],
   "source": [
    "# Step 6e: Define Training and Evaluation Functions\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# TRAINING FUNCTION: Train One Epoch\n",
    "# ========================================\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, device, epoch_num):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to train\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for updating model parameters\n",
    "    device : torch.device\n",
    "        Device to train on (CPU or CUDA)\n",
    "    epoch_num : int\n",
    "        Current epoch number (for logging)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (average_loss, elapsed_time)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move batch to device\n",
    "        player_ids = batch['player_id'].to(device)\n",
    "        opening_ids = batch['opening_id'].to(device)\n",
    "        confidence = batch['confidence'].to(device)\n",
    "        targets = batch['score'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(player_ids, opening_ids)\n",
    "        \n",
    "        # Compute loss with confidence weighting\n",
    "        loss = mse_loss(predictions, targets, confidence)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 10 batches or on last batch\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print_progress(epoch_num, batch_idx, num_batches, loss.item(), elapsed)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    elapsed_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Clear progress line and print summary\n",
    "    print()  # New line after progress bar\n",
    "    \n",
    "    return avg_loss, elapsed_time\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# EVALUATION FUNCTION: Evaluate Model\n",
    "# ========================================\n",
    "\n",
    "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to evaluate\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for evaluation data\n",
    "    device : torch.device\n",
    "        Device to evaluate on (CPU or CUDA)\n",
    "    dataset_name : str\n",
    "        Name of dataset for logging (e.g., \"Validation\", \"Test\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (mse, rmse)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_mse = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in data_loader:\n",
    "            # Move batch to device\n",
    "            player_ids = batch['player_id'].to(device)\n",
    "            opening_ids = batch['opening_id'].to(device)\n",
    "            confidence = batch['confidence'].to(device)\n",
    "            targets = batch['score'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(player_ids, opening_ids)\n",
    "            \n",
    "            # Compute MSE with confidence weighting\n",
    "            batch_mse = mse_loss(predictions, targets, confidence)\n",
    "            \n",
    "            # Track weighted sum of MSE\n",
    "            total_mse += batch_mse.item() * len(targets)\n",
    "            total_samples += len(targets)\n",
    "    \n",
    "    # Calculate average MSE and RMSE\n",
    "    avg_mse = total_mse / total_samples\n",
    "    avg_rmse = avg_mse ** 0.5\n",
    "    \n",
    "    return avg_mse, avg_rmse\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST FUNCTIONS (with dummy model)\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ Training and evaluation functions defined:\")\n",
    "print(f\"   ‚Ä¢ train_one_epoch(): Train model for one epoch\")\n",
    "print(f\"   ‚Ä¢ evaluate_model(): Evaluate model on a dataset\")\n",
    "\n",
    "print(f\"\\nüí° Function features:\")\n",
    "print(f\"   ‚Ä¢ Automatic device handling (CPU/CUDA)\")\n",
    "print(f\"   ‚Ä¢ Progress tracking with ETA\")\n",
    "print(f\"   ‚Ä¢ Confidence-weighted loss\")\n",
    "print(f\"   ‚Ä¢ Batch processing via DataLoader\")\n",
    "print(f\"   ‚Ä¢ Train/eval mode switching\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TRAINING AND EVALUATION FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã Training workflow:\")\n",
    "print(f\"   1. Initialize model and optimizer\")\n",
    "print(f\"   2. For each epoch:\")\n",
    "print(f\"      a. Call train_one_epoch()\")\n",
    "print(f\"      b. Call evaluate_model() on validation set\")\n",
    "print(f\"      c. Save checkpoint with save_checkpoint()\")\n",
    "print(f\"      d. Log metrics\")\n",
    "print(f\"   3. After training, evaluate on test set\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready to define the model architecture (next cell)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50c5eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6F: DEFINE MODEL ARCHITECTURE\n",
      "============================================================\n",
      "\n",
      "‚úÖ ChessOpeningRecommender model class defined\n",
      "\n",
      "üìä Model Architecture:\n",
      "   Player Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(13990, 50)\n",
      "      ‚Ä¢ Biases: Embedding(13990, 1)\n",
      "      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\n",
      "      ‚Ä¢ Combiner: Linear(51, 50)\n",
      "\n",
      "   Opening Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(2560, 50)\n",
      "      ‚Ä¢ Biases: Embedding(2560, 1)\n",
      "      ‚Ä¢ ECO letter embedding: Embedding(5, 4)\n",
      "      ‚Ä¢ ECO number embedding: Embedding(100, 4)\n",
      "      ‚Ä¢ Combiner: Linear(58, 50)\n",
      "\n",
      "   Prediction:\n",
      "      ‚Ä¢ Dot product of player and opening representations\n",
      "      ‚Ä¢ + player bias + opening bias + global bias\n",
      "      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\n",
      "\n",
      "üí° Model Statistics:\n",
      "   ‚Ä¢ Total trainable parameters: 850,021\n",
      "   ‚Ä¢ Player parameters: 716,090\n",
      "   ‚Ä¢ Opening parameters: 133,930\n",
      "   ‚Ä¢ Global parameters: 1\n",
      "\n",
      "============================================================\n",
      "‚úÖ MODEL ARCHITECTURE COMPLETE\n",
      "============================================================\n",
      "\n",
      "üöÄ Ready for Step 7: Training Loop!\n"
     ]
    }
   ],
   "source": [
    "# Step 6f: Define Model Architecture\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 6F: DEFINE MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Chess Opening Recommender Model\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningRecommender(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for chess opening recommendations.\n",
    "    \n",
    "    The model learns latent factors for players and openings, incorporating\n",
    "    side information:\n",
    "    - Player ratings (normalized)\n",
    "    - Opening ECO codes (letter and number as categorical features)\n",
    "    \n",
    "    Architecture:\n",
    "    - Player embedding: learnable latent factors\n",
    "    - Opening embedding: learnable latent factors\n",
    "    - Player rating: fixed side information (no embedding)\n",
    "    - ECO letter/number: categorical embeddings\n",
    "    \n",
    "    Prediction: dot product of player and opening representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_players, num_openings, num_factors,\n",
    "                 player_ratings, opening_eco_letters, opening_eco_numbers,\n",
    "                 num_eco_letters, num_eco_numbers, eco_embed_dim=4):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_players : int\n",
    "            Total number of unique players\n",
    "        num_openings : int\n",
    "            Total number of unique openings\n",
    "        num_factors : int\n",
    "            Dimensionality of latent factor embeddings\n",
    "        player_ratings : torch.Tensor\n",
    "            Z-score normalized ratings for all players (shape: [num_players])\n",
    "        opening_eco_letters : torch.Tensor\n",
    "            ECO letter categories for all openings (shape: [num_openings])\n",
    "        opening_eco_numbers : torch.Tensor\n",
    "            ECO number categories for all openings (shape: [num_openings])\n",
    "        num_eco_letters : int\n",
    "            Number of unique ECO letter categories\n",
    "        num_eco_numbers : int\n",
    "            Number of unique ECO number categories\n",
    "        eco_embed_dim : int\n",
    "            Dimensionality of ECO categorical embeddings (default: 4)\n",
    "        \"\"\"\n",
    "        super(ChessOpeningRecommender, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_players = num_players\n",
    "        self.num_openings = num_openings\n",
    "        self.num_factors = num_factors\n",
    "        self.eco_embed_dim = eco_embed_dim\n",
    "        \n",
    "        # ========================================\n",
    "        # Player Components\n",
    "        # ========================================\n",
    "        \n",
    "        # Player latent factors (learnable)\n",
    "        self.player_factors = torch.nn.Embedding(num_players, num_factors)\n",
    "        \n",
    "        # Player biases (learnable)\n",
    "        self.player_biases = torch.nn.Embedding(num_players, 1)\n",
    "        \n",
    "        # Player ratings (fixed side information - registered as buffer, not parameter)\n",
    "        self.register_buffer('player_ratings', player_ratings)\n",
    "        \n",
    "        # ========================================\n",
    "        # Opening Components\n",
    "        # ========================================\n",
    "        \n",
    "        # Opening latent factors (learnable)\n",
    "        self.opening_factors = torch.nn.Embedding(num_openings, num_factors)\n",
    "        \n",
    "        # Opening biases (learnable)\n",
    "        self.opening_biases = torch.nn.Embedding(num_openings, 1)\n",
    "        \n",
    "        # ECO letter and number (fixed side information - registered as buffers)\n",
    "        self.register_buffer('opening_eco_letters', opening_eco_letters)\n",
    "        self.register_buffer('opening_eco_numbers', opening_eco_numbers)\n",
    "        \n",
    "        # ECO embeddings (learnable)\n",
    "        self.eco_letter_embedding = torch.nn.Embedding(num_eco_letters, eco_embed_dim)\n",
    "        self.eco_number_embedding = torch.nn.Embedding(num_eco_numbers, eco_embed_dim)\n",
    "        \n",
    "        # ========================================\n",
    "        # Combination Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Combine player latent factors with rating\n",
    "        # Input: [num_factors + 1] ‚Üí Output: [num_factors]\n",
    "        self.player_combiner = torch.nn.Linear(num_factors + 1, num_factors)\n",
    "        \n",
    "        # Combine opening latent factors with ECO embeddings\n",
    "        # Input: [num_factors + 2*eco_embed_dim] ‚Üí Output: [num_factors]\n",
    "        self.opening_combiner = torch.nn.Linear(num_factors + 2 * eco_embed_dim, num_factors)\n",
    "        \n",
    "        # ========================================\n",
    "        # Global Bias\n",
    "        # ========================================\n",
    "        \n",
    "        # Global bias term (learnable scalar)\n",
    "        self.global_bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # ========================================\n",
    "        # Initialize Weights\n",
    "        # ========================================\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        torch.nn.init.normal_(self.player_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.opening_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_letter_embedding.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_number_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        torch.nn.init.zeros_(self.player_biases.weight)\n",
    "        torch.nn.init.zeros_(self.opening_biases.weight)\n",
    "        \n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        torch.nn.init.xavier_uniform_(self.player_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.player_combiner.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.opening_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.opening_combiner.bias)\n",
    "    \n",
    "    def forward(self, player_ids, opening_ids):\n",
    "        \"\"\"\n",
    "        Forward pass: predict player-opening scores.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (shape: [batch_size])\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (shape: [batch_size])\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Predicted scores (shape: [batch_size])\n",
    "        \"\"\"\n",
    "        # ========================================\n",
    "        # Get Player Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get player latent factors [batch_size, num_factors]\n",
    "        player_embed = self.player_factors(player_ids)\n",
    "        \n",
    "        # Get player ratings [batch_size, 1]\n",
    "        player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "        \n",
    "        # Concatenate player factors with rating [batch_size, num_factors + 1]\n",
    "        player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "        \n",
    "        # Combine into final player representation [batch_size, num_factors]\n",
    "        player_repr = self.player_combiner(player_concat)\n",
    "        \n",
    "        # Get player bias [batch_size]\n",
    "        player_bias = self.player_biases(player_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Get Opening Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get opening latent factors [batch_size, num_factors]\n",
    "        opening_embed = self.opening_factors(opening_ids)\n",
    "        \n",
    "        # Get ECO embeddings\n",
    "        eco_letters = self.opening_eco_letters[opening_ids]  # [batch_size]\n",
    "        eco_numbers = self.opening_eco_numbers[opening_ids]  # [batch_size]\n",
    "        \n",
    "        eco_letter_embed = self.eco_letter_embedding(eco_letters)  # [batch_size, eco_embed_dim]\n",
    "        eco_number_embed = self.eco_number_embedding(eco_numbers)  # [batch_size, eco_embed_dim]\n",
    "        \n",
    "        # Concatenate opening factors with ECO embeddings\n",
    "        # [batch_size, num_factors + 2*eco_embed_dim]\n",
    "        opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "        \n",
    "        # Combine into final opening representation [batch_size, num_factors]\n",
    "        opening_repr = self.opening_combiner(opening_concat)\n",
    "        \n",
    "        # Get opening bias [batch_size]\n",
    "        opening_bias = self.opening_biases(opening_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Compute Prediction\n",
    "        # ========================================\n",
    "        \n",
    "        # Dot product of player and opening representations [batch_size]\n",
    "        interaction = (player_repr * opening_repr).sum(dim=1)\n",
    "        \n",
    "        # Add biases and global bias\n",
    "        prediction = interaction + player_bias + opening_bias + self.global_bias\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] range (since scores are win rates)\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def get_player_embedding(self, player_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific player.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_id : int\n",
    "            Player ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Player representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            player_ids = torch.tensor([player_id], device=self.player_factors.weight.device)\n",
    "            player_embed = self.player_factors(player_ids)\n",
    "            player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "            player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "            player_repr = self.player_combiner(player_concat)\n",
    "            return player_repr.squeeze()\n",
    "    \n",
    "    def get_opening_embedding(self, opening_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific opening.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        opening_id : int\n",
    "            Opening ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Opening representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            opening_ids = torch.tensor([opening_id], device=self.opening_factors.weight.device)\n",
    "            opening_embed = self.opening_factors(opening_ids)\n",
    "            \n",
    "            eco_letters = self.opening_eco_letters[opening_ids]\n",
    "            eco_numbers = self.opening_eco_numbers[opening_ids]\n",
    "            \n",
    "            eco_letter_embed = self.eco_letter_embedding(eco_letters)\n",
    "            eco_number_embed = self.eco_number_embedding(eco_numbers)\n",
    "            \n",
    "            opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "            opening_repr = self.opening_combiner(opening_concat)\n",
    "            return opening_repr.squeeze()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model Summary\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ ChessOpeningRecommender model class defined\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Player Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_PLAYERS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_PLAYERS}, 1)\")\n",
    "print(f\"      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 1}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Opening Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_OPENINGS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_OPENINGS}, 1)\")\n",
    "print(f\"      ‚Ä¢ ECO letter embedding: Embedding({NUM_ECO_LETTERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ ECO number embedding: Embedding({NUM_ECO_NUMBERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 8}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Prediction:\")\n",
    "print(f\"      ‚Ä¢ Dot product of player and opening representations\")\n",
    "print(f\"      ‚Ä¢ + player bias + opening bias + global bias\")\n",
    "print(f\"      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\")\n",
    "\n",
    "# Calculate total parameters\n",
    "player_factor_params = NUM_PLAYERS * NUM_FACTORS\n",
    "player_bias_params = NUM_PLAYERS\n",
    "player_combiner_params = (NUM_FACTORS + 1) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "opening_factor_params = NUM_OPENINGS * NUM_FACTORS\n",
    "opening_bias_params = NUM_OPENINGS\n",
    "eco_letter_params = NUM_ECO_LETTERS * 4\n",
    "eco_number_params = NUM_ECO_NUMBERS * 4\n",
    "opening_combiner_params = (NUM_FACTORS + 8) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "global_bias_params = 1\n",
    "\n",
    "total_params = (player_factor_params + player_bias_params + player_combiner_params +\n",
    "                opening_factor_params + opening_bias_params + \n",
    "                eco_letter_params + eco_number_params + opening_combiner_params +\n",
    "                global_bias_params)\n",
    "\n",
    "print(f\"\\nüí° Model Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total trainable parameters: {total_params:,}\")\n",
    "print(f\"   ‚Ä¢ Player parameters: {player_factor_params + player_bias_params + player_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Opening parameters: {opening_factor_params + opening_bias_params + eco_letter_params + eco_number_params + opening_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Global parameters: {global_bias_params}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ MODEL ARCHITECTURE COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Step 7: Training Loop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc2c6d",
   "metadata": {},
   "source": [
    "## Step 7: Training Loop\n",
    "\n",
    "Now we'll implement the main training loop that:\n",
    "- Initializes the model with player and opening embeddings\n",
    "- Trains for multiple epochs using mini-batch SGD\n",
    "- Evaluates on validation set after each epoch\n",
    "- Saves checkpoints after each epoch\n",
    "- Logs MSE/RMSE metrics throughout training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da549756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 7: TRAINING LOOP\n",
      "============================================================\n",
      "\n",
      "üì¶ Initializing model...\n",
      "‚úÖ Model initialized on cpu\n",
      "   ‚Ä¢ Player embeddings: 13990 √ó 50\n",
      "   ‚Ä¢ Opening embeddings: 2560 √ó 50\n",
      "   ‚Ä¢ Player ratings: 13990 (z-score normalized)\n",
      "   ‚Ä¢ ECO letters: 5 categories\n",
      "   ‚Ä¢ ECO numbers: 100 categories\n",
      "\n",
      "üîß Initializing optimizer...\n",
      "‚úÖ SGD optimizer initialized:\n",
      "   ‚Ä¢ Learning rate: 0.01\n",
      "   ‚Ä¢ Momentum: 0.9\n",
      "   ‚Ä¢ Weight decay: 0.0\n",
      "\n",
      "============================================================\n",
      "üöÄ STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Training configuration:\n",
      "   ‚Ä¢ Epochs: 20\n",
      "   ‚Ä¢ Batch size: 1024\n",
      "   ‚Ä¢ Training samples: 666,255\n",
      "   ‚Ä¢ Validation samples: 133,251\n",
      "   ‚Ä¢ Batches per epoch: 651\n",
      "\n",
      "============================================================\n",
      "TRAINING PROGRESS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/20\n",
      "============================================================\n",
      "‚úÖ SGD optimizer initialized:\n",
      "   ‚Ä¢ Learning rate: 0.01\n",
      "   ‚Ä¢ Momentum: 0.9\n",
      "   ‚Ä¢ Weight decay: 0.0\n",
      "\n",
      "============================================================\n",
      "üöÄ STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Training configuration:\n",
      "   ‚Ä¢ Epochs: 20\n",
      "   ‚Ä¢ Batch size: 1024\n",
      "   ‚Ä¢ Training samples: 666,255\n",
      "   ‚Ä¢ Validation samples: 133,251\n",
      "   ‚Ä¢ Batches per epoch: 651\n",
      "\n",
      "============================================================\n",
      "TRAINING PROGRESS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/20\n",
      "============================================================\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001698 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001698 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 1 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001778\n",
      "   ‚Ä¢ Time: 14.14s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001746\n",
      "   ‚Ä¢ RMSE: 0.041785\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_001.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 2/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 1 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001778\n",
      "   ‚Ä¢ Time: 14.14s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001746\n",
      "   ‚Ä¢ RMSE: 0.041785\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_001.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 2/20\n",
      "============================================================\n",
      "   Epoch 2 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001679 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 2 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001679 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 2 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001749\n",
      "   ‚Ä¢ Time: 10.96s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001731\n",
      "   ‚Ä¢ RMSE: 0.041600\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_002.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 3/20\n",
      "============================================================\n",
      "   Epoch 3 [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   1.54% | Loss: 0.001823 | ETA: 11.2s\n",
      "============================================================\n",
      "EPOCH 2 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001749\n",
      "   ‚Ä¢ Time: 10.96s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001731\n",
      "   ‚Ä¢ RMSE: 0.041600\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_002.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 3/20\n",
      "============================================================\n",
      "   Epoch 3 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001685 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 3 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001685 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 3 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001733\n",
      "   ‚Ä¢ Time: 10.63s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001717\n",
      "   ‚Ä¢ RMSE: 0.041432\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_003.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 4/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 3 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001733\n",
      "   ‚Ä¢ Time: 10.63s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001717\n",
      "   ‚Ä¢ RMSE: 0.041432\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_003.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 4/20\n",
      "============================================================\n",
      "   Epoch 4 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001862 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 4 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001862 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 4 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001719\n",
      "   ‚Ä¢ Time: 10.77s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001704\n",
      "   ‚Ä¢ RMSE: 0.041276\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_004.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 5/20\n",
      "============================================================\n",
      "   Epoch 5 [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   1.54% | Loss: 0.001849 | ETA: 11.2s\n",
      "============================================================\n",
      "EPOCH 4 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001719\n",
      "   ‚Ä¢ Time: 10.77s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001704\n",
      "   ‚Ä¢ RMSE: 0.041276\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_004.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 5/20\n",
      "============================================================\n",
      "   Epoch 5 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001549 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 5 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001549 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 5 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001706\n",
      "   ‚Ä¢ Time: 10.82s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001692\n",
      "   ‚Ä¢ RMSE: 0.041130\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_005.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 6/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 5 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001706\n",
      "   ‚Ä¢ Time: 10.82s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001692\n",
      "   ‚Ä¢ RMSE: 0.041130\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_005.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 6/20\n",
      "============================================================\n",
      "   Epoch 6 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001551 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 6 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001551 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 6 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001694\n",
      "   ‚Ä¢ Time: 10.87s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001681\n",
      "   ‚Ä¢ RMSE: 0.040994\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_006.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 7/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 6 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001694\n",
      "   ‚Ä¢ Time: 10.87s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001681\n",
      "   ‚Ä¢ RMSE: 0.040994\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_006.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 7/20\n",
      "============================================================\n",
      "   Epoch 7 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001730 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 7 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001730 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 7 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001683\n",
      "   ‚Ä¢ Time: 15.21s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001670\n",
      "   ‚Ä¢ RMSE: 0.040868\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_007.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 8/20\n",
      "============================================================\n",
      "   Epoch 8 [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   1.54% | Loss: 0.001681 | ETA: 11.1s\n",
      "============================================================\n",
      "EPOCH 7 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001683\n",
      "   ‚Ä¢ Time: 15.21s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001670\n",
      "   ‚Ä¢ RMSE: 0.040868\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_007.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 8/20\n",
      "============================================================\n",
      "   Epoch 8 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001616 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 8 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001616 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 8 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001672\n",
      "   ‚Ä¢ Time: 13.56s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001660\n",
      "   ‚Ä¢ RMSE: 0.040748\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_008.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 9/20\n",
      "============================================================\n",
      "   Epoch 9 [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   1.54% | Loss: 0.001653 | ETA: 10.7s\n",
      "============================================================\n",
      "EPOCH 8 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001672\n",
      "   ‚Ä¢ Time: 13.56s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001660\n",
      "   ‚Ä¢ RMSE: 0.040748\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_008.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 9/20\n",
      "============================================================\n",
      "   Epoch 9 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001789 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 9 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001789 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 9 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001663\n",
      "   ‚Ä¢ Time: 12.78s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001651\n",
      "   ‚Ä¢ RMSE: 0.040634\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_009.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 10/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 9 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001663\n",
      "   ‚Ä¢ Time: 12.78s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001651\n",
      "   ‚Ä¢ RMSE: 0.040634\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_009.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 10/20\n",
      "============================================================\n",
      "   Epoch 10 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001648 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 10 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001648 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 10 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001653\n",
      "   ‚Ä¢ Time: 11.65s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001642\n",
      "   ‚Ä¢ RMSE: 0.040528\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_010.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 11/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 10 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001653\n",
      "   ‚Ä¢ Time: 11.65s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001642\n",
      "   ‚Ä¢ RMSE: 0.040528\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_010.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 11/20\n",
      "============================================================\n",
      "   Epoch 11 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001711 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 11 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001711 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 11 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001644\n",
      "   ‚Ä¢ Time: 13.25s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001634\n",
      "   ‚Ä¢ RMSE: 0.040426\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_011.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 12/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 11 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001644\n",
      "   ‚Ä¢ Time: 13.25s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001634\n",
      "   ‚Ä¢ RMSE: 0.040426\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_011.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 12/20\n",
      "============================================================\n",
      "   Epoch 12 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001570 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 12 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001570 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 12 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001636\n",
      "   ‚Ä¢ Time: 13.39s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001626\n",
      "   ‚Ä¢ RMSE: 0.040329\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_012.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 13/20\n",
      "============================================================\n",
      "   Epoch 13 [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   1.54% | Loss: 0.001747 | ETA: 10.6s\n",
      "============================================================\n",
      "EPOCH 12 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001636\n",
      "   ‚Ä¢ Time: 13.39s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001626\n",
      "   ‚Ä¢ RMSE: 0.040329\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_012.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 13/20\n",
      "============================================================\n",
      "   Epoch 13 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001688 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 13 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001688 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 13 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001628\n",
      "   ‚Ä¢ Time: 12.70s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001619\n",
      "   ‚Ä¢ RMSE: 0.040237\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_013.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 14/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 13 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001628\n",
      "   ‚Ä¢ Time: 12.70s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001619\n",
      "   ‚Ä¢ RMSE: 0.040237\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_013.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 14/20\n",
      "============================================================\n",
      "   Epoch 14 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001642 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 14 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001642 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 14 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001621\n",
      "   ‚Ä¢ Time: 12.64s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001612\n",
      "   ‚Ä¢ RMSE: 0.040149\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_014.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 15/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 14 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001621\n",
      "   ‚Ä¢ Time: 12.64s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001612\n",
      "   ‚Ä¢ RMSE: 0.040149\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_014.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 15/20\n",
      "============================================================\n",
      "   Epoch 15 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001863 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 15 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001863 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 15 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001613\n",
      "   ‚Ä¢ Time: 13.06s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001605\n",
      "   ‚Ä¢ RMSE: 0.040065\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_015.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 16/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 15 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001613\n",
      "   ‚Ä¢ Time: 13.06s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001605\n",
      "   ‚Ä¢ RMSE: 0.040065\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_015.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 16/20\n",
      "============================================================\n",
      "   Epoch 16 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001644 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 16 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001644 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 16 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001606\n",
      "   ‚Ä¢ Time: 12.53s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001599\n",
      "   ‚Ä¢ RMSE: 0.039985\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_016.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 17/20\n",
      "============================================================\n",
      "   Epoch 17 [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   1.54% | Loss: 0.001670 | ETA: 10.4s\n",
      "============================================================\n",
      "EPOCH 16 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001606\n",
      "   ‚Ä¢ Time: 12.53s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001599\n",
      "   ‚Ä¢ RMSE: 0.039985\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_016.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 17/20\n",
      "============================================================\n",
      "   Epoch 17 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001700 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 17 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001700 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 17 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001600\n",
      "   ‚Ä¢ Time: 13.16s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001593\n",
      "   ‚Ä¢ RMSE: 0.039908\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_017.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 18/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 17 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001600\n",
      "   ‚Ä¢ Time: 13.16s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001593\n",
      "   ‚Ä¢ RMSE: 0.039908\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_017.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 18/20\n",
      "============================================================\n",
      "   Epoch 18 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001446 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 18 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001446 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 18 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001593\n",
      "   ‚Ä¢ Time: 14.84s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001587\n",
      "   ‚Ä¢ RMSE: 0.039833\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_018.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 19/20\n",
      "============================================================\n",
      "   Epoch 19 [‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]   1.54% | Loss: 0.001618 | ETA: 10.7s\n",
      "============================================================\n",
      "EPOCH 18 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001593\n",
      "   ‚Ä¢ Time: 14.84s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001587\n",
      "   ‚Ä¢ RMSE: 0.039833\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_018.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 19/20\n",
      "============================================================\n",
      "   Epoch 19 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001439 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 19 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001439 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 19 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001587\n",
      "   ‚Ä¢ Time: 11.72s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001581\n",
      "   ‚Ä¢ RMSE: 0.039762\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_019.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 20/20\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 19 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001587\n",
      "   ‚Ä¢ Time: 11.72s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001581\n",
      "   ‚Ä¢ RMSE: 0.039762\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_019.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 20/20\n",
      "============================================================\n",
      "   Epoch 20 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001521 | ETA: 0.0ss\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "   Epoch 20 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.00% | Loss: 0.001521 | ETA: 0.0s\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "\n",
      "============================================================\n",
      "EPOCH 20 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001582\n",
      "   ‚Ä¢ Time: 11.52s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001576\n",
      "   ‚Ä¢ RMSE: 0.039693\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_020.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Training Summary:\n",
      "   ‚Ä¢ Total epochs: 20\n",
      "   ‚Ä¢ Best epoch: 20\n",
      "   ‚Ä¢ Best validation RMSE: 0.039693\n",
      "   ‚Ä¢ Final validation RMSE: 0.039693\n",
      "\n",
      "‚è±Ô∏è  Training Time:\n",
      "   ‚Ä¢ Total: 250.19s (4.17m)\n",
      "   ‚Ä¢ Average per epoch: 12.51s\n",
      "\n",
      "üìà Training History:\n",
      " epoch  train_loss  train_time  val_mse  val_rmse  learning_rate\n",
      "     1    0.001778   14.144902 0.001746  0.041785           0.01\n",
      "     2    0.001749   10.958447 0.001731  0.041600           0.01\n",
      "     3    0.001733   10.628502 0.001717  0.041432           0.01\n",
      "     4    0.001719   10.766961 0.001704  0.041276           0.01\n",
      "     5    0.001706   10.819318 0.001692  0.041130           0.01\n",
      "     6    0.001694   10.873668 0.001681  0.040994           0.01\n",
      "     7    0.001683   15.205449 0.001670  0.040868           0.01\n",
      "     8    0.001672   13.563247 0.001660  0.040748           0.01\n",
      "     9    0.001663   12.782222 0.001651  0.040634           0.01\n",
      "    10    0.001653   11.650695 0.001642  0.040528           0.01\n",
      "    11    0.001644   13.252013 0.001634  0.040426           0.01\n",
      "    12    0.001636   13.389938 0.001626  0.040329           0.01\n",
      "    13    0.001628   12.697149 0.001619  0.040237           0.01\n",
      "    14    0.001621   12.635688 0.001612  0.040149           0.01\n",
      "    15    0.001613   13.062347 0.001605  0.040065           0.01\n",
      "    16    0.001606   12.525286 0.001599  0.039985           0.01\n",
      "    17    0.001600   13.156441 0.001593  0.039908           0.01\n",
      "    18    0.001593   14.835915 0.001587  0.039833           0.01\n",
      "    19    0.001587   11.722978 0.001581  0.039762           0.01\n",
      "    20    0.001582   11.515457 0.001576  0.039693           0.01\n",
      "\n",
      "============================================================\n",
      "üéØ Next Step: Evaluate on test set (Step 8)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 20 SUMMARY\n",
      "============================================================\n",
      "Training:\n",
      "   ‚Ä¢ Average loss: 0.001582\n",
      "   ‚Ä¢ Time: 11.52s\n",
      "Validation:\n",
      "   ‚Ä¢ MSE: 0.001576\n",
      "   ‚Ä¢ RMSE: 0.039693\n",
      "   üåü New best validation RMSE!\n",
      "   üíæ Checkpoint saved to: /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/model_checkpoints/checkpoint_epoch_020.pt\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Training Summary:\n",
      "   ‚Ä¢ Total epochs: 20\n",
      "   ‚Ä¢ Best epoch: 20\n",
      "   ‚Ä¢ Best validation RMSE: 0.039693\n",
      "   ‚Ä¢ Final validation RMSE: 0.039693\n",
      "\n",
      "‚è±Ô∏è  Training Time:\n",
      "   ‚Ä¢ Total: 250.19s (4.17m)\n",
      "   ‚Ä¢ Average per epoch: 12.51s\n",
      "\n",
      "üìà Training History:\n",
      " epoch  train_loss  train_time  val_mse  val_rmse  learning_rate\n",
      "     1    0.001778   14.144902 0.001746  0.041785           0.01\n",
      "     2    0.001749   10.958447 0.001731  0.041600           0.01\n",
      "     3    0.001733   10.628502 0.001717  0.041432           0.01\n",
      "     4    0.001719   10.766961 0.001704  0.041276           0.01\n",
      "     5    0.001706   10.819318 0.001692  0.041130           0.01\n",
      "     6    0.001694   10.873668 0.001681  0.040994           0.01\n",
      "     7    0.001683   15.205449 0.001670  0.040868           0.01\n",
      "     8    0.001672   13.563247 0.001660  0.040748           0.01\n",
      "     9    0.001663   12.782222 0.001651  0.040634           0.01\n",
      "    10    0.001653   11.650695 0.001642  0.040528           0.01\n",
      "    11    0.001644   13.252013 0.001634  0.040426           0.01\n",
      "    12    0.001636   13.389938 0.001626  0.040329           0.01\n",
      "    13    0.001628   12.697149 0.001619  0.040237           0.01\n",
      "    14    0.001621   12.635688 0.001612  0.040149           0.01\n",
      "    15    0.001613   13.062347 0.001605  0.040065           0.01\n",
      "    16    0.001606   12.525286 0.001599  0.039985           0.01\n",
      "    17    0.001600   13.156441 0.001593  0.039908           0.01\n",
      "    18    0.001593   14.835915 0.001587  0.039833           0.01\n",
      "    19    0.001587   11.722978 0.001581  0.039762           0.01\n",
      "    20    0.001582   11.515457 0.001576  0.039693           0.01\n",
      "\n",
      "============================================================\n",
      "üéØ Next Step: Evaluate on test set (Step 8)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Training Loop\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 7: TRAINING LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Initialize Model\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüì¶ Initializing model...\")\n",
    "model = ChessOpeningRecommender(\n",
    "    num_players=NUM_PLAYERS,\n",
    "    num_openings=NUM_OPENINGS,\n",
    "    num_factors=NUM_FACTORS,\n",
    "    player_ratings=player_ratings_tensor,\n",
    "    opening_eco_letters=opening_eco_letter_tensor,\n",
    "    opening_eco_numbers=opening_eco_number_tensor,\n",
    "    num_eco_letters=NUM_ECO_LETTERS,\n",
    "    num_eco_numbers=NUM_ECO_NUMBERS\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"‚úÖ Model initialized on {DEVICE}\")\n",
    "print(f\"   ‚Ä¢ Player embeddings: {NUM_PLAYERS} √ó {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ Opening embeddings: {NUM_OPENINGS} √ó {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ Player ratings: {NUM_PLAYERS} (z-score normalized)\")\n",
    "print(f\"   ‚Ä¢ ECO letters: {NUM_ECO_LETTERS} categories\")\n",
    "print(f\"   ‚Ä¢ ECO numbers: {NUM_ECO_NUMBERS} categories\")\n",
    "\n",
    "# ========================================\n",
    "# Initialize Optimizer\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüîß Initializing optimizer...\")\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ SGD optimizer initialized:\")\n",
    "print(f\"   ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ Momentum: {MOMENTUM}\")\n",
    "print(f\"   ‚Ä¢ Weight decay: {WEIGHT_DECAY}\")\n",
    "\n",
    "# ========================================\n",
    "# Training Loop\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"   ‚Ä¢ Epochs: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   ‚Ä¢ Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   ‚Ä¢ Batches per epoch: {len(train_loader):,}\")\n",
    "\n",
    "# Track best validation RMSE for early stopping info\n",
    "best_val_rmse = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "# Training history\n",
    "training_history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_time': [],\n",
    "    'val_mse': [],\n",
    "    'val_rmse': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING PROGRESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch}/{N_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_time = train_one_epoch(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=DEVICE,\n",
    "        epoch_num=epoch\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(f\"\\nüìä Evaluating on validation set...\")\n",
    "    val_mse, val_rmse = evaluate_model(\n",
    "        model=model,\n",
    "        data_loader=val_loader,\n",
    "        device=DEVICE,\n",
    "        dataset_name=\"Validation\"\n",
    "    )\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch} SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training:\")\n",
    "    print(f\"   ‚Ä¢ Average loss: {train_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Time: {train_time:.2f}s\")\n",
    "    print(f\"Validation:\")\n",
    "    print(f\"   ‚Ä¢ MSE: {val_mse:.6f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE: {val_rmse:.6f}\")\n",
    "    \n",
    "    # Track best model\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_epoch = epoch\n",
    "        print(f\"   üåü New best validation RMSE!\")\n",
    "    \n",
    "    # Store history\n",
    "    training_history['epoch'].append(epoch)\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['train_time'].append(train_time)\n",
    "    training_history['val_mse'].append(val_mse)\n",
    "    training_history['val_rmse'].append(val_rmse)\n",
    "    training_history['learning_rate'].append(LEARNING_RATE)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_filename = f\"checkpoint_epoch_{epoch:03d}.pt\"\n",
    "    checkpoint_path = MODEL_SAVE_DIR / checkpoint_filename\n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        train_loss=train_loss,\n",
    "        val_loss=val_mse,\n",
    "        filepath=checkpoint_path\n",
    "    )\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ========================================\n",
    "# Training Complete\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"   ‚Ä¢ Total epochs: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ Best epoch: {best_epoch}\")\n",
    "print(f\"   ‚Ä¢ Best validation RMSE: {best_val_rmse:.6f}\")\n",
    "print(f\"   ‚Ä¢ Final validation RMSE: {training_history['val_rmse'][-1]:.6f}\")\n",
    "\n",
    "# Calculate total training time\n",
    "total_training_time = sum(training_history['train_time'])\n",
    "print(f\"\\n‚è±Ô∏è  Training Time:\")\n",
    "print(f\"   ‚Ä¢ Total: {total_training_time:.2f}s ({total_training_time/60:.2f}m)\")\n",
    "print(f\"   ‚Ä¢ Average per epoch: {total_training_time/N_EPOCHS:.2f}s\")\n",
    "\n",
    "# Convert history to DataFrame for easy viewing\n",
    "import pandas as pd\n",
    "history_df = pd.DataFrame(training_history)\n",
    "print(f\"\\nüìà Training History:\")\n",
    "print(history_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Next Step: Evaluate on test set (Step 8)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc479f",
   "metadata": {},
   "source": [
    "## Step 8: Evaluation on Test Set\n",
    "\n",
    "Now that training is complete, we'll evaluate the final model on the held-out test set to get an unbiased estimate of model performance. We'll also:\n",
    "- Calculate MSE and RMSE metrics\n",
    "- Analyze prediction accuracy across different rating ranges\n",
    "- Examine predicted vs actual scores\n",
    "- Identify best and worst predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d8f0315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 8: EVALUATION ON TEST SET\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating model on test set...\n",
      "\n",
      "============================================================\n",
      "TEST SET RESULTS\n",
      "============================================================\n",
      "   ‚Ä¢ MSE: 0.001576\n",
      "   ‚Ä¢ RMSE: 0.039702\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "Final Training Loss:   0.001582\n",
      "Final Validation RMSE: 0.039693\n",
      "Test RMSE:             0.039702\n",
      "\n",
      "‚úÖ Test and validation RMSE are similar (within 10%)\n",
      "   Model generalizes appropriately.\n",
      "\n",
      "============================================================\n",
      "GENERATING PREDICTIONS FOR ANALYSIS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TEST SET RESULTS\n",
      "============================================================\n",
      "   ‚Ä¢ MSE: 0.001576\n",
      "   ‚Ä¢ RMSE: 0.039702\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "Final Training Loss:   0.001582\n",
      "Final Validation RMSE: 0.039693\n",
      "Test RMSE:             0.039702\n",
      "\n",
      "‚úÖ Test and validation RMSE are similar (within 10%)\n",
      "   Model generalizes appropriately.\n",
      "\n",
      "============================================================\n",
      "GENERATING PREDICTIONS FOR ANALYSIS\n",
      "============================================================\n",
      "‚úÖ Generated 88,835 predictions\n",
      "\n",
      "============================================================\n",
      "PREDICTION STATISTICS\n",
      "============================================================\n",
      "\n",
      "Prediction Range:\n",
      "   ‚Ä¢ Min: 0.4689\n",
      "   ‚Ä¢ Max: 0.5612\n",
      "   ‚Ä¢ Mean: 0.5140\n",
      "   ‚Ä¢ Std: 0.0077\n",
      "\n",
      "Actual Score Range:\n",
      "   ‚Ä¢ Min: 0.2500\n",
      "   ‚Ä¢ Max: 0.7759\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0416\n",
      "\n",
      "Prediction Errors:\n",
      "   ‚Ä¢ Mean Error: 0.002029\n",
      "   ‚Ä¢ Mean Absolute Error: 0.030828\n",
      "   ‚Ä¢ Median Absolute Error: 0.025590\n",
      "   ‚Ä¢ Max Error: 0.277177\n",
      "\n",
      "============================================================\n",
      "ERROR DISTRIBUTION\n",
      "============================================================\n",
      "\n",
      "Predictions within error threshold:\n",
      "   ‚Ä¢ ¬±0.01: 18,562 (20.9%)\n",
      "   ‚Ä¢ ¬±0.02: 35,882 (40.4%)\n",
      "   ‚Ä¢ ¬±0.05: 71,845 (80.9%)\n",
      "   ‚Ä¢ ¬±0.10: 87,579 (98.6%)\n",
      "   ‚Ä¢ ¬±0.15: 88,703 (99.9%)\n",
      "\n",
      "============================================================\n",
      "BEST AND WORST PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "üéØ Top 10 Best Predictions (lowest error):\n",
      " Player ID   Opening ID   Actual  Predicted    Error\n",
      "------------------------------------------------------------\n",
      "      2398         1807   0.5166     0.5166  -0.0000\n",
      "      3812         1588   0.5148     0.5148  -0.0000\n",
      "        78         1002   0.5128     0.5129   0.0000\n",
      "     11662          907   0.5167     0.5166  -0.0000\n",
      "       240          788   0.5113     0.5113   0.0000\n",
      "     12454         1369   0.5148     0.5148   0.0000\n",
      "      2798          905   0.5165     0.5165  -0.0000\n",
      "     11139          322   0.5014     0.5014   0.0000\n",
      "      5390          629   0.5213     0.5213  -0.0000\n",
      "     13731         1510   0.5124     0.5124   0.0000\n",
      "\n",
      "‚ùå Top 10 Worst Predictions (highest error):\n",
      " Player ID   Opening ID   Actual  Predicted    Error\n",
      "------------------------------------------------------------\n",
      "      3733         1535   0.2500     0.5272   0.2772\n",
      "      4579         2406   0.7759     0.5176  -0.2582\n",
      "      9682          546   0.2526     0.5076   0.2550\n",
      "      4534            7   0.7533     0.5104  -0.2429\n",
      "      5584         2406   0.7477     0.5166  -0.2310\n",
      "      3448          664   0.7444     0.5159  -0.2285\n",
      "     12258         2151   0.2904     0.5154   0.2250\n",
      "     10079         1564   0.7615     0.5376  -0.2239\n",
      "      3440         2306   0.7494     0.5263  -0.2230\n",
      "      1719         2302   0.2850     0.5073   0.2223\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE BY SCORE RANGE\n",
      "============================================================\n",
      "\n",
      "               Range    Count   Mean Error      MAE     RMSE\n",
      "--------------------------------------------------------------------\n",
      "       Low (0.0-0.3)        4     0.244880 0.244880 0.245924\n",
      " Below Avg (0.3-0.4)      375     0.123001 0.123001 0.124592\n",
      "   Average (0.4-0.5)   33,661     0.038790 0.038866 0.044118\n",
      " Above Avg (0.5-0.6)   53,094    -0.018859 0.022827 0.029835\n",
      "      High (0.6-1.0)    1,701    -0.100675 0.100675 0.103856\n",
      "\n",
      "============================================================\n",
      "‚úÖ EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Final Test Set Metrics:\n",
      "   ‚Ä¢ Test MSE: 0.001576\n",
      "   ‚Ä¢ Test RMSE: 0.039702\n",
      "   ‚Ä¢ Mean Absolute Error: 0.030828\n",
      "   ‚Ä¢ Predictions within ¬±0.05: 80.9%\n",
      "\n",
      "üí° Interpretation:\n",
      "   ‚Ä¢ RMSE of 0.0397 means average prediction error is ~0.0397\n",
      "   ‚Ä¢ For a player with true win rate of 0.50, model typically predicts within ¬±0.0397\n",
      "   ‚Ä¢ This is equivalent to ¬±3.97 percentage points\n",
      "\n",
      "============================================================\n",
      "üéØ Next: Save model and create inference pipeline\n",
      "============================================================\n",
      "‚úÖ Generated 88,835 predictions\n",
      "\n",
      "============================================================\n",
      "PREDICTION STATISTICS\n",
      "============================================================\n",
      "\n",
      "Prediction Range:\n",
      "   ‚Ä¢ Min: 0.4689\n",
      "   ‚Ä¢ Max: 0.5612\n",
      "   ‚Ä¢ Mean: 0.5140\n",
      "   ‚Ä¢ Std: 0.0077\n",
      "\n",
      "Actual Score Range:\n",
      "   ‚Ä¢ Min: 0.2500\n",
      "   ‚Ä¢ Max: 0.7759\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0416\n",
      "\n",
      "Prediction Errors:\n",
      "   ‚Ä¢ Mean Error: 0.002029\n",
      "   ‚Ä¢ Mean Absolute Error: 0.030828\n",
      "   ‚Ä¢ Median Absolute Error: 0.025590\n",
      "   ‚Ä¢ Max Error: 0.277177\n",
      "\n",
      "============================================================\n",
      "ERROR DISTRIBUTION\n",
      "============================================================\n",
      "\n",
      "Predictions within error threshold:\n",
      "   ‚Ä¢ ¬±0.01: 18,562 (20.9%)\n",
      "   ‚Ä¢ ¬±0.02: 35,882 (40.4%)\n",
      "   ‚Ä¢ ¬±0.05: 71,845 (80.9%)\n",
      "   ‚Ä¢ ¬±0.10: 87,579 (98.6%)\n",
      "   ‚Ä¢ ¬±0.15: 88,703 (99.9%)\n",
      "\n",
      "============================================================\n",
      "BEST AND WORST PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "üéØ Top 10 Best Predictions (lowest error):\n",
      " Player ID   Opening ID   Actual  Predicted    Error\n",
      "------------------------------------------------------------\n",
      "      2398         1807   0.5166     0.5166  -0.0000\n",
      "      3812         1588   0.5148     0.5148  -0.0000\n",
      "        78         1002   0.5128     0.5129   0.0000\n",
      "     11662          907   0.5167     0.5166  -0.0000\n",
      "       240          788   0.5113     0.5113   0.0000\n",
      "     12454         1369   0.5148     0.5148   0.0000\n",
      "      2798          905   0.5165     0.5165  -0.0000\n",
      "     11139          322   0.5014     0.5014   0.0000\n",
      "      5390          629   0.5213     0.5213  -0.0000\n",
      "     13731         1510   0.5124     0.5124   0.0000\n",
      "\n",
      "‚ùå Top 10 Worst Predictions (highest error):\n",
      " Player ID   Opening ID   Actual  Predicted    Error\n",
      "------------------------------------------------------------\n",
      "      3733         1535   0.2500     0.5272   0.2772\n",
      "      4579         2406   0.7759     0.5176  -0.2582\n",
      "      9682          546   0.2526     0.5076   0.2550\n",
      "      4534            7   0.7533     0.5104  -0.2429\n",
      "      5584         2406   0.7477     0.5166  -0.2310\n",
      "      3448          664   0.7444     0.5159  -0.2285\n",
      "     12258         2151   0.2904     0.5154   0.2250\n",
      "     10079         1564   0.7615     0.5376  -0.2239\n",
      "      3440         2306   0.7494     0.5263  -0.2230\n",
      "      1719         2302   0.2850     0.5073   0.2223\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE BY SCORE RANGE\n",
      "============================================================\n",
      "\n",
      "               Range    Count   Mean Error      MAE     RMSE\n",
      "--------------------------------------------------------------------\n",
      "       Low (0.0-0.3)        4     0.244880 0.244880 0.245924\n",
      " Below Avg (0.3-0.4)      375     0.123001 0.123001 0.124592\n",
      "   Average (0.4-0.5)   33,661     0.038790 0.038866 0.044118\n",
      " Above Avg (0.5-0.6)   53,094    -0.018859 0.022827 0.029835\n",
      "      High (0.6-1.0)    1,701    -0.100675 0.100675 0.103856\n",
      "\n",
      "============================================================\n",
      "‚úÖ EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Final Test Set Metrics:\n",
      "   ‚Ä¢ Test MSE: 0.001576\n",
      "   ‚Ä¢ Test RMSE: 0.039702\n",
      "   ‚Ä¢ Mean Absolute Error: 0.030828\n",
      "   ‚Ä¢ Predictions within ¬±0.05: 80.9%\n",
      "\n",
      "üí° Interpretation:\n",
      "   ‚Ä¢ RMSE of 0.0397 means average prediction error is ~0.0397\n",
      "   ‚Ä¢ For a player with true win rate of 0.50, model typically predicts within ¬±0.0397\n",
      "   ‚Ä¢ This is equivalent to ¬±3.97 percentage points\n",
      "\n",
      "============================================================\n",
      "üéØ Next: Save model and create inference pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Evaluation on Test Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 8: EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Evaluate on Test Set\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüìä Evaluating model on test set...\")\n",
    "test_mse, test_rmse = evaluate_model(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    device=DEVICE,\n",
    "    dataset_name=\"Test\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   ‚Ä¢ MSE: {test_mse:.6f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {test_rmse:.6f}\")\n",
    "\n",
    "# ========================================\n",
    "# Compare with Training and Validation\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_train_loss = training_history['train_loss'][-1]\n",
    "final_val_rmse = training_history['val_rmse'][-1]\n",
    "\n",
    "print(f\"\\nFinal Training Loss:   {final_train_loss:.6f}\")\n",
    "print(f\"Final Validation RMSE: {final_val_rmse:.6f}\")\n",
    "print(f\"Test RMSE:             {test_rmse:.6f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "if test_rmse > final_val_rmse * 1.1:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Test RMSE is {(test_rmse/final_val_rmse - 1)*100:.1f}% higher than validation RMSE\")\n",
    "    print(f\"   This suggests possible overfitting.\")\n",
    "elif test_rmse < final_val_rmse * 0.9:\n",
    "    print(f\"\\n‚úÖ Test RMSE is {(1 - test_rmse/final_val_rmse)*100:.1f}% lower than validation RMSE\")\n",
    "    print(f\"   Model generalizes well!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Test and validation RMSE are similar (within 10%)\")\n",
    "    print(f\"   Model generalizes appropriately.\")\n",
    "\n",
    "# ========================================\n",
    "# Get Predictions for Analysis\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING PREDICTIONS FOR ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "all_player_ids = []\n",
    "all_opening_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        player_ids = batch['player_id'].to(DEVICE)\n",
    "        opening_ids = batch['opening_id'].to(DEVICE)\n",
    "        targets = batch['score'].to(DEVICE)\n",
    "        \n",
    "        predictions = model(player_ids, opening_ids)\n",
    "        \n",
    "        all_predictions.extend(predictions.detach().cpu().tolist())\n",
    "        all_actuals.extend(targets.detach().cpu().tolist())\n",
    "        all_player_ids.extend(player_ids.detach().cpu().tolist())\n",
    "        all_opening_ids.extend(opening_ids.detach().cpu().tolist())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_actuals = np.array(all_actuals)\n",
    "all_player_ids = np.array(all_player_ids)\n",
    "all_opening_ids = np.array(all_opening_ids)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(all_predictions):,} predictions\")\n",
    "\n",
    "# ========================================\n",
    "# Prediction Statistics\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "errors = all_predictions - all_actuals\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "print(f\"\\nPrediction Range:\")\n",
    "print(f\"   ‚Ä¢ Min: {all_predictions.min():.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {all_predictions.max():.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {all_predictions.mean():.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {all_predictions.std():.4f}\")\n",
    "\n",
    "print(f\"\\nActual Score Range:\")\n",
    "print(f\"   ‚Ä¢ Min: {all_actuals.min():.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {all_actuals.max():.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {all_actuals.mean():.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {all_actuals.std():.4f}\")\n",
    "\n",
    "print(f\"\\nPrediction Errors:\")\n",
    "print(f\"   ‚Ä¢ Mean Error: {errors.mean():.6f}\")\n",
    "print(f\"   ‚Ä¢ Mean Absolute Error: {abs_errors.mean():.6f}\")\n",
    "print(f\"   ‚Ä¢ Median Absolute Error: {np.median(abs_errors):.6f}\")\n",
    "print(f\"   ‚Ä¢ Max Error: {abs_errors.max():.6f}\")\n",
    "\n",
    "# ========================================\n",
    "# Error Distribution\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ERROR DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count predictions within various error thresholds\n",
    "thresholds = [0.01, 0.02, 0.05, 0.10, 0.15]\n",
    "print(f\"\\nPredictions within error threshold:\")\n",
    "for threshold in thresholds:\n",
    "    count = np.sum(abs_errors <= threshold)\n",
    "    pct = 100.0 * count / len(abs_errors)\n",
    "    print(f\"   ‚Ä¢ ¬±{threshold:.2f}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# ========================================\n",
    "# Best and Worst Predictions\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST AND WORST PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sort by absolute error\n",
    "sorted_indices = np.argsort(abs_errors)\n",
    "\n",
    "print(f\"\\nüéØ Top 10 Best Predictions (lowest error):\")\n",
    "print(f\"{'Player ID':>10} {'Opening ID':>12} {'Actual':>8} {'Predicted':>10} {'Error':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(min(10, len(sorted_indices))):\n",
    "    idx = sorted_indices[i]\n",
    "    print(f\"{all_player_ids[idx]:>10.0f} {all_opening_ids[idx]:>12.0f} \"\n",
    "          f\"{all_actuals[idx]:>8.4f} {all_predictions[idx]:>10.4f} \"\n",
    "          f\"{errors[idx]:>8.4f}\")\n",
    "\n",
    "print(f\"\\n‚ùå Top 10 Worst Predictions (highest error):\")\n",
    "print(f\"{'Player ID':>10} {'Opening ID':>12} {'Actual':>8} {'Predicted':>10} {'Error':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(min(10, len(sorted_indices))):\n",
    "    idx = sorted_indices[-(i+1)]\n",
    "    print(f\"{all_player_ids[idx]:>10.0f} {all_opening_ids[idx]:>12.0f} \"\n",
    "          f\"{all_actuals[idx]:>8.4f} {all_predictions[idx]:>10.4f} \"\n",
    "          f\"{errors[idx]:>8.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# Analyze by Score Range\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE BY SCORE RANGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "score_ranges = [\n",
    "    (0.0, 0.3, \"Low (0.0-0.3)\"),\n",
    "    (0.3, 0.4, \"Below Avg (0.3-0.4)\"),\n",
    "    (0.4, 0.5, \"Average (0.4-0.5)\"),\n",
    "    (0.5, 0.6, \"Above Avg (0.5-0.6)\"),\n",
    "    (0.6, 1.0, \"High (0.6-1.0)\")\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Range':>20} {'Count':>8} {'Mean Error':>12} {'MAE':>8} {'RMSE':>8}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "for low, high, label in score_ranges:\n",
    "    mask = (all_actuals >= low) & (all_actuals < high)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    range_errors = errors[mask]\n",
    "    range_abs_errors = abs_errors[mask]\n",
    "    range_rmse = np.sqrt(np.mean(range_errors ** 2))\n",
    "    \n",
    "    print(f\"{label:>20} {mask.sum():>8,} {range_errors.mean():>12.6f} \"\n",
    "          f\"{range_abs_errors.mean():>8.6f} {range_rmse:>8.6f}\")\n",
    "\n",
    "# ========================================\n",
    "# Summary\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Final Test Set Metrics:\")\n",
    "print(f\"   ‚Ä¢ Test MSE: {test_mse:.6f}\")\n",
    "print(f\"   ‚Ä¢ Test RMSE: {test_rmse:.6f}\")\n",
    "print(f\"   ‚Ä¢ Mean Absolute Error: {abs_errors.mean():.6f}\")\n",
    "print(f\"   ‚Ä¢ Predictions within ¬±0.05: {100.0 * np.sum(abs_errors <= 0.05) / len(abs_errors):.1f}%\")\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   ‚Ä¢ RMSE of {test_rmse:.4f} means average prediction error is ~{test_rmse:.4f}\")\n",
    "print(f\"   ‚Ä¢ For a player with true win rate of 0.50, model typically predicts within ¬±{test_rmse:.4f}\")\n",
    "print(f\"   ‚Ä¢ This is equivalent to ¬±{test_rmse*100:.2f} percentage points\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Next: Save model and create inference pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
