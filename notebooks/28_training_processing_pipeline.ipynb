{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e163a7",
   "metadata": {},
   "source": [
    "# Notebook 28 ‚Äî Opening Recommender Model: Training Pipeline\n",
    "\n",
    "### 0. Overview and Goals\n",
    "\n",
    "This notebook defines the full pipeline for training the chess opening recommender model.\n",
    "The objective is to predict **player‚Äìopening performance scores** ((wins + (0.5 * draws) / num games)) for openings a player hasn‚Äôt yet played, based on their results in the openings they *have* played.\n",
    "\n",
    "The model will use **matrix factorization** with **stochastic gradient descent (SGD)** to learn latent factors representing player and opening characteristics.\n",
    "All computations will be implemented in **PyTorch**, with data loaded from my local **DuckDB** database.\n",
    "\n",
    "**High-level specs:**\n",
    "- Use only *White* openings initially (we‚Äôll extend to Black later).\n",
    "- Data source: processed player‚Äìopening stats from local DuckDB.\n",
    "- Predict: normalized ‚Äúscore‚Äù = win rate ((wins + 0.5 x draws) / total games).\n",
    "- Filter: only include entries with ‚â• `MIN_GAMES_THRESHOLD` (default = 10).\n",
    "- Ignore: rating differences, time controls, and other metadata for the base model.\n",
    "- Model parameters (to be defined in appropriate places for easy editing):\n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_PLAYERS_TO_PROCESS`\n",
    "- Logging and checkpoints throughout for reproducibility.\n",
    "- All random operations seeded for deterministic runs.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Extraction\n",
    "- Connect to local DuckDB and pull all processed player‚Äìopening statistics.\n",
    "- Verify schema consistency and include row-count sanity checks.\n",
    "- Filter for players with ratings above a minimum threshold (e.g., 1200).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Sanitization & Normalization\n",
    "- Apply confidence weighting using hierarchical Bayesian shrinkage to adjust scores for low-game-count entries.\n",
    "- Normalize player ratings (z-score) for use as side information.\n",
    "- Resequence player and opening IDs to be contiguous integers for embedding layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Splits\n",
    "- Split data into train, validation, and test sets (e.g., 75/15/10).\n",
    "- Ensure splits are handled correctly to avoid data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Enumerate Categorical Variables\n",
    "- Process ECO codes into categorical features (e.g., `eco_letter`, `eco_number`).\n",
    "- Store these as opening-level side information.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Data Structure\n",
    "- Convert the final, processed DataFrames into PyTorch Tensors.\n",
    "- Create custom `Dataset` and `DataLoader` classes for efficient batching.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Cross-Validation & Hyperparameter Tuning\n",
    "- Define ranges for hyperparameters (`NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`).\n",
    "- Perform k-fold cross-validation on a subset of the training data to find the best hyperparameter combination before the final training run.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Final Model Training Setup\n",
    "- Define constants and hyperparameters for the final model (using results from CV).\n",
    "- Instantiate the PyTorch model, optimizer (SGD), and learning rate scheduler.\n",
    "- Implement helper functions for training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Final Model Training Loop\n",
    "- Initialize player and opening embeddings.\n",
    "- Iterate through epochs with mini-batch SGD.\n",
    "- Compute and log training and validation metrics (e.g., RMSE) per epoch.\n",
    "- Save model checkpoints locally.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Evaluation\n",
    "- Evaluate the final trained model on the held-out test set.\n",
    "- Report final metrics (MSE, RMSE) and create visualizations (e.g., predicted vs. actual scores).\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Next Steps\n",
    "- Extend model to include Black openings.\n",
    "- Experiment with more complex architectures or hybrid inputs.\n",
    "- Integrate the trained model into an API for serving recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "- Every random seed and parameter definition will be explicit.\n",
    "- Every major step includes row-count, schema, and type validation.\n",
    "- Model artifacts and logs will be saved locally for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d823",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "Connect to DuckDB and extract all player-opening statistics.\n",
    "Verify schema and perform sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcea569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA EXTRACTION\n",
      "üìÅ Database exists: True\n",
      "üé® Color filter: White\n",
      "üîí Minimum holdout players: 1,000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd() / 'utils'))\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = Path.cwd().parent / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "COLOR_FILTER = 'w'  # 'w' for white, 'b' for black\n",
    "MIN_HOLDOUT_PLAYERS = 1000  # Minimum number of players to reserve for fold-in verification. These will not be used at all in this notebook for training, test/val or anything else.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: DATA EXTRACTION\")\n",
    "print(f\"üìÅ Database exists: {DB_PATH.exists()}\")\n",
    "print(f\"üé® Color filter: {'White' if COLOR_FILTER == 'w' else 'Black'}\")\n",
    "print(f\"üîí Minimum holdout players: {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9435033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£  Extracting player-opening stats (color: 'w')...\n",
      "   ‚Ä¢ Minimum rating filter: 1200\n",
      "\n",
      "2Ô∏è‚É£  Selecting holdout players for fold-in verification...\n",
      "   ‚Ä¢ Holdout size: 1,000 players minimum\n",
      "   ‚Ä¢ Total eligible players: 49,551\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 48,551\n",
      "   ‚Ä¢ Holdout percentage: 2.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20ed209da2e4013af4192952774280f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Extracted 11,560,276 rows\n",
      "   ‚úì All required columns present: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "   ‚Ä¢ player_id: int32\n",
      "   ‚Ä¢ opening_id: int32\n",
      "   ‚Ä¢ num_games: int32\n",
      "   ‚Ä¢ score: float64\n",
      "   ‚Ä¢ eco: object\n",
      "\n",
      "6Ô∏è‚É£  Data statistics...\n",
      "   ‚Ä¢ Total rows: 11,560,276\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "   ‚Ä¢ Total games (sum): 228,935,516\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 50000\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 13462\n",
      "   ‚Ä¢ Mean: 19.8\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5007\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0          1          39          6  0.666667  A00\n",
      "1          1          49          2  0.500000  A00\n",
      "2          1          53          1  0.000000  A00\n",
      "3          1         182          1  0.000000  A04\n",
      "4          1         187          1  1.000000  A04\n",
      "5          1         671          3  1.000000  B00\n",
      "6          1         675          3  0.000000  B00\n",
      "7          1         677          6  0.333333  B00\n",
      "8          1         688         25  0.620000  B00\n",
      "9          1         717          1  1.000000  B00\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (11560276, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚úì Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# Get our training database\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "MAX_PLAYERS = 50_000\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player-opening stats (color: '{COLOR_FILTER}')...\")\n",
    "\n",
    "    # Extract stats with calculated score and num_games\n",
    "    # Filter by color, minimum rating, and calculate score in the database\n",
    "    MIN_RATING = 1200\n",
    "    print(f\"   ‚Ä¢ Minimum rating filter: {MIN_RATING}\")\n",
    "\n",
    "    print(f\"\\n2Ô∏è‚É£  Selecting holdout players for fold-in verification...\")\n",
    "    print(f\"   ‚Ä¢ Holdout size: {MIN_HOLDOUT_PLAYERS:,} players minimum\")\n",
    "\n",
    "    player_query = f\"\"\"\n",
    "        SELECT DISTINCT p.id as player_id\n",
    "        FROM player p\n",
    "        JOIN player_opening_stats pos ON p.id = pos.player_id\n",
    "        WHERE p.rating >= {MIN_RATING}\n",
    "        AND pos.color = '{COLOR_FILTER}'\n",
    "        LIMIT {MAX_PLAYERS}\n",
    "    \"\"\"\n",
    "\n",
    "    all_eligible_players = pd.DataFrame(con.execute(player_query).df())\n",
    "    total_eligible = len(all_eligible_players)\n",
    "    print(f\"   ‚Ä¢ Total eligible players: {total_eligible:,}\")\n",
    "\n",
    "    if total_eligible < MIN_HOLDOUT_PLAYERS:\n",
    "        raise ValueError(f\"Not enough eligible players ({total_eligible:,}) to create holdout set of {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "    # Randomly sample holdout players (deterministic with seed)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    holdout_player_ids = np.random.choice(\n",
    "        all_eligible_players['player_id'].values,\n",
    "        size=MIN_HOLDOUT_PLAYERS,\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    training_player_ids = set(all_eligible_players['player_id'].values) - set(holdout_player_ids)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Holdout players selected: {len(holdout_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Training players available: {len(training_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Holdout percentage: {100 * len(holdout_player_ids) / total_eligible:.1f}%\")\n",
    "\n",
    "    # SQL-friendly string\n",
    "    training_player_ids_str = ','.join(map(str, training_player_ids))\n",
    "\n",
    "    # Extract data ONLY for training players\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            pos.player_id,\n",
    "            pos.opening_id,\n",
    "            pos.num_wins + pos.num_draws + pos.num_losses as num_games,\n",
    "            (pos.num_wins + (pos.num_draws * 0.5)) / \n",
    "                NULLIF(pos.num_wins + pos.num_draws + pos.num_losses, 0) as score,\n",
    "            o.eco\n",
    "        FROM player_opening_stats pos\n",
    "        JOIN opening o ON pos.opening_id = o.id\n",
    "        JOIN player p ON pos.player_id = p.id\n",
    "        WHERE pos.color = '{COLOR_FILTER}'\n",
    "        AND p.rating >= {MIN_RATING}\n",
    "        AND pos.player_id IN ({training_player_ids_str})\n",
    "        ORDER BY pos.player_id, pos.opening_id\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data = pd.DataFrame(con.execute(query).df())\n",
    "\n",
    "    print(f\"   ‚úì Extracted {len(raw_data):,} rows\")\n",
    "\n",
    "    # Also save holdout player IDs for later use\n",
    "    holdout_players_df = pd.DataFrame({'player_id': holdout_player_ids})\n",
    "\n",
    "    # Schema verification\n",
    "    required_columns = ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
    "\n",
    "    for col in required_columns:\n",
    "        if col not in raw_data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    print(f\"   ‚úì All required columns present: {required_columns}\")\n",
    "\n",
    "    # Data types verification\n",
    "    print(f\"   ‚Ä¢ player_id: {raw_data['player_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ opening_id: {raw_data['opening_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ num_games: {raw_data['num_games'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ score: {raw_data['score'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ eco: {raw_data['eco'].dtype}\")\n",
    "\n",
    "    print(\"\\n6Ô∏è‚É£  Data statistics...\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {len(raw_data):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Total games (sum): {raw_data['num_games'].sum():,}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Player ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['player_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['player_id'].max()}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Opening ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['opening_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['opening_id'].max()}\")\n",
    "\n",
    "    print(f\"\\n   Games per entry:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['num_games'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['num_games'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['num_games'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['num_games'].median():.0f}\")\n",
    "\n",
    "    print(f\"\\n   Score distribution:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['score'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['score'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['score'].mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['score'].median():.4f}\")\n",
    "\n",
    "    print(\"\\n7Ô∏è‚É£  Checking for null values...\")\n",
    "    null_counts = raw_data.isnull().sum()\n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"   ‚úì No null values found\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "\n",
    "    print(\"\\n8Ô∏è‚É£  Sample of extracted data (first 10 rows):\")\n",
    "    print(raw_data.head(10).to_string())\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nData shape: {raw_data.shape}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "\n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"\\n‚úì Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad110b85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "519d8f2f",
   "metadata": {},
   "source": [
    "## Step 2: Data Sanitization & Normalization\n",
    "\n",
    "Filter low-quality data, handle duplicates, and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74f0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: DATA SANITIZATION & NORMALIZATION\n",
      "\n",
      "üìä Starting data shape: (11560276, 5)\n",
      "   ‚Ä¢ Rows: 11,560,276\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Before: 11,560,276 rows\n",
      "   ‚Ä¢ After: 2,897,177 rows\n",
      "   ‚Ä¢ Filtered out: 8,663,099 rows (74.9%)\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Data statistics:\n",
      "   ‚Ä¢ Total rows: 2,897,177\n",
      "   ‚Ä¢ Unique players: 48,469\n",
      "   ‚Ä¢ Unique openings: 2,716\n",
      "   ‚Ä¢ Total games: 206,352,788\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1066.7\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5110\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "946223       15939         737         38  0.736842  B00\n",
      "1472537      24921         520         11  0.545455  A50\n",
      "773434       12998         897         30  0.416667  B08\n",
      "1093775      18482        1733        151  0.536424  C34\n",
      "427701        7213        3214         48  0.552083  A40\n",
      "2649621      44856        1959         23  0.652174  C44\n",
      "351570        5952        2061         32  0.593750  C50\n",
      "2009274      33938        2231         15  0.266667  C64\n",
      "2419844      40939         794         10  0.200000  B02\n",
      "2400523      40605         132         94  0.547872  A01\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2897177, 5)\n",
      "Data reduction: 74.9%\n"
     ]
    }
   ],
   "source": [
    "# 2a. Filter low-quality data, handle duplicates, and prepare for training.\n",
    "\n",
    "# Configuration\n",
    "MIN_GAMES_THRESHOLD = 10\n",
    "\n",
    "print(\"STEP 2: DATA SANITIZATION & NORMALIZATION\")\n",
    "\n",
    "# Start with raw_data from Step 1\n",
    "print(f\"\\nüìä Starting data shape: {raw_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Rows: {len(raw_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "\n",
    "# 1. Filter by minimum games threshold\n",
    "print(f\"\\n1Ô∏è‚É£  Filtering entries with < {MIN_GAMES_THRESHOLD} games...\")\n",
    "before_filter = len(raw_data)\n",
    "clean_data = raw_data.query(f'num_games >= {MIN_GAMES_THRESHOLD}').copy()\n",
    "num_rows_after_filter = len(clean_data)\n",
    "num_rows_filtered_out = before_filter - num_rows_after_filter\n",
    "\n",
    "print(f\"   ‚Ä¢ Before: {before_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ After: {num_rows_after_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ Filtered out: {num_rows_filtered_out:,} rows ({100*num_rows_filtered_out/before_filter:.1f}%)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "num_duplicates = clean_data.duplicated(subset=['player_id', 'opening_id']).sum()\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {num_duplicates} duplicate player-opening entries\")\n",
    "    dup_mask = clean_data.duplicated(subset=['player_id', 'opening_id'], keep=False)\n",
    "    print(\"\\n   Sample of duplicates:\")\n",
    "    print(clean_data[dup_mask].head(10).to_string())\n",
    "    \n",
    "    # Keep only first occurrence of any duplicate player-opening pair\n",
    "    print(\"\\n   Removing duplicates (keeping first occurrence)...\")\n",
    "    clean_data = pd.DataFrame.drop_duplicates(clean_data, subset=['player_id', 'opening_id'], keep='first')\n",
    "    print(f\"   ‚úì After deduplication: {len(clean_data):,} rows\")\n",
    "\n",
    "# 3. Remove players with no qualifying openings\n",
    "\n",
    "# Note that a few players only play stuff like the Van't Kruijs which we've excluded, so a small numer of players will be excluded here\n",
    "players_before = clean_data['player_id'].nunique()\n",
    "\n",
    "# Count openings per player\n",
    "num_openings_per_player = pd.DataFrame(clean_data.groupby('player_id').size(), columns=['count'])\n",
    "players_with_data = num_openings_per_player[num_openings_per_player['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter\n",
    "clean_data = clean_data[clean_data['player_id'].isin(players_with_data)]\n",
    "players_after = clean_data['player_id'].nunique()\n",
    "\n",
    "# 4. Remove openings with no qualifying players\n",
    "num_openings_before = clean_data['opening_id'].nunique()\n",
    "\n",
    "num_players_per_opening = pd.DataFrame(clean_data.groupby('opening_id').size(), columns=['count'])\n",
    "openings_with_data = num_players_per_opening[num_players_per_opening['count'] > 0].index.tolist()\n",
    "\n",
    "clean_data = clean_data[clean_data['opening_id'].isin(openings_with_data)]\n",
    "openings_after = clean_data['opening_id'].nunique()\n",
    "\n",
    "# 5. Verify no null values using pd.isna()\n",
    "null_counts = pd.DataFrame.isna(clean_data).sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"   ‚úì No null values found\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "    for col, count in null_counts[null_counts > 0].items():\n",
    "        print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    # Drop rows with nulls using pd.DataFrame.dropna()\n",
    "    clean_data = pd.DataFrame.dropna(clean_data)\n",
    "    print(f\"   ‚úì Dropped null rows. New shape: {clean_data.shape}\")\n",
    "\n",
    "# Reset index using pd.DataFrame.reset_index()\n",
    "clean_data = pd.DataFrame.reset_index(clean_data, drop=True)\n",
    "\n",
    "# Final statistics using pd functions\n",
    "print(f\"\\n6Ô∏è‚É£  Data statistics:\")\n",
    "print(f\"   ‚Ä¢ Total rows: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {pd.Series.nunique(clean_data['player_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {pd.Series.nunique(clean_data['opening_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Total games: {pd.Series.sum(clean_data['num_games']):,}\")\n",
    "print(f\"   ‚Ä¢ Avg games per entry: {pd.Series.mean(clean_data['num_games']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg openings per player: {len(clean_data) / pd.Series.nunique(clean_data['player_id']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg players per opening: {len(clean_data) / pd.Series.nunique(clean_data['opening_id']):.1f}\")\n",
    "\n",
    "# Score distribution using pd functions\n",
    "print(f\"\\n   Score statistics:\")\n",
    "print(f\"   ‚Ä¢ Min: {pd.Series.min(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {pd.Series.quantile(clean_data['score'], 0.25):.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {pd.Series.median(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {pd.Series.quantile(clean_data['score'], 0.75):.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {pd.Series.max(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {pd.Series.mean(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {pd.Series.std(clean_data['score']):.4f}\")\n",
    "\n",
    "# Sample of cleaned data using pd.DataFrame.sample()\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of cleaned data (10 random rows):\")\n",
    "print(pd.DataFrame.sample(clean_data, min(10, len(clean_data)), random_state=42).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SANITIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCleaned data shape: {clean_data.shape}\")\n",
    "print(f\"Data reduction: {100 * (1 - len(clean_data)/len(raw_data)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e671b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\n",
      "============================================================\n",
      "\n",
      "üìä Global statistics:\n",
      "   ‚Ä¢ Global mean score: 0.5110\n",
      "   ‚Ä¢ Total entries: 2,897,177\n",
      "   ‚Ä¢ Unique openings: 2,716\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1667\n",
      "   ‚Ä¢ 25th percentile: 0.4963\n",
      "   ‚Ä¢ Median: 0.5162\n",
      "   ‚Ä¢ 75th percentile: 0.5365\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0505\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4794\n",
      "   ‚Ä¢ Players per opening (median): 156\n",
      "   ‚Ä¢ Total games range: [10, 5502381]\n",
      "   ‚Ä¢ Players range: [1, 42896]\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,897,177 entries\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Range: [0.1667, 0.9963]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000959\n",
      "   ‚Ä¢ Std adjustment: 0.076765\n",
      "   ‚Ä¢ Max adjustment: 0.457585\n",
      "   ‚Ä¢ Min adjustment: -0.457446\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003637\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001787\n",
      "   ‚Ä¢ 75th percentile (n=66 games): avg adjustment = -0.000402\n",
      "   ‚Ä¢ >75th percentile (n>66 games): avg adjustment = -0.001360\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ Median: 0.5117\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   Player  9134 | Opening 2053 | Games:  18 | Opening mean: 0.5275 | Original: 0.6389 ‚Üí Adjusted: 0.5570 | Diff: -0.0819 | Confidence: 0.265\n",
      "   Player 14508 | Opening  886 | Games:  11 | Opening mean: 0.4943 | Original: 0.2727 ‚Üí Adjusted: 0.4543 | Diff: +0.1816 | Confidence: 0.180\n",
      "   Player 46495 | Opening  138 | Games:  12 | Opening mean: 0.5059 | Original: 0.4167 ‚Üí Adjusted: 0.4886 | Diff: +0.0720 | Confidence: 0.194\n",
      "   Player 47375 | Opening 2561 | Games:  13 | Opening mean: 0.5459 | Original: 0.4615 ‚Üí Adjusted: 0.5285 | Diff: +0.0670 | Confidence: 0.206\n",
      "   Player 22951 | Opening 1314 | Games:  11 | Opening mean: 0.4866 | Original: 0.7273 ‚Üí Adjusted: 0.5300 | Diff: -0.1973 | Confidence: 0.180\n",
      "   Player 21134 | Opening  218 | Games:  12 | Opening mean: 0.4699 | Original: 0.4167 ‚Üí Adjusted: 0.4596 | Diff: +0.0429 | Confidence: 0.194\n",
      "   Player 25675 | Opening  591 | Games:  14 | Opening mean: 0.5378 | Original: 0.4286 ‚Üí Adjusted: 0.5139 | Diff: +0.0853 | Confidence: 0.219\n",
      "   Player 19414 | Opening 2478 | Games:  18 | Opening mean: 0.5161 | Original: 0.3056 ‚Üí Adjusted: 0.4603 | Diff: +0.1548 | Confidence: 0.265\n",
      "   Player 31163 | Opening 1394 | Games:  16 | Opening mean: 0.4638 | Original: 0.4375 ‚Üí Adjusted: 0.4574 | Diff: +0.0199 | Confidence: 0.242\n",
      "   Player 42904 | Opening 1046 | Games:  16 | Opening mean: 0.4823 | Original: 0.4375 ‚Üí Adjusted: 0.4714 | Diff: +0.0339 | Confidence: 0.242\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 47620 | Opening 3204 | Games:  94 | Opening mean: 0.5172 | Original: 0.4202 ‚Üí Adjusted: 0.4539 | Diff: +0.0337 | Confidence: 0.653\n",
      "   Player 23643 | Opening  382 | Games: 100 | Opening mean: 0.5072 | Original: 0.5450 ‚Üí Adjusted: 0.5324 | Diff: -0.0126 | Confidence: 0.667\n",
      "   Player  9596 | Opening 1854 | Games:  72 | Opening mean: 0.5257 | Original: 0.4931 ‚Üí Adjusted: 0.5064 | Diff: +0.0134 | Confidence: 0.590\n",
      "   Player 19945 | Opening  376 | Games:  69 | Opening mean: 0.5178 | Original: 0.6014 ‚Üí Adjusted: 0.5663 | Diff: -0.0352 | Confidence: 0.580\n",
      "   Player 23907 | Opening 1385 | Games:  69 | Opening mean: 0.4856 | Original: 0.4275 ‚Üí Adjusted: 0.4519 | Diff: +0.0244 | Confidence: 0.580\n",
      "   Player 31319 | Opening  730 | Games:  74 | Opening mean: 0.5185 | Original: 0.6284 ‚Üí Adjusted: 0.5841 | Diff: -0.0443 | Confidence: 0.597\n",
      "   Player 11966 | Opening 2519 | Games:  71 | Opening mean: 0.5229 | Original: 0.4859 ‚Üí Adjusted: 0.5012 | Diff: +0.0153 | Confidence: 0.587\n",
      "   Player 14516 | Opening 3143 | Games:  89 | Opening mean: 0.5118 | Original: 0.4382 ‚Üí Adjusted: 0.4647 | Diff: +0.0265 | Confidence: 0.640\n",
      "   Player 37651 | Opening 3214 | Games:  98 | Opening mean: 0.5145 | Original: 0.6071 ‚Üí Adjusted: 0.5759 | Diff: -0.0313 | Confidence: 0.662\n",
      "   Player 44127 | Opening  744 | Games:  53 | Opening mean: 0.5134 | Original: 0.4528 ‚Üí Adjusted: 0.4822 | Diff: +0.0294 | Confidence: 0.515\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 39330 | Opening  230 | Games: 342 | Opening mean: 0.5426 | Original: 0.5541 ‚Üí Adjusted: 0.5526 | Diff: -0.0015 | Confidence: 0.872\n",
      "   Player 42033 | Opening 1642 | Games: 380 | Opening mean: 0.5456 | Original: 0.5487 ‚Üí Adjusted: 0.5483 | Diff: -0.0004 | Confidence: 0.884\n",
      "   Player 25563 | Opening 1167 | Games: 220 | Opening mean: 0.4962 | Original: 0.4886 ‚Üí Adjusted: 0.4900 | Diff: +0.0014 | Confidence: 0.815\n",
      "   Player 21519 | Opening  910 | Games: 404 | Opening mean: 0.5002 | Original: 0.4406 ‚Üí Adjusted: 0.4472 | Diff: +0.0066 | Confidence: 0.890\n",
      "   Player 43603 | Opening 1356 | Games: 225 | Opening mean: 0.5053 | Original: 0.5378 ‚Üí Adjusted: 0.5319 | Diff: -0.0059 | Confidence: 0.818\n",
      "   Player 43781 | Opening 1407 | Games: 518 | Opening mean: 0.5183 | Original: 0.5309 ‚Üí Adjusted: 0.5298 | Diff: -0.0011 | Confidence: 0.912\n",
      "   Player 17505 | Opening 1555 | Games: 203 | Opening mean: 0.5318 | Original: 0.6281 ‚Üí Adjusted: 0.6090 | Diff: -0.0190 | Confidence: 0.802\n",
      "   Player 14808 | Opening  261 | Games: 432 | Opening mean: 0.5063 | Original: 0.4444 ‚Üí Adjusted: 0.4509 | Diff: +0.0064 | Confidence: 0.896\n",
      "   Player 14702 | Opening 1167 | Games: 247 | Opening mean: 0.4962 | Original: 0.5466 ‚Üí Adjusted: 0.5381 | Diff: -0.0085 | Confidence: 0.832\n",
      "   Player   512 | Opening  838 | Games: 220 | Opening mean: 0.5096 | Original: 0.5409 ‚Üí Adjusted: 0.5351 | Diff: -0.0058 | Confidence: 0.815\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4890 vs global) | 1 player entries\n",
      "   Opening 3551 (C19): mean = 0.8000 (+0.2890 vs global) | 1 player entries\n",
      "   Opening 2881 (E10): mean = 0.8000 (+0.2890 vs global) | 1 player entries\n",
      "   Opening 2046 (C49): mean = 0.7857 (+0.2747 vs global) | 1 player entries\n",
      "   Opening 1659 (C30): mean = 0.7692 (+0.2582 vs global) | 1 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 2593 (D26): mean = 0.1667 (-0.3444 vs global) | 1 player entries\n",
      "   Opening  636 (A83): mean = 0.2006 (-0.3104 vs global) | 2 player entries\n",
      "   Opening 3496 (A40): mean = 0.2273 (-0.2838 vs global) | 1 player entries\n",
      "   Opening 1763 (C37): mean = 0.2417 (-0.2694 vs global) | 2 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 39505 | Opening 3292 (C54) | Games: 10 | Opening mean: 0.7306 | Original: 0.7000 ‚Üí 0.7255\n",
      "      If we'd shrunk to global mean: 0.5425 (would lose +0.1830 of deserved credit)\n",
      "   Player 44311 | Opening 3292 (C54) | Games: 10 | Opening mean: 0.7306 | Original: 1.0000 ‚Üí 0.7755\n",
      "      If we'd shrunk to global mean: 0.5925 (would lose +0.1830 of deserved credit)\n",
      "   Player  7217 | Opening 3292 (C54) | Games: 17 | Opening mean: 0.7306 | Original: 0.8235 ‚Üí 0.7542\n",
      "      If we'd shrunk to global mean: 0.5903 (would lose +0.1639 of deserved credit)\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player  1857 | Opening 2293 (C71) | Games: 13 | Opening mean: 0.2665 | Original: 0.1538 ‚Üí 0.2433\n",
      "      If we'd shrunk to global mean: 0.4373 (would unfairly boost by +0.1941)\n",
      "   Player 40878 | Opening  443 (A43) | Games: 11 | Opening mean: 0.3182 | Original: 0.3182 ‚Üí 0.3182\n",
      "      If we'd shrunk to global mean: 0.4763 (would unfairly boost by +0.1581)\n",
      "   Player  4605 | Opening 1779 (C37) | Games: 13 | Opening mean: 0.3313 | Original: 0.3846 ‚Üí 0.3423\n",
      "      If we'd shrunk to global mean: 0.4850 (would unfairly boost by +0.1427)\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (2897177, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n"
     ]
    }
   ],
   "source": [
    "# 2b. Apply hierarchical Bayesian shrinkage to adjust scores based on sample size confidence\n",
    "\n",
    "# Check if confidence already exists - if so, skip this processing\n",
    "if 'confidence' in clean_data.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'confidence' column already exists in data\")\n",
    "else:\n",
    "    # Define the processing function\n",
    "    # This is a long function, I recommend you fold it down in your editor\n",
    "    def apply_hierarchical_bayesian_shrinkage(data, k_player=50):\n",
    "        \"\"\"\n",
    "        Apply two-level hierarchical Bayesian shrinkage to adjust scores.\n",
    "        \n",
    "        A lot of our player-opening entries have a small number of games played, because openings are so specific.\n",
    "        This introduces sample size issues.\n",
    "        \n",
    "        We use TWO-LEVEL shrinkage:\n",
    "        Level 1: Calculate opening-specific means (these are our \"ground truth\" for each opening)\n",
    "        Level 2: Shrink individual player-opening scores toward their opening's mean\n",
    "        This is better than shrinking toward global mean because different openings have different baseline win rates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Clean data with columns: player_id, opening_id, score, num_games, eco\n",
    "        k_player : int\n",
    "            Shrinkage constant for player-opening scores (default: 50)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Data with adjusted scores and new 'confidence' column\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Calculate global mean score for comparison\n",
    "        global_mean_score = data[\"score\"].mean()\n",
    "        print(f\"\\nüìä Global statistics:\")\n",
    "        print(f\"   ‚Ä¢ Global mean score: {global_mean_score:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Total entries: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique openings: {data['opening_id'].nunique():,}\")\n",
    "        \n",
    "        # Store original scores for comparison\n",
    "        data = data.copy()\n",
    "        data[\"score_original\"] = data[\"score\"].copy()\n",
    "        \n",
    "        # LEVEL 1: Calculate opening-specific means and statistics        \n",
    "        opening_stats = (\n",
    "            data.groupby(\"opening_id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"score\": \"mean\",\n",
    "                    \"num_games\": \"sum\",\n",
    "                    \"player_id\": \"count\",  # Number of players who played this opening\n",
    "                }\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"score\": \"opening_mean\",\n",
    "                    \"num_games\": \"opening_total_games\",\n",
    "                    \"player_id\": \"opening_num_players\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        # Opening mean statistics\n",
    "        print(f\"\\n   Opening mean score distribution:\")\n",
    "        print(f\"   ‚Ä¢ Min: {opening_stats['opening_mean'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {opening_stats['opening_mean'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {opening_stats['opening_mean'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {opening_stats['opening_mean'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {opening_stats['opening_mean'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {opening_stats['opening_mean'].std():.4f}\")\n",
    "        \n",
    "        # Show distribution of opening sizes\n",
    "        print(f\"\\n   Opening sample size distribution:\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games per opening (median): {opening_stats['opening_total_games'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players per opening (median): {opening_stats['opening_num_players'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games range: [{opening_stats['opening_total_games'].min():.0f}, {opening_stats['opening_total_games'].max():.0f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players range: [{opening_stats['opening_num_players'].min():.0f}, {opening_stats['opening_num_players'].max():.0f}]\"\n",
    "        )\n",
    "        \n",
    "        # Merge opening means back into main dataframe\n",
    "        data = data.merge(\n",
    "            opening_stats[[\"opening_mean\"]], left_on=\"opening_id\", right_index=True, how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # LEVEL 2: Shrink player-opening scores toward opening-specific means\n",
    "        print(\n",
    "            f\"   Formula: adjusted_score = (num_games √ó player_score + {k_player} √ó opening_mean) / (num_games + {k_player})\"\n",
    "        )\n",
    "        \n",
    "        numerator = (data[\"num_games\"] * data[\"score_original\"]) + (\n",
    "            k_player * data[\"opening_mean\"]\n",
    "        )\n",
    "        denominator = data[\"num_games\"] + k_player\n",
    "        data[\"score\"] = numerator / denominator\n",
    "        \n",
    "        print(f\"   ‚úì Scores adjusted for {len(data):,} entries\")\n",
    "        \n",
    "        # Calculate confidence weights (will be used in loss function later)\n",
    "        data[\"confidence\"] = data[\"num_games\"] / (\n",
    "            data[\"num_games\"] + k_player\n",
    "        )\n",
    "        print(f\"   ‚úì Confidence weights calculated\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Range: [{data['confidence'].min():.4f}, {data['confidence'].max():.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # Statistics on the adjustment\n",
    "        score_diff = data[\"score\"] - data[\"score_original\"]\n",
    "        print(f\"\\n4Ô∏è‚É£  Adjustment statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean adjustment: {score_diff.mean():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Std adjustment: {score_diff.std():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Max adjustment: {score_diff.max():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Min adjustment: {score_diff.min():.6f}\")\n",
    "        \n",
    "        # Show distribution of adjustments\n",
    "        print(f\"\\n   Adjustment by num_games quartiles:\")\n",
    "        quartiles = data[\"num_games\"].quantile([0.25, 0.5, 0.75])\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 25th percentile (n={quartiles[0.25]:.0f} games): avg adjustment = {score_diff[data['num_games'] <= quartiles[0.25]].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 50th percentile (n={quartiles[0.5]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.25]) & (data['num_games'] <= quartiles[0.5])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 75th percentile (n={quartiles[0.75]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.5]) & (data['num_games'] <= quartiles[0.75])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ >75th percentile (n>{quartiles[0.75]:.0f} games): avg adjustment = {score_diff[data['num_games'] > quartiles[0.75]].mean():.6f}\"\n",
    "        )\n",
    "        \n",
    "        # New score distribution after adjustment\n",
    "        print(f\"\\n5Ô∏è‚É£  Adjusted score statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {data['score'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {data['score'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {data['score'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {data['score'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {data['score'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {data['score'].mean():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {data['score'].std():.4f}\")\n",
    "        \n",
    "        # Detailed sample showing the effect across different game counts\n",
    "        print(f\"\\n6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\")\n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        low_game_sample = data[\n",
    "            (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in low_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Medium-game entries (50-100 games) - MODERATE shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        med_game_sample = data[\n",
    "            (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in med_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   High-game entries (200+ games) - LOW shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        high_game_sample = data[data[\"num_games\"] >= 200].sample(\n",
    "            min(10, len(data[data[\"num_games\"] >= 200])), random_state=42\n",
    "        )\n",
    "        for idx, row in high_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        # Show extreme cases - comparing to both opening mean AND global mean\n",
    "        print(f\"\\n7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\")\n",
    "        \n",
    "        # Find entries where opening mean differs significantly from global mean\n",
    "        data[\"opening_deviation_from_global\"] = (\n",
    "            data[\"opening_mean\"] - global_mean_score\n",
    "        ).abs()\n",
    "        \n",
    "        print(f\"\\n   Openings with HIGHEST win rates (strong for White):\")\n",
    "        strong_openings = data.nlargest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in strong_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"(+{deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   Openings with LOWEST win rates (weak for White):\")\n",
    "        weak_openings = data.nsmallest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in weak_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"({deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        # Show specific examples where hierarchical shrinkage made a difference\n",
    "        print(f\"\\n8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\")\n",
    "        \n",
    "        # Find entries with strong openings where player did well\n",
    "        strong_opening_ids = data.nlargest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        strong_examples = data[\n",
    "            (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] > 0.6)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] > 0.6)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"\\n   Strong opening + good player performance (shrunk toward HIGH opening mean):\"\n",
    "        )\n",
    "        for idx, row in strong_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would lose {difference:+.4f} of deserved credit)\"\n",
    "            )\n",
    "        \n",
    "        # Find entries with weak openings where player did poorly\n",
    "        weak_opening_ids = data.nsmallest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        weak_examples = data[\n",
    "            (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] < 0.45)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] < 0.45)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Weak opening + poor player performance (shrunk toward LOW opening mean):\")\n",
    "        for idx, row in weak_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would unfairly boost by {-difference:+.4f})\"\n",
    "            )\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        data = data.drop(\n",
    "            columns=[\"score_original\", \"opening_mean\", \"opening_deviation_from_global\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nFinal data shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        print(f\"\\nNew columns added:\")\n",
    "        print(f\"   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\")\n",
    "        print(f\"   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\")\n",
    "        print(f\"\\nKey improvement over simple shrinkage:\")\n",
    "        print(f\"   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\")\n",
    "        print(f\"   ‚Ä¢ Preserves opening difficulty differences\")\n",
    "        print(f\"   ‚Ä¢ More accurate for both strong and weak openings\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    K_PLAYER = 50  # Shrinkage constant for player-opening scores\n",
    "    \n",
    "    clean_data = apply_hierarchical_bayesian_shrinkage(clean_data, k_player=K_PLAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2C: PLAYER RATING STATISTICS\n",
      "============================================================\n",
      "   ‚úì Retrieved ratings for 48,469 players\n",
      "   ‚úì All players have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 48,469\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1765.15\n",
      "   ‚Ä¢ Median: 1762\n",
      "   ‚Ä¢ Std Dev: 249.21\n",
      "\n",
      "5Ô∏è‚É£  Ratings Percentiles\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1359    ‚ñà‚ñà‚ñà\n",
      "      10%          1435    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1494    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1541    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1584    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1623    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1660    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1696    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1729    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1762    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1797    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1829    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1864    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1901    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1936    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1980    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2027    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2088    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2183    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400       3,585      7.40%      ‚ñà‚ñà\n",
      "   1400-1600       9,411     19.42%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800      13,788     28.45%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000      12,889     26.59%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       6,597     13.61%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400       1,844      3.80%      ‚ñà\n",
      "   2400-2600         330      0.68%      \n",
      "   2600-3000          25      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 352\n",
      "   ‚Ä¢ 10th-90th percentile range: 653\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1435):\n",
      "      Player 29638: glhc - Rating: 1435\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1584):\n",
      "      Player 45186: luyen1984 - Rating: 1584\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1762):\n",
      "      Player 45685: loshchini - Rating: 1762\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1936):\n",
      "      Player 10674: KENANGA47 - Rating: 1936\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2088):\n",
      "      Player 38252: pristinewaters - Rating: 2088\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "   ‚Ä¢ Total players: 48,469\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1765 ¬± 249\n",
      "   ‚Ä¢ Median: 1762\n"
     ]
    }
   ],
   "source": [
    "# 2c. Gather player rating statistics\n",
    "\n",
    "print(\"STEP 2C: PLAYER RATING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to database and extract player ratings\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:    \n",
    "    unique_player_ids = clean_data['player_id'].unique()\n",
    "    player_ids_str = ','.join(map(str, unique_player_ids))\n",
    "    \n",
    "    # Query to get player ratings\n",
    "    rating_query = f\"\"\"\n",
    "        SELECT \n",
    "            id as player_id,\n",
    "            name,\n",
    "            title,\n",
    "            rating\n",
    "        FROM player\n",
    "        WHERE id IN ({player_ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    player_ratings = pd.DataFrame(con.execute(rating_query).df())\n",
    "    print(f\"   ‚úì Retrieved ratings for {len(player_ratings):,} players\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Check for missing ratings\n",
    "missing_ratings = player_ratings['rating'].isna().sum()\n",
    "if missing_ratings > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: {missing_ratings:,} players have missing ratings\")\n",
    "else:\n",
    "    print(f\"   ‚úì All players have ratings\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Basic rating statistics:\")\n",
    "print(f\"   ‚Ä¢ Count: {player_ratings['rating'].notna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Missing: {player_ratings['rating'].isna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Min: {player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Max: {player_ratings['rating'].max():.0f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {player_ratings['rating'].mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"   ‚Ä¢ Std Dev: {player_ratings['rating'].std():.2f}\")\n",
    "\n",
    "\n",
    "print(f\"\\n5Ô∏è‚É£  Ratings Percentiles\")\n",
    "percentiles = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,\n",
    "               0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "print(f\"\\n   {'Percentile':<12} {'Rating':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*12} {'-'*10} {'-'*40}\")\n",
    "\n",
    "for p in percentiles:\n",
    "    rating_value = player_ratings['rating'].quantile(p)\n",
    "    # Create a simple bar visualization\n",
    "    bar_length = int((rating_value - player_ratings['rating'].min()) / \n",
    "                     (player_ratings['rating'].max() - player_ratings['rating'].min()) * 40)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {p*100:>5.0f}%       {rating_value:>7.0f}    {bar}\")\n",
    "\n",
    "# Rating ranges and counts\n",
    "print(f\"\\n6Ô∏è‚É£  Rating distribution by range:\")\n",
    "rating_ranges = [\n",
    "    (0, 1000), (1000, 1200), (1200, 1400), (1400, 1600), \n",
    "    (1600, 1800), (1800, 2000), (2000, 2200), (2200, 2400), \n",
    "    (2400, 2600), (2600, 3000)\n",
    "]\n",
    "\n",
    "print(f\"\\n   {'Range':<15} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*15} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "\n",
    "for low, high in rating_ranges:\n",
    "    count = len(player_ratings[(player_ratings['rating'] >= low) & (player_ratings['rating'] < high)])\n",
    "    pct = 100 * count / len(player_ratings)\n",
    "    bar_length = int(pct * 0.4)  # Scale for visualization\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {low:>4}-{high:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Interquartile range\n",
    "iqr = player_ratings['rating'].quantile(0.75) - player_ratings['rating'].quantile(0.25)\n",
    "print(f\"\\n7Ô∏è‚É£  Spread statistics:\")\n",
    "print(f\"   ‚Ä¢ Range: {player_ratings['rating'].max() - player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Interquartile Range (IQR): {iqr:.0f}\")\n",
    "print(f\"   ‚Ä¢ 10th-90th percentile range: {player_ratings['rating'].quantile(0.90) - player_ratings['rating'].quantile(0.10):.0f}\")\n",
    "\n",
    "# Sample of players at different rating levels\n",
    "print(f\"\\n9Ô∏è‚É£  Sample players at different rating levels:\")\n",
    "sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "for p in sample_percentiles:\n",
    "    rating_threshold = player_ratings['rating'].quantile(p)\n",
    "    # Get a player near this rating\n",
    "    sample_player = player_ratings.iloc[(player_ratings['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "    print(f\"\\n   ~{p*100:.0f}th percentile (rating ‚âà {rating_threshold:.0f}):\")\n",
    "    for idx, row in sample_player.iterrows():\n",
    "        # print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f} {f'({row['title']})' if pd.notna(row['title']) else ''}\")\n",
    "        title_str = f\" ({row['title']})\" if pd.notna(row['title']) else \"\"\n",
    "        print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f}{title_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ RATING STATISTICS COMPLETE\")\n",
    "print(f\"   ‚Ä¢ Total players: {len(player_ratings):,}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings['rating'].min():.0f}, {player_ratings['rating'].max():.0f}]\")\n",
    "print(f\"   ‚Ä¢ Mean ¬± std: {player_ratings['rating'].mean():.0f} ¬± {player_ratings['rating'].std():.0f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ab6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Normalization parameters (calculated from 48,469 players):\n",
      "   ‚Ä¢ Mean: 1765.15\n",
      "   ‚Ä¢ Std Dev: 249.21\n",
      "\n",
      "2Ô∏è‚É£  Normalized rating statistics:\n",
      "   ‚Ä¢ Min: -2.2678\n",
      "   ‚Ä¢ Max: 4.2448\n",
      "   ‚Ä¢ Mean: -0.000000 (should be ~0)\n",
      "   ‚Ä¢ Std: 1.000000 (should be ~1)\n",
      "   ‚Ä¢ Range: [-2.27, 4.24]\n",
      "\n",
      "3Ô∏è‚É£  Sample normalized ratings across skill levels:\n",
      "   ~10th percentile: Player 28741 | Rating: 1435 ‚Üí Z-score: -1.325\n",
      "   ~25th percentile: Player 43805 | Rating: 1584 ‚Üí Z-score: -0.727\n",
      "   ~50th percentile: Player 44292 | Rating: 1762 ‚Üí Z-score: -0.013\n",
      "   ~75th percentile: Player 10320 | Rating: 1936 ‚Üí Z-score:  0.686\n",
      "   ~90th percentile: Player 37084 | Rating: 2088 ‚Üí Z-score:  1.295\n",
      "\n",
      "4Ô∏è‚É£  Interpretation guide:\n",
      "   ‚Ä¢ rating_z ‚âà -2.3: 1200 player (minimum)\n",
      "   ‚Ä¢ rating_z ‚âà -0.7: 1584 player (25th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà  0.0: 1765 player (mean)\n",
      "   ‚Ä¢ rating_z ‚âà 0.7: 1936 player (75th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà 4.2: 2823 player (maximum)\n",
      "\n",
      "5Ô∏è‚É£  Player side information table structure:\n",
      "   ‚Ä¢ Shape: (48469, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'rating', 'rating_z']\n",
      "\n",
      "6Ô∏è‚É£  Sample entries from side information table:\n",
      "   Player  1960 | Rating: 1401 ‚Üí Z-score: -1.461\n",
      "   Player 37690 | Rating: 1305 ‚Üí Z-score: -1.846\n",
      "   Player 46265 | Rating: 2029 ‚Üí Z-score:  1.059\n",
      "   Player 48531 | Rating: 1688 ‚Üí Z-score: -0.310\n",
      "   Player 23804 | Rating: 1322 ‚Üí Z-score: -1.778\n",
      "   Player 26421 | Rating: 1843 ‚Üí Z-score:  0.312\n",
      "   Player 29429 | Rating: 1867 ‚Üí Z-score:  0.409\n",
      "   Player 17348 | Rating: 1506 ‚Üí Z-score: -1.040\n",
      "   Player 37092 | Rating: 1610 ‚Üí Z-score: -0.623\n",
      "   Player  5384 | Rating: 1463 ‚Üí Z-score: -1.212\n",
      "   ‚úì All 48,469 players in clean_data have side information\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING NORMALIZATION COMPLETE\n",
      "\n",
      "Created: player_side_info\n",
      "   ‚Ä¢ Shape: (48469, 1)\n",
      "   ‚Ä¢ Index: player_id\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ clean_data: 2,897,177 rows (player-opening interactions)\n",
      "   ‚Ä¢ player_side_info: 48,469 rows (one per player)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\n",
      "   RATING_MEAN = 1765.15\n",
      "   RATING_STD = 249.21\n",
      "\n",
      "   You'll need them to normalize ratings for new users at inference time.\n"
     ]
    }
   ],
   "source": [
    "# 2d. Normalize player ratings using z-score normalization (for use as side information in MF model)\n",
    "\n",
    "# Check if we've already normalized ratings\n",
    "if 'player_side_info' in globals() and 'rating_z' in player_side_info.columns:\n",
    "    print(\"   SKIPPING STEP 2D: RATING NORMALIZATION\")\n",
    "    print(\"Ratings have already been normalized\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing normalized rating statistics:\")\n",
    "    print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "    print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "\n",
    "else:\n",
    "    def normalize_player_ratings(player_ratings_df):\n",
    "        \"\"\"\n",
    "        Apply z-score normalization to player ratings for use as side information.\n",
    "        \n",
    "        This creates a SEPARATE table of player-level features, NOT merged into clean_data.\n",
    "        Rating is side information - it describes the player, not the player-opening interaction.\n",
    "        \n",
    "        During training, the model will LOOK UP each player's rating_z from this table.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ratings_df : pd.DataFrame\n",
    "            Player ratings with columns: player_id, name, title, rating\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (player_side_info DataFrame, RATING_MEAN, RATING_STD)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calculate normalization parameters\n",
    "        RATING_MEAN = player_ratings_df['rating'].mean()\n",
    "        RATING_STD = player_ratings_df['rating'].std()\n",
    "        \n",
    "        print(f\"\\n1Ô∏è‚É£  Normalization parameters (calculated from {len(player_ratings_df):,} players):\")\n",
    "        print(f\"   ‚Ä¢ Mean: {RATING_MEAN:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Std Dev: {RATING_STD:.2f}\")\n",
    "        \n",
    "        # Create side information table - only keep player_id and rating for now\n",
    "        player_side_info = player_ratings_df[['player_id', 'rating']].copy()\n",
    "        player_side_info['rating_z'] = (player_side_info['rating'] - RATING_MEAN) / RATING_STD\n",
    "        \n",
    "        print(f\"\\n2Ô∏è‚É£  Normalized rating statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "        print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "        print(f\"   ‚Ä¢ Range: [{player_side_info['rating_z'].min():.2f}, {player_side_info['rating_z'].max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\n3Ô∏è‚É£  Sample normalized ratings across skill levels:\")\n",
    "        sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "        for p in sample_percentiles:\n",
    "            rating_threshold = player_side_info['rating'].quantile(p)\n",
    "            sample_player = player_side_info.iloc[(player_side_info['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "            for idx, row in sample_player.iterrows():\n",
    "                print(f\"   ~{p*100:.0f}th percentile: Player {idx} | \"\n",
    "                      f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n4Ô∏è‚É£  Interpretation guide:\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(1200 - RATING_MEAN)/RATING_STD:.1f}: 1200 player (minimum)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.25) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.25):.0f} player (25th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà  0.0: {RATING_MEAN:.0f} player (mean)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.75) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.75):.0f} player (75th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].max() - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].max():.0f} player (maximum)\")\n",
    "        \n",
    "        print(f\"\\n5Ô∏è‚É£  Player side information table structure:\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        # Set player_id as index for fast lookups\n",
    "        player_side_info = player_side_info.set_index('player_id')\n",
    "        \n",
    "        print(f\"\\n6Ô∏è‚É£  Sample entries from side information table:\")\n",
    "        sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "        for idx, row in sample_data.iterrows():\n",
    "            print(f\"   Player {idx:>5} | Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        # Drop rating column - we only need rating_z for the model\n",
    "        player_side_info = player_side_info.drop(columns=['rating'])\n",
    "        \n",
    "        # This is important - make sure every player in clean_data has a rating\n",
    "        missing_players = set(clean_data['player_id'].unique()) - set(player_side_info.index)\n",
    "        if len(missing_players) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in clean_data are missing from side_info!\")\n",
    "            print(f\"   Missing player IDs: {sorted(list(missing_players))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(player_side_info):,} players in clean_data have side information\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ RATING NORMALIZATION COMPLETE\")\n",
    "        print(f\"\\nCreated: player_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: player_id\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ clean_data: {clean_data.shape[0]:,} rows (player-opening interactions)\")\n",
    "        print(f\"   ‚Ä¢ player_side_info: {len(player_side_info):,} rows (one per player)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\")\n",
    "        print(f\"   RATING_MEAN = {RATING_MEAN:.2f}\")\n",
    "        print(f\"   RATING_STD = {RATING_STD:.2f}\")\n",
    "        print(f\"\\n   You'll need them to normalize ratings for new users at inference time.\")\n",
    "        \n",
    "        return player_side_info, RATING_MEAN, RATING_STD\n",
    "    \n",
    "    player_side_info, RATING_MEAN, RATING_STD = normalize_player_ratings(player_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14eba9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         player_id  opening_id  num_games     score  eco  confidence\n",
      "23708          417        2829         44  0.483255  D91    0.468085\n",
      "589727        9920        3245         12  0.506432  C50    0.193548\n",
      "1273193      21590         751         10  0.513905  B01    0.166667\n",
      "1325149      22471         191         53  0.508753  A04    0.514563\n",
      "1252392      21237         744         11  0.543741  B00    0.180328\n",
      "842800       14171         853         11  0.482078  B06    0.180328\n",
      "1530786      25878        1167         60  0.475557  B40    0.545455\n",
      "1920837      32441         610         19  0.499434  A80    0.275362\n",
      "518670        8726        3468         28  0.454026  A40    0.358974\n",
      "596815       10038          19         72  0.527406  A00    0.590164\n",
      "39447          694        1829         22  0.514681  C40    0.305556\n",
      "1455480      24643         730         12  0.458441  B00    0.193548\n",
      "2360213      39917         400         18  0.531076  A40    0.264706\n",
      "2496673      42269         868         32  0.488423  B06    0.390244\n",
      "1567661      26514         748         20  0.564417  B00    0.285714\n",
      "1425296      24148        3255         27  0.570200  C00    0.350649\n",
      "1478599      25022        2216        195  0.545440  C62    0.795918\n",
      "1070441      18064         696         11  0.569764  B00    0.180328\n",
      "712885       11978        1538         47  0.464301  C21    0.484536\n",
      "2724452      46247        3216         17  0.493586  C00    0.253731\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac119ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           rating_z\n",
      "player_id          \n",
      "29029     -1.236531\n",
      "42655      0.244161\n",
      "28067     -1.292709\n",
      "40825     -1.453218\n",
      "20260     -0.478128\n",
      "33613      1.893387\n",
      "39956      0.003398\n",
      "28361      1.167085\n",
      "23046      2.647777\n",
      "45688     -0.165136\n"
     ]
    }
   ],
   "source": [
    "print(player_side_info.sample(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30280db",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test/Val splits\n",
    "\n",
    "Here, I split my data and drop columns that are no longer needed. We're very close to being able to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf3849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: TRAIN/VALIDATION/TEST SPLIT\n",
      "\n",
      "1Ô∏è‚É£  Preparing data for split...\n",
      "   ‚Ä¢ Features (X): (2897177, 4)\n",
      "   ‚Ä¢ Target (y): (2897177,)\n",
      "   ‚Ä¢ Feature columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ Train: 2,172,882 samples (75.0%)\n",
      "   ‚Ä¢ Val: 434,577 samples (15.0%)\n",
      "   ‚Ä¢ Test: 289,718 samples (10.0%)\n",
      "   ‚Ä¢ Total samples: 2,897,177 (should equal 2,897,177)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 48,425 unique players\n",
      "   ‚Ä¢ Val: 47,498 unique players\n",
      "   ‚Ä¢ Test: 46,531 unique players\n",
      "   ‚Ä¢ Total unique: 48,469 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,694 unique openings\n",
      "   ‚Ä¢ Val: 2,450 unique openings\n",
      "   ‚Ä¢ Test: 2,354 unique openings\n",
      "   ‚Ä¢ Total unique: 2,716 openings\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 27 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 14 (0.6%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 21 (0.0%)\n",
      "   ‚Ä¢ Openings not in train: 11 (0.5%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "\n",
      "   Train y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1138\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ Max: 0.8251\n",
      "\n",
      "   Test y:\n",
      "   ‚Ä¢ Mean: 0.5121\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.2162\n",
      "   ‚Ä¢ Max: 0.7920\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train confidence:\n",
      "   ‚Ä¢ Mean: 0.4149\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation confidence:\n",
      "   ‚Ä¢ Mean: 0.4148\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test confidence:\n",
      "   ‚Ä¢ Mean: 0.4140\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 2,172,882 samples (75%)\n",
      "   ‚Ä¢ Validation data: 434,577 samples (15%)\n",
      "   ‚Ä¢ Test data: 289,718 samples (10%)\n",
      "   ‚Ä¢ Player side info: 48,469 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train/Validation/Test Split (75/15/10)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 3: TRAIN/VALIDATION/TEST SPLIT\")\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Preparing data for split...\")\n",
    "\n",
    "# We don't need num_games for modeling because we have the `confidence` based on num_games\n",
    "# Keep: player_id, opening_id, score, eco, confidence\n",
    "X = clean_data[[\"player_id\", \"opening_id\", \"eco\", \"confidence\"]].copy()\n",
    "y = clean_data[\"score\"].copy()\n",
    "\n",
    "print(f\"   ‚Ä¢ Features (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Target (y): {y.shape}\")\n",
    "print(f\"   ‚Ä¢ Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Clean up player_side_info - only keep rating_z\n",
    "# May regret this if we add more side info later and forget we took this step\n",
    "# God help me that would be a nasty bug\n",
    "player_side_info_clean = player_side_info[[\"rating_z\"]].copy()\n",
    "\n",
    "#  Use index-based splitting to avoid DataFrame copies\n",
    "idx = np.arange(len(X))\n",
    "\n",
    "# First split: separate out test set (10%)\n",
    "idx_temp, idx_test = train_test_split(idx, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: split remaining into train (75%) and val (15%)\n",
    "# 15% of original = 15/90 ‚âà 0.1667 of temp\n",
    "idx_train, idx_val = train_test_split(idx_temp, test_size=15/90, random_state=42, shuffle=True)\n",
    "\n",
    "# Create splits using iloc (view, not copy)\n",
    "X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
    "X_test, y_test = X.iloc[idx_test], y.iloc[idx_test]\n",
    "\n",
    "print(f\"   ‚Ä¢ Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Val: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"   ‚Ä¢ Total samples: {total:,} (should equal {len(X):,})\")\n",
    "print(f\"   ‚Ä¢ Train %: {len(X_train)/total*100:.2f}% (target: 75%)\")\n",
    "print(f\"   ‚Ä¢ Val %: {len(X_val)/total*100:.2f}% (target: 15%)\")\n",
    "print(f\"   ‚Ä¢ Test %: {len(X_test)/total*100:.2f}% (target: 10%)\")\n",
    "\n",
    "# Pre-compute unique arrays once\n",
    "players_train = X_train[\"player_id\"].unique()\n",
    "players_val = X_val[\"player_id\"].unique()\n",
    "players_test = X_test[\"player_id\"].unique()\n",
    "\n",
    "openings_train = X_train[\"opening_id\"].unique()\n",
    "openings_val = X_val[\"opening_id\"].unique()\n",
    "openings_test = X_test[\"opening_id\"].unique()\n",
    "\n",
    "print(f\"\\n   Players:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(players_train):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Val: {len(players_val):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Test: {len(players_test):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['player_id'].nunique():,} players\")\n",
    "\n",
    "print(f\"\\n   Openings:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(openings_train):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Val: {len(openings_val):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Test: {len(openings_test):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['opening_id'].nunique():,} openings\")\n",
    "\n",
    "# Use NumPy setdiff1d for cold-start analysis (C-speed)\n",
    "\n",
    "val_cold_players = np.setdiff1d(players_val, players_train, assume_unique=True)\n",
    "val_cold_openings = np.setdiff1d(openings_val, openings_train, assume_unique=True)\n",
    "\n",
    "test_cold_players = np.setdiff1d(players_test, players_train, assume_unique=True)\n",
    "test_cold_openings = np.setdiff1d(openings_test, openings_train, assume_unique=True)\n",
    "\n",
    "print(f\"\\n   Validation set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(val_cold_players):,} ({len(val_cold_players)/len(players_val)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(val_cold_openings):,} ({len(val_cold_openings)/len(openings_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(test_cold_players):,} ({len(test_cold_players)/len(players_test)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(test_cold_openings):,} ({len(test_cold_openings)/len(openings_test)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n7Ô∏è‚É£  Score distribution across splits:\")\n",
    "\n",
    "y_train_stats = y_train.describe()\n",
    "y_val_stats = y_val.describe()\n",
    "y_test_stats = y_test.describe()\n",
    "\n",
    "print(f\"\\n   Train y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_train_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_train_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_train_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_val_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_val_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_val_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_test_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_test_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_test_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n8Ô∏è‚É£  Confidence distribution across splits:\")\n",
    "\n",
    "conf_train_stats = X_train['confidence'].describe()\n",
    "conf_val_stats = X_val['confidence'].describe()\n",
    "conf_test_stats = X_test['confidence'].describe()\n",
    "\n",
    "print(f\"\\n   Train confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_train_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_val_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_test_stats['50%']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SPLIT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   ‚Ä¢ Training data: {len(X_train):,} samples (75%)\")\n",
    "print(f\"   ‚Ä¢ Validation data: {len(X_val):,} samples (15%)\")\n",
    "print(f\"   ‚Ä¢ Test data: {len(X_test):,} samples (10%)\")\n",
    "print(f\"   ‚Ä¢ Player side info: {len(player_side_info_clean):,} players\")\n",
    "print(f\"   ‚Ä¢ Side info columns: {list(player_side_info_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781a382",
   "metadata": {},
   "source": [
    "## Step 3b: Remap Player and Opening IDs to Sequential Integers\n",
    "\n",
    "**Why remap IDs?**\n",
    "- Database IDs may have gaps (e.g., [1, 5, 10, 15, ...]) from deleted entries\n",
    "- Embedding layers need 0-based contiguous indices for efficiency\n",
    "- Remapping saves memory (no unused embedding slots)\n",
    "\n",
    "**Process:**\n",
    "1. Check if IDs are already sequential (0 or 1-based with no gaps)\n",
    "2. If not, create mappings: old_id ‚Üí new_sequential_id\n",
    "3. Remap all DataFrames and side info tables\n",
    "4. Verify mappings with spot checks\n",
    "\n",
    "This ensures embeddings use minimal memory and indices align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81b4c409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\n",
      "\n",
      "1Ô∏è‚É£  Processing player IDs...\n",
      "Checking player IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique players: 48469\n",
      "   ‚Ä¢ ID range: [1, 50000]\n",
      "   ‚ö†Ô∏è  player IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1531\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      player_id 1 ‚Üí 0\n",
      "      player_id 2 ‚Üí 1\n",
      "      player_id 3 ‚Üí 2\n",
      "      player_id 4 ‚Üí 3\n",
      "      player_id 5 ‚Üí 4\n",
      "      player_id 49996 ‚Üí 48464\n",
      "      player_id 49997 ‚Üí 48465\n",
      "      player_id 49998 ‚Üí 48466\n",
      "      player_id 49999 ‚Üí 48467\n",
      "      player_id 50000 ‚Üí 48468\n",
      "\n",
      "   Remapping 5 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2716\n",
      "   ‚Ä¢ ID range: [2, 3589]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 872\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 3 ‚Üí 1\n",
      "      opening_id 4 ‚Üí 2\n",
      "      opening_id 5 ‚Üí 3\n",
      "      opening_id 6 ‚Üí 4\n",
      "      opening_id 3571 ‚Üí 2711\n",
      "      opening_id 3572 ‚Üí 2712\n",
      "      opening_id 3575 ‚Üí 2713\n",
      "      opening_id 3584 ‚Üí 2714\n",
      "      opening_id 3589 ‚Üí 2715\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/4\n",
      "   ‚úì Remapped DataFrame 2/4\n",
      "   ‚úì Remapped DataFrame 3/4\n",
      "   ‚úì Remapped DataFrame 4/4\n",
      "\n",
      "   ‚úÖ Opening ID remapping complete!\n",
      "\n",
      "3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\n",
      "   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\n",
      "\n",
      "   Sampling 10 entries from X_train (before remapping was applied)...\n",
      "   ‚Ä¢ Sample indices: [0, 241431, 482862, 724293, 965724, 1207155, 1448586, 1690017, 1931448, 2172881]\n",
      "\n",
      "   Verification checks:\n",
      "   #    Row Idx    New Player   New Opening  Confidence   Status         \n",
      "   ---- ---------- ------------ ------------ ------------ ---------------\n",
      "   1    0          27535        599          0.6503       ‚úì PASS         \n",
      "   2    241431     20431        1556         0.1667       ‚úì PASS         \n",
      "   3    482862     42619        2040         0.2647       ‚úì PASS         \n",
      "   4    724293     44106        532          0.3506       ‚úì PASS         \n",
      "   5    965724     28354        728          0.3243       ‚úì PASS         \n",
      "   6    1207155    1395         1057         0.9246       ‚úì PASS         \n",
      "   7    1448586    22254        2656         0.4253       ‚úì PASS         \n",
      "   8    1690017    37480        1594         0.3506       ‚úì PASS         \n",
      "   9    1931448    12338        1146         0.1803       ‚úì PASS         \n",
      "   10   2172881    34499        1648         0.3421       ‚úì PASS         \n",
      "\n",
      "   Reverse mapping verification (sample of 3 entries):\n",
      "   #    New‚ÜíOld Player                 New‚ÜíOld Opening               \n",
      "   ---- ------------------------------ ------------------------------\n",
      "   1    27535 ‚Üí 28400                  599 ‚Üí 774                     \n",
      "   6    1395 ‚Üí 1442                    1057 ‚Üí 1356                   \n",
      "   10   34499 ‚Üí 35569                  1648 ‚Üí 2155                   \n",
      "\n",
      "   ‚úÖ All spot checks passed! ID mappings are correct.\n",
      "\n",
      "4Ô∏è‚É£  Summary:\n",
      "\n",
      "   Player IDs:\n",
      "   ‚Ä¢ Original range: [1, 50000]\n",
      "   ‚Ä¢ New range: [0, 48468]\n",
      "   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\n",
      "\n",
      "   Opening IDs:\n",
      "   ‚Ä¢ Original range: [2, 3589]\n",
      "   ‚Ä¢ New range: [0, 2715]\n",
      "   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\n",
      "\n",
      "   Updated DataFrames:\n",
      "   ‚Ä¢ X_train: (2172882, 4)\n",
      "   ‚Ä¢ X_val: (434577, 4)\n",
      "   ‚Ä¢ X_test: (289718, 4)\n",
      "   ‚Ä¢ clean_data: (2897177, 6)\n",
      "   ‚Ä¢ player_side_info: (48469, 1)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ID REMAPPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Important:\n",
      "   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\n",
      "   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\n",
      "   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\n",
      "   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\n"
     ]
    }
   ],
   "source": [
    "# Step 3b: Remap player and opening IDs to 0-based sequential integers\n",
    "\n",
    "print(\"STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\")\n",
    "\n",
    "def check_and_remap_ids(df_list, id_column, entity_name):\n",
    "    \"\"\"\n",
    "    Check if IDs are sequential starting from 0, and remap if not.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of DataFrames\n",
    "        List of DataFrames containing the ID column to check/remap\n",
    "    id_column : str\n",
    "        Name of the ID column ('player_id' or 'opening_id')\n",
    "    entity_name : str\n",
    "        Name for logging ('player' or 'opening')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (df_list with remapped IDs, id_to_idx mapping dict, needs_remapping bool)\n",
    "    \"\"\"\n",
    "    # Get all unique IDs across all dataframes\n",
    "    all_ids = pd.concat([df[id_column] for df in df_list]).unique()\n",
    "    all_ids_sorted = sorted(all_ids)\n",
    "    \n",
    "    print(f\"Checking {entity_name} IDs...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ‚Ä¢ Total unique {entity_name}s: {len(all_ids_sorted)}\")\n",
    "    print(f\"   ‚Ä¢ ID range: [{all_ids_sorted[0]}, {all_ids_sorted[-1]}]\")\n",
    "    \n",
    "    # Check if IDs are already 0-based sequential (0, 1, 2, ...)\n",
    "    expected_sequential = list(range(len(all_ids_sorted)))\n",
    "    is_sequential = (all_ids_sorted == expected_sequential)\n",
    "    \n",
    "    if is_sequential:\n",
    "        print(f\"   ‚úì {entity_name} IDs are already 0-based sequential - no remapping needed!\")\n",
    "        return df_list, None, False\n",
    "    \n",
    "    # Check if IDs are 1-based sequential (1, 2, 3, ...)\n",
    "    expected_sequential_1based = list(range(1, len(all_ids_sorted) + 1))\n",
    "    is_sequential_1based = (all_ids_sorted == expected_sequential_1based)\n",
    "    \n",
    "    if is_sequential_1based:\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs are 1-based sequential - will remap to 0-based\")\n",
    "    else:\n",
    "        # Calculate gaps\n",
    "        num_gaps = (all_ids_sorted[-1] - all_ids_sorted[0] + 1) - len(all_ids_sorted)\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs have gaps - will remap to 0-based sequential\")\n",
    "        print(f\"   ‚Ä¢ Number of gaps: {num_gaps}\")\n",
    "    \n",
    "    # Create mapping: old_id -> new_idx (0-based)\n",
    "    id_to_idx = {old_id: new_idx for new_idx, old_id in enumerate(all_ids_sorted)}\n",
    "    idx_to_id = {new_idx: old_id for old_id, new_idx in id_to_idx.items()}\n",
    "    \n",
    "    print(f\"\\n   Creating mapping...\")\n",
    "    print(f\"   ‚Ä¢ Example mappings:\")\n",
    "    sample_ids = all_ids_sorted[:5] + all_ids_sorted[-5:]\n",
    "    for old_id in sample_ids[:10]:  # Show first 5 and last 5\n",
    "        print(f\"      {entity_name}_id {old_id} ‚Üí {id_to_idx[old_id]}\")\n",
    "    \n",
    "    # Remap all DataFrames\n",
    "    print(f\"\\n   Remapping {len(df_list)} DataFrames...\")\n",
    "    remapped_dfs = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[id_column] = df_copy[id_column].map(id_to_idx)\n",
    "        remapped_dfs.append(df_copy)\n",
    "        print(f\"   ‚úì Remapped DataFrame {i+1}/{len(df_list)}\")\n",
    "    \n",
    "    return remapped_dfs, (id_to_idx, idx_to_id), True\n",
    "\n",
    "# 1. Remap player IDs\n",
    "print(f\"\\n1Ô∏è‚É£  Processing player IDs...\")\n",
    "player_dfs = [X_train, X_val, X_test, clean_data, player_side_info.reset_index()]\n",
    "remapped_player_dfs, player_mappings, player_remapped = check_and_remap_ids(\n",
    "    player_dfs, 'player_id', 'player'\n",
    ")\n",
    "\n",
    "if player_remapped:\n",
    "    X_train, X_val, X_test, clean_data, player_side_info_remapped = remapped_player_dfs\n",
    "    player_id_to_idx, player_idx_to_id = player_mappings\n",
    "    player_side_info = player_side_info_remapped.set_index('player_id')\n",
    "    print(f\"\\n   ‚úÖ Player ID remapping complete!\")\n",
    "else:\n",
    "    player_id_to_idx, player_idx_to_id = None, None\n",
    "\n",
    "# 2. Remap opening IDs\n",
    "print(f\"\\n2Ô∏è‚É£  Processing opening IDs...\")\n",
    "opening_dfs = [X_train, X_val, X_test, clean_data]\n",
    "remapped_opening_dfs, opening_mappings, opening_remapped = check_and_remap_ids(\n",
    "    opening_dfs, 'opening_id', 'opening'\n",
    ")\n",
    "\n",
    "if opening_remapped:\n",
    "    X_train, X_val, X_test, clean_data = remapped_opening_dfs\n",
    "    opening_id_to_idx, opening_idx_to_id = opening_mappings\n",
    "    print(f\"\\n   ‚úÖ Opening ID remapping complete!\")\n",
    "else:\n",
    "    opening_id_to_idx, opening_idx_to_id = None, None\n",
    "\n",
    "# 3. Spot checks to verify mappings\n",
    "print(f\"\\n3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\")\n",
    "print(f\"   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\")\n",
    "\n",
    "# Sample 10 entries: first, last, and 8 in between\n",
    "print(f\"\\n   Sampling 10 entries from X_train (before remapping was applied)...\")\n",
    "total_rows = len(X_train)\n",
    "# Get indices: first, last, and 8 evenly spaced in between\n",
    "sample_indices = [0]  # First row\n",
    "step = (total_rows - 1) // 9  # Divide remaining rows into 9 parts\n",
    "sample_indices.extend([min(i * step, total_rows - 1) for i in range(1, 9)])\n",
    "sample_indices.append(total_rows - 1)  # Last row\n",
    "\n",
    "print(f\"   ‚Ä¢ Sample indices: {sample_indices}\")\n",
    "\n",
    "# Store samples with their NEW (remapped) IDs\n",
    "samples = []\n",
    "for idx in sample_indices:\n",
    "    row = X_train.iloc[idx]\n",
    "    samples.append({\n",
    "        'index': idx,\n",
    "        'new_player_id': row['player_id'],\n",
    "        'new_opening_id': row['opening_id'],\n",
    "        'confidence': row['confidence']\n",
    "    })\n",
    "\n",
    "print(f\"\\n   Verification checks:\")\n",
    "print(f\"   {'#':<4} {'Row Idx':<10} {'New Player':<12} {'New Opening':<12} {'Confidence':<12} {'Status':<15}\")\n",
    "print(f\"   {'-'*4} {'-'*10} {'-'*12} {'-'*12} {'-'*12} {'-'*15}\")\n",
    "\n",
    "all_checks_passed = True\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    new_player_id = sample['new_player_id']\n",
    "    new_opening_id = sample['new_opening_id']\n",
    "    confidence = sample['confidence']\n",
    "    idx = sample['index']\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: New player ID is valid (0-based sequential)\n",
    "    if player_remapped:\n",
    "        player_valid = 0 <= new_player_id < len(player_id_to_idx)\n",
    "        checks.append((\"player_id\", player_valid))\n",
    "    else:\n",
    "        checks.append((\"player_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 2: New opening ID is valid (0-based sequential)\n",
    "    if opening_remapped:\n",
    "        opening_valid = 0 <= new_opening_id < len(opening_id_to_idx)\n",
    "        checks.append((\"opening_id\", opening_valid))\n",
    "    else:\n",
    "        checks.append((\"opening_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 3: Player exists in player_side_info\n",
    "    player_exists = new_player_id in player_side_info.index\n",
    "    checks.append((\"in_side_info\", player_exists))\n",
    "    \n",
    "    # Check 4: Opening exists in clean_data\n",
    "    opening_exists = new_opening_id in clean_data['opening_id'].values\n",
    "    checks.append((\"in_clean\", opening_exists))\n",
    "    \n",
    "    # All checks must pass\n",
    "    all_pass = all(check[1] for check in checks)\n",
    "    status = \"‚úì PASS\" if all_pass else f\"‚úó FAIL ({','.join([c[0] for c in checks if not c[1]])})\"\n",
    "    \n",
    "    if not all_pass:\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(f\"   {i:<4} {idx:<10} {new_player_id:<12} {new_opening_id:<12} {confidence:<12.4f} {status:<15}\")\n",
    "\n",
    "# Additional verification: check if we can reverse map to original IDs\n",
    "if player_remapped or opening_remapped:\n",
    "    print(f\"\\n   Reverse mapping verification (sample of 3 entries):\")\n",
    "    print(f\"   {'#':<4} {'New‚ÜíOld Player':<30} {'New‚ÜíOld Opening':<30}\")\n",
    "    print(f\"   {'-'*4} {'-'*30} {'-'*30}\")\n",
    "    \n",
    "    for i in [0, len(samples)//2, len(samples)-1]:  # First, middle, last\n",
    "        sample = samples[i]\n",
    "        \n",
    "        # Reverse map player\n",
    "        if player_remapped:\n",
    "            old_player = player_idx_to_id.get(sample['new_player_id'], 'NOT_FOUND')\n",
    "            player_str = f\"{sample['new_player_id']} ‚Üí {old_player}\"\n",
    "        else:\n",
    "            player_str = f\"{sample['new_player_id']} (unchanged)\"\n",
    "        \n",
    "        # Reverse map opening\n",
    "        if opening_remapped:\n",
    "            old_opening = opening_idx_to_id.get(sample['new_opening_id'], 'NOT_FOUND')\n",
    "            opening_str = f\"{sample['new_opening_id']} ‚Üí {old_opening}\"\n",
    "        else:\n",
    "            opening_str = f\"{sample['new_opening_id']} (unchanged)\"\n",
    "        \n",
    "        print(f\"   {i+1:<4} {player_str:<30} {opening_str:<30}\")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(f\"\\n   ‚úÖ All spot checks passed! ID mappings are correct.\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Some spot checks failed - investigate immediately!\")\n",
    "    raise ValueError(\"ID remapping verification failed!\")\n",
    "\n",
    "# 4. Summary\n",
    "print(f\"\\n4Ô∏è‚É£  Summary:\")\n",
    "print(f\"\\n   Player IDs:\")\n",
    "if player_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{player_idx_to_id[0]}, {player_idx_to_id[len(player_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(player_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "    \n",
    "print(f\"\\n   Opening IDs:\")\n",
    "if opening_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{opening_idx_to_id[0]}, {opening_idx_to_id[len(opening_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(opening_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "\n",
    "print(f\"\\n   Updated DataFrames:\")\n",
    "print(f\"   ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ X_val: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ clean_data: {clean_data.shape}\")\n",
    "print(f\"   ‚Ä¢ player_side_info: {player_side_info.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ID REMAPPING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Important:\")\n",
    "print(f\"   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\")\n",
    "print(f\"   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\")\n",
    "print(f\"   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\")\n",
    "print(f\"   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a580c0",
   "metadata": {},
   "source": [
    "## 4. Enumerate Categorical Variables\n",
    "\n",
    "I believe the only variable we need to enumerate here is `eco`. That's the broad categorization of a specific opening.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- One ECO code will have many openings\n",
    "- They're sorted by letter, then further by number. For instance, C21 and C44 are in the `C` family.\n",
    "- Maybe we make this side information?\n",
    "\n",
    "First, let's get some data on ECO codes to help us better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9504a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: ECO CODE STATISTICS\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,897,177\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 6325.7\n",
      "   ‚Ä¢ Median entries per ECO: 822.5\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 201193\n",
      "   ‚Ä¢ Std: 18377.3\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        510,079     17.61%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        1,047,700     36.16%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        987,082     34.07%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D        279,380      9.64%      ‚ñà‚ñà‚ñà\n",
      "   E         72,936      2.52%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00    201,193      6.94%      ‚ñà‚ñà\n",
      "   2      B01    197,219      6.81%      ‚ñà‚ñà\n",
      "   3      A40    125,678      4.34%      ‚ñà\n",
      "   4      C50    101,353      3.50%      ‚ñà\n",
      "   5      C00     87,225      3.01%      \n",
      "   6      C40     81,403      2.81%      \n",
      "   7      B06     73,295      2.53%      \n",
      "   8      C42     65,671      2.27%      \n",
      "   9      C44     60,489      2.09%      \n",
      "   10     B10     56,867      1.96%      \n",
      "   11     C41     55,370      1.91%      \n",
      "   12     B40     52,287      1.80%      \n",
      "   13     B12     48,802      1.68%      \n",
      "   14     A00     48,423      1.67%      \n",
      "   15     C02     47,943      1.65%      \n",
      "   16     D00     41,265      1.42%      \n",
      "   17     B21     37,782      1.30%      \n",
      "   18     A43     35,791      1.24%      \n",
      "   19     B32     33,859      1.17%      \n",
      "   20     A04     32,877      1.13%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      D62          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2      D29          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3      D98          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4      A97          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   5      D68          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   6      E78          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   7      E79          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   8      A63          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   9      A72          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   10     C76          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   11     D49          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   12     C94          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   13     E55          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   14     D84          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   15     A99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   16     B69          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   17     E96          2    ‚ñà‚ñà\n",
      "   18     A93          2    ‚ñà‚ñà\n",
      "   19     E36          1    ‚ñà\n",
      "   20     A98          1    ‚ñà\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   A07, A20, A24, A58, B13, B22, B53, C03, C43, C48, C66, D36, D74, D92, E10, E30, E59, E78, E96, E97\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   ‚Ä¢ Unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,172,882\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 448\n",
      "   ‚Ä¢ Total entries: 434,577\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 440\n",
      "   ‚Ä¢ Total entries: 289,718\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C94    0.6200             5\n",
      "   B77    0.5962           550\n",
      "   E96    0.5860             2\n",
      "   B71    0.5858           227\n",
      "   B85    0.5844            16\n",
      "   D57    0.5838            23\n",
      "   D29    0.5818             8\n",
      "   D49    0.5764             6\n",
      "   E99    0.5733            26\n",
      "   A63    0.5678             7\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   A57    0.4709         4,487\n",
      "   B70    0.4702         4,143\n",
      "   B58    0.4669            10\n",
      "   A58    0.4627           955\n",
      "   C76    0.4616             6\n",
      "   C38    0.4581         1,661\n",
      "   C99    0.4345            14\n",
      "   E55    0.3855             5\n",
      "   E36    0.3500             1\n",
      "   A98    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   A97    0.0079       0.0889             8\n",
      "   A93    0.0069       0.0829             2\n",
      "   C57    0.0068       0.0823        11,883\n",
      "   E45    0.0068       0.0823            17\n",
      "   C37    0.0044       0.0666         5,944\n",
      "   C51    0.0038       0.0619         3,972\n",
      "   C39    0.0036       0.0601         1,449\n",
      "   C56    0.0036       0.0597         9,532\n",
      "   C31    0.0036       0.0597        10,986\n",
      "   C52    0.0035       0.0592         1,910\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Most common family: B (1,047,700 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (201,193 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: ECO Code Statistics (no mutations, just exploration)\n",
    "\n",
    "print(\"STEP 4: ECO CODE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Missing ECO values: {clean_data['eco'].isna().sum()}\")\n",
    "\n",
    "# ECO value counts\n",
    "eco_counts = clean_data['eco'].value_counts().sort_index()\n",
    "print(f\"\\n2Ô∏è‚É£  Distribution of entries per ECO code:\")\n",
    "print(f\"   ‚Ä¢ Mean entries per ECO: {eco_counts.mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Median entries per ECO: {eco_counts.median():.1f}\")\n",
    "print(f\"   ‚Ä¢ Min entries: {eco_counts.min()}\")\n",
    "print(f\"   ‚Ä¢ Max entries: {eco_counts.max()}\")\n",
    "print(f\"   ‚Ä¢ Std: {eco_counts.std():.1f}\")\n",
    "\n",
    "# ECO by first letter (family)\n",
    "print(f\"\\n3Ô∏è‚É£  ECO families (by first letter):\")\n",
    "eco_families = clean_data['eco'].str[0].value_counts().sort_index()\n",
    "print(f\"\\n   {'Family':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "for family, count in eco_families.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.4)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {family:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Top 20 most common ECO codes\n",
    "print(f\"\\n4Ô∏è‚É£  Top 20 most common ECO codes:\")\n",
    "top_eco = clean_data['eco'].value_counts().head(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*12} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(top_eco.items(), 1):\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.3)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Bottom 20 least common ECO codes\n",
    "print(f\"\\n5Ô∏è‚É£  Bottom 20 least common ECO codes:\")\n",
    "bottom_eco = clean_data['eco'].value_counts().tail(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(bottom_eco.items(), 1):\n",
    "    bar_length = min(count, 30)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {bar}\")\n",
    "\n",
    "# Check for any unusual ECO codes\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of ECO codes:\")\n",
    "sample_eco = clean_data['eco'].drop_duplicates().sample(min(20, clean_data['eco'].nunique()), random_state=42).sort_values()\n",
    "print(f\"   {', '.join(sample_eco.values)}\")\n",
    "\n",
    "# ECO statistics by split\n",
    "print(f\"\\n8Ô∏è‚É£  ECO distribution across splits:\")\n",
    "print(f\"\\n   Train split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_train['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\n   Validation split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_val['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_val):,}\")\n",
    "val_new_eco = set(X_val['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(val_new_eco)}\")\n",
    "\n",
    "print(f\"\\n   Test split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_test['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_test):,}\")\n",
    "test_new_eco = set(X_test['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(test_new_eco)}\")\n",
    "\n",
    "# Average score by ECO code (top 10 and bottom 10)\n",
    "print(f\"\\n9Ô∏è‚É£  Average score by ECO code:\")\n",
    "eco_scores = clean_data.groupby('eco')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\n   Top 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.head(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(f\"\\n   Bottom 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.tail(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# ECO codes with high variance in scores\n",
    "print(f\"\\nüîü  ECO codes with highest score variance (may indicate difficulty):\")\n",
    "eco_variance = clean_data.groupby('eco')['score'].agg(['var', 'std', 'count']).sort_values('var', ascending=False).head(10)\n",
    "print(f\"\\n   {'ECO':<6} {'Variance':<12} {'Std Dev':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_variance.iterrows():\n",
    "    print(f\"   {eco:<6} {row['var']:<12.4f} {row['std']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ECO CODE STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Key takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Most common family: {eco_families.idxmax()} ({eco_families.max():,} entries)\")\n",
    "print(f\"   ‚Ä¢ Most common ECO: {top_eco.index[0]} ({top_eco.iloc[0]:,} entries)\")\n",
    "print(f\"   ‚Ä¢ ECO codes appear in all splits (good for training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbad18",
   "metadata": {},
   "source": [
    "## 4b. Create ECO Side Information\n",
    "\n",
    "**Why ECO is Side Information:**\n",
    "- ECO codes describe **opening characteristics**, not individual player-opening interactions\n",
    "- Similar to how player ratings describe players, ECO describes openings\n",
    "- Each opening has ONE ECO code (not per player-opening pair)\n",
    "\n",
    "**Implementation Strategy:**\n",
    "- Split ECO codes into two categorical features:\n",
    "  - `eco_letter`: A, B, C, D, or E ‚Üí encoded as integers 0-4\n",
    "  - `eco_number`: The numeric part (e.g., \"21\" from \"C21\") ‚Üí encoded as sequential integers\n",
    "- Store in a separate `opening_side_info` lookup table (indexed by opening_id)\n",
    "- Remove `eco` from train/test/val DataFrames (it's not interaction data)\n",
    "- During training, model will lookup opening_id ‚Üí (eco_letter, eco_number)\n",
    "\n",
    "**Why Split ECO into Letter and Number:**\n",
    "- ECO families (A-E) represent fundamentally different opening types:\n",
    "  - **A**: Flank openings (English, R√©ti, Bird's, etc.)\n",
    "  - **B**: Semi-Open games (Sicilian, French, Caro-Kann, etc.)\n",
    "  - **C**: Open games (King's pawn openings, Spanish, Italian, etc.)\n",
    "  - **D**: Closed games (Queen's Gambit variations)\n",
    "  - **E**: Indian defenses (King's Indian, Nimzo-Indian, etc.)\n",
    "- Numbers within each family represent variations (C20-C29, C30-C39, etc.)\n",
    "- Model can learn separate embeddings for family vs variation\n",
    "\n",
    "**Categorical Encoding:**\n",
    "- Both features will be treated as categorical (not ordinal)\n",
    "- Higher numbers don't mean \"better\" openings\n",
    "- Model will learn embedding vectors for each category\n",
    "- This allows the model to capture non-linear relationships between ECO codes and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64124475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4B: CREATE ECO SIDE INFORMATION\n",
      "============================================================\n",
      "   ‚úì Extracted 2,716 unique openings\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['B' 'C' 'A' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (577 openings)\n",
      "      'B' ‚Üí 1 (590 openings)\n",
      "      'C' ‚Üí 2 (894 openings)\n",
      "      'D' ‚Üí 3 (419 openings)\n",
      "      'E' ‚Üí 4 (236 openings)\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 289 openings\n",
      "      '40' (‚Üí 40):  98 openings\n",
      "      '20' (‚Üí 20):  70 openings\n",
      "      '01' (‚Üí  1):  70 openings\n",
      "      '42' (‚Üí 42):  68 openings\n",
      "      '02' (‚Üí  2):  67 openings\n",
      "      '45' (‚Üí 45):  64 openings\n",
      "      '10' (‚Üí 10):  63 openings\n",
      "      '21' (‚Üí 21):  61 openings\n",
      "      '15' (‚Üí 15):  58 openings\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2716, 2)\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ X_train before: (2172882, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (434577, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (289718, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   142          0               4               A04               \n",
      "   1267         2               27              C27               \n",
      "   1629         2               52              C52               \n",
      "   521          1               0               B00               \n",
      "   86           0               0               A00               \n",
      "   927          1               40              B40               \n",
      "   2333         4               70              E70               \n",
      "   1625         2               52              C52               \n",
      "   1631         2               53              C53               \n",
      "   214          0               13              A13               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            577     21.24%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            590     21.72%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            894     32.92%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            419     15.43%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            236      8.69%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2716, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 2,172,882 rows, 3 features\n",
      "   ‚Ä¢ X_val: 434,577 rows, 3 features\n",
      "   ‚Ä¢ X_test: 289,718 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,716 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n"
     ]
    }
   ],
   "source": [
    "# 4b. Create ECO side information and remove ECO from train/test/val DataFrames\n",
    "\n",
    "# Check if ECO processing has already been done\n",
    "if 'eco' not in X_train.columns and 'opening_side_info' in globals():\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 4B: ECO SIDE INFORMATION CREATION\")\n",
    "    print(\"\\n‚úì ECO column already removed from train/test/val data\")\n",
    "    print(f\"\\nOpening side info shape: {opening_side_info.shape}\")\n",
    "    print(f\"Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "    print(\"opening side info:\", opening_side_info.head().to_string())\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing ECO encoding statistics:\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_letter values: {opening_side_info['eco_letter_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_number values: {opening_side_info['eco_number_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ eco_letter range: [{opening_side_info['eco_letter_cat'].min()}, {opening_side_info['eco_letter_cat'].max()}]\")\n",
    "    print(f\"   ‚Ä¢ eco_number range: [{opening_side_info['eco_number_cat'].min()}, {opening_side_info['eco_number_cat'].max()}]\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing ECO encoding:\")\n",
    "    sample_data = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "    print(sample_data.to_string())\n",
    "else:\n",
    "    def create_eco_side_information(clean_data_df, X_train_df, X_val_df, X_test_df):\n",
    "        \"\"\"\n",
    "        Create ECO side information table and remove ECO from train/test/val DataFrames.\n",
    "        \n",
    "        ECO codes are opening-level features, not player-opening interaction features.\n",
    "        We split each ECO code (e.g., \"C21\") into:\n",
    "        - eco_letter: The letter part (A, B, C, D, or E)\n",
    "        - eco_number: The numeric part (e.g., 21)\n",
    "        \n",
    "        Both are encoded as sequential integers for use as categorical features in embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        clean_data_df : pd.DataFrame\n",
    "            Full cleaned data with opening_id and eco columns\n",
    "        X_train_df, X_val_df, X_test_df : pd.DataFrame\n",
    "            Train/val/test feature DataFrames (will be modified to remove 'eco')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 4B: CREATE ECO SIDE INFORMATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Extract unique opening ‚Üí ECO mappings\n",
    "        opening_eco_map = clean_data_df[['opening_id', 'eco']].drop_duplicates().set_index('opening_id')\n",
    "        print(f\"   ‚úì Extracted {len(opening_eco_map):,} unique openings\")\n",
    "        \n",
    "        # Verify one-to-one mapping\n",
    "        eco_per_opening = clean_data_df.groupby('opening_id')['eco'].nunique()\n",
    "        if (eco_per_opening > 1).any():\n",
    "            problematic = eco_per_opening[eco_per_opening > 1]\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(problematic)} openings have multiple ECO codes!\")\n",
    "            print(f\"   Problematic opening IDs: {problematic.index.tolist()[:10]}...\")\n",
    "        \n",
    "        # Split ECO into letter and number components\n",
    "        opening_eco_map['eco_letter_str'] = opening_eco_map['eco'].str[0]  # First character (A-E)\n",
    "        opening_eco_map['eco_number_str'] = opening_eco_map['eco'].str[1:]  # Remaining characters (numeric)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted letter and number components\")\n",
    "        print(f\"   ‚Ä¢ Unique letters: {opening_eco_map['eco_letter_str'].unique()}\")\n",
    "        print(f\"   ‚Ä¢ Unique numbers: {opening_eco_map['eco_number_str'].nunique()}\")\n",
    "        \n",
    "        # Create encoding mappings for eco_letter (A-E ‚Üí 0-4)\n",
    "        unique_letters = sorted(opening_eco_map['eco_letter_str'].unique())\n",
    "        eco_letter_to_int = {letter: idx for idx, letter in enumerate(unique_letters)}\n",
    "        eco_int_to_letter = {idx: letter for letter, idx in eco_letter_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_letter_cat'] = opening_eco_map['eco_letter_str'].map(eco_letter_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Letter encoding created:\")\n",
    "        for letter, idx in sorted(eco_letter_to_int.items()):\n",
    "            count = (opening_eco_map['eco_letter_str'] == letter).sum()\n",
    "            print(f\"      '{letter}' ‚Üí {idx} ({count:,} openings)\")\n",
    "        \n",
    "        # Create encoding mappings for eco_number (00-99 ‚Üí sequential integers)\n",
    "        unique_numbers = sorted(opening_eco_map['eco_number_str'].unique())\n",
    "        eco_number_to_int = {num: idx for idx, num in enumerate(unique_numbers)}\n",
    "        eco_int_to_number = {idx: num for num, idx in eco_number_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_number_cat'] = opening_eco_map['eco_number_str'].map(eco_number_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Number encoding created:\")\n",
    "        print(f\"      {len(unique_numbers)} unique numbers mapped to [0, {len(unique_numbers)-1}]\")\n",
    "        print(f\"      Range: '{unique_numbers[0]}' ‚Üí 0, ..., '{unique_numbers[-1]}' ‚Üí {len(unique_numbers)-1}\")\n",
    "        \n",
    "        # Show distribution of numbers\n",
    "        print(f\"\\n   Distribution of ECO numbers (top 10):\")\n",
    "        number_counts = opening_eco_map['eco_number_str'].value_counts().head(10)\n",
    "        for num, count in number_counts.items():\n",
    "            encoded = eco_number_to_int[num]\n",
    "            print(f\"      '{num}' (‚Üí {encoded:>2}): {count:>3} openings\")\n",
    "        \n",
    "        # Create final opening_side_info table\n",
    "        # Only keep the encoded categorical columns and rename them for clarity\n",
    "        opening_side_info = opening_eco_map[['eco_letter_cat', 'eco_number_cat']].copy()\n",
    "\n",
    "        print(f\"   ‚úì Created opening_side_info\")\n",
    "        print(f\"      ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"      ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "        # Verify all openings in train/val/test have ECO info\n",
    "        all_openings = set(X_train_df['opening_id'].unique()) | \\\n",
    "                       set(X_val_df['opening_id'].unique()) | \\\n",
    "                       set(X_test_df['opening_id'].unique())\n",
    "        \n",
    "        missing_openings = all_openings - set(opening_side_info.index)\n",
    "        if len(missing_openings) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits are missing ECO info!\")\n",
    "            print(f\"   Missing opening IDs: {sorted(list(missing_openings))[:10]}...\")\n",
    "        \n",
    "        # Remove ECO from train/val/test DataFrames\n",
    "        print(f\"   ‚Ä¢ X_train before: {X_train_df.shape}, columns: {list(X_train_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val before: {X_val_df.shape}, columns: {list(X_val_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test before: {X_test_df.shape}, columns: {list(X_test_df.columns)}\")\n",
    "        \n",
    "        X_train_clean = X_train_df.drop(columns=['eco'])\n",
    "        X_val_clean = X_val_df.drop(columns=['eco'])\n",
    "        X_test_clean = X_test_df.drop(columns=['eco'])\n",
    "        \n",
    "        # Sample data showing the transformation\n",
    "        print(f\"\\n8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\")\n",
    "        sample_openings = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "        \n",
    "        # For display purposes, reconstruct original values from the categorical encodings\n",
    "        print(f\"\\n   {'Opening ID':<12} {'Letter (Cat)':<15} {'Number (Cat)':<15} {'Reconstructed ECO':<18}\")\n",
    "        print(f\"   {'-'*12} {'-'*15} {'-'*15} {'-'*18}\")\n",
    "        for idx, row in sample_openings.iterrows():\n",
    "            letter_str = eco_int_to_letter[row['eco_letter_cat']]\n",
    "            number_str = eco_int_to_number[row['eco_number_cat']]\n",
    "            reconstructed = f\"{letter_str}{number_str}\"\n",
    "            print(f\"   {idx:<12} {row['eco_letter_cat']:<15} {row['eco_number_cat']:<15} {reconstructed:<18}\")\n",
    "        \n",
    "        # Show ECO family distribution\n",
    "        print(f\"\\n9Ô∏è‚É£  ECO family distribution in opening_side_info:\")\n",
    "        letter_dist = opening_side_info['eco_letter_cat'].value_counts().sort_index()\n",
    "        print(f\"\\n   {'Encoded':<8} {'Letter':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "        print(f\"   {'-'*8} {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "        for encoded_val, count in letter_dist.items():\n",
    "            letter = eco_int_to_letter[encoded_val]\n",
    "            pct = 100 * count / len(opening_side_info)\n",
    "            bar_length = int(pct * 0.4)\n",
    "            bar = '‚ñà' * bar_length\n",
    "            print(f\"   {encoded_val:<8} {letter:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "        \n",
    "        print(f\"\\nCreated: opening_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape[0]:,} rows, {X_train_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape[0]:,} rows, {X_val_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape[0]:,} rows, {X_test_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ opening_side_info: {len(opening_side_info):,} openings (one per opening)\")\n",
    "        print(f\"   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\")\n",
    "        print(f\"   ‚Ä¢ eco_letter_to_int: {eco_letter_to_int}\")\n",
    "        print(f\"   ‚Ä¢ eco_number_to_int: (dict with {len(eco_number_to_int)} entries)\")\n",
    "        print(f\"\\n   You'll need them to encode ECO codes for new openings at inference time.\")\n",
    "\n",
    "        return opening_side_info, eco_letter_to_int, eco_number_to_int, X_train_clean, X_val_clean, X_test_clean\n",
    "    \n",
    "    # Call the function\n",
    "    opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test = create_eco_side_information(\n",
    "        clean_data, X_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Create reverse mappings for decoding (needed for verification and debugging)\n",
    "    eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "    eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "    print(f\"\\n‚úì Created reverse mappings for ECO decoding:\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_letter: {len(eco_int_to_letter)} entries\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_number: {len(eco_int_to_number)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4b4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICATION: FINAL DATA STRUCTURE\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\n",
      "\n",
      "   X_train:\n",
      "   ‚Ä¢ Shape: (2172882, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ Sample:\n",
      "         player_id  opening_id  confidence\n",
      "1680611      27535         599    0.650350\n",
      "1723993      28227        1500    0.532710\n",
      "999505       16315          78    0.557522\n",
      "\n",
      "   X_val:\n",
      "   ‚Ä¢ Shape: (434577, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "   X_test:\n",
      "   ‚Ä¢ Shape: (289718, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Side Information Tables:\n",
      "\n",
      "   player_side_info (indexed by player_id):\n",
      "   ‚Ä¢ Shape: (48469, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Sample:\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.609318\n",
      "1          1.058742\n",
      "2          0.561165\n",
      "\n",
      "   opening_side_info (indexed by opening_id):\n",
      "   ‚Ä¢ Shape: (2716, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Sample:\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "530                      1               0\n",
      "564                      1               0\n",
      "571                      1               0\n",
      "\n",
      "3Ô∏è‚É£  Encoding Mappings (for inference):\n",
      "\n",
      "   eco_letter_map:\n",
      "      'A' ‚Üí 0\n",
      "      'B' ‚Üí 1\n",
      "      'C' ‚Üí 2\n",
      "      'D' ‚Üí 3\n",
      "      'E' ‚Üí 4\n",
      "\n",
      "   eco_number_map (first 10):\n",
      "      '00' ‚Üí 0\n",
      "      '01' ‚Üí 1\n",
      "      '02' ‚Üí 2\n",
      "      '03' ‚Üí 3\n",
      "      '04' ‚Üí 4\n",
      "      '05' ‚Üí 5\n",
      "      '06' ‚Üí 6\n",
      "      '07' ‚Üí 7\n",
      "      '08' ‚Üí 8\n",
      "      '09' ‚Üí 9\n",
      "      ... (100 total)\n",
      "\n",
      "4Ô∏è‚É£  Example: Lookup flow for a random train sample:\n",
      "\n",
      "   Sample interaction:\n",
      "   ‚Ä¢ player_id: 33158.0\n",
      "   ‚Ä¢ opening_id: 1058.0\n",
      "   ‚Ä¢ confidence: 0.1803\n",
      "\n",
      "   Player side info lookup:\n",
      "   ‚Ä¢ rating_z: -0.6467\n",
      "\n",
      "   Opening side info lookup:\n",
      "   ‚Ä¢ eco_letter_cat: 2 (letter: C)\n",
      "   ‚Ä¢ eco_number_cat: 0 (number: 00)\n",
      "\n",
      "============================================================\n",
      "‚úÖ VERIFICATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Ready for PyTorch tensor conversion:\n",
      "   ‚Ä¢ Features: player_id, opening_id, confidence\n",
      "   ‚Ä¢ Target: score (in y_train, y_val, y_test)\n",
      "   ‚Ä¢ Player side info: rating_z\n",
      "   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\n",
      "\n",
      "   All ECO and rating data is now properly separated as side information!\n",
      "\n",
      "   Side info tables are clean - only contain necessary model inputs!\n"
     ]
    }
   ],
   "source": [
    "# Verify final data structure after ECO processing\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION: FINAL DATA STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\")\n",
    "print(f\"\\n   X_train:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_train.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(X_train.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   X_val:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_val.columns)}\")\n",
    "\n",
    "print(f\"\\n   X_test:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_test.columns)}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Side Information Tables:\")\n",
    "\n",
    "print(f\"\\n   player_side_info (indexed by player_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(player_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   opening_side_info (indexed by opening_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(opening_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Encoding Mappings (for inference):\")\n",
    "print(f\"\\n   eco_letter_map:\")\n",
    "for k, v in sorted(eco_letter_map.items()):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "\n",
    "print(f\"\\n   eco_number_map (first 10):\")\n",
    "for i, (k, v) in enumerate(sorted(eco_number_map.items())[:10]):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "print(f\"      ... ({len(eco_number_map)} total)\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Example: Lookup flow for a random train sample:\")\n",
    "sample = X_train.sample(1, random_state=42).iloc[0]\n",
    "player_id = sample['player_id']\n",
    "opening_id = sample['opening_id']\n",
    "\n",
    "print(f\"\\n   Sample interaction:\")\n",
    "print(f\"   ‚Ä¢ player_id: {player_id}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {opening_id}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Player side info lookup:\")\n",
    "player_info = player_side_info.loc[player_id]\n",
    "print(f\"   ‚Ä¢ rating_z: {player_info['rating_z']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Opening side info lookup:\")\n",
    "opening_info = opening_side_info.loc[opening_id]\n",
    "print(f\"   ‚Ä¢ eco_letter_cat: {opening_info['eco_letter_cat']} (letter: {eco_int_to_letter[opening_info['eco_letter_cat']]})\")\n",
    "print(f\"   ‚Ä¢ eco_number_cat: {opening_info['eco_number_cat']} (number: {eco_int_to_number[opening_info['eco_number_cat']]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Ready for PyTorch tensor conversion:\")\n",
    "print(f\"   ‚Ä¢ Features: player_id, opening_id, confidence\")\n",
    "print(f\"   ‚Ä¢ Target: score (in y_train, y_val, y_test)\")\n",
    "print(f\"   ‚Ä¢ Player side info: rating_z\")\n",
    "print(f\"   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\")\n",
    "print(f\"\\n   All ECO and rating data is now properly separated as side information!\")\n",
    "print(f\"\\n   Side info tables are clean - only contain necessary model inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43752aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "============================================================\n",
      "STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\n",
      "============================================================\n",
      "Final DataFrame for correlation created.\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'score', 'confidence', 'rating_z', 'eco_letter_cat', 'eco_number_cat']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRELATIONS WITH TARGET VARIABLE (score)\n",
      "================================================================================\n",
      "eco_letter_cat    0.116489\n",
      "confidence        0.098032\n",
      "eco_number_cat    0.055375\n",
      "rating_z          0.042969\n"
     ]
    }
   ],
   "source": [
    "# Step 4c: Correlation Analysis of Processed Data\n",
    "\n",
    "%pip install seaborn --quiet\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Start with the main interaction data (already has remapped IDs, adjusted score, confidence)\n",
    "corr_df = clean_data[[\"player_id\", \"opening_id\", \"score\", \"confidence\"]].copy()\n",
    "\n",
    "# Merge player side information (rating_z)\n",
    "# player_side_info is indexed by the remapped player_id\n",
    "corr_df = corr_df.merge(\n",
    "    player_side_info[[\"rating_z\"]], left_on=\"player_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# Merge opening side information (eco categories)\n",
    "# opening_side_info is indexed by the remapped opening_id\n",
    "corr_df = corr_df.merge(\n",
    "    opening_side_info[[\"eco_letter_cat\", \"eco_number_cat\"]],\n",
    "    left_on=\"opening_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(f\"Final DataFrame for correlation created.\")\n",
    "print(f\"   ‚Ä¢ Columns: {corr_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "correlation_matrix = corr_df.corr().drop(columns=['player_id', 'opening_id']).drop(index=['player_id', 'opening_id'])\n",
    "\n",
    "\n",
    "# 3. Visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Model Features\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 4. Analyze correlations with the target variable 'score'\n",
    "print(\"CORRELATIONS WITH TARGET VARIABLE (score)\")\n",
    "print(\"=\" * 80)\n",
    "score_correlations = (\n",
    "    correlation_matrix[\"score\"].drop(\"score\").sort_values(ascending=False)\n",
    ")\n",
    "print(score_correlations.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831d7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "017515d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\n",
      "====================================================================================================\n",
      "   ‚Ä¢ Converting 81 NEW opening IDs to OLD database IDs for query\n",
      "   ‚Ä¢ Example: NEW ID 1058 ‚Üí OLD ID 1357\n",
      "#    Player   Opening   ECO (DB)   Reconstructed Match  Opening Name                                      \n",
      "====================================================================================================\n",
      "1    33158    1058      C00        C00           ‚úì      French Defense: La Bourdonnais Variation          \n",
      "2    46639    2070      D45        D45           ‚úì      Semi-Slav Defense: Accelerated Meran Variation    \n",
      "3    42206    542       B00        B00           ‚úì      Nimzowitsch Defense: Kennedy Variation, Links...  \n",
      "4    27777    1708      C65        C65           ‚úì      Ruy Lopez: Berlin Defense                         \n",
      "5    13541    2203      E12        E12           ‚úì      Nimzo-Indian Defense: Three Knights Variation...  \n",
      "6    16884    1223      C23        C23           ‚úì      Bishop's Opening: Khan Gambit                     \n",
      "7    27617    2456      C54        C54           ‚úì      Italian Game: Classical Variation, Giuoco Pia...  \n",
      "8    38108    1556      C46        C46           ‚úì      Three Knights Opening                             \n",
      "9    23515    1237      C25        C25           ‚úì      Vienna Game                                       \n",
      "10   44268    598       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "11   39363    630       B03        B03           ‚úì      Alekhine Defense                                  \n",
      "12   25724    1411      C40        C40           ‚úì      King's Pawn Game: McConnell Defense               \n",
      "13   34577    1286      C30        C30           ‚úì      King's Gambit Declined: Keene Defense             \n",
      "14   28791    2427      C50        C50           ‚úì      Italian Game: Paris Defense                       \n",
      "15   2963     2439      B50        B50           ‚úì      Sicilian Defense: Delayed Alapin Variation, w...  \n",
      "16   7475     1392      C39        C39           ‚úì      King's Gambit Accepted: Kieseritzky Gambit, B...  \n",
      "17   488      1869      D02        D02           ‚úì      Queen's Gambit Declined: Baltic Defense, Pseu...  \n",
      "18   31651    921       B40        B40           ‚úì      Sicilian Defense: French Variation, Normal        \n",
      "19   38494    897       B32        B32           ‚úì      Sicilian Defense: Open                            \n",
      "20   30574    848       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Dragon         \n",
      "21   37930    1281      C30        C30           ‚úì      King's Gambit                                     \n",
      "22   3435     1648      C55        C55           ‚úì      Italian Game: Two Knights Defense, Modern Bis...  \n",
      "23   8460     1556      C46        C46           ‚úì      Three Knights Opening                             \n",
      "24   6857     564       B00        B00           ‚úì      Owen Defense                                      \n",
      "25   26499    1406      C40        C40           ‚úì      King's Knight Opening                             \n",
      "26   31218    593       B01        B01           ‚úì      Scandinavian Defense: Main Line                   \n",
      "27   25595    705       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "28   30065    49        A00        A00           ‚úì      Polish Opening                                    \n",
      "29   39699    705       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "30   7037     1924      D11        D11           ‚úì      Slav Defense: Quiet Variation                     \n",
      "31   20018    471       A80        A80           ‚úì      Dutch Defense                                     \n",
      "32   23956    559       B00        B00           ‚úì      Nimzowitsch Defense: Scandinavian Variation, ...  \n",
      "33   13952    341       A43        A43           ‚úì      Benoni Defense: French Benoni                     \n",
      "34   30505    587       B01        B01           ‚úì      Scandinavian Defense: Gubinsky-Melts Defense      \n",
      "35   12406    1923      D11        D11           ‚úì      Slav Defense: Modern Line                         \n",
      "36   26415    598       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "37   13448    2450      D00        D00           ‚úì      Queen's Pawn Game: Accelerated London System      \n",
      "38   29792    1647      C55        C55           ‚úì      Italian Game: Two Knights Defense                 \n",
      "39   13751    1722      C66        C66           ‚úì      Ruy Lopez: Berlin Defense, Improved Steinitz ...  \n",
      "40   27637    1687      C61        C61           ‚úì      Ruy Lopez: Bird Variation                         \n",
      "41   46978    1407      C40        C40           ‚úì      King's Pawn Game: Busch-Gass Gambit               \n",
      "42   3219     1504      C44        C44           ‚úì      Ponziani Opening: Neumann Gambit                  \n",
      "43   18528    733       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Van der...  \n",
      "44   41092    1288      C30        C30           ‚úì      King's Gambit Declined: Mafia Defense             \n",
      "45   20749    1455      C41        C41           ‚úì      Philidor Defense: Steinitz Variation              \n",
      "46   36071    1413      C40        C40           ‚úì      Latvian Gambit Accepted                           \n",
      "47   21618    2430      B07        B07           ‚úì      King's Pawn Game: Mar√≥czy Defense                 \n",
      "48   684      1231      C24        C24           ‚úì      Bishop's Opening: Berlin Defense                  \n",
      "49   9688     633       B03        B03           ‚úì      Alekhine Defense: Four Pawns Attack               \n",
      "50   13049    807       B21        B21           ‚úì      Sicilian Defense: Smith-Morra Gambit              \n",
      "51   24586    230       A16        A16           ‚úì      English Opening: Anglo-Gr√ºnfeld Defense           \n",
      "52   9992     848       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Dragon         \n",
      "53   32475    542       B00        B00           ‚úì      Nimzowitsch Defense: Kennedy Variation, Links...  \n",
      "54   9412     620       B02        B02           ‚úì      Alekhine Defense: Scandinavian Variation, Ges...  \n",
      "55   21695    530       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "56   3306     1428      C41        C41           ‚úì      Philidor Defense                                  \n",
      "57   5962     652       B06        B06           ‚úì      Modern Defense                                    \n",
      "58   9397     705       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "59   41232    2406      A29        A29           ‚úì      English Opening: King's English Variation, Fo...  \n",
      "60   5216     1344      C34        C34           ‚úì      King's Gambit Accepted: Bonsch-Osmolovsky Var...  \n",
      "61   24270    1074      C00        C00           ‚úì      Rat Defense: Small Center Defense                 \n",
      "62   19299    564       B00        B00           ‚úì      Owen Defense                                      \n",
      "63   24645    600       B01        B01           ‚úì      Scandinavian Defense: Panov Transfer              \n",
      "64   36047    1954      D20        D20           ‚úì      Queen's Gambit Accepted: Central Variation, A...  \n",
      "65   17433    581       B01        B01           ‚úì      Scandinavian Defense: Blackburne Gambit           \n",
      "66   11333    666       B06        B06           ‚úì      Modern Defense: Three Pawns Attack                \n",
      "67   33266    1513      C44        C44           ‚úì      Scotch Game                                       \n",
      "68   28331    630       B03        B03           ‚úì      Alekhine Defense                                  \n",
      "69   37479    891       B32        B32           ‚úì      Sicilian Defense: Franco-Sicilian Variation       \n",
      "70   25332    1472      C42        C42           ‚úì      Russian Game: Damiano Variation                   \n",
      "71   32099    848       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Dragon         \n",
      "72   3468     728       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Botvinn...  \n",
      "73   10531    554       B00        B00           ‚úì      Nimzowitsch Defense: Scandinavian Variation, ...  \n",
      "74   11023    471       A80        A80           ‚úì      Dutch Defense                                     \n",
      "75   5979     1702      C64        C64           ‚úì      Ruy Lopez: Classical Variation                    \n",
      "76   2492     384       A46        A46           ‚úì      Queen's Pawn Game: Veresov Attack, Classical ...  \n",
      "77   8092     1472      C42        C42           ‚úì      Russian Game: Damiano Variation                   \n",
      "78   15476    666       B06        B06           ‚úì      Modern Defense: Three Pawns Attack                \n",
      "79   25234    910       B35        B35           ‚úì      Sicilian Defense: Accelerated Dragon, Modern ...  \n",
      "80   39069    1505      C44        C44           ‚úì      Ponziani Opening: Ponziani Countergambit          \n",
      "81   30156    542       B00        B00           ‚úì      Nimzowitsch Defense: Kennedy Variation, Links...  \n",
      "82   433      294       A40        A40           ‚úì      English Defense                                   \n",
      "83   24643    807       B21        B21           ‚úì      Sicilian Defense: Smith-Morra Gambit              \n",
      "84   14638    2463      C00        C00           ‚úì      French Defense: Franco-Sicilian Defense           \n",
      "85   13752    1588      C50        C50           ‚úì      Italian Game: Anti-Fried Liver Defense            \n",
      "86   12694    1456      C42        C42           ‚úì      Russian Game                                      \n",
      "87   39806    1592      C50        C50           ‚úì      Italian Game: Giuoco Pianissimo, Canal Variation  \n",
      "88   32888    581       B01        B01           ‚úì      Scandinavian Defense: Blackburne Gambit           \n",
      "89   10961    1896      D06        D06           ‚úì      Queen's Gambit Declined: Marshall Defense         \n",
      "90   14108    2007      D31        D31           ‚úì      Semi-Slav Defense: Accelerated Move Order         \n",
      "91   25397    1124      C11        C11           ‚úì      French Defense: Classical Variation, Steinitz...  \n",
      "92   24027    782       B20        B20           ‚úì      Sicilian Defense: Bowdler Attack                  \n",
      "93   41283    471       A80        A80           ‚úì      Dutch Defense                                     \n",
      "94   28188    1448      C41        C41           ‚úì      Philidor Defense: Nimzowitsch Variation           \n",
      "95   26998    2115      D59        D59           ‚úì      Queen's Gambit Declined: Tartakower Defense, ...  \n",
      "96   31150    1675      C59        C59           ‚úì      Italian Game: Two Knights Defense, Polerio De...  \n",
      "97   34542    1455      C41        C41           ‚úì      Philidor Defense: Steinitz Variation              \n",
      "98   29118    542       B00        B00           ‚úì      Nimzowitsch Defense: Kennedy Variation, Links...  \n",
      "99   35202    2251      E22        E22           ‚úì      Nimzo-Indian Defense: Spielmann Variation         \n",
      "100  4677     807       B21        B21           ‚úì      Sicilian Defense: Smith-Morra Gambit              \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Verification Results:\n",
      "   ‚Ä¢ Total samples: 100\n",
      "   ‚Ä¢ Matches: 100/100 (100.0%)\n",
      "   ‚Ä¢ Mismatches: 0\n",
      "\n",
      " All ECO codes reconstructed correctly\n",
      "   ‚Ä¢ ID remapping preserved all ECO code mappings\n"
     ]
    }
   ],
   "source": [
    "# Verification: Sample player-opening pairs with reconstructed ECO codes and opening names\n",
    "# Doing this to make sure that our ECO encoding/decoding is correct\n",
    "\n",
    "print(\"VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Sample random player-opening pairs from training data\n",
    "sample_size = 100\n",
    "sample_data = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "\n",
    "# Get unique opening IDs from sample (these are NEW remapped IDs)\n",
    "new_opening_ids = sample_data['opening_id'].unique()\n",
    "\n",
    "# Convert NEW opening IDs back to OLD database IDs for query\n",
    "if opening_remapped:\n",
    "    old_opening_ids = [opening_idx_to_id[int(new_id)] for new_id in new_opening_ids]\n",
    "    print(f\"   ‚Ä¢ Converting {len(new_opening_ids)} NEW opening IDs to OLD database IDs for query\")\n",
    "    print(f\"   ‚Ä¢ Example: NEW ID {new_opening_ids[0]} ‚Üí OLD ID {old_opening_ids[0]}\")\n",
    "else:\n",
    "    old_opening_ids = new_opening_ids\n",
    "    print(f\"   ‚Ä¢ No remapping was done - using opening IDs directly\")\n",
    "\n",
    "opening_ids_str = ','.join(map(str, old_opening_ids))\n",
    "\n",
    "# Query database for opening names using OLD IDs\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    opening_query = f\"\"\"\n",
    "        SELECT id, name, eco\n",
    "        FROM opening\n",
    "        WHERE id IN ({opening_ids_str})\n",
    "    \"\"\"\n",
    "    opening_names = pd.DataFrame(con.execute(opening_query).df()).set_index('id')\n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Create reverse mappings for ECO decoding\n",
    "eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "\n",
    "# Build verification table\n",
    "print(f\"{'#':<4} {'Player':<8} {'Opening':<9} {'ECO (DB)':<10} {'Reconstructed':<13} {'Match':<6} {'Opening Name':<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "matches = 0\n",
    "for i, (idx, row) in enumerate(sample_data.iterrows(), 1):\n",
    "    player_id = int(row['player_id'])\n",
    "    new_opening_id = int(row['opening_id'])\n",
    "    \n",
    "    # Convert NEW opening ID to OLD database ID for lookup\n",
    "    if opening_remapped:\n",
    "        old_opening_id = opening_idx_to_id[new_opening_id]\n",
    "    else:\n",
    "        old_opening_id = new_opening_id\n",
    "    \n",
    "    # Lookup opening side info using NEW ID\n",
    "    opening_info = opening_side_info.loc[new_opening_id]\n",
    "    \n",
    "    # Reconstruct ECO from encoded categorical values\n",
    "    eco_letter_encoded = opening_info['eco_letter_cat']\n",
    "    eco_number_encoded = opening_info['eco_number_cat']\n",
    "    \n",
    "    eco_letter_decoded = eco_int_to_letter[eco_letter_encoded]\n",
    "    eco_number_decoded = eco_int_to_number[eco_number_encoded]\n",
    "    \n",
    "    reconstructed_eco = f\"{eco_letter_decoded}{eco_number_decoded}\"\n",
    "    \n",
    "    # Get original ECO from database using OLD ID\n",
    "    db_eco = opening_names.loc[old_opening_id, 'eco']\n",
    "    opening_name = opening_names.loc[old_opening_id, 'name']\n",
    "    \n",
    "    # Check if they match\n",
    "    match = \"‚úì\" if reconstructed_eco == db_eco else \"‚úó\"\n",
    "    if reconstructed_eco == db_eco:\n",
    "        matches += 1\n",
    "    \n",
    "    # Truncate opening name if too long\n",
    "    if len(opening_name) > 48:\n",
    "        opening_name = opening_name[:45] + \"...\"\n",
    "    \n",
    "    # Display using NEW opening ID (what's in the data now)\n",
    "    print(f\"{i:<4} {player_id:<8} {new_opening_id:<9} {db_eco:<10} {reconstructed_eco:<13} {match:<6} {opening_name:<50}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n‚úÖ Verification Results:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(sample_data)}\")\n",
    "print(f\"   ‚Ä¢ Matches: {matches}/{len(sample_data)} ({100*matches/len(sample_data):.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Mismatches: {len(sample_data) - matches}\")\n",
    "\n",
    "if matches == len(sample_data):\n",
    "    print(f\"\\n All ECO codes reconstructed correctly\")\n",
    "    print(f\"   ‚Ä¢ ID remapping preserved all ECO code mappings\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Some ECO codes did not match. Investigate mismatches above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf4878",
   "metadata": {},
   "source": [
    "## 5. Data Verification and Examination\n",
    "We're almost there. Let's examine our data structures to check for any obvious flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f76afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train \n",
      "          player_id  opening_id  confidence\n",
      "1680611      27535         599    0.650350\n",
      "1723993      28227        1500    0.532710\n",
      "999505       16315          78    0.557522\n",
      "2695812      44314         963    0.315068\n",
      "218410        3548        1288    0.253731\n",
      "============================================================\n",
      "X_val \n",
      "          player_id  opening_id  confidence\n",
      "2727998      44899        2427    0.504950\n",
      "1045809      17082        1411    0.468085\n",
      "27238          462        2414    0.242424\n",
      "1628427      26693        2090    0.295775\n",
      "1532631      25111         965    0.253731\n",
      "============================================================\n",
      "X_test \n",
      "          player_id  opening_id  confidence\n",
      "946223       15424         568    0.431818\n",
      "1472537      24158         399    0.180328\n",
      "773434       12570         692    0.375000\n",
      "1093775      17895        1345    0.751244\n",
      "427701        6964        2461    0.489796\n",
      "============================================================\n",
      "y_train \n",
      " 1680611    0.514827\n",
      "1723993    0.528596\n",
      "999505     0.462689\n",
      "2695812    0.538425\n",
      "218410     0.493765\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_val \n",
      " 2727998    0.527043\n",
      "1045809    0.564437\n",
      "27238      0.497873\n",
      "1628427    0.519393\n",
      "1532631    0.481872\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_test \n",
      " 946223     0.607061\n",
      "1472537    0.501109\n",
      "773434     0.475295\n",
      "1093775    0.534658\n",
      "427701     0.532926\n",
      "Name: score, dtype: float64\n",
      "player_side_info \n",
      "            rating_z\n",
      "player_id          \n",
      "0          0.609318\n",
      "1          1.058742\n",
      "2          0.561165\n",
      "3          0.617343\n",
      "4         -2.167481\n",
      "============================================================\n",
      "opening_side_info \n",
      "             eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "530                      1               0\n",
      "564                      1               0\n",
      "571                      1               0\n",
      "578                      1               1\n",
      "588                      1               1\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train \\n\", X_train.head())\n",
    "print(\"=\"*60)\n",
    "print(\"X_val \\n\", X_val.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"X_test \\n\", X_test.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_train \\n\", y_train.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_val \\n\", y_val.head())\n",
    "print(\"=\" * 60) \n",
    "print(\"y_test \\n\", y_test.head())\n",
    "\n",
    "# Now side information\n",
    "print(\"player_side_info \\n\", player_side_info.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"opening_side_info \\n\", opening_side_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52936cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANED SIDE INFORMATION TABLES\n",
      "============================================================\n",
      "\n",
      "üìä player_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (48469, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Index: player_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.609318\n",
      "1          1.058742\n",
      "2          0.561165\n",
      "3          0.617343\n",
      "4         -2.167481\n",
      "\n",
      "üìä opening_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (2716, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Index: opening_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "530                      1               0\n",
      "564                      1               0\n",
      "571                      1               0\n",
      "578                      1               1\n",
      "588                      1               1\n"
     ]
    }
   ],
   "source": [
    "# Final verification: Display cleaned side info tables\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANED SIDE INFORMATION TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä player_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: player_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(player_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\nüìä opening_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: opening_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(opening_side_info.head().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19883cba",
   "metadata": {},
   "source": [
    "## Step 5: Convert to PyTorch Tensors\n",
    "\n",
    "**What are tensors?**\n",
    "Tensors are PyTorch's version of arrays - just multi-dimensional data structures optimized for deep learning. Think of them like fancy NumPy arrays that can run on GPUs.\n",
    "\n",
    "**What we need to convert:**\n",
    "\n",
    "1. **Main features** (X_train, X_val, X_test):\n",
    "   - `player_id` ‚Üí long tensor (integer IDs)\n",
    "   - `opening_id` ‚Üí long tensor (integer IDs)\n",
    "   - `confidence` ‚Üí float tensor (weights for loss function)\n",
    "\n",
    "2. **Targets** (y_train, y_val, y_test):\n",
    "   - `score` ‚Üí float tensor (what we're predicting)\n",
    "\n",
    "3. **Player side info** (player_side_info):\n",
    "   - `rating_z` ‚Üí float tensor (normalized ratings)\n",
    "   - Indexed by player_id for fast lookup\n",
    "\n",
    "4. **Opening side info** (opening_side_info):\n",
    "   - `eco_letter_cat` ‚Üí long tensor (categorical)\n",
    "   - `eco_number_cat` ‚Üí long tensor (categorical)\n",
    "   - Indexed by opening_id for fast lookup\n",
    "\n",
    "**Why these data types?**\n",
    "- `long` (int64): For IDs and categorical features that will be embedded\n",
    "- `float` (float32): For continuous values like scores, confidence, and normalized ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "350be514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4c/gt84bby56mqdnzwsg72zvd680000gn/T/ipykernel_38287/1794647647.py\", line 6, in <module>\n",
      "    import torch\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# It says subprocess is unused but complains at me if I don't import it\n",
    "# Something to do with incompatible versions of blah blah blah\n",
    "# Don't really care as long as it works\n",
    "import subprocess\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda01b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: CONVERT TO PYTORCH TENSORS\n",
      "============================================================\n",
      "   ‚Ä¢ CUDA available: False\n",
      "   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\n",
      "\n",
      "0Ô∏è‚É£  Index alignment sanity checks...\n",
      "   ‚úì player_side_info index is sorted\n",
      "   ‚Ä¢ Index range: [0, 48468]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\n",
      "   ‚Ä¢ Index range: [0, 2715]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "\n",
      "   Checking if indices are contiguous 0-based...\n",
      "   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\n",
      "   ‚Ä¢ Training tensors:\n",
      "   ‚Ä¢ player_ids_train: torch.Size([2172882]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_train: torch.Size([2172882]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_train: torch.Size([2172882]), dtype=torch.float32\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([434577]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([434577]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([434577]), dtype=torch.float32\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([289718]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([289718]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([289718]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   ‚Ä¢ scores_train: torch.Size([2172882]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([434577]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([289718]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1138, 1.0000]\n",
      "   ‚Ä¢ Val: [0.1004, 0.8251]\n",
      "   ‚Ä¢ Test: [0.2162, 0.7920]\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([48469]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([48469]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.2678, 4.2448]\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2716]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2716]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2716]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "   ‚Ä¢ Train: 2,172,882 samples\n",
      "   ‚Ä¢ Val: 434,577 samples\n",
      "   ‚Ä¢ Test: 289,718 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 48,469\n",
      "   ‚Ä¢ Openings: 2,716\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 48469 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2716 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 66.54 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Convert all data to PyTorch tensors\n",
    "\n",
    "print(\"STEP 5: CONVERT TO PYTORCH TENSORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\")\n",
    "\n",
    "# 0. Index alignment sanity checks\n",
    "print(f\"\\n0Ô∏è‚É£  Index alignment sanity checks...\")\n",
    "\n",
    "# Check player_side_info is properly indexed\n",
    "player_ids_sorted = sorted(player_side_info.index.values)\n",
    "if player_ids_sorted != list(player_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  player_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì player_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{player_side_info.index.min()}, {player_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {player_side_info.index.dtype}\")\n",
    "\n",
    "# Check opening_side_info is properly indexed\n",
    "opening_ids_sorted = sorted(opening_side_info.index.values)\n",
    "if opening_ids_sorted != list(opening_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì opening_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{opening_side_info.index.min()}, {opening_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {opening_side_info.index.dtype}\")\n",
    "\n",
    "# CRITICAL: Check if indices are contiguous 0-based\n",
    "# If opening_side_info.index = [0, 1, 2, ..., N-1], then we can use opening_id as direct array index\n",
    "# If not (e.g., [10, 15, 23, ...]), we'll need a mapping dictionary\n",
    "print(f\"\\n   Checking if indices are contiguous 0-based...\")\n",
    "player_ids_are_contiguous = (player_side_info.index == range(len(player_side_info))).all()\n",
    "opening_ids_are_contiguous = (opening_side_info.index == range(len(opening_side_info))).all()\n",
    "\n",
    "\n",
    "if not player_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Player IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "if not opening_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "\n",
    "# 1. Convert main features (train/val/test)\n",
    "\n",
    "# Train set\n",
    "player_ids_train = torch.tensor(X_train['player_id'].values, dtype=torch.long)\n",
    "opening_ids_train = torch.tensor(X_train['opening_id'].values, dtype=torch.long)\n",
    "confidence_train = torch.tensor(X_train['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ Training tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_train: {player_ids_train.shape}, dtype={player_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_train: {opening_ids_train.shape}, dtype={opening_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_train: {confidence_train.shape}, dtype={confidence_train.dtype}\")\n",
    "\n",
    "# Validation set\n",
    "player_ids_val = torch.tensor(X_val['player_id'].values, dtype=torch.long)\n",
    "opening_ids_val = torch.tensor(X_val['opening_id'].values, dtype=torch.long)\n",
    "confidence_val = torch.tensor(X_val['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Validation tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_val: {player_ids_val.shape}, dtype={player_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_val: {opening_ids_val.shape}, dtype={opening_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_val: {confidence_val.shape}, dtype={confidence_val.dtype}\")\n",
    "\n",
    "# Test set\n",
    "player_ids_test = torch.tensor(X_test['player_id'].values, dtype=torch.long)\n",
    "opening_ids_test = torch.tensor(X_test['opening_id'].values, dtype=torch.long)\n",
    "confidence_test = torch.tensor(X_test['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Test tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_test: {player_ids_test.shape}, dtype={player_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_test: {opening_ids_test.shape}, dtype={opening_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_test: {confidence_test.shape}, dtype={confidence_test.dtype}\")\n",
    "\n",
    "# 2. Convert targets (scores)\n",
    "print(f\"\\n2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\")\n",
    "\n",
    "scores_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "scores_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "scores_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ scores_train: {scores_train.shape}, dtype={scores_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_val: {scores_val.shape}, dtype={scores_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_test: {scores_test.shape}, dtype={scores_test.dtype}\")\n",
    "\n",
    "print(f\"\\n   Score ranges (sanity check):\")\n",
    "print(f\"   ‚Ä¢ Train: [{scores_train.min():.4f}, {scores_train.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Val: [{scores_val.min():.4f}, {scores_val.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Test: [{scores_test.min():.4f}, {scores_test.max():.4f}]\")\n",
    "\n",
    "# 3. Convert player side information\n",
    "\n",
    "# Create tensor of all player ratings (indexed by player_id)\n",
    "# Since player_side_info is indexed by player_id, we need to ensure coverage\n",
    "player_ratings_tensor = torch.tensor(player_side_info['rating_z'].values, dtype=torch.float32)\n",
    "player_ids_in_side_info = torch.tensor(player_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ player_ratings_tensor: {player_ratings_tensor.shape}, dtype={player_ratings_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ player_ids_in_side_info: {player_ids_in_side_info.shape}, dtype={player_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings_tensor.min():.4f}, {player_ratings_tensor.max():.4f}]\")\n",
    "\n",
    "# Verify all player IDs in train/val/test are covered\n",
    "all_player_ids = torch.cat([player_ids_train, player_ids_val, player_ids_test]).unique()\n",
    "missing_players = set(all_player_ids.tolist()) - set(player_ids_in_side_info.tolist())\n",
    "if len(missing_players) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in splits missing from side_info!\")\n",
    "\n",
    "# 4. Convert opening side information\n",
    "print(f\"\\n4Ô∏è‚É£  Converting opening side information...\")\n",
    "\n",
    "# Verify column names exist (using eco_letter_cat and eco_number_cat consistently)\n",
    "if 'eco_letter_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_letter_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "if 'eco_number_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_number_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "\n",
    "# Create tensors for opening ECO features (indexed by opening_id)\n",
    "opening_eco_letter_tensor = torch.tensor(opening_side_info['eco_letter_cat'].values, dtype=torch.long)\n",
    "opening_eco_number_tensor = torch.tensor(opening_side_info['eco_number_cat'].values, dtype=torch.long)\n",
    "opening_ids_in_side_info = torch.tensor(opening_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ opening_eco_letter_tensor: {opening_eco_letter_tensor.shape}, dtype={opening_eco_letter_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_eco_number_tensor: {opening_eco_number_tensor.shape}, dtype={opening_eco_number_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_in_side_info: {opening_ids_in_side_info.shape}, dtype={opening_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ ECO letter range: [{opening_eco_letter_tensor.min()}, {opening_eco_letter_tensor.max()}]\")\n",
    "print(f\"   ‚Ä¢ ECO number range: [{opening_eco_number_tensor.min()}, {opening_eco_number_tensor.max()}]\")\n",
    "\n",
    "# Verify all opening IDs in train/val/test are covered\n",
    "all_opening_ids = torch.cat([opening_ids_train, opening_ids_val, opening_ids_test]).unique()\n",
    "missing_openings = set(all_opening_ids.tolist()) - set(opening_ids_in_side_info.tolist())\n",
    "if len(missing_openings) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits missing from side_info!\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(f\"\\n5Ô∏è‚É£  Summary statistics:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(scores_train):,} samples\")\n",
    "print(f\"   ‚Ä¢ Val: {len(scores_val):,} samples\")\n",
    "print(f\"   ‚Ä¢ Test: {len(scores_test):,} samples\")\n",
    "\n",
    "print(f\"\\n   Unique entities:\")\n",
    "print(f\"   ‚Ä¢ Players: {len(player_ids_in_side_info):,}\")\n",
    "print(f\"   ‚Ä¢ Openings: {len(opening_ids_in_side_info):,}\")\n",
    "\n",
    "print(f\"\\n   Vocabulary sizes (for embedding layers):\")\n",
    "print(f\"   ‚Ä¢ num_players: {player_ids_in_side_info.max() + 1} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_openings: {opening_ids_in_side_info.max() + 1} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_letters: {opening_eco_letter_tensor.max() + 1} (max eco_letter_cat + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_numbers: {opening_eco_number_tensor.max() + 1} (max eco_number_cat + 1)\")\n",
    "\n",
    "# 6. Memory usage (approximate - using simple calculation)\n",
    "print(f\"\\n6Ô∏è‚É£  Approximate memory usage:\")\n",
    "def tensor_memory_mb(t):\n",
    "    \"\"\"Calculate tensor memory in MB\"\"\"\n",
    "    return (t.element_size() * t.nelement()) / (1024 * 1024)\n",
    "\n",
    "tensors = [\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train,\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val,\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test,\n",
    "    player_ratings_tensor, opening_eco_letter_tensor, opening_eco_number_tensor\n",
    "]\n",
    "total_memory_mb = sum(tensor_memory_mb(t) for t in tensors)\n",
    "print(f\"   ‚Ä¢ Total tensor memory: {total_memory_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TENSOR CONVERSION COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fc308",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup\n",
    "\n",
    "Define all constants, loss functions, optimizer, and helper functions for training.\n",
    "\n",
    "**What we need to set up:**\n",
    "\n",
    "1. **Constants and Hyperparameters:**\n",
    "   - `NUM_FACTORS`: Dimensionality of latent factor embeddings (e.g., 50)\n",
    "   - `LEARNING_RATE`: Step size for SGD optimizer (e.g., 0.01)\n",
    "   - `BATCH_SIZE`: Number of samples per training batch (e.g., 1024)\n",
    "   - `N_EPOCHS`: Number of full passes through training data (e.g., 10)\n",
    "   - Random seeds for reproducibility\n",
    "\n",
    "2. **Loss Functions:**\n",
    "   - MSE (Mean Squared Error): Main training loss\n",
    "   - RMSE (Root Mean Squared Error): Evaluation metric\n",
    "   - Confidence-weighted loss: Down-weight uncertain predictions\n",
    "\n",
    "3. **Optimizer:**\n",
    "   - SGD (Stochastic Gradient Descent) with momentum\n",
    "\n",
    "4. **Helper Functions:**\n",
    "   - `train_one_epoch()`: Train for one epoch\n",
    "   - `evaluate_model()`: Evaluate on validation/test set\n",
    "   - `calculate_rmse()`: Compute RMSE metric\n",
    "   - `save_checkpoint()`: Save model state\n",
    "\n",
    "5. **Logging:**\n",
    "   - Progress bars with ETA\n",
    "   - Loss tracking per epoch\n",
    "   - Model checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54d13c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\n",
      "============================================================\n",
      "\n",
      "üìã Hyperparameters:\n",
      "   ‚Ä¢ NUM_FACTORS: 50\n",
      "   ‚Ä¢ LEARNING_RATE: 0.01\n",
      "   ‚Ä¢ MOMENTUM: 0.9\n",
      "   ‚Ä¢ BATCH_SIZE: 1024\n",
      "   ‚Ä¢ N_EPOCHS: 20\n",
      "   ‚Ä¢ WEIGHT_DECAY: 0.0\n",
      "\n",
      "üé≤ Random Seed:\n",
      "   ‚Ä¢ RANDOM_SEED: 42\n",
      "\n",
      "üíª Device Configuration:\n",
      "   ‚Ä¢ DEVICE: cpu\n",
      "\n",
      "üíæ Model Saving:\n",
      "   ‚Ä¢ Save directory: /Users/a/Documents/personalprojects/chess-opening-recommender/data/models\n",
      "   ‚úì Python random seed set to 42\n",
      "   ‚úì NumPy random seed set to 42\n",
      "   ‚úì PyTorch CPU random seed set to 42\n",
      "\n",
      "üìä Vocabulary Sizes (for embedding layers):\n",
      "   ‚Ä¢ NUM_PLAYERS: 48,469 (max player_id + 1)\n",
      "   ‚Ä¢ NUM_OPENINGS: 2,716 (max opening_id + 1)\n",
      "   ‚Ä¢ NUM_ECO_LETTERS: 5 (ECO letter categories)\n",
      "   ‚Ä¢ NUM_ECO_NUMBERS: 100 (ECO number categories)\n",
      "\n",
      "üìà Dataset Statistics:\n",
      "   ‚Ä¢ Train samples: 2,172,882\n",
      "   ‚Ä¢ Validation samples: 434,577\n",
      "   ‚Ä¢ Test samples: 289,718\n",
      "   ‚Ä¢ Total samples: 2,897,177\n",
      "\n",
      "   ‚Ä¢ Batches per epoch: 2,121\n",
      "   ‚Ä¢ Training iterations (total): 42,420\n",
      "\n",
      "============================================================\n",
      "‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\n"
     ]
    }
   ],
   "source": [
    "# Step 6a: Define Hyperparameters and Constants\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HYPERPARAMETERS AND CONSTANTS CONFIG\n",
    "# ========================================\n",
    "\n",
    "NUM_FACTORS = 50  # Dimensionality of latent factor embeddings\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9  # SGD momentum\n",
    "BATCH_SIZE = 1024 \n",
    "N_EPOCHS = 20\n",
    "\n",
    "# Regularization (if needed)\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_SAVE_DIR = Path.cwd().parent / \"data\" / \"models\"  # Saves to projectroot/data/models\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"\\nüìã Hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ NUM_FACTORS: {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ LEARNING_RATE: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ MOMENTUM: {MOMENTUM}\")\n",
    "print(f\"   ‚Ä¢ BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ N_EPOCHS: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ WEIGHT_DECAY: {WEIGHT_DECAY}\")\n",
    "\n",
    "print(f\"\\nüé≤ Random Seed:\")\n",
    "print(f\"   ‚Ä¢ RANDOM_SEED: {RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\nüíª Device Configuration:\")\n",
    "print(f\"   ‚Ä¢ DEVICE: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ‚Ä¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüíæ Model Saving:\")\n",
    "print(f\"   ‚Ä¢ Save directory: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# ========================================\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# ========================================\n",
    "\n",
    "# Python random\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì Python random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì NumPy random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# PyTorch random\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì PyTorch CPU random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU\n",
    "    print(f\"   ‚úì PyTorch GPU random seed set to {RANDOM_SEED}\")\n",
    "    \n",
    "    # Additional CUDA reproducibility settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"   ‚úì CUDA deterministic mode enabled\")\n",
    "\n",
    "# ========================================\n",
    "# CALCULATE VOCABULARY SIZES\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Sizes (for embedding layers):\")\n",
    "\n",
    "# Calculate from our tensors\n",
    "NUM_PLAYERS = int(player_ids_in_side_info.max()) + 1\n",
    "NUM_OPENINGS = int(opening_ids_in_side_info.max()) + 1\n",
    "NUM_ECO_LETTERS = int(opening_eco_letter_tensor.max()) + 1\n",
    "NUM_ECO_NUMBERS = int(opening_eco_number_tensor.max()) + 1\n",
    "\n",
    "print(f\"   ‚Ä¢ NUM_PLAYERS: {NUM_PLAYERS:,} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_OPENINGS: {NUM_OPENINGS:,} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_LETTERS: {NUM_ECO_LETTERS} (ECO letter categories)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_NUMBERS: {NUM_ECO_NUMBERS} (ECO number categories)\")\n",
    "\n",
    "# ========================================\n",
    "# DATASET STATISTICS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Train samples: {len(scores_train):,}\")\n",
    "print(f\"   ‚Ä¢ Validation samples: {len(scores_val):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(scores_test):,}\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(scores_train) + len(scores_val) + len(scores_test):,}\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Batches per epoch: {len(scores_train) // BATCH_SIZE:,}\")\n",
    "print(f\"   ‚Ä¢ Training iterations (total): {(len(scores_train) // BATCH_SIZE) * N_EPOCHS:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a93e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6B: DEFINE LOSS FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "üß™ Testing loss functions with dummy data...\n",
      "\n",
      "   Dummy data:\n",
      "   ‚Ä¢ Predictions: [0.5, 0.6000000238418579, 0.699999988079071, 0.800000011920929]\n",
      "   ‚Ä¢ Targets: [0.550000011920929, 0.6200000047683716, 0.6800000071525574, 0.75]\n",
      "   ‚Ä¢ Confidence: [0.800000011920929, 0.8999999761581421, 0.699999988079071, 1.0]\n",
      "\n",
      "   MSE (unweighted): 0.001450\n",
      "   MSE (weighted): 0.001512\n",
      "\n",
      "   RMSE (unweighted): 0.038079\n",
      "   RMSE (weighted): 0.038881\n",
      "\n",
      "   calculate_rmse() result: 0.038881\n",
      "   ‚úì Returns Python float (not tensor): <class 'float'>\n",
      "\n",
      "============================================================\n",
      "‚úÖ LOSS FUNCTIONS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6b: Define Loss Functions\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "print(\"STEP 6B: DEFINE LOSS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "def mse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss with optional confidence weighting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        Higher confidence = larger loss contribution\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Compute squared error\n",
    "    squared_error = (predictions - targets) ** 2\n",
    "    \n",
    "    # Apply confidence weighting if provided\n",
    "    if confidence_weights is not None:\n",
    "        weighted_squared_error = squared_error * confidence_weights\n",
    "        # Average over all samples (sum of weighted errors / sum of weights)\n",
    "        loss = weighted_squared_error.sum() / confidence_weights.sum()\n",
    "    else:\n",
    "        # Standard MSE (average over all samples)\n",
    "        loss = squared_error.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def rmse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error loss.\n",
    "    \n",
    "    This is the square root of MSE and is our primary evaluation metric.\n",
    "    RMSE is in the same units as the target (score), making it interpretable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar RMSE value\n",
    "    \"\"\"\n",
    "    mse = mse_loss(predictions, targets, confidence_weights)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def calculate_rmse(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Calculate RMSE metric (convenience function for evaluation).\n",
    "    \n",
    "    This is the same as rmse_loss but with clearer naming for evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        RMSE value (Python float, not tensor)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Don't compute gradients for evaluation\n",
    "        rmse = rmse_loss(predictions, targets, confidence_weights)\n",
    "    return rmse.item()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüß™ Testing loss functions with dummy data...\")\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_predictions = torch.tensor([0.5, 0.6, 0.7, 0.8], dtype=torch.float32)\n",
    "dummy_targets = torch.tensor([0.55, 0.62, 0.68, 0.75], dtype=torch.float32)\n",
    "dummy_confidence = torch.tensor([0.8, 0.9, 0.7, 1.0], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Dummy data:\")\n",
    "print(f\"   ‚Ä¢ Predictions: {dummy_predictions.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Targets: {dummy_targets.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Confidence: {dummy_confidence.tolist()}\")\n",
    "\n",
    "# Test MSE without confidence weighting\n",
    "mse_unweighted = mse_loss(dummy_predictions, dummy_targets)\n",
    "print(f\"\\n   MSE (unweighted): {mse_unweighted.item():.6f}\")\n",
    "\n",
    "# Test MSE with confidence weighting\n",
    "mse_weighted = mse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"   MSE (weighted): {mse_weighted.item():.6f}\")\n",
    "\n",
    "# Test RMSE\n",
    "rmse_unweighted = rmse_loss(dummy_predictions, dummy_targets)\n",
    "rmse_weighted = rmse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   RMSE (unweighted): {rmse_unweighted.item():.6f}\")\n",
    "print(f\"   RMSE (weighted): {rmse_weighted.item():.6f}\")\n",
    "\n",
    "# Test calculate_rmse\n",
    "rmse_eval = calculate_rmse(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   calculate_rmse() result: {rmse_eval:.6f}\")\n",
    "print(f\"   ‚úì Returns Python float (not tensor): {type(rmse_eval)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LOSS FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35e00b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Creating PyTorch Datasets...\n",
      "   ‚úì Train dataset: 2,172,882 samples\n",
      "   ‚úì Validation dataset: 434,577 samples\n",
      "   ‚úì Test dataset: 289,718 samples\n",
      "\n",
      "2Ô∏è‚É£  Testing dataset access...\n",
      "   Sample from train_dataset[0]:\n",
      "   ‚Ä¢ player_id: 27535\n",
      "   ‚Ä¢ opening_id: 599\n",
      "   ‚Ä¢ confidence: 0.6503\n",
      "   ‚Ä¢ score: 0.5148\n",
      "\n",
      "3Ô∏è‚É£  Creating PyTorch DataLoaders...\n"
     ]
    }
   ],
   "source": [
    "# Step 6c: Create PyTorch Dataset and DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# CUSTOM DATASET CLASS\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for chess player-opening interactions.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - player_id: Integer ID for embedding lookup\n",
    "    - opening_id: Integer ID for embedding lookup\n",
    "    - confidence: Confidence weight for loss function\n",
    "    - score: Target value (win rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, player_ids, opening_ids, confidence, scores):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (long tensor)\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (long tensor)\n",
    "        confidence : torch.Tensor\n",
    "            Confidence weights (float tensor)\n",
    "        scores : torch.Tensor\n",
    "            Target scores (float tensor)\n",
    "        \"\"\"\n",
    "        self.player_ids = player_ids\n",
    "        self.opening_ids = opening_ids\n",
    "        self.confidence = confidence\n",
    "        self.scores = scores\n",
    "        \n",
    "        # Validate shapes\n",
    "        assert len(player_ids) == len(opening_ids) == len(confidence) == len(scores), \\\n",
    "            \"All tensors must have the same length\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.player_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with keys: 'player_id', 'opening_id', 'confidence', 'score'\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'player_id': self.player_ids[idx],\n",
    "            'opening_id': self.opening_ids[idx],\n",
    "            'confidence': self.confidence[idx],\n",
    "            'score': self.scores[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATASETS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Creating PyTorch Datasets...\")\n",
    "\n",
    "train_dataset = ChessOpeningDataset(\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train\n",
    ")\n",
    "val_dataset = ChessOpeningDataset(\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val\n",
    ")\n",
    "test_dataset = ChessOpeningDataset(\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"   ‚úì Validation dataset: {len(val_dataset):,} samples\")\n",
    "print(f\"   ‚úì Test dataset: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Test dataset access\n",
    "print(f\"\\n2Ô∏è‚É£  Testing dataset access...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"   Sample from train_dataset[0]:\")\n",
    "print(f\"   ‚Ä¢ player_id: {sample['player_id']}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {sample['opening_id']}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "print(f\"   ‚Ä¢ score: {sample['score']:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATALOADERS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Creating PyTorch DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle training data each epoch\n",
    "    num_workers=0,  # Number of subprocesses for data loading (0 = main process)\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle test data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e5fbb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6D: DEFINE HELPER FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "   format_time() tests:\n",
      "   ‚Ä¢ 30 seconds ‚Üí 30.0s\n",
      "   ‚Ä¢ 90 seconds ‚Üí 1m 30s\n",
      "   ‚Ä¢ 300 seconds ‚Üí 5m 0s\n",
      "   ‚Ä¢ 3661 seconds ‚Üí 1h 1m 1s\n",
      "   ‚Ä¢ 7384 seconds ‚Üí 2h 3m 4s\n",
      "\n",
      "   print_progress() test (simulating batch progress):\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6d: Define Helper Functions for Training\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"STEP 6D: DEFINE HELPER FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Save Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to save\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer state to save\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    train_loss : float\n",
    "        Training loss for this epoch\n",
    "    val_loss : float\n",
    "        Validation loss for this epoch\n",
    "    filepath : Path or str\n",
    "        Where to save the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        \n",
    "        # Save hyperparameters for reproducibility\n",
    "        'hyperparameters': {\n",
    "            'num_factors': NUM_FACTORS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'momentum': MOMENTUM,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'num_players': NUM_PLAYERS,\n",
    "            'num_openings': NUM_OPENINGS,\n",
    "            'num_eco_letters': NUM_ECO_LETTERS,\n",
    "            'num_eco_numbers': NUM_ECO_NUMBERS,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"   Checkpoint saved to: {filepath}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Load Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to load weights into\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to load state into\n",
    "    filepath : Path or str\n",
    "        Where to load the checkpoint from\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        The epoch number from the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"    Checkpoint loaded from: {filepath}\")\n",
    "    print(f\"   ‚Ä¢ Epoch: {epoch}\")\n",
    "    print(f\"   ‚Ä¢ Train loss: {checkpoint['train_loss']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Val loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    return epoch\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Format Time\n",
    "# ========================================\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"\n",
    "    Format seconds as human-readable time string.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seconds : float\n",
    "        Number of seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted time string (e.g., \"1h 23m 45s\")\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Print Progress\n",
    "# ========================================\n",
    "\n",
    "def print_progress(epoch, batch_idx, total_batches, loss, elapsed_time):\n",
    "    \"\"\"\n",
    "    Print training progress with ETA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    batch_idx : int\n",
    "        Current batch index (0-based)\n",
    "    total_batches : int\n",
    "        Total number of batches per epoch\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    elapsed_time : float\n",
    "        Elapsed time in seconds since epoch start\n",
    "    \"\"\"\n",
    "    # Calculate progress\n",
    "    progress_pct = 100.0 * (batch_idx + 1) / total_batches\n",
    "    \n",
    "    # Estimate time remaining\n",
    "    time_per_batch = elapsed_time / (batch_idx + 1)\n",
    "    remaining_batches = total_batches - (batch_idx + 1)\n",
    "    eta_seconds = time_per_batch * remaining_batches\n",
    "    \n",
    "    # Print progress bar\n",
    "    bar_length = 40\n",
    "    filled_length = int(bar_length * (batch_idx + 1) / total_batches)\n",
    "    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)\n",
    "    \n",
    "    print(f\"   Epoch {epoch} [{bar}] {progress_pct:>6.2f}% | \"\n",
    "          f\"Loss: {loss:.6f} | \"\n",
    "          f\"ETA: {format_time(eta_seconds)}\", end='\\r')\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST HELPER FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "# Test format_time\n",
    "print(f\"\\n   format_time() tests:\")\n",
    "test_times = [30, 90, 300, 3661, 7384]\n",
    "for t in test_times:\n",
    "    print(f\"   ‚Ä¢ {t} seconds ‚Üí {format_time(t)}\")\n",
    "\n",
    "# Test print_progress (will overwrite line)\n",
    "print(f\"\\n   print_progress() test (simulating batch progress):\")\n",
    "for i in range(0, 101, 20):\n",
    "    total = 100\n",
    "    loss = 0.05 - i * 0.0003\n",
    "    elapsed = i * 0.5\n",
    "    print_progress(epoch=1, batch_idx=i, total_batches=total, loss=loss, elapsed_time=elapsed)\n",
    "    time.sleep(0.1)  # Brief pause to show animation\n",
    "print()  # New line after progress bar\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HELPER FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\n",
      "============================================================\n",
      " TRAINING AND EVALUATION FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìã Training workflow:\n",
      "   1. Initialize model and optimizer\n",
      "   2. For each epoch:\n",
      "      a. Call train_one_epoch()\n",
      "      b. Call evaluate_model() on validation set\n",
      "      c. Save checkpoint with save_checkpoint()\n",
      "      d. Log metrics\n",
      "   3. After training, evaluate on test set\n"
     ]
    }
   ],
   "source": [
    "# Step 6e: Define Training and Evaluation Functions\n",
    "\n",
    "print(\"STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# TRAINING FUNCTION: Train One Epoch\n",
    "# ========================================\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, device, epoch_num):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch and compute metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to train\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for updating model parameters\n",
    "    device : torch.device\n",
    "        Device to train on (CPU or CUDA)\n",
    "    epoch_num : int\n",
    "        Current epoch number (for logging)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing: loss, rmse, mae, huber, spearman, baseline_delta, time\n",
    "    \"\"\"\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Collect predictions and targets for metrics\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_conf = []\n",
    "    \n",
    "    # Huber loss function\n",
    "    huber_fn = torch.nn.HuberLoss(reduction='none', delta=1.0)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move batch to device\n",
    "        player_ids = batch['player_id'].to(device)\n",
    "        opening_ids = batch['opening_id'].to(device)\n",
    "        confidence = batch['confidence'].to(device)\n",
    "        targets = batch['score'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(player_ids, opening_ids)\n",
    "        \n",
    "        # Compute loss with confidence weighting\n",
    "        loss = mse_loss(predictions, targets, confidence)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Collect for metrics (detach to save memory)\n",
    "        all_preds.append(predictions.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "        all_conf.append(confidence.detach().cpu())\n",
    "        \n",
    "        # Print progress every 10 batches or on last batch\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print_progress(epoch_num, batch_idx, num_batches, loss.item(), elapsed)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    elapsed_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Clear progress line and print summary\n",
    "    print()  # New line after progress bar\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    preds_all = torch.cat(all_preds)\n",
    "    targets_all = torch.cat(all_targets)\n",
    "    conf_all = torch.cat(all_conf)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    mse = ((preds_all - targets_all) ** 2 * conf_all).sum() / conf_all.sum()\n",
    "    rmse = torch.sqrt(mse).item()\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = torch.abs(preds_all - targets_all).mean().item()\n",
    "    \n",
    "    # Calculate Huber Loss\n",
    "    huber_losses = huber_fn(preds_all, targets_all)\n",
    "    weighted_huber = (huber_losses * conf_all).sum() / conf_all.sum()\n",
    "    huber = weighted_huber.item()\n",
    "    \n",
    "    # Calculate Spearman correlation\n",
    "    spearman_corr, _ = spearmanr(preds_all.numpy(), targets_all.numpy())\n",
    "    \n",
    "    # Calculate baseline delta\n",
    "    mean_target = targets_all.mean()\n",
    "    baseline_mse = ((targets_all - mean_target) ** 2).mean()\n",
    "    baseline_rmse = torch.sqrt(baseline_mse).item()\n",
    "    baseline_delta = baseline_rmse - rmse\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'huber': huber,\n",
    "        'spearman': spearman_corr,\n",
    "        'baseline_delta': baseline_delta,\n",
    "        'time': elapsed_time\n",
    "    }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# EVALUATION FUNCTION: Evaluate Model\n",
    "# ========================================\n",
    "\n",
    "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset with comprehensive metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to evaluate\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for evaluation data\n",
    "    device : torch.device\n",
    "        Device to evaluate on (CPU or CUDA)\n",
    "    dataset_name : str\n",
    "        Name of dataset for logging (e.g., \"Validation\", \"Test\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing: mse, rmse, mae, huber, spearman, baseline_delta\n",
    "    \"\"\"\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_mse = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Collect all predictions and targets for additional metrics\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_conf = []\n",
    "    \n",
    "    # Huber loss function\n",
    "    huber_fn = torch.nn.HuberLoss(reduction='none', delta=1.0)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in data_loader:\n",
    "            # Move batch to device\n",
    "            player_ids = batch['player_id'].to(device)\n",
    "            opening_ids = batch['opening_id'].to(device)\n",
    "            confidence = batch['confidence'].to(device)\n",
    "            targets = batch['score'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(player_ids, opening_ids)\n",
    "            \n",
    "            # Compute MSE with confidence weighting\n",
    "            batch_mse = mse_loss(predictions, targets, confidence)\n",
    "            \n",
    "            # Track weighted sum of MSE\n",
    "            total_mse += batch_mse.item() * len(targets)\n",
    "            total_samples += len(targets)\n",
    "            \n",
    "            # Collect for additional metrics\n",
    "            all_preds.append(predictions.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "            all_conf.append(confidence.cpu())\n",
    "    \n",
    "    # Calculate average MSE and RMSE\n",
    "    avg_mse = total_mse / total_samples\n",
    "    avg_rmse = avg_mse ** 0.5\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    preds_all = torch.cat(all_preds)\n",
    "    targets_all = torch.cat(all_targets)\n",
    "    conf_all = torch.cat(all_conf)\n",
    "    \n",
    "    # Calculate MAE (Mean Absolute Error)\n",
    "    mae = torch.abs(preds_all - targets_all).mean().item()\n",
    "    \n",
    "    # Calculate Huber Loss\n",
    "    huber_losses = huber_fn(preds_all, targets_all)\n",
    "    # Weight by confidence and average\n",
    "    weighted_huber = (huber_losses * conf_all).sum() / conf_all.sum()\n",
    "    huber = weighted_huber.item()\n",
    "    \n",
    "    # Calculate Spearman correlation\n",
    "    spearman_corr, _ = spearmanr(preds_all.numpy(), targets_all.numpy())\n",
    "    \n",
    "    # Calculate baseline delta (improvement over predicting mean)\n",
    "    mean_target = targets_all.mean()\n",
    "    baseline_mse = ((targets_all - mean_target) ** 2).mean()\n",
    "    baseline_rmse = torch.sqrt(baseline_mse).item()\n",
    "    baseline_delta = baseline_rmse - avg_rmse  # Positive = improvement\n",
    "    \n",
    "    return {\n",
    "        'mse': avg_mse,\n",
    "        'rmse': avg_rmse,\n",
    "        'mae': mae,\n",
    "        'huber': huber,\n",
    "        'spearman': spearman_corr,\n",
    "        'baseline_delta': baseline_delta\n",
    "    }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST FUNCTIONS (with dummy model)\n",
    "# ========================================\n",
    "\n",
    "print(\" TRAINING AND EVALUATION FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã Training workflow:\")\n",
    "print(f\"   1. Initialize model and optimizer\")\n",
    "print(f\"   2. For each epoch:\")\n",
    "print(f\"      a. Call train_one_epoch()\")\n",
    "print(f\"      b. Call evaluate_model() on validation set\")\n",
    "print(f\"      c. Save checkpoint with save_checkpoint()\")\n",
    "print(f\"      d. Log metrics\")\n",
    "print(f\"   3. After training, evaluate on test set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50c5eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6F: DEFINE MODEL ARCHITECTURE\n",
      "============================================================\n",
      "\n",
      "‚úÖ ChessOpeningRecommender model class defined\n",
      "\n",
      "üìä Model Architecture:\n",
      "   Player Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(48469, 50)\n",
      "      ‚Ä¢ Biases: Embedding(48469, 1)\n",
      "      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\n",
      "      ‚Ä¢ Combiner: Linear(51, 50)\n",
      "\n",
      "   Opening Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(2716, 50)\n",
      "      ‚Ä¢ Biases: Embedding(2716, 1)\n",
      "      ‚Ä¢ ECO letter embedding: Embedding(5, 4)\n",
      "      ‚Ä¢ ECO number embedding: Embedding(100, 4)\n",
      "      ‚Ä¢ Combiner: Linear(58, 50)\n",
      "\n",
      "   Prediction:\n",
      "      ‚Ä¢ Dot product of player and opening representations\n",
      "      ‚Ä¢ + player bias + opening bias + global bias\n",
      "      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\n",
      "\n",
      " Model Statistics:\n",
      "   ‚Ä¢ Total trainable parameters: 2,616,406\n",
      "   ‚Ä¢ Player parameters: 2,474,519\n",
      "   ‚Ä¢ Opening parameters: 141,886\n",
      "   ‚Ä¢ Global parameters: 1\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6f: Define Model Architecture\n",
    "\n",
    "print(\"STEP 6F: DEFINE MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Chess Opening Recommender Model\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningRecommender(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for chess opening recommendations.\n",
    "    \n",
    "    The model learns latent factors for players and openings, incorporating\n",
    "    side information:\n",
    "    - Player ratings (normalized)\n",
    "    - Opening ECO codes (letter and number as categorical features)\n",
    "    \n",
    "    Architecture:\n",
    "    - Player embedding: learnable latent factors\n",
    "    - Opening embedding: learnable latent factors\n",
    "    - Player rating: fixed side information (no embedding)\n",
    "    - ECO letter/number: categorical embeddings\n",
    "    \n",
    "    Prediction: dot product of player and opening representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_players, num_openings, num_factors,\n",
    "                 player_ratings, opening_eco_letters, opening_eco_numbers,\n",
    "                 num_eco_letters, num_eco_numbers, eco_embed_dim=4):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_players : int\n",
    "            Total number of unique players\n",
    "        num_openings : int\n",
    "            Total number of unique openings\n",
    "        num_factors : int\n",
    "            Dimensionality of latent factor embeddings\n",
    "        player_ratings : torch.Tensor\n",
    "            Z-score normalized ratings for all players (shape: [num_players])\n",
    "        opening_eco_letters : torch.Tensor\n",
    "            ECO letter categories for all openings (shape: [num_openings])\n",
    "        opening_eco_numbers : torch.Tensor\n",
    "            ECO number categories for all openings (shape: [num_openings])\n",
    "        num_eco_letters : int\n",
    "            Number of unique ECO letter categories\n",
    "        num_eco_numbers : int\n",
    "            Number of unique ECO number categories\n",
    "        eco_embed_dim : int\n",
    "            Dimensionality of ECO categorical embeddings (default: 4)\n",
    "        \"\"\"\n",
    "        super(ChessOpeningRecommender, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_players = num_players\n",
    "        self.num_openings = num_openings\n",
    "        self.num_factors = num_factors\n",
    "        self.eco_embed_dim = eco_embed_dim\n",
    "        \n",
    "        # ========================================\n",
    "        # Player Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.player_factors = torch.nn.Embedding(num_players, num_factors)\n",
    "        \n",
    "        self.player_biases = torch.nn.Embedding(num_players, 1)\n",
    "        \n",
    "        # Player ratings (fixed side information - registered as buffer, not parameter)\n",
    "        self.register_buffer('player_ratings', player_ratings)\n",
    "        \n",
    "        # ========================================\n",
    "        # Opening Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.opening_factors = torch.nn.Embedding(num_openings, num_factors)\n",
    "        \n",
    "        self.opening_biases = torch.nn.Embedding(num_openings, 1)\n",
    "        \n",
    "        # ECO letter and number (fixed side information - registered as buffers)\n",
    "        self.register_buffer('opening_eco_letters', opening_eco_letters)\n",
    "        self.register_buffer('opening_eco_numbers', opening_eco_numbers)\n",
    "        \n",
    "        # ECO embeddings (learnable)\n",
    "        self.eco_letter_embedding = torch.nn.Embedding(num_eco_letters, eco_embed_dim)\n",
    "        self.eco_number_embedding = torch.nn.Embedding(num_eco_numbers, eco_embed_dim)\n",
    "        \n",
    "        # ========================================\n",
    "        # Combination Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Combine player latent factors with rating\n",
    "        # Input: [num_factors + 1] ‚Üí Output: [num_factors]\n",
    "        self.player_combiner = torch.nn.Linear(num_factors + 1, num_factors)\n",
    "        \n",
    "        # Combine opening latent factors with ECO embeddings\n",
    "        # Input: [num_factors + 2*eco_embed_dim] ‚Üí Output: [num_factors]\n",
    "        self.opening_combiner = torch.nn.Linear(num_factors + 2 * eco_embed_dim, num_factors)\n",
    "        \n",
    "        # ========================================\n",
    "        # Global Bias\n",
    "        # ========================================\n",
    "        \n",
    "        # Global bias term (learnable scalar)\n",
    "        self.global_bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # ========================================\n",
    "        # Initialize Weights\n",
    "        # ========================================\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        torch.nn.init.normal_(self.player_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.opening_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_letter_embedding.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_number_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        torch.nn.init.zeros_(self.player_biases.weight)\n",
    "        torch.nn.init.zeros_(self.opening_biases.weight)\n",
    "        \n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        torch.nn.init.xavier_uniform_(self.player_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.player_combiner.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.opening_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.opening_combiner.bias)\n",
    "    \n",
    "    def forward(self, player_ids, opening_ids):\n",
    "        \"\"\"\n",
    "        Forward pass: predict player-opening scores.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (shape: [batch_size])\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (shape: [batch_size])\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Predicted scores (shape: [batch_size])\n",
    "        \"\"\"\n",
    "        # ========================================\n",
    "        # Get Player Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get player latent factors [batch_size, num_factors]\n",
    "        player_embed = self.player_factors(player_ids)\n",
    "        \n",
    "        # Get player ratings [batch_size, 1]\n",
    "        player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "        \n",
    "        # Concatenate player factors with rating [batch_size, num_factors + 1]\n",
    "        player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "        \n",
    "        # Combine into final player representation [batch_size, num_factors]\n",
    "        player_repr = self.player_combiner(player_concat)\n",
    "        \n",
    "        # Get player bias [batch_size]\n",
    "        player_bias = self.player_biases(player_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Get Opening Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get opening latent factors [batch_size, num_factors]\n",
    "        opening_embed = self.opening_factors(opening_ids)\n",
    "        \n",
    "        # Get ECO embeddings\n",
    "        eco_letters = self.opening_eco_letters[opening_ids]  # [batch_size]\n",
    "        eco_numbers = self.opening_eco_numbers[opening_ids]  # [batch_size]\n",
    "        \n",
    "        eco_letter_embed = self.eco_letter_embedding(eco_letters)  # [batch_size, eco_embed_dim]\n",
    "        eco_number_embed = self.eco_number_embedding(eco_numbers)  # [batch_size, eco_embed_dim]\n",
    "        \n",
    "        # Concatenate opening factors with ECO embeddings\n",
    "        # [batch_size, num_factors + 2*eco_embed_dim]\n",
    "        opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "        \n",
    "        # Combine into final opening representation [batch_size, num_factors]\n",
    "        opening_repr = self.opening_combiner(opening_concat)\n",
    "        \n",
    "        # Get opening bias [batch_size]\n",
    "        opening_bias = self.opening_biases(opening_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Compute Prediction\n",
    "        # ========================================\n",
    "        \n",
    "        # Dot product of player and opening representations [batch_size]\n",
    "        interaction = (player_repr * opening_repr).sum(dim=1)\n",
    "        \n",
    "        # Add biases and global bias\n",
    "        prediction = interaction + player_bias + opening_bias + self.global_bias\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] range (since scores are win rates)\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def get_player_embedding(self, player_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific player.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_id : int\n",
    "            Player ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Player representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            player_ids = torch.tensor([player_id], device=self.player_factors.weight.device)\n",
    "            player_embed = self.player_factors(player_ids)\n",
    "            player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "            player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "            player_repr = self.player_combiner(player_concat)\n",
    "            return player_repr.squeeze()\n",
    "    \n",
    "    def get_opening_embedding(self, opening_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific opening.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        opening_id : int\n",
    "            Opening ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Opening representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            opening_ids = torch.tensor([opening_id], device=self.opening_factors.weight.device)\n",
    "            opening_embed = self.opening_factors(opening_ids)\n",
    "            \n",
    "            eco_letters = self.opening_eco_letters[opening_ids]\n",
    "            eco_numbers = self.opening_eco_numbers[opening_ids]\n",
    "            \n",
    "            eco_letter_embed = self.eco_letter_embedding(eco_letters)\n",
    "            eco_number_embed = self.eco_number_embedding(eco_numbers)\n",
    "            \n",
    "            opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "            opening_repr = self.opening_combiner(opening_concat)\n",
    "            return opening_repr.squeeze()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model Summary\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ ChessOpeningRecommender model class defined\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Player Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_PLAYERS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_PLAYERS}, 1)\")\n",
    "print(f\"      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 1}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Opening Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_OPENINGS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_OPENINGS}, 1)\")\n",
    "print(f\"      ‚Ä¢ ECO letter embedding: Embedding({NUM_ECO_LETTERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ ECO number embedding: Embedding({NUM_ECO_NUMBERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 8}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Prediction:\")\n",
    "print(f\"      ‚Ä¢ Dot product of player and opening representations\")\n",
    "print(f\"      ‚Ä¢ + player bias + opening bias + global bias\")\n",
    "print(f\"      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\")\n",
    "\n",
    "# Calculate total parameters\n",
    "player_factor_params = NUM_PLAYERS * NUM_FACTORS\n",
    "player_bias_params = NUM_PLAYERS\n",
    "player_combiner_params = (NUM_FACTORS + 1) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "opening_factor_params = NUM_OPENINGS * NUM_FACTORS\n",
    "opening_bias_params = NUM_OPENINGS\n",
    "eco_letter_params = NUM_ECO_LETTERS * 4\n",
    "eco_number_params = NUM_ECO_NUMBERS * 4\n",
    "opening_combiner_params = (NUM_FACTORS + 8) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "global_bias_params = 1\n",
    "\n",
    "total_params = (player_factor_params + player_bias_params + player_combiner_params +\n",
    "                opening_factor_params + opening_bias_params + \n",
    "                eco_letter_params + eco_number_params + opening_combiner_params +\n",
    "                global_bias_params)\n",
    "\n",
    "print(f\"\\n Model Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total trainable parameters: {total_params:,}\")\n",
    "print(f\"   ‚Ä¢ Player parameters: {player_factor_params + player_bias_params + player_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Opening parameters: {opening_factor_params + opening_bias_params + eco_letter_params + eco_number_params + opening_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Global parameters: {global_bias_params}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc2c6d",
   "metadata": {},
   "source": [
    "## Step 7: Final Model Training Setup\n",
    "\n",
    "Now we'll set up the training components for the **final model**: the model instance, optimizer, and learning rate scheduler.\n",
    "\n",
    "We'll use the best hyperparameters found during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da549756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: FINAL MODEL TRAINING SETUP\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# ==================================\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Checkpoint and Model Save Paths\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# ==================================\u001b[39;00m\n\u001b[32m     50\u001b[39m CHECKPOINT_DIR = \u001b[33m\"\u001b[39m\u001b[33mdata/models\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m BEST_MODEL_PATH = \u001b[43mos\u001b[49m.path.join(CHECKPOINT_DIR, \u001b[33m\"\u001b[39m\u001b[33mbest_model.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Ensure the directory exists\u001b[39;00m\n\u001b[32m     54\u001b[39m os.makedirs(CHECKPOINT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"STEP 7: FINAL MODEL TRAINING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Hyperparameters for Final Training\n",
    "# ==================================\n",
    "# These would be set by the results from the CV step.\n",
    "# For now, we'll use the initial defaults.\n",
    "NUM_FACTORS = 40\n",
    "LEARNING_RATE = 0.06\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# ==================================\n",
    "# Model Instantiation\n",
    "# ==================================\n",
    "final_model = ChessOpeningRecommender(\n",
    "    num_players=NUM_PLAYERS,\n",
    "    num_openings=NUM_OPENINGS,\n",
    "    num_factors=NUM_FACTORS,\n",
    "    player_ratings=player_ratings_tensor,\n",
    "    opening_eco_letters=opening_eco_letter_tensor,\n",
    "    opening_eco_numbers=opening_eco_number_tensor,\n",
    "    num_eco_letters=NUM_ECO_LETTERS,\n",
    "    num_eco_numbers=NUM_ECO_NUMBERS\n",
    ").to(DEVICE)\n",
    "\n",
    "# ==================================\n",
    "# Optimizer\n",
    "# ==================================\n",
    "# We'll use SGD with momentum, a classic choice for matrix factorization.\n",
    "optimizer = torch.optim.SGD(final_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# ==================================\n",
    "# Learning Rate Scheduler\n",
    "# ==================================\n",
    "# Reduces the learning rate when a metric has stopped improving.\n",
    "# This helps to fine-tune the model in the later stages of training.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',      # The scheduler will step when the quantity monitored has stopped decreasing\n",
    "    factor=0.1,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "    patience=2,      # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=True     # If True, prints a message to stdout for each update\n",
    ")\n",
    "\n",
    "# ==================================\n",
    "# Checkpoint and Model Save Paths\n",
    "# ==================================\n",
    "CHECKPOINT_DIR = \"data/models\"\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.pt\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Final model hyperparameters:\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Num Factors: {NUM_FACTORS}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Num Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"\\nModel, optimizer, and scheduler initialized.\")\n",
    "print(f\"Checkpoints will be saved in: '{CHECKPOINT_DIR}'\")\n",
    "print(f\"Best model will be saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ SETUP COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becba99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0dc479f",
   "metadata": {},
   "source": [
    "## Step 8: Final Model Training Loop\n",
    "\n",
    "Here's the main training loop for the final model. We'll iterate for a specified number of epochs, training the model and evaluating its performance on the validation set periodically. We'll also save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"STEP 8: TRAINING THE FINAL MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Training State Tracking\n",
    "# ==================================\n",
    "history = defaultdict(list)\n",
    "best_val_rmse = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "\n",
    "# ==================================\n",
    "# Main Training Loop\n",
    "# ==================================\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    train_metrics = train_one_epoch(final_model, train_loader, optimizer, DEVICE, epoch)\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    val_metrics = evaluate_model(final_model, val_loader, DEVICE)\n",
    "    \n",
    "    # --- Learning Rate Scheduler Step ---\n",
    "    scheduler.step(val_metrics['rmse'])\n",
    "    \n",
    "    # --- Logging ---\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['train_rmse'].append(train_metrics['rmse'])\n",
    "    history['train_mae'].append(train_metrics['mae'])\n",
    "    history['train_huber'].append(train_metrics['huber'])\n",
    "    history['train_spearman'].append(train_metrics['spearman'])\n",
    "    history['train_baseline_delta'].append(train_metrics['baseline_delta'])\n",
    "    \n",
    "    history['val_mse'].append(val_metrics['mse'])\n",
    "    history['val_rmse'].append(val_metrics['rmse'])\n",
    "    history['val_mae'].append(val_metrics['mae'])\n",
    "    history['val_huber'].append(val_metrics['huber'])\n",
    "    history['val_spearman'].append(val_metrics['spearman'])\n",
    "    history['val_baseline_delta'].append(val_metrics['baseline_delta'])\n",
    "    \n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
    "          f\"Train RMSE: {train_metrics['rmse']:.4f}, MAE: {train_metrics['mae']:.4f}, \"\n",
    "          f\"Huber: {train_metrics['huber']:.4f} | \"\n",
    "          f\"Val RMSE: {val_metrics['rmse']:.4f}, MAE: {val_metrics['mae']:.4f}, \"\n",
    "          f\"Huber: {val_metrics['huber']:.4f}\")\n",
    "    print(f\"         Spearman - Train: {train_metrics['spearman']:.3f}, Val: {val_metrics['spearman']:.3f} | \"\n",
    "          f\"Baseline Œî - Train: {train_metrics['baseline_delta']:.4f}, Val: {val_metrics['baseline_delta']:.4f} | \"\n",
    "          f\"Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "    # --- Checkpoint Saving ---\n",
    "    # Save a checkpoint every epoch\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch:03d}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_metrics': val_metrics,\n",
    "        'train_metrics': train_metrics,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Save the best model based on validation RMSE\n",
    "    if val_metrics['rmse'] < best_val_rmse:\n",
    "        best_val_rmse = val_metrics['rmse']\n",
    "        epochs_since_improvement = 0\n",
    "        torch.save(final_model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  -> New best model saved with Val RMSE: {val_metrics['rmse']:.4f}\")\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "\n",
    "# ==================================\n",
    "# Post-Training Summary\n",
    "# ==================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ FINAL TRAINING COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best Validation RMSE: {best_val_rmse:.4f}\")\n",
    "print(f\"Best model saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(f\"Last checkpoint saved to: '{checkpoint_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68202800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7617822",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Training Metrics\n",
    "\n",
    "Plot the training history to understand model performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef56cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 9: VISUALIZE TRAINING METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "fig.suptitle('Training History - Final Model', fontsize=16, fontweight='bold')\n",
    "\n",
    "epochs = range(1, len(history['train_rmse']) + 1)\n",
    "\n",
    "# 1. RMSE\n",
    "axes[0, 0].plot(epochs, history['train_rmse'], 'o-', label='Train', linewidth=2, markersize=6)\n",
    "axes[0, 0].plot(epochs, history['val_rmse'], 's-', label='Validation', linewidth=2, markersize=6)\n",
    "axes[0, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('RMSE', fontweight='bold')\n",
    "axes[0, 0].set_title('Root Mean Squared Error', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MAE\n",
    "axes[0, 1].plot(epochs, history['train_mae'], 'o-', label='Train', linewidth=2, markersize=6)\n",
    "axes[0, 1].plot(epochs, history['val_mae'], 's-', label='Validation', linewidth=2, markersize=6)\n",
    "axes[0, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('MAE', fontweight='bold')\n",
    "axes[0, 1].set_title('Mean Absolute Error', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Huber Loss\n",
    "axes[1, 0].plot(epochs, history['train_huber'], 'o-', label='Train', linewidth=2, markersize=6)\n",
    "axes[1, 0].plot(epochs, history['val_huber'], 's-', label='Validation', linewidth=2, markersize=6)\n",
    "axes[1, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Huber Loss', fontweight='bold')\n",
    "axes[1, 0].set_title('Huber Loss (Œ¥=1.0)', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Spearman Correlation\n",
    "axes[1, 1].plot(epochs, history['train_spearman'], 'o-', label='Train', linewidth=2, markersize=6)\n",
    "axes[1, 1].plot(epochs, history['val_spearman'], 's-', label='Validation', linewidth=2, markersize=6)\n",
    "axes[1, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Spearman œÅ', fontweight='bold')\n",
    "axes[1, 1].set_title('Spearman Rank Correlation (Higher is Better)', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Baseline Delta\n",
    "axes[2, 0].plot(epochs, history['train_baseline_delta'], 'o-', label='Train', linewidth=2, markersize=6)\n",
    "axes[2, 0].plot(epochs, history['val_baseline_delta'], 's-', label='Validation', linewidth=2, markersize=6)\n",
    "axes[2, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Baseline (predict mean)')\n",
    "axes[2, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[2, 0].set_ylabel('RMSE Improvement', fontweight='bold')\n",
    "axes[2, 0].set_title('Improvement Over Baseline (Higher is Better)', fontweight='bold')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Training Loss\n",
    "axes[2, 1].plot(epochs, history['train_loss'], 'o-', label='Train Loss (MSE)', linewidth=2, markersize=6)\n",
    "axes[2, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[2, 1].set_ylabel('Loss', fontweight='bold')\n",
    "axes[2, 1].set_title('Training Loss', fontweight='bold')\n",
    "axes[2, 1].legend()\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL METRICS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nFinal Epoch Metrics:\")\n",
    "print(f\"  Train RMSE:         {history['train_rmse'][-1]:.4f}\")\n",
    "print(f\"  Val RMSE:           {history['val_rmse'][-1]:.4f}\")\n",
    "print(f\"  Train MAE:          {history['train_mae'][-1]:.4f}\")\n",
    "print(f\"  Val MAE:            {history['val_mae'][-1]:.4f}\")\n",
    "print(f\"  Train Huber:        {history['train_huber'][-1]:.4f}\")\n",
    "print(f\"  Val Huber:          {history['val_huber'][-1]:.4f}\")\n",
    "print(f\"  Train Spearman:     {history['train_spearman'][-1]:.3f}\")\n",
    "print(f\"  Val Spearman:       {history['val_spearman'][-1]:.3f}\")\n",
    "print(f\"  Train Baseline Œî:   {history['train_baseline_delta'][-1]:.4f}\")\n",
    "print(f\"  Val Baseline Œî:     {history['val_baseline_delta'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nBest Validation Metrics:\")\n",
    "best_epoch = np.argmin(history['val_rmse']) + 1\n",
    "print(f\"  Best Epoch:         {best_epoch}\")\n",
    "print(f\"  Best Val RMSE:      {min(history['val_rmse']):.4f}\")\n",
    "print(f\"  Val MAE:            {history['val_mae'][best_epoch-1]:.4f}\")\n",
    "print(f\"  Val Huber:          {history['val_huber'][best_epoch-1]:.4f}\")\n",
    "print(f\"  Val Spearman:       {history['val_spearman'][best_epoch-1]:.3f}\")\n",
    "print(f\"  Val Baseline Œî:     {history['val_baseline_delta'][best_epoch-1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VISUALIZATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
