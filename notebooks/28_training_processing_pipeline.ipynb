{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e163a7",
   "metadata": {},
   "source": [
    "# Notebook 28 ‚Äî Opening Recommender Model: Training Pipeline\n",
    "\n",
    "### 0. Overview and Goals\n",
    "\n",
    "This notebook defines the full pipeline for training the chess opening recommender model.\n",
    "The objective is to predict **player‚Äìopening performance scores** ((wins + (0.5 * draws) / num games)) for openings a player hasn‚Äôt yet played, based on their results in the openings they *have* played.\n",
    "\n",
    "The model will use **matrix factorization** with **stochastic gradient descent (SGD)** to learn latent factors representing player and opening characteristics.\n",
    "All computations will be implemented in **PyTorch**, with data loaded from my local **DuckDB** database.\n",
    "\n",
    "**High-level specs:**\n",
    "- Use only *White* openings initially (we‚Äôll extend to Black later).\n",
    "- Data source: processed player‚Äìopening stats from local DuckDB.\n",
    "- Predict: normalized ‚Äúscore‚Äù = win rate ((wins + 0.5 x draws) / total games).\n",
    "- Filter: only include entries with ‚â• `MIN_GAMES_THRESHOLD` (default = 10).\n",
    "- Ignore: rating differences, time controls, and other metadata for the base model.\n",
    "- Model parameters (to be defined in appropriate places for easy editing):\n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_PLAYERS_TO_PROCESS`\n",
    "- Logging and checkpoints throughout for reproducibility.\n",
    "- All random operations seeded for deterministic runs.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Extraction\n",
    "- Connect to local DuckDB and pull all processed player‚Äìopening statistics.\n",
    "- Verify schema consistency and include row-count sanity checks.\n",
    "- Filter for players with ratings above a minimum threshold (e.g., 1200).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Sanitization & Normalization\n",
    "- Apply confidence weighting using hierarchical Bayesian shrinkage to adjust scores for low-game-count entries.\n",
    "- Normalize player ratings (z-score) for use as side information.\n",
    "- Resequence player and opening IDs to be contiguous integers for embedding layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Splits\n",
    "- Split data into train, validation, and test sets (e.g., 75/15/10).\n",
    "- Ensure splits are handled correctly to avoid data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Enumerate Categorical Variables\n",
    "- Process ECO codes into categorical features (e.g., `eco_letter`, `eco_number`).\n",
    "- Store these as opening-level side information.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Data Structure\n",
    "- Convert the final, processed DataFrames into PyTorch Tensors.\n",
    "- Create custom `Dataset` and `DataLoader` classes for efficient batching.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Cross-Validation & Hyperparameter Tuning\n",
    "- Define ranges for hyperparameters (`NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`).\n",
    "- Perform k-fold cross-validation on a subset of the training data to find the best hyperparameter combination before the final training run.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Final Model Training Setup\n",
    "- Define constants and hyperparameters for the final model (using results from CV).\n",
    "- Instantiate the PyTorch model, optimizer (SGD), and learning rate scheduler.\n",
    "- Implement helper functions for training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Final Model Training Loop\n",
    "- Initialize player and opening embeddings.\n",
    "- Iterate through epochs with mini-batch SGD.\n",
    "- Compute and log training and validation metrics (e.g., RMSE) per epoch.\n",
    "- Save model checkpoints locally.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Evaluation\n",
    "- Evaluate the final trained model on the held-out test set.\n",
    "- Report final metrics (MSE, RMSE) and create visualizations (e.g., predicted vs. actual scores).\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Next Steps\n",
    "- Extend model to include Black openings.\n",
    "- Experiment with more complex architectures or hybrid inputs.\n",
    "- Integrate the trained model into an API for serving recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "- Every random seed and parameter definition will be explicit.\n",
    "- Every major step includes row-count, schema, and type validation.\n",
    "- Model artifacts and logs will be saved locally for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d823",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "Connect to DuckDB and extract all player-opening statistics.\n",
    "Verify schema and perform sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fcea569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA EXTRACTION\n",
      "üìÅ Database exists: True\n",
      "üé® Color filter: White\n",
      "üîí Minimum holdout players: 1,000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd() / 'utils'))\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = Path.cwd().parent / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "COLOR_FILTER = 'w'  # 'w' for white, 'b' for black\n",
    "MIN_HOLDOUT_PLAYERS = 1000  # Minimum number of players to reserve for fold-in verification. These will not be used at all in this notebook for training, test/val or anything else.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: DATA EXTRACTION\")\n",
    "print(f\"üìÅ Database exists: {DB_PATH.exists()}\")\n",
    "print(f\"üé® Color filter: {'White' if COLOR_FILTER == 'w' else 'Black'}\")\n",
    "print(f\"üîí Minimum holdout players: {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9435033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£  Extracting player-opening stats (color: 'w')...\n",
      "   ‚Ä¢ Minimum rating filter: 1200\n",
      "\n",
      "2Ô∏è‚É£  Selecting holdout players for fold-in verification...\n",
      "   ‚Ä¢ Holdout size: 1,000 players minimum\n",
      "   ‚Ä¢ Total eligible players: 49,551\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 48,551\n",
      "   ‚Ä¢ Holdout percentage: 2.0%\n",
      "   ‚Ä¢ Total eligible players: 49,551\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 48,551\n",
      "   ‚Ä¢ Holdout percentage: 2.0%\n",
      "\n",
      "‚úì Database connection closed\n",
      "\n",
      "‚úì Database connection closed\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Query interrupted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Extract data ONLY for training players\u001b[39;00m\n\u001b[32m     52\u001b[39m query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[33m    SELECT \u001b[39m\n\u001b[32m     54\u001b[39m \u001b[33m        pos.player_id,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33m    ORDER BY pos.player_id, pos.opening_id\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m raw_data = pd.DataFrame(\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m.df())\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úì Extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(raw_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Also save holdout player IDs for later use\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Query interrupted"
     ]
    }
   ],
   "source": [
    "# Get our training database\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "MAX_PLAYERS = 50_000\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player-opening stats (color: '{COLOR_FILTER}')...\")\n",
    "\n",
    "    # Extract stats with calculated score and num_games\n",
    "    # Filter by color, minimum rating, and calculate score in the database\n",
    "    MIN_RATING = 1200\n",
    "    print(f\"   ‚Ä¢ Minimum rating filter: {MIN_RATING}\")\n",
    "\n",
    "    print(f\"\\n2Ô∏è‚É£  Selecting holdout players for fold-in verification...\")\n",
    "    print(f\"   ‚Ä¢ Holdout size: {MIN_HOLDOUT_PLAYERS:,} players minimum\")\n",
    "\n",
    "    player_query = f\"\"\"\n",
    "        SELECT DISTINCT p.id as player_id\n",
    "        FROM player p\n",
    "        JOIN player_opening_stats pos ON p.id = pos.player_id\n",
    "        WHERE p.rating >= {MIN_RATING}\n",
    "        AND pos.color = '{COLOR_FILTER}'\n",
    "        LIMIT {MAX_PLAYERS}\n",
    "    \"\"\"\n",
    "\n",
    "    all_eligible_players = pd.DataFrame(con.execute(player_query).df())\n",
    "    total_eligible = len(all_eligible_players)\n",
    "    print(f\"   ‚Ä¢ Total eligible players: {total_eligible:,}\")\n",
    "\n",
    "    if total_eligible < MIN_HOLDOUT_PLAYERS:\n",
    "        raise ValueError(f\"Not enough eligible players ({total_eligible:,}) to create holdout set of {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "    # Randomly sample holdout players (deterministic with seed)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    holdout_player_ids = np.random.choice(\n",
    "        all_eligible_players['player_id'].values,\n",
    "        size=MIN_HOLDOUT_PLAYERS,\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    training_player_ids = set(all_eligible_players['player_id'].values) - set(holdout_player_ids)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Holdout players selected: {len(holdout_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Training players available: {len(training_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Holdout percentage: {100 * len(holdout_player_ids) / total_eligible:.1f}%\")\n",
    "\n",
    "    # SQL-friendly string\n",
    "    training_player_ids_str = ','.join(map(str, training_player_ids))\n",
    "\n",
    "    # Extract data ONLY for training players\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            pos.player_id,\n",
    "            pos.opening_id,\n",
    "            pos.num_wins + pos.num_draws + pos.num_losses as num_games,\n",
    "            (pos.num_wins + (pos.num_draws * 0.5)) / \n",
    "                NULLIF(pos.num_wins + pos.num_draws + pos.num_losses, 0) as score,\n",
    "            o.eco\n",
    "        FROM player_opening_stats pos\n",
    "        JOIN opening o ON pos.opening_id = o.id\n",
    "        JOIN player p ON pos.player_id = p.id\n",
    "        WHERE pos.color = '{COLOR_FILTER}'\n",
    "        AND p.rating >= {MIN_RATING}\n",
    "        AND pos.player_id IN ({training_player_ids_str})\n",
    "        ORDER BY pos.player_id, pos.opening_id\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data = pd.DataFrame(con.execute(query).df())\n",
    "\n",
    "    print(f\"   ‚úì Extracted {len(raw_data):,} rows\")\n",
    "\n",
    "    # Also save holdout player IDs for later use\n",
    "    holdout_players_df = pd.DataFrame({'player_id': holdout_player_ids})\n",
    "\n",
    "    # Schema verification\n",
    "    required_columns = ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
    "\n",
    "    for col in required_columns:\n",
    "        if col not in raw_data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    print(f\"   ‚úì All required columns present: {required_columns}\")\n",
    "\n",
    "    # Data types verification\n",
    "    print(f\"   ‚Ä¢ player_id: {raw_data['player_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ opening_id: {raw_data['opening_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ num_games: {raw_data['num_games'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ score: {raw_data['score'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ eco: {raw_data['eco'].dtype}\")\n",
    "\n",
    "    print(\"\\n6Ô∏è‚É£  Data statistics...\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {len(raw_data):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Total games (sum): {raw_data['num_games'].sum():,}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Player ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['player_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['player_id'].max()}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Opening ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['opening_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['opening_id'].max()}\")\n",
    "\n",
    "    print(f\"\\n   Games per entry:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['num_games'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['num_games'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['num_games'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['num_games'].median():.0f}\")\n",
    "\n",
    "    print(f\"\\n   Score distribution:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['score'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['score'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['score'].mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['score'].median():.4f}\")\n",
    "\n",
    "    print(\"\\n7Ô∏è‚É£  Checking for null values...\")\n",
    "    null_counts = raw_data.isnull().sum()\n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"   ‚úì No null values found\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "\n",
    "    print(\"\\n8Ô∏è‚É£  Sample of extracted data (first 10 rows):\")\n",
    "    print(raw_data.head(10).to_string())\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nData shape: {raw_data.shape}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "\n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"\\n‚úì Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad110b85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "519d8f2f",
   "metadata": {},
   "source": [
    "## Step 2: Data Sanitization & Normalization\n",
    "\n",
    "Filter low-quality data, handle duplicates, and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: DATA SANITIZATION & NORMALIZATION\n",
      "\n",
      "üìä Starting data shape: (11571380, 5)\n",
      "   ‚Ä¢ Rows: 11,571,380\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Before: 11,571,380 rows\n",
      "   ‚Ä¢ After: 2,898,426 rows\n",
      "   ‚Ä¢ Filtered out: 8,672,954 rows (75.0%)\n",
      "   ‚Ä¢ Before: 11,571,380 rows\n",
      "   ‚Ä¢ After: 2,898,426 rows\n",
      "   ‚Ä¢ Filtered out: 8,672,954 rows (75.0%)\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Data statistics:\n",
      "   ‚Ä¢ Total rows: 2,898,426\n",
      "   ‚Ä¢ Unique players: 48,469\n",
      "   ‚Ä¢ Unique openings: 2,717\n",
      "   ‚Ä¢ Total games: 206,310,936\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1066.8\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Data statistics:\n",
      "   ‚Ä¢ Total rows: 2,898,426\n",
      "   ‚Ä¢ Unique players: 48,469\n",
      "   ‚Ä¢ Unique openings: 2,717\n",
      "   ‚Ä¢ Total games: 206,310,936\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1066.8\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5111\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "1768009      29872        2716         10  0.400000  D46\n",
      "2579236      43609        2478        100  0.505000  D05\n",
      "2452387      41485         910        322  0.560559  B10\n",
      "2098818      35476        2462         24  0.375000  D02\n",
      "1875067      31673        1582         67  0.350746  C24\n",
      "594098        9981        2539         31  0.403226  D15\n",
      "1422152      24089        1001        248  0.522177  B20\n",
      "1589081      26857        1356         19  0.526316  C00\n",
      "1267779      21499         696         28  0.392857  B00\n",
      "687697       11532         838        187  0.548128  B06\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2898426, 5)\n",
      "Data reduction: 75.0%\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5111\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "1768009      29872        2716         10  0.400000  D46\n",
      "2579236      43609        2478        100  0.505000  D05\n",
      "2452387      41485         910        322  0.560559  B10\n",
      "2098818      35476        2462         24  0.375000  D02\n",
      "1875067      31673        1582         67  0.350746  C24\n",
      "594098        9981        2539         31  0.403226  D15\n",
      "1422152      24089        1001        248  0.522177  B20\n",
      "1589081      26857        1356         19  0.526316  C00\n",
      "1267779      21499         696         28  0.392857  B00\n",
      "687697       11532         838        187  0.548128  B06\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2898426, 5)\n",
      "Data reduction: 75.0%\n"
     ]
    }
   ],
   "source": [
    "# 2a. Filter low-quality data, handle duplicates, and prepare for training.\n",
    "\n",
    "# Configuration\n",
    "MIN_GAMES_THRESHOLD = 10\n",
    "\n",
    "print(\"STEP 2: DATA SANITIZATION & NORMALIZATION\")\n",
    "\n",
    "# Start with raw_data from Step 1\n",
    "print(f\"\\nüìä Starting data shape: {raw_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Rows: {len(raw_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "\n",
    "# 1. Filter by minimum games threshold\n",
    "print(f\"\\n1Ô∏è‚É£  Filtering entries with < {MIN_GAMES_THRESHOLD} games...\")\n",
    "before_filter = len(raw_data)\n",
    "clean_data = raw_data.query(f'num_games >= {MIN_GAMES_THRESHOLD}').copy()\n",
    "num_rows_after_filter = len(clean_data)\n",
    "num_rows_filtered_out = before_filter - num_rows_after_filter\n",
    "\n",
    "print(f\"   ‚Ä¢ Before: {before_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ After: {num_rows_after_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ Filtered out: {num_rows_filtered_out:,} rows ({100*num_rows_filtered_out/before_filter:.1f}%)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "num_duplicates = clean_data.duplicated(subset=['player_id', 'opening_id']).sum()\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {num_duplicates} duplicate player-opening entries\")\n",
    "    dup_mask = clean_data.duplicated(subset=['player_id', 'opening_id'], keep=False)\n",
    "    print(\"\\n   Sample of duplicates:\")\n",
    "    print(clean_data[dup_mask].head(10).to_string())\n",
    "    \n",
    "    # Keep only first occurrence of any duplicate player-opening pair\n",
    "    print(\"\\n   Removing duplicates (keeping first occurrence)...\")\n",
    "    clean_data = pd.DataFrame.drop_duplicates(clean_data, subset=['player_id', 'opening_id'], keep='first')\n",
    "    print(f\"   ‚úì After deduplication: {len(clean_data):,} rows\")\n",
    "\n",
    "# 3. Remove players with no qualifying openings\n",
    "\n",
    "# Note that a few players only play stuff like the Van't Kruijs which we've excluded, so a small numer of players will be excluded here\n",
    "players_before = clean_data['player_id'].nunique()\n",
    "\n",
    "# Count openings per player\n",
    "num_openings_per_player = pd.DataFrame(clean_data.groupby('player_id').size(), columns=['count'])\n",
    "players_with_data = num_openings_per_player[num_openings_per_player['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter\n",
    "clean_data = clean_data[clean_data['player_id'].isin(players_with_data)]\n",
    "players_after = clean_data['player_id'].nunique()\n",
    "\n",
    "# 4. Remove openings with no qualifying players\n",
    "num_openings_before = clean_data['opening_id'].nunique()\n",
    "\n",
    "num_players_per_opening = pd.DataFrame(clean_data.groupby('opening_id').size(), columns=['count'])\n",
    "openings_with_data = num_players_per_opening[num_players_per_opening['count'] > 0].index.tolist()\n",
    "\n",
    "clean_data = clean_data[clean_data['opening_id'].isin(openings_with_data)]\n",
    "openings_after = clean_data['opening_id'].nunique()\n",
    "\n",
    "# 5. Verify no null values using pd.isna()\n",
    "null_counts = pd.DataFrame.isna(clean_data).sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"   ‚úì No null values found\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "    for col, count in null_counts[null_counts > 0].items():\n",
    "        print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    # Drop rows with nulls using pd.DataFrame.dropna()\n",
    "    clean_data = pd.DataFrame.dropna(clean_data)\n",
    "    print(f\"   ‚úì Dropped null rows. New shape: {clean_data.shape}\")\n",
    "\n",
    "# Reset index using pd.DataFrame.reset_index()\n",
    "clean_data = pd.DataFrame.reset_index(clean_data, drop=True)\n",
    "\n",
    "# Final statistics using pd functions\n",
    "print(f\"\\n6Ô∏è‚É£  Data statistics:\")\n",
    "print(f\"   ‚Ä¢ Total rows: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {pd.Series.nunique(clean_data['player_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {pd.Series.nunique(clean_data['opening_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Total games: {pd.Series.sum(clean_data['num_games']):,}\")\n",
    "print(f\"   ‚Ä¢ Avg games per entry: {pd.Series.mean(clean_data['num_games']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg openings per player: {len(clean_data) / pd.Series.nunique(clean_data['player_id']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg players per opening: {len(clean_data) / pd.Series.nunique(clean_data['opening_id']):.1f}\")\n",
    "\n",
    "# Score distribution using pd functions\n",
    "print(f\"\\n   Score statistics:\")\n",
    "print(f\"   ‚Ä¢ Min: {pd.Series.min(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {pd.Series.quantile(clean_data['score'], 0.25):.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {pd.Series.median(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {pd.Series.quantile(clean_data['score'], 0.75):.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {pd.Series.max(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {pd.Series.mean(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {pd.Series.std(clean_data['score']):.4f}\")\n",
    "\n",
    "# Sample of cleaned data using pd.DataFrame.sample()\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of cleaned data (10 random rows):\")\n",
    "print(pd.DataFrame.sample(clean_data, min(10, len(clean_data)), random_state=42).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SANITIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCleaned data shape: {clean_data.shape}\")\n",
    "print(f\"Data reduction: {100 * (1 - len(clean_data)/len(raw_data)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\n",
      "============================================================\n",
      "\n",
      "üìä Global statistics:\n",
      "   ‚Ä¢ Global mean score: 0.5111\n",
      "   ‚Ä¢ Total entries: 2,898,426\n",
      "   ‚Ä¢ Unique openings: 2,717\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1667\n",
      "   ‚Ä¢ 25th percentile: 0.4962\n",
      "   ‚Ä¢ Median: 0.5162\n",
      "   ‚Ä¢ 75th percentile: 0.5364\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0509\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4811\n",
      "   ‚Ä¢ Players per opening (median): 155\n",
      "   ‚Ä¢ Total games range: [10, 5503030]\n",
      "   ‚Ä¢ Players range: [1, 42888]\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1667\n",
      "   ‚Ä¢ 25th percentile: 0.4962\n",
      "   ‚Ä¢ Median: 0.5162\n",
      "   ‚Ä¢ 75th percentile: 0.5364\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0509\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4811\n",
      "   ‚Ä¢ Players per opening (median): 155\n",
      "   ‚Ä¢ Total games range: [10, 5503030]\n",
      "   ‚Ä¢ Players range: [1, 42888]\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,898,426 entries\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Range: [0.1667, 0.9963]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000962\n",
      "   ‚Ä¢ Std adjustment: 0.076777\n",
      "   ‚Ä¢ Max adjustment: 0.457327\n",
      "   ‚Ä¢ Min adjustment: -0.457520\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,898,426 entries\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Range: [0.1667, 0.9963]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000962\n",
      "   ‚Ä¢ Std adjustment: 0.076777\n",
      "   ‚Ä¢ Max adjustment: 0.457327\n",
      "   ‚Ä¢ Min adjustment: -0.457520\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003659\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001793\n",
      "   ‚Ä¢ 75th percentile (n=65 games): avg adjustment = -0.000408\n",
      "   ‚Ä¢ >75th percentile (n>65 games): avg adjustment = -0.001364\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003659\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001793\n",
      "   ‚Ä¢ 75th percentile (n=65 games): avg adjustment = -0.000408\n",
      "   ‚Ä¢ >75th percentile (n>65 games): avg adjustment = -0.001364\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ Median: 0.5118\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   ‚Ä¢ Median: 0.5118\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   Player 38179 | Opening 1367 | Games:  13 | Opening mean: 0.5176 | Original: 0.5000 ‚Üí Adjusted: 0.5140 | Diff: +0.0140 | Confidence: 0.206\n",
      "   Player 29283 | Opening  536 | Games:  11 | Opening mean: 0.5083 | Original: 0.6364 ‚Üí Adjusted: 0.5314 | Diff: -0.1050 | Confidence: 0.180\n",
      "   Player 43835 | Opening  696 | Games:  12 | Opening mean: 0.5257 | Original: 0.5417 ‚Üí Adjusted: 0.5288 | Diff: -0.0129 | Confidence: 0.194\n",
      "   Player 31680 | Opening 3116 | Games:  20 | Opening mean: 0.5107 | Original: 0.3750 ‚Üí Adjusted: 0.4719 | Diff: +0.0969 | Confidence: 0.286\n",
      "   Player 30843 | Opening  768 | Games:  14 | Opening mean: 0.4875 | Original: 0.7857 ‚Üí Adjusted: 0.5528 | Diff: -0.2330 | Confidence: 0.219\n",
      "   Player 34217 | Opening 1972 | Games:  15 | Opening mean: 0.5039 | Original: 0.9000 ‚Üí Adjusted: 0.5953 | Diff: -0.3047 | Confidence: 0.231\n",
      "   Player 19258 | Opening 2312 | Games:  10 | Opening mean: 0.4933 | Original: 0.5000 ‚Üí Adjusted: 0.4944 | Diff: -0.0056 | Confidence: 0.167\n",
      "   Player 10029 | Opening 2879 | Games:  18 | Opening mean: 0.5235 | Original: 0.5556 ‚Üí Adjusted: 0.5320 | Diff: -0.0235 | Confidence: 0.265\n",
      "   Player 17177 | Opening 2316 | Games:  13 | Opening mean: 0.5358 | Original: 0.4231 ‚Üí Adjusted: 0.5125 | Diff: +0.0895 | Confidence: 0.206\n",
      "   Player 33371 | Opening 1167 | Games:  19 | Opening mean: 0.4964 | Original: 0.4211 ‚Üí Adjusted: 0.4756 | Diff: +0.0546 | Confidence: 0.275\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 35593 | Opening  376 | Games:  97 | Opening mean: 0.5178 | Original: 0.4330 ‚Üí Adjusted: 0.4618 | Diff: +0.0288 | Confidence: 0.660\n",
      "   Player 10582 | Opening  207 | Games:  79 | Opening mean: 0.4618 | Original: 0.5063 ‚Üí Adjusted: 0.4891 | Diff: -0.0172 | Confidence: 0.612\n",
      "   Player 26753 | Opening 1194 | Games:  57 | Opening mean: 0.4954 | Original: 0.4825 ‚Üí Adjusted: 0.4885 | Diff: +0.0060 | Confidence: 0.533\n",
      "   Player 24340 | Opening 1570 | Games:  61 | Opening mean: 0.5174 | Original: 0.5984 ‚Üí Adjusted: 0.5619 | Diff: -0.0365 | Confidence: 0.550\n",
      "   Player  7457 | Opening 2246 | Games:  75 | Opening mean: 0.5783 | Original: 0.4733 ‚Üí Adjusted: 0.5153 | Diff: +0.0420 | Confidence: 0.600\n",
      "   Player 33807 | Opening 1374 | Games:  67 | Opening mean: 0.5214 | Original: 0.5896 ‚Üí Adjusted: 0.5604 | Diff: -0.0291 | Confidence: 0.573\n",
      "   Player 38831 | Opening  768 | Games:  58 | Opening mean: 0.4875 | Original: 0.4052 ‚Üí Adjusted: 0.4433 | Diff: +0.0381 | Confidence: 0.537\n",
      "   Player  6542 | Opening 1124 | Games:  53 | Opening mean: 0.5158 | Original: 0.4906 ‚Üí Adjusted: 0.5028 | Diff: +0.0123 | Confidence: 0.515\n",
      "   Player  2257 | Opening 2519 | Games:  67 | Opening mean: 0.5228 | Original: 0.5672 ‚Üí Adjusted: 0.5482 | Diff: -0.0190 | Confidence: 0.573\n",
      "   Player  3711 | Opening  799 | Games:  59 | Opening mean: 0.4845 | Original: 0.5508 ‚Üí Adjusted: 0.5204 | Diff: -0.0304 | Confidence: 0.541\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 38179 | Opening 1367 | Games:  13 | Opening mean: 0.5176 | Original: 0.5000 ‚Üí Adjusted: 0.5140 | Diff: +0.0140 | Confidence: 0.206\n",
      "   Player 29283 | Opening  536 | Games:  11 | Opening mean: 0.5083 | Original: 0.6364 ‚Üí Adjusted: 0.5314 | Diff: -0.1050 | Confidence: 0.180\n",
      "   Player 43835 | Opening  696 | Games:  12 | Opening mean: 0.5257 | Original: 0.5417 ‚Üí Adjusted: 0.5288 | Diff: -0.0129 | Confidence: 0.194\n",
      "   Player 31680 | Opening 3116 | Games:  20 | Opening mean: 0.5107 | Original: 0.3750 ‚Üí Adjusted: 0.4719 | Diff: +0.0969 | Confidence: 0.286\n",
      "   Player 30843 | Opening  768 | Games:  14 | Opening mean: 0.4875 | Original: 0.7857 ‚Üí Adjusted: 0.5528 | Diff: -0.2330 | Confidence: 0.219\n",
      "   Player 34217 | Opening 1972 | Games:  15 | Opening mean: 0.5039 | Original: 0.9000 ‚Üí Adjusted: 0.5953 | Diff: -0.3047 | Confidence: 0.231\n",
      "   Player 19258 | Opening 2312 | Games:  10 | Opening mean: 0.4933 | Original: 0.5000 ‚Üí Adjusted: 0.4944 | Diff: -0.0056 | Confidence: 0.167\n",
      "   Player 10029 | Opening 2879 | Games:  18 | Opening mean: 0.5235 | Original: 0.5556 ‚Üí Adjusted: 0.5320 | Diff: -0.0235 | Confidence: 0.265\n",
      "   Player 17177 | Opening 2316 | Games:  13 | Opening mean: 0.5358 | Original: 0.4231 ‚Üí Adjusted: 0.5125 | Diff: +0.0895 | Confidence: 0.206\n",
      "   Player 33371 | Opening 1167 | Games:  19 | Opening mean: 0.4964 | Original: 0.4211 ‚Üí Adjusted: 0.4756 | Diff: +0.0546 | Confidence: 0.275\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 35593 | Opening  376 | Games:  97 | Opening mean: 0.5178 | Original: 0.4330 ‚Üí Adjusted: 0.4618 | Diff: +0.0288 | Confidence: 0.660\n",
      "   Player 10582 | Opening  207 | Games:  79 | Opening mean: 0.4618 | Original: 0.5063 ‚Üí Adjusted: 0.4891 | Diff: -0.0172 | Confidence: 0.612\n",
      "   Player 26753 | Opening 1194 | Games:  57 | Opening mean: 0.4954 | Original: 0.4825 ‚Üí Adjusted: 0.4885 | Diff: +0.0060 | Confidence: 0.533\n",
      "   Player 24340 | Opening 1570 | Games:  61 | Opening mean: 0.5174 | Original: 0.5984 ‚Üí Adjusted: 0.5619 | Diff: -0.0365 | Confidence: 0.550\n",
      "   Player  7457 | Opening 2246 | Games:  75 | Opening mean: 0.5783 | Original: 0.4733 ‚Üí Adjusted: 0.5153 | Diff: +0.0420 | Confidence: 0.600\n",
      "   Player 33807 | Opening 1374 | Games:  67 | Opening mean: 0.5214 | Original: 0.5896 ‚Üí Adjusted: 0.5604 | Diff: -0.0291 | Confidence: 0.573\n",
      "   Player 38831 | Opening  768 | Games:  58 | Opening mean: 0.4875 | Original: 0.4052 ‚Üí Adjusted: 0.4433 | Diff: +0.0381 | Confidence: 0.537\n",
      "   Player  6542 | Opening 1124 | Games:  53 | Opening mean: 0.5158 | Original: 0.4906 ‚Üí Adjusted: 0.5028 | Diff: +0.0123 | Confidence: 0.515\n",
      "   Player  2257 | Opening 2519 | Games:  67 | Opening mean: 0.5228 | Original: 0.5672 ‚Üí Adjusted: 0.5482 | Diff: -0.0190 | Confidence: 0.573\n",
      "   Player  3711 | Opening  799 | Games:  59 | Opening mean: 0.4845 | Original: 0.5508 ‚Üí Adjusted: 0.5204 | Diff: -0.0304 | Confidence: 0.541\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player  1981 | Opening  132 | Games: 291 | Opening mean: 0.5207 | Original: 0.4983 ‚Üí Adjusted: 0.5016 | Diff: +0.0033 | Confidence: 0.853\n",
      "   Player 28213 | Opening  476 | Games: 568 | Opening mean: 0.5150 | Original: 0.5000 ‚Üí Adjusted: 0.5012 | Diff: +0.0012 | Confidence: 0.919\n",
      "   Player 22217 | Opening  218 | Games: 641 | Opening mean: 0.4700 | Original: 0.5328 ‚Üí Adjusted: 0.5282 | Diff: -0.0045 | Confidence: 0.928\n",
      "   Player 49218 | Opening  178 | Games: 334 | Opening mean: 0.5197 | Original: 0.4805 ‚Üí Adjusted: 0.4856 | Diff: +0.0051 | Confidence: 0.870\n",
      "   Player 43455 | Opening 2478 | Games: 764 | Opening mean: 0.5160 | Original: 0.5491 ‚Üí Adjusted: 0.5471 | Diff: -0.0020 | Confidence: 0.939\n",
      "   Player 27038 | Opening  772 | Games: 417 | Opening mean: 0.5163 | Original: 0.5360 ‚Üí Adjusted: 0.5339 | Diff: -0.0021 | Confidence: 0.893\n",
      "   Player 10236 | Opening  772 | Games: 256 | Opening mean: 0.5163 | Original: 0.5273 ‚Üí Adjusted: 0.5255 | Diff: -0.0018 | Confidence: 0.837\n",
      "   Player 18346 | Opening 3461 | Games: 216 | Opening mean: 0.4929 | Original: 0.4514 ‚Üí Adjusted: 0.4592 | Diff: +0.0078 | Confidence: 0.812\n",
      "   Player 41245 | Opening 1355 | Games: 552 | Opening mean: 0.5037 | Original: 0.5226 ‚Üí Adjusted: 0.5211 | Diff: -0.0016 | Confidence: 0.917\n",
      "   Player 32963 | Opening 2478 | Games: 1064 | Opening mean: 0.5160 | Original: 0.5329 ‚Üí Adjusted: 0.5321 | Diff: -0.0008 | Confidence: 0.955\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Player  1981 | Opening  132 | Games: 291 | Opening mean: 0.5207 | Original: 0.4983 ‚Üí Adjusted: 0.5016 | Diff: +0.0033 | Confidence: 0.853\n",
      "   Player 28213 | Opening  476 | Games: 568 | Opening mean: 0.5150 | Original: 0.5000 ‚Üí Adjusted: 0.5012 | Diff: +0.0012 | Confidence: 0.919\n",
      "   Player 22217 | Opening  218 | Games: 641 | Opening mean: 0.4700 | Original: 0.5328 ‚Üí Adjusted: 0.5282 | Diff: -0.0045 | Confidence: 0.928\n",
      "   Player 49218 | Opening  178 | Games: 334 | Opening mean: 0.5197 | Original: 0.4805 ‚Üí Adjusted: 0.4856 | Diff: +0.0051 | Confidence: 0.870\n",
      "   Player 43455 | Opening 2478 | Games: 764 | Opening mean: 0.5160 | Original: 0.5491 ‚Üí Adjusted: 0.5471 | Diff: -0.0020 | Confidence: 0.939\n",
      "   Player 27038 | Opening  772 | Games: 417 | Opening mean: 0.5163 | Original: 0.5360 ‚Üí Adjusted: 0.5339 | Diff: -0.0021 | Confidence: 0.893\n",
      "   Player 10236 | Opening  772 | Games: 256 | Opening mean: 0.5163 | Original: 0.5273 ‚Üí Adjusted: 0.5255 | Diff: -0.0018 | Confidence: 0.837\n",
      "   Player 18346 | Opening 3461 | Games: 216 | Opening mean: 0.4929 | Original: 0.4514 ‚Üí Adjusted: 0.4592 | Diff: +0.0078 | Confidence: 0.812\n",
      "   Player 41245 | Opening 1355 | Games: 552 | Opening mean: 0.5037 | Original: 0.5226 ‚Üí Adjusted: 0.5211 | Diff: -0.0016 | Confidence: 0.917\n",
      "   Player 32963 | Opening 2478 | Games: 1064 | Opening mean: 0.5160 | Original: 0.5329 ‚Üí Adjusted: 0.5321 | Diff: -0.0008 | Confidence: 0.955\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4889 vs global) | 1 player entries\n",
      "   Opening 3551 (C19): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2881 (E10): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2046 (C49): mean = 0.7857 (+0.2746 vs global) | 1 player entries\n",
      "   Opening 1659 (C30): mean = 0.7692 (+0.2582 vs global) | 1 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4889 vs global) | 1 player entries\n",
      "   Opening 3551 (C19): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2881 (E10): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2046 (C49): mean = 0.7857 (+0.2746 vs global) | 1 player entries\n",
      "   Opening 1659 (C30): mean = 0.7692 (+0.2582 vs global) | 1 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 2593 (D26): mean = 0.1667 (-0.3444 vs global) | 1 player entries\n",
      "   Opening 1918 (C42): mean = 0.2000 (-0.3111 vs global) | 1 player entries\n",
      "   Opening  636 (A83): mean = 0.2006 (-0.3104 vs global) | 2 player entries\n",
      "   Opening 3496 (A40): mean = 0.2273 (-0.2838 vs global) | 1 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "   Opening 2593 (D26): mean = 0.1667 (-0.3444 vs global) | 1 player entries\n",
      "   Opening 1918 (C42): mean = 0.2000 (-0.3111 vs global) | 1 player entries\n",
      "   Opening  636 (A83): mean = 0.2006 (-0.3104 vs global) | 2 player entries\n",
      "   Opening 3496 (A40): mean = 0.2273 (-0.2838 vs global) | 1 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 21108 | Opening 1811 (C39) | Games: 10 | Opening mean: 1.0000 | Original: 1.0000 ‚Üí 1.0000\n",
      "      If we'd shrunk to global mean: 0.5926 (would lose +0.4074 of deserved credit)\n",
      "   Player 16788 | Opening 3292 (C54) | Games: 15 | Opening mean: 0.7286 | Original: 0.7333 ‚Üí 0.7297\n",
      "      If we'd shrunk to global mean: 0.5624 (would lose +0.1673 of deserved credit)\n",
      "   Player 16548 | Opening 3292 (C54) | Games: 19 | Opening mean: 0.7286 | Original: 0.7895 ‚Üí 0.7453\n",
      "      If we'd shrunk to global mean: 0.5877 (would lose +0.1576 of deserved credit)\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 21108 | Opening 1811 (C39) | Games: 10 | Opening mean: 1.0000 | Original: 1.0000 ‚Üí 1.0000\n",
      "      If we'd shrunk to global mean: 0.5926 (would lose +0.4074 of deserved credit)\n",
      "   Player 16788 | Opening 3292 (C54) | Games: 15 | Opening mean: 0.7286 | Original: 0.7333 ‚Üí 0.7297\n",
      "      If we'd shrunk to global mean: 0.5624 (would lose +0.1673 of deserved credit)\n",
      "   Player 16548 | Opening 3292 (C54) | Games: 19 | Opening mean: 0.7286 | Original: 0.7895 ‚Üí 0.7453\n",
      "      If we'd shrunk to global mean: 0.5877 (would lose +0.1576 of deserved credit)\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player  1857 | Opening 2293 (C71) | Games: 13 | Opening mean: 0.2665 | Original: 0.1538 ‚Üí 0.2433\n",
      "      If we'd shrunk to global mean: 0.4374 (would unfairly boost by +0.1941)\n",
      "   Player 19116 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3366 | Original: 0.2000 ‚Üí 0.3138\n",
      "      If we'd shrunk to global mean: 0.4592 (would unfairly boost by +0.1454)\n",
      "   Player  4605 | Opening 1779 (C37) | Games: 13 | Opening mean: 0.3366 | Original: 0.3846 ‚Üí 0.3465\n",
      "      If we'd shrunk to global mean: 0.4850 (would unfairly boost by +0.1385)\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (2898426, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player  1857 | Opening 2293 (C71) | Games: 13 | Opening mean: 0.2665 | Original: 0.1538 ‚Üí 0.2433\n",
      "      If we'd shrunk to global mean: 0.4374 (would unfairly boost by +0.1941)\n",
      "   Player 19116 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3366 | Original: 0.2000 ‚Üí 0.3138\n",
      "      If we'd shrunk to global mean: 0.4592 (would unfairly boost by +0.1454)\n",
      "   Player  4605 | Opening 1779 (C37) | Games: 13 | Opening mean: 0.3366 | Original: 0.3846 ‚Üí 0.3465\n",
      "      If we'd shrunk to global mean: 0.4850 (would unfairly boost by +0.1385)\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (2898426, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n"
     ]
    }
   ],
   "source": [
    "# 2b. Apply hierarchical Bayesian shrinkage to adjust scores based on sample size confidence\n",
    "\n",
    "# Check if confidence already exists - if so, skip this processing\n",
    "if 'confidence' in clean_data.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'confidence' column already exists in data\")\n",
    "else:\n",
    "    # Define the processing function\n",
    "    # This is a long function, I recommend you fold it down in your editor\n",
    "    def apply_hierarchical_bayesian_shrinkage(data, k_player=50):\n",
    "        \"\"\"\n",
    "        Apply two-level hierarchical Bayesian shrinkage to adjust scores.\n",
    "        \n",
    "        A lot of our player-opening entries have a small number of games played, because openings are so specific.\n",
    "        This introduces sample size issues.\n",
    "        \n",
    "        We use TWO-LEVEL shrinkage:\n",
    "        Level 1: Calculate opening-specific means (these are our \"ground truth\" for each opening)\n",
    "        Level 2: Shrink individual player-opening scores toward their opening's mean\n",
    "        This is better than shrinking toward global mean because different openings have different baseline win rates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Clean data with columns: player_id, opening_id, score, num_games, eco\n",
    "        k_player : int\n",
    "            Shrinkage constant for player-opening scores (default: 50)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Data with adjusted scores and new 'confidence' column\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Calculate global mean score for comparison\n",
    "        global_mean_score = data[\"score\"].mean()\n",
    "        print(f\"\\nüìä Global statistics:\")\n",
    "        print(f\"   ‚Ä¢ Global mean score: {global_mean_score:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Total entries: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique openings: {data['opening_id'].nunique():,}\")\n",
    "        \n",
    "        # Store original scores for comparison\n",
    "        data = data.copy()\n",
    "        data[\"score_original\"] = data[\"score\"].copy()\n",
    "        \n",
    "        # LEVEL 1: Calculate opening-specific means and statistics        \n",
    "        opening_stats = (\n",
    "            data.groupby(\"opening_id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"score\": \"mean\",\n",
    "                    \"num_games\": \"sum\",\n",
    "                    \"player_id\": \"count\",  # Number of players who played this opening\n",
    "                }\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"score\": \"opening_mean\",\n",
    "                    \"num_games\": \"opening_total_games\",\n",
    "                    \"player_id\": \"opening_num_players\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        # Opening mean statistics\n",
    "        print(f\"\\n   Opening mean score distribution:\")\n",
    "        print(f\"   ‚Ä¢ Min: {opening_stats['opening_mean'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {opening_stats['opening_mean'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {opening_stats['opening_mean'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {opening_stats['opening_mean'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {opening_stats['opening_mean'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {opening_stats['opening_mean'].std():.4f}\")\n",
    "        \n",
    "        # Show distribution of opening sizes\n",
    "        print(f\"\\n   Opening sample size distribution:\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games per opening (median): {opening_stats['opening_total_games'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players per opening (median): {opening_stats['opening_num_players'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games range: [{opening_stats['opening_total_games'].min():.0f}, {opening_stats['opening_total_games'].max():.0f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players range: [{opening_stats['opening_num_players'].min():.0f}, {opening_stats['opening_num_players'].max():.0f}]\"\n",
    "        )\n",
    "        \n",
    "        # Merge opening means back into main dataframe\n",
    "        data = data.merge(\n",
    "            opening_stats[[\"opening_mean\"]], left_on=\"opening_id\", right_index=True, how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # LEVEL 2: Shrink player-opening scores toward opening-specific means\n",
    "        print(\n",
    "            f\"   Formula: adjusted_score = (num_games √ó player_score + {k_player} √ó opening_mean) / (num_games + {k_player})\"\n",
    "        )\n",
    "        \n",
    "        numerator = (data[\"num_games\"] * data[\"score_original\"]) + (\n",
    "            k_player * data[\"opening_mean\"]\n",
    "        )\n",
    "        denominator = data[\"num_games\"] + k_player\n",
    "        data[\"score\"] = numerator / denominator\n",
    "        \n",
    "        print(f\"   ‚úì Scores adjusted for {len(data):,} entries\")\n",
    "        \n",
    "        # Calculate confidence weights (will be used in loss function later)\n",
    "        data[\"confidence\"] = data[\"num_games\"] / (\n",
    "            data[\"num_games\"] + k_player\n",
    "        )\n",
    "        print(f\"   ‚úì Confidence weights calculated\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Range: [{data['confidence'].min():.4f}, {data['confidence'].max():.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # Statistics on the adjustment\n",
    "        score_diff = data[\"score\"] - data[\"score_original\"]\n",
    "        print(f\"\\n4Ô∏è‚É£  Adjustment statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean adjustment: {score_diff.mean():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Std adjustment: {score_diff.std():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Max adjustment: {score_diff.max():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Min adjustment: {score_diff.min():.6f}\")\n",
    "        \n",
    "        # Show distribution of adjustments\n",
    "        print(f\"\\n   Adjustment by num_games quartiles:\")\n",
    "        quartiles = data[\"num_games\"].quantile([0.25, 0.5, 0.75])\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 25th percentile (n={quartiles[0.25]:.0f} games): avg adjustment = {score_diff[data['num_games'] <= quartiles[0.25]].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 50th percentile (n={quartiles[0.5]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.25]) & (data['num_games'] <= quartiles[0.5])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 75th percentile (n={quartiles[0.75]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.5]) & (data['num_games'] <= quartiles[0.75])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ >75th percentile (n>{quartiles[0.75]:.0f} games): avg adjustment = {score_diff[data['num_games'] > quartiles[0.75]].mean():.6f}\"\n",
    "        )\n",
    "        \n",
    "        # New score distribution after adjustment\n",
    "        print(f\"\\n5Ô∏è‚É£  Adjusted score statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {data['score'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {data['score'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {data['score'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {data['score'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {data['score'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {data['score'].mean():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {data['score'].std():.4f}\")\n",
    "        \n",
    "        # Detailed sample showing the effect across different game counts\n",
    "        print(f\"\\n6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\")\n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        low_game_sample = data[\n",
    "            (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in low_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Medium-game entries (50-100 games) - MODERATE shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        med_game_sample = data[\n",
    "            (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in med_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   High-game entries (200+ games) - LOW shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        high_game_sample = data[data[\"num_games\"] >= 200].sample(\n",
    "            min(10, len(data[data[\"num_games\"] >= 200])), random_state=42\n",
    "        )\n",
    "        for idx, row in high_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        # Show extreme cases - comparing to both opening mean AND global mean\n",
    "        print(f\"\\n7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\")\n",
    "        \n",
    "        # Find entries where opening mean differs significantly from global mean\n",
    "        data[\"opening_deviation_from_global\"] = (\n",
    "            data[\"opening_mean\"] - global_mean_score\n",
    "        ).abs()\n",
    "        \n",
    "        print(f\"\\n   Openings with HIGHEST win rates (strong for White):\")\n",
    "        strong_openings = data.nlargest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in strong_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"(+{deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   Openings with LOWEST win rates (weak for White):\")\n",
    "        weak_openings = data.nsmallest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in weak_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"({deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        # Show specific examples where hierarchical shrinkage made a difference\n",
    "        print(f\"\\n8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\")\n",
    "        \n",
    "        # Find entries with strong openings where player did well\n",
    "        strong_opening_ids = data.nlargest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        strong_examples = data[\n",
    "            (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] > 0.6)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] > 0.6)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"\\n   Strong opening + good player performance (shrunk toward HIGH opening mean):\"\n",
    "        )\n",
    "        for idx, row in strong_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would lose {difference:+.4f} of deserved credit)\"\n",
    "            )\n",
    "        \n",
    "        # Find entries with weak openings where player did poorly\n",
    "        weak_opening_ids = data.nsmallest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        weak_examples = data[\n",
    "            (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] < 0.45)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] < 0.45)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Weak opening + poor player performance (shrunk toward LOW opening mean):\")\n",
    "        for idx, row in weak_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would unfairly boost by {-difference:+.4f})\"\n",
    "            )\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        data = data.drop(\n",
    "            columns=[\"score_original\", \"opening_mean\", \"opening_deviation_from_global\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nFinal data shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        print(f\"\\nNew columns added:\")\n",
    "        print(f\"   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\")\n",
    "        print(f\"   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\")\n",
    "        print(f\"\\nKey improvement over simple shrinkage:\")\n",
    "        print(f\"   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\")\n",
    "        print(f\"   ‚Ä¢ Preserves opening difficulty differences\")\n",
    "        print(f\"   ‚Ä¢ More accurate for both strong and weak openings\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    K_PLAYER = 50  # Shrinkage constant for player-opening scores\n",
    "    \n",
    "    clean_data = apply_hierarchical_bayesian_shrinkage(clean_data, k_player=K_PLAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2C: PLAYER RATING STATISTICS\n",
      "============================================================\n",
      "   ‚úì Retrieved ratings for 48,469 players\n",
      "   ‚úì All players have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 48,469\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1765.57\n",
      "   ‚Ä¢ Median: 1763\n",
      "   ‚Ä¢ Std Dev: 249.55\n",
      "\n",
      "5Ô∏è‚É£  Ratings Percentiles\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1359    ‚ñà‚ñà‚ñà\n",
      "      10%          1435    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1494    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1541    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1584    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1623    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1660    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1696    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1729    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1763    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1798    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1830    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1865    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1902    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1937    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1981    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2028    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2089    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2185    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400       3,578      7.38%      ‚ñà‚ñà\n",
      "   1400-1600       9,425     19.45%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800      13,744     28.36%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000      12,871     26.56%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       6,636     13.69%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400       1,859      3.84%      ‚ñà\n",
      "   2400-2600         331      0.68%      \n",
      "   2600-3000          25      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 353\n",
      "   ‚Ä¢ 10th-90th percentile range: 654\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1435):\n",
      "      Player 43145: vitaliy49 - Rating: 1435\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1584):\n",
      "      Player 3259: Blancachess - Rating: 1584\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1763):\n",
      "      Player 47874: Natala2807 - Rating: 1763\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1937):\n",
      "      Player 31108: isaidyow - Rating: 1937\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2089):\n",
      "      Player 34562: maranhense01 - Rating: 2089\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "   ‚Ä¢ Total players: 48,469\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1766 ¬± 250\n",
      "   ‚Ä¢ Median: 1763\n",
      "   ‚úì Retrieved ratings for 48,469 players\n",
      "   ‚úì All players have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 48,469\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1765.57\n",
      "   ‚Ä¢ Median: 1763\n",
      "   ‚Ä¢ Std Dev: 249.55\n",
      "\n",
      "5Ô∏è‚É£  Ratings Percentiles\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1359    ‚ñà‚ñà‚ñà\n",
      "      10%          1435    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1494    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1541    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1584    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1623    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1660    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1696    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1729    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1763    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1798    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1830    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1865    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1902    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1937    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1981    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2028    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2089    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2185    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400       3,578      7.38%      ‚ñà‚ñà\n",
      "   1400-1600       9,425     19.45%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800      13,744     28.36%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000      12,871     26.56%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       6,636     13.69%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400       1,859      3.84%      ‚ñà\n",
      "   2400-2600         331      0.68%      \n",
      "   2600-3000          25      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 353\n",
      "   ‚Ä¢ 10th-90th percentile range: 654\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1435):\n",
      "      Player 43145: vitaliy49 - Rating: 1435\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1584):\n",
      "      Player 3259: Blancachess - Rating: 1584\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1763):\n",
      "      Player 47874: Natala2807 - Rating: 1763\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1937):\n",
      "      Player 31108: isaidyow - Rating: 1937\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2089):\n",
      "      Player 34562: maranhense01 - Rating: 2089\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "   ‚Ä¢ Total players: 48,469\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1766 ¬± 250\n",
      "   ‚Ä¢ Median: 1763\n"
     ]
    }
   ],
   "source": [
    "# 2c. Gather player rating statistics\n",
    "\n",
    "print(\"STEP 2C: PLAYER RATING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to database and extract player ratings\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:    \n",
    "    unique_player_ids = clean_data['player_id'].unique()\n",
    "    player_ids_str = ','.join(map(str, unique_player_ids))\n",
    "    \n",
    "    # Query to get player ratings\n",
    "    rating_query = f\"\"\"\n",
    "        SELECT \n",
    "            id as player_id,\n",
    "            name,\n",
    "            title,\n",
    "            rating\n",
    "        FROM player\n",
    "        WHERE id IN ({player_ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    player_ratings = pd.DataFrame(con.execute(rating_query).df())\n",
    "    print(f\"   ‚úì Retrieved ratings for {len(player_ratings):,} players\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Check for missing ratings\n",
    "missing_ratings = player_ratings['rating'].isna().sum()\n",
    "if missing_ratings > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: {missing_ratings:,} players have missing ratings\")\n",
    "else:\n",
    "    print(f\"   ‚úì All players have ratings\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Basic rating statistics:\")\n",
    "print(f\"   ‚Ä¢ Count: {player_ratings['rating'].notna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Missing: {player_ratings['rating'].isna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Min: {player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Max: {player_ratings['rating'].max():.0f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {player_ratings['rating'].mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"   ‚Ä¢ Std Dev: {player_ratings['rating'].std():.2f}\")\n",
    "\n",
    "\n",
    "print(f\"\\n5Ô∏è‚É£  Ratings Percentiles\")\n",
    "percentiles = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,\n",
    "               0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "print(f\"\\n   {'Percentile':<12} {'Rating':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*12} {'-'*10} {'-'*40}\")\n",
    "\n",
    "for p in percentiles:\n",
    "    rating_value = player_ratings['rating'].quantile(p)\n",
    "    # Create a simple bar visualization\n",
    "    bar_length = int((rating_value - player_ratings['rating'].min()) / \n",
    "                     (player_ratings['rating'].max() - player_ratings['rating'].min()) * 40)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {p*100:>5.0f}%       {rating_value:>7.0f}    {bar}\")\n",
    "\n",
    "# Rating ranges and counts\n",
    "print(f\"\\n6Ô∏è‚É£  Rating distribution by range:\")\n",
    "rating_ranges = [\n",
    "    (0, 1000), (1000, 1200), (1200, 1400), (1400, 1600), \n",
    "    (1600, 1800), (1800, 2000), (2000, 2200), (2200, 2400), \n",
    "    (2400, 2600), (2600, 3000)\n",
    "]\n",
    "\n",
    "print(f\"\\n   {'Range':<15} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*15} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "\n",
    "for low, high in rating_ranges:\n",
    "    count = len(player_ratings[(player_ratings['rating'] >= low) & (player_ratings['rating'] < high)])\n",
    "    pct = 100 * count / len(player_ratings)\n",
    "    bar_length = int(pct * 0.4)  # Scale for visualization\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {low:>4}-{high:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Interquartile range\n",
    "iqr = player_ratings['rating'].quantile(0.75) - player_ratings['rating'].quantile(0.25)\n",
    "print(f\"\\n7Ô∏è‚É£  Spread statistics:\")\n",
    "print(f\"   ‚Ä¢ Range: {player_ratings['rating'].max() - player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Interquartile Range (IQR): {iqr:.0f}\")\n",
    "print(f\"   ‚Ä¢ 10th-90th percentile range: {player_ratings['rating'].quantile(0.90) - player_ratings['rating'].quantile(0.10):.0f}\")\n",
    "\n",
    "# Sample of players at different rating levels\n",
    "print(f\"\\n9Ô∏è‚É£  Sample players at different rating levels:\")\n",
    "sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "for p in sample_percentiles:\n",
    "    rating_threshold = player_ratings['rating'].quantile(p)\n",
    "    # Get a player near this rating\n",
    "    sample_player = player_ratings.iloc[(player_ratings['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "    print(f\"\\n   ~{p*100:.0f}th percentile (rating ‚âà {rating_threshold:.0f}):\")\n",
    "    for idx, row in sample_player.iterrows():\n",
    "        # print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f} {f'({row['title']})' if pd.notna(row['title']) else ''}\")\n",
    "        title_str = f\" ({row['title']})\" if pd.notna(row['title']) else \"\"\n",
    "        print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f}{title_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ RATING STATISTICS COMPLETE\")\n",
    "print(f\"   ‚Ä¢ Total players: {len(player_ratings):,}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings['rating'].min():.0f}, {player_ratings['rating'].max():.0f}]\")\n",
    "print(f\"   ‚Ä¢ Mean ¬± std: {player_ratings['rating'].mean():.0f} ¬± {player_ratings['rating'].std():.0f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Normalization parameters (calculated from 48,469 players):\n",
      "   ‚Ä¢ Mean: 1765.57\n",
      "   ‚Ä¢ Std Dev: 249.55\n",
      "\n",
      "2Ô∏è‚É£  Normalized rating statistics:\n",
      "   ‚Ä¢ Min: -2.2664\n",
      "   ‚Ä¢ Max: 4.2374\n",
      "   ‚Ä¢ Mean: 0.000000 (should be ~0)\n",
      "   ‚Ä¢ Std: 1.000000 (should be ~1)\n",
      "   ‚Ä¢ Range: [-2.27, 4.24]\n",
      "\n",
      "3Ô∏è‚É£  Sample normalized ratings across skill levels:\n",
      "   ~10th percentile: Player 41838 | Rating: 1435 ‚Üí Z-score: -1.325\n",
      "   ~25th percentile: Player 3170 | Rating: 1584 ‚Üí Z-score: -0.728\n",
      "   ~50th percentile: Player 46413 | Rating: 1763 ‚Üí Z-score: -0.010\n",
      "   ~75th percentile: Player 30163 | Rating: 1937 ‚Üí Z-score:  0.687\n",
      "   ~90th percentile: Player 33512 | Rating: 2089 ‚Üí Z-score:  1.296\n",
      "\n",
      "4Ô∏è‚É£  Interpretation guide:\n",
      "   ‚Ä¢ rating_z ‚âà -2.3: 1200 player (minimum)\n",
      "   ‚Ä¢ rating_z ‚âà -0.7: 1584 player (25th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà  0.0: 1766 player (mean)\n",
      "   ‚Ä¢ rating_z ‚âà 0.7: 1937 player (75th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà 4.2: 2823 player (maximum)\n",
      "\n",
      "5Ô∏è‚É£  Player side information table structure:\n",
      "   ‚Ä¢ Shape: (48469, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'rating', 'rating_z']\n",
      "\n",
      "6Ô∏è‚É£  Sample entries from side information table:\n",
      "   Player  1955 | Rating: 1624 ‚Üí Z-score: -0.567\n",
      "   Player 37697 | Rating: 1494 ‚Üí Z-score: -1.088\n",
      "   Player 46253 | Rating: 1853 ‚Üí Z-score:  0.350\n",
      "   Player 48529 | Rating: 2119 ‚Üí Z-score:  1.416\n",
      "   Player 23816 | Rating: 1722 ‚Üí Z-score: -0.175\n",
      "   Player 26420 | Rating: 1799 ‚Üí Z-score:  0.134\n",
      "   Player 29434 | Rating: 1502 ‚Üí Z-score: -1.056\n",
      "   Player 17344 | Rating: 1721 ‚Üí Z-score: -0.179\n",
      "   Player 37103 | Rating: 1723 ‚Üí Z-score: -0.171\n",
      "   Player  5359 | Rating: 2133 ‚Üí Z-score:  1.472\n",
      "   ‚úì All 48,469 players in clean_data have side information\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING NORMALIZATION COMPLETE\n",
      "\n",
      "Created: player_side_info\n",
      "   ‚Ä¢ Shape: (48469, 1)\n",
      "   ‚Ä¢ Index: player_id\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ clean_data: 2,898,426 rows (player-opening interactions)\n",
      "   ‚Ä¢ player_side_info: 48,469 rows (one per player)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\n",
      "   RATING_MEAN = 1765.57\n",
      "   RATING_STD = 249.55\n",
      "\n",
      "   You'll need them to normalize ratings for new users at inference time.\n"
     ]
    }
   ],
   "source": [
    "# 2d. Normalize player ratings using z-score normalization (for use as side information in MF model)\n",
    "\n",
    "# Check if we've already normalized ratings\n",
    "if 'player_side_info' in globals() and 'rating_z' in player_side_info.columns:\n",
    "    print(\"   SKIPPING STEP 2D: RATING NORMALIZATION\")\n",
    "    print(\"Ratings have already been normalized\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing normalized rating statistics:\")\n",
    "    print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "    print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "\n",
    "else:\n",
    "    def normalize_player_ratings(player_ratings_df):\n",
    "        \"\"\"\n",
    "        Apply z-score normalization to player ratings for use as side information.\n",
    "        \n",
    "        This creates a SEPARATE table of player-level features, NOT merged into clean_data.\n",
    "        Rating is side information - it describes the player, not the player-opening interaction.\n",
    "        \n",
    "        During training, the model will LOOK UP each player's rating_z from this table.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ratings_df : pd.DataFrame\n",
    "            Player ratings with columns: player_id, name, title, rating\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (player_side_info DataFrame, RATING_MEAN, RATING_STD)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calculate normalization parameters\n",
    "        RATING_MEAN = player_ratings_df['rating'].mean()\n",
    "        RATING_STD = player_ratings_df['rating'].std()\n",
    "        \n",
    "        print(f\"\\n1Ô∏è‚É£  Normalization parameters (calculated from {len(player_ratings_df):,} players):\")\n",
    "        print(f\"   ‚Ä¢ Mean: {RATING_MEAN:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Std Dev: {RATING_STD:.2f}\")\n",
    "        \n",
    "        # Create side information table - only keep player_id and rating for now\n",
    "        player_side_info = player_ratings_df[['player_id', 'rating']].copy()\n",
    "        player_side_info['rating_z'] = (player_side_info['rating'] - RATING_MEAN) / RATING_STD\n",
    "        \n",
    "        print(f\"\\n2Ô∏è‚É£  Normalized rating statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "        print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "        print(f\"   ‚Ä¢ Range: [{player_side_info['rating_z'].min():.2f}, {player_side_info['rating_z'].max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\n3Ô∏è‚É£  Sample normalized ratings across skill levels:\")\n",
    "        sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "        for p in sample_percentiles:\n",
    "            rating_threshold = player_side_info['rating'].quantile(p)\n",
    "            sample_player = player_side_info.iloc[(player_side_info['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "            for idx, row in sample_player.iterrows():\n",
    "                print(f\"   ~{p*100:.0f}th percentile: Player {idx} | \"\n",
    "                      f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n4Ô∏è‚É£  Interpretation guide:\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(1200 - RATING_MEAN)/RATING_STD:.1f}: 1200 player (minimum)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.25) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.25):.0f} player (25th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà  0.0: {RATING_MEAN:.0f} player (mean)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.75) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.75):.0f} player (75th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].max() - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].max():.0f} player (maximum)\")\n",
    "        \n",
    "        print(f\"\\n5Ô∏è‚É£  Player side information table structure:\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        # Set player_id as index for fast lookups\n",
    "        player_side_info = player_side_info.set_index('player_id')\n",
    "        \n",
    "        print(f\"\\n6Ô∏è‚É£  Sample entries from side information table:\")\n",
    "        sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "        for idx, row in sample_data.iterrows():\n",
    "            print(f\"   Player {idx:>5} | Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        # Drop rating column - we only need rating_z for the model\n",
    "        player_side_info = player_side_info.drop(columns=['rating'])\n",
    "        \n",
    "        # This is important - make sure every player in clean_data has a rating\n",
    "        missing_players = set(clean_data['player_id'].unique()) - set(player_side_info.index)\n",
    "        if len(missing_players) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in clean_data are missing from side_info!\")\n",
    "            print(f\"   Missing player IDs: {sorted(list(missing_players))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(player_side_info):,} players in clean_data have side information\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ RATING NORMALIZATION COMPLETE\")\n",
    "        print(f\"\\nCreated: player_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: player_id\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ clean_data: {clean_data.shape[0]:,} rows (player-opening interactions)\")\n",
    "        print(f\"   ‚Ä¢ player_side_info: {len(player_side_info):,} rows (one per player)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\")\n",
    "        print(f\"   RATING_MEAN = {RATING_MEAN:.2f}\")\n",
    "        print(f\"   RATING_STD = {RATING_STD:.2f}\")\n",
    "        print(f\"\\n   You'll need them to normalize ratings for new users at inference time.\")\n",
    "        \n",
    "        return player_side_info, RATING_MEAN, RATING_STD\n",
    "    \n",
    "    player_side_info, RATING_MEAN, RATING_STD = normalize_player_ratings(player_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eba9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         player_id  opening_id  num_games     score  eco  confidence\n",
      "946982       15945        1921         98  0.424165  C42    0.662162\n",
      "2306368      38993         937         12  0.447450  B12    0.193548\n",
      "69641         1201        1169         19  0.473111  B40    0.275362\n",
      "2637374      44608        3219         22  0.566383  D00    0.305556\n",
      "838901       14094        2060        259  0.533555  C50    0.838188\n",
      "1546239      26139        3178         13  0.527407  C50    0.206349\n",
      "548259        9225         751         16  0.497488  B01    0.242424\n",
      "2252482      38085        3241         89  0.562086  D32    0.640288\n",
      "493398        8299        1555         17  0.486303  C23    0.253731\n",
      "2448981      41428        1975         11  0.477031  C44    0.180328\n",
      "2522908      42686        1431         10  0.452411  C11    0.166667\n",
      "696253       11674         309         33  0.540370  A20    0.397590\n",
      "1161927      19652         856         18  0.562542  B06    0.264706\n",
      "2727576      46269         737         48  0.437967  B00    0.489796\n",
      "2869432      49223        1304         12  0.448212  B84    0.193548\n",
      "217128        3644        2628        112  0.516229  D31    0.691358\n",
      "550867        9266         738         16  0.471339  B00    0.242424\n",
      "985080       16592        1050         52  0.559541  B22    0.509804\n",
      "625890       10522        2836         50  0.505553  D94    0.500000\n",
      "1893373      31992         835         15  0.478809  B05    0.230769\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac119ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           rating_z\n",
      "player_id          \n",
      "26135     -1.629231\n",
      "29456     -1.577136\n",
      "23533     -0.238709\n",
      "42572      1.508459\n",
      "43649     -0.563298\n",
      "21332     -0.206651\n",
      "18958      0.554730\n",
      "22264      0.843253\n",
      "36376      0.799173\n",
      "15170      1.031594\n"
     ]
    }
   ],
   "source": [
    "print(player_side_info.sample(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30280db",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test/Val splits\n",
    "\n",
    "Here, I split my data and drop columns that are no longer needed. We're very close to being able to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: TRAIN/VALIDATION/TEST SPLIT\n",
      "\n",
      "1Ô∏è‚É£  Preparing data for split...\n",
      "   ‚Ä¢ Features (X): (2898426, 4)\n",
      "   ‚Ä¢ Target (y): (2898426,)\n",
      "   ‚Ä¢ Feature columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ Train: 2,173,819 samples (75.0%)\n",
      "   ‚Ä¢ Val: 434,764 samples (15.0%)\n",
      "   ‚Ä¢ Test: 289,843 samples (10.0%)\n",
      "   ‚Ä¢ Total samples: 2,898,426 (should equal 2,898,426)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 48,423 unique players\n",
      "   ‚Ä¢ Val: 47,500 unique players\n",
      "   ‚Ä¢ Test: 46,620 unique players\n",
      "   ‚Ä¢ Total unique: 48,469 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,690 unique openings\n",
      "   ‚Ä¢ Val: 2,442 unique openings\n",
      "   ‚Ä¢ Test: 2,377 unique openings\n",
      "   ‚Ä¢ Total unique: 2,717 openings\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 27 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 13 (0.5%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 25 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 15 (0.6%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "   ‚Ä¢ Train: 2,173,819 samples (75.0%)\n",
      "   ‚Ä¢ Val: 434,764 samples (15.0%)\n",
      "   ‚Ä¢ Test: 289,843 samples (10.0%)\n",
      "   ‚Ä¢ Total samples: 2,898,426 (should equal 2,898,426)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 48,423 unique players\n",
      "   ‚Ä¢ Val: 47,500 unique players\n",
      "   ‚Ä¢ Test: 46,620 unique players\n",
      "   ‚Ä¢ Total unique: 48,469 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,690 unique openings\n",
      "   ‚Ä¢ Val: 2,442 unique openings\n",
      "   ‚Ä¢ Test: 2,377 unique openings\n",
      "   ‚Ä¢ Total unique: 2,717 openings\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 27 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 13 (0.5%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 25 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 15 (0.6%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "\n",
      "   Train y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation y:\n",
      "   ‚Ä¢ Mean: 0.5121\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1882\n",
      "   ‚Ä¢ Max: 0.7718\n",
      "\n",
      "   Test y:\n",
      "   ‚Ä¢ Mean: 0.5121\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1352\n",
      "   ‚Ä¢ Max: 0.8251\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train confidence:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation confidence:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test confidence:\n",
      "   ‚Ä¢ Mean: 0.4145\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 2,173,819 samples (75%)\n",
      "   ‚Ä¢ Validation data: 434,764 samples (15%)\n",
      "   ‚Ä¢ Test data: 289,843 samples (10%)\n",
      "   ‚Ä¢ Player side info: 48,469 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n",
      "\n",
      "   Train y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation y:\n",
      "   ‚Ä¢ Mean: 0.5121\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1882\n",
      "   ‚Ä¢ Max: 0.7718\n",
      "\n",
      "   Test y:\n",
      "   ‚Ä¢ Mean: 0.5121\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1352\n",
      "   ‚Ä¢ Max: 0.8251\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train confidence:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation confidence:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test confidence:\n",
      "   ‚Ä¢ Mean: 0.4145\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 2,173,819 samples (75%)\n",
      "   ‚Ä¢ Validation data: 434,764 samples (15%)\n",
      "   ‚Ä¢ Test data: 289,843 samples (10%)\n",
      "   ‚Ä¢ Player side info: 48,469 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train/Validation/Test Split (75/15/10)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 3: TRAIN/VALIDATION/TEST SPLIT\")\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Preparing data for split...\")\n",
    "\n",
    "# We don't need num_games for modeling because we have the `confidence` based on num_games\n",
    "# Keep: player_id, opening_id, score, eco, confidence\n",
    "X = clean_data[[\"player_id\", \"opening_id\", \"eco\", \"confidence\"]].copy()\n",
    "y = clean_data[\"score\"].copy()\n",
    "\n",
    "print(f\"   ‚Ä¢ Features (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Target (y): {y.shape}\")\n",
    "print(f\"   ‚Ä¢ Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Clean up player_side_info - only keep rating_z\n",
    "# May regret this if we add more side info later and forget we took this step\n",
    "# God help me that would be a nasty bug\n",
    "player_side_info_clean = player_side_info[[\"rating_z\"]].copy()\n",
    "\n",
    "#  Use index-based splitting to avoid DataFrame copies\n",
    "idx = np.arange(len(X))\n",
    "\n",
    "# First split: separate out test set (10%)\n",
    "idx_temp, idx_test = train_test_split(idx, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: split remaining into train (75%) and val (15%)\n",
    "# 15% of original = 15/90 ‚âà 0.1667 of temp\n",
    "idx_train, idx_val = train_test_split(idx_temp, test_size=15/90, random_state=42, shuffle=True)\n",
    "\n",
    "# Create splits using iloc (view, not copy)\n",
    "X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
    "X_test, y_test = X.iloc[idx_test], y.iloc[idx_test]\n",
    "\n",
    "print(f\"   ‚Ä¢ Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Val: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"   ‚Ä¢ Total samples: {total:,} (should equal {len(X):,})\")\n",
    "print(f\"   ‚Ä¢ Train %: {len(X_train)/total*100:.2f}% (target: 75%)\")\n",
    "print(f\"   ‚Ä¢ Val %: {len(X_val)/total*100:.2f}% (target: 15%)\")\n",
    "print(f\"   ‚Ä¢ Test %: {len(X_test)/total*100:.2f}% (target: 10%)\")\n",
    "\n",
    "# Pre-compute unique arrays once\n",
    "players_train = X_train[\"player_id\"].unique()\n",
    "players_val = X_val[\"player_id\"].unique()\n",
    "players_test = X_test[\"player_id\"].unique()\n",
    "\n",
    "openings_train = X_train[\"opening_id\"].unique()\n",
    "openings_val = X_val[\"opening_id\"].unique()\n",
    "openings_test = X_test[\"opening_id\"].unique()\n",
    "\n",
    "print(f\"\\n   Players:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(players_train):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Val: {len(players_val):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Test: {len(players_test):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['player_id'].nunique():,} players\")\n",
    "\n",
    "print(f\"\\n   Openings:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(openings_train):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Val: {len(openings_val):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Test: {len(openings_test):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['opening_id'].nunique():,} openings\")\n",
    "\n",
    "# Use NumPy setdiff1d for cold-start analysis (C-speed)\n",
    "\n",
    "val_cold_players = np.setdiff1d(players_val, players_train, assume_unique=True)\n",
    "val_cold_openings = np.setdiff1d(openings_val, openings_train, assume_unique=True)\n",
    "\n",
    "test_cold_players = np.setdiff1d(players_test, players_train, assume_unique=True)\n",
    "test_cold_openings = np.setdiff1d(openings_test, openings_train, assume_unique=True)\n",
    "\n",
    "print(f\"\\n   Validation set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(val_cold_players):,} ({len(val_cold_players)/len(players_val)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(val_cold_openings):,} ({len(val_cold_openings)/len(openings_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(test_cold_players):,} ({len(test_cold_players)/len(players_test)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(test_cold_openings):,} ({len(test_cold_openings)/len(openings_test)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n7Ô∏è‚É£  Score distribution across splits:\")\n",
    "\n",
    "y_train_stats = y_train.describe()\n",
    "y_val_stats = y_val.describe()\n",
    "y_test_stats = y_test.describe()\n",
    "\n",
    "print(f\"\\n   Train y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_train_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_train_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_train_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_val_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_val_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_val_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_test_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_test_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_test_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n8Ô∏è‚É£  Confidence distribution across splits:\")\n",
    "\n",
    "conf_train_stats = X_train['confidence'].describe()\n",
    "conf_val_stats = X_val['confidence'].describe()\n",
    "conf_test_stats = X_test['confidence'].describe()\n",
    "\n",
    "print(f\"\\n   Train confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_train_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_val_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_test_stats['50%']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SPLIT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   ‚Ä¢ Training data: {len(X_train):,} samples (75%)\")\n",
    "print(f\"   ‚Ä¢ Validation data: {len(X_val):,} samples (15%)\")\n",
    "print(f\"   ‚Ä¢ Test data: {len(X_test):,} samples (10%)\")\n",
    "print(f\"   ‚Ä¢ Player side info: {len(player_side_info_clean):,} players\")\n",
    "print(f\"   ‚Ä¢ Side info columns: {list(player_side_info_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781a382",
   "metadata": {},
   "source": [
    "## Step 3b: Remap Player and Opening IDs to Sequential Integers\n",
    "\n",
    "**Why remap IDs?**\n",
    "- Database IDs may have gaps (e.g., [1, 5, 10, 15, ...]) from deleted entries\n",
    "- Embedding layers need 0-based contiguous indices for efficiency\n",
    "- Remapping saves memory (no unused embedding slots)\n",
    "\n",
    "**Process:**\n",
    "1. Check if IDs are already sequential (0 or 1-based with no gaps)\n",
    "2. If not, create mappings: old_id ‚Üí new_sequential_id\n",
    "3. Remap all DataFrames and side info tables\n",
    "4. Verify mappings with spot checks\n",
    "\n",
    "This ensures embeddings use minimal memory and indices align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4c409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\n",
      "\n",
      "1Ô∏è‚É£  Processing player IDs...\n",
      "Checking player IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique players: 48469\n",
      "   ‚Ä¢ ID range: [1, 50000]\n",
      "   ‚ö†Ô∏è  player IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1531\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      player_id 1 ‚Üí 0\n",
      "      player_id 2 ‚Üí 1\n",
      "      player_id 3 ‚Üí 2\n",
      "      player_id 4 ‚Üí 3\n",
      "      player_id 5 ‚Üí 4\n",
      "      player_id 49996 ‚Üí 48464\n",
      "      player_id 49997 ‚Üí 48465\n",
      "      player_id 49998 ‚Üí 48466\n",
      "      player_id 49999 ‚Üí 48467\n",
      "      player_id 50000 ‚Üí 48468\n",
      "\n",
      "   Remapping 5 DataFrames...\n",
      "Checking player IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique players: 48469\n",
      "   ‚Ä¢ ID range: [1, 50000]\n",
      "   ‚ö†Ô∏è  player IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1531\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      player_id 1 ‚Üí 0\n",
      "      player_id 2 ‚Üí 1\n",
      "      player_id 3 ‚Üí 2\n",
      "      player_id 4 ‚Üí 3\n",
      "      player_id 5 ‚Üí 4\n",
      "      player_id 49996 ‚Üí 48464\n",
      "      player_id 49997 ‚Üí 48465\n",
      "      player_id 49998 ‚Üí 48466\n",
      "      player_id 49999 ‚Üí 48467\n",
      "      player_id 50000 ‚Üí 48468\n",
      "\n",
      "   Remapping 5 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2717\n",
      "   ‚Ä¢ ID range: [2, 3589]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 871\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 3 ‚Üí 1\n",
      "      opening_id 4 ‚Üí 2\n",
      "      opening_id 5 ‚Üí 3\n",
      "      opening_id 6 ‚Üí 4\n",
      "      opening_id 3571 ‚Üí 2712\n",
      "      opening_id 3572 ‚Üí 2713\n",
      "      opening_id 3575 ‚Üí 2714\n",
      "      opening_id 3584 ‚Üí 2715\n",
      "      opening_id 3589 ‚Üí 2716\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2717\n",
      "   ‚Ä¢ ID range: [2, 3589]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 871\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 3 ‚Üí 1\n",
      "      opening_id 4 ‚Üí 2\n",
      "      opening_id 5 ‚Üí 3\n",
      "      opening_id 6 ‚Üí 4\n",
      "      opening_id 3571 ‚Üí 2712\n",
      "      opening_id 3572 ‚Üí 2713\n",
      "      opening_id 3575 ‚Üí 2714\n",
      "      opening_id 3584 ‚Üí 2715\n",
      "      opening_id 3589 ‚Üí 2716\n",
      "\n",
      "   Remapping 4 DataFrames...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2Ô∏è‚É£  Processing opening IDs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m opening_dfs = [X_train, X_val, X_test, clean_data]\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m remapped_opening_dfs, opening_mappings, opening_remapped = \u001b[43mcheck_and_remap_ids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopening_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopening_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopening\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     92\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opening_remapped:\n\u001b[32m     95\u001b[39m     X_train, X_val, X_test, clean_data = remapped_opening_dfs\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mcheck_and_remap_ids\u001b[39m\u001b[34m(df_list, id_column, entity_name)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_list):\n\u001b[32m     65\u001b[39m     df_copy = df.copy()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     df_copy[id_column] = \u001b[43mdf_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mid_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_to_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     remapped_dfs.append(df_copy)\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úì Remapped DataFrame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/series.py:4711\u001b[39m, in \u001b[36mSeries.map\u001b[39m\u001b[34m(self, arg, na_action)\u001b[39m\n\u001b[32m   4631\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\n\u001b[32m   4632\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4633\u001b[39m     arg: Callable | Mapping | Series,\n\u001b[32m   4634\u001b[39m     na_action: Literal[\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   4635\u001b[39m ) -> Series:\n\u001b[32m   4636\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4637\u001b[39m \u001b[33;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[32m   4638\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4709\u001b[39m \u001b[33;03m    dtype: object\u001b[39;00m\n\u001b[32m   4710\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4711\u001b[39m     new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4712\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor(new_values, index=\u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mFalse\u001b[39;00m).__finalize__(\n\u001b[32m   4713\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmap\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4714\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1733\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1730\u001b[39m     \u001b[38;5;66;03m# Since values were input this means we came from either\u001b[39;00m\n\u001b[32m   1731\u001b[39m     \u001b[38;5;66;03m# a dict or a series and mapper should be an index\u001b[39;00m\n\u001b[32m   1732\u001b[39m     indexer = mapper.index.get_indexer(arr)\n\u001b[32m-> \u001b[39m\u001b[32m1733\u001b[39m     new_values = \u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1735\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_values\n\u001b[32m   1737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arr):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/array_algos/take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/array_algos/take.py:133\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    131\u001b[39m     indexer = ensure_platform_int(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m dtype, fill_value, mask_info = \u001b[43m_take_preprocess_indexer_and_fill_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m flip_order = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arr.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arr.flags.f_contiguous:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/pandas/core/array_algos/take.py:586\u001b[39m, in \u001b[36m_take_preprocess_indexer_and_fill_value\u001b[39m\u001b[34m(arr, indexer, fill_value, allow_fill, mask)\u001b[39m\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    585\u001b[39m     mask = indexer == -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m     needs_masking = \u001b[38;5;28mbool\u001b[39m(\u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    587\u001b[39m mask_info = mask, needs_masking\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_masking:\n\u001b[32m    589\u001b[39m     \u001b[38;5;66;03m# if not, then depromote, set fill_value to dummy\u001b[39;00m\n\u001b[32m    590\u001b[39m     \u001b[38;5;66;03m# (it won't be used but we don't want the cython code\u001b[39;00m\n\u001b[32m    591\u001b[39m     \u001b[38;5;66;03m# to crash when trying to cast it to dtype)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:57\u001b[39m, in \u001b[36m_any\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     54\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_prod(a, axis, dtype, out, keepdims, initial, where)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_any\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m, *, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# By default, return a boolean for any and all\u001b[39;00m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     60\u001b[39m         dtype = bool_dt\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 3b: Remap player and opening IDs to 0-based sequential integers\n",
    "\n",
    "print(\"STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\")\n",
    "\n",
    "def check_and_remap_ids(df_list, id_column, entity_name):\n",
    "    \"\"\"\n",
    "    Check if IDs are sequential starting from 0, and remap if not.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of DataFrames\n",
    "        List of DataFrames containing the ID column to check/remap\n",
    "    id_column : str\n",
    "        Name of the ID column ('player_id' or 'opening_id')\n",
    "    entity_name : str\n",
    "        Name for logging ('player' or 'opening')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (df_list with remapped IDs, id_to_idx mapping dict, needs_remapping bool)\n",
    "    \"\"\"\n",
    "    # Get all unique IDs across all dataframes\n",
    "    all_ids = pd.concat([df[id_column] for df in df_list]).unique()\n",
    "    all_ids_sorted = sorted(all_ids)\n",
    "    \n",
    "    print(f\"Checking {entity_name} IDs...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ‚Ä¢ Total unique {entity_name}s: {len(all_ids_sorted)}\")\n",
    "    print(f\"   ‚Ä¢ ID range: [{all_ids_sorted[0]}, {all_ids_sorted[-1]}]\")\n",
    "    \n",
    "    # Check if IDs are already 0-based sequential (0, 1, 2, ...)\n",
    "    expected_sequential = list(range(len(all_ids_sorted)))\n",
    "    is_sequential = (all_ids_sorted == expected_sequential)\n",
    "    \n",
    "    if is_sequential:\n",
    "        print(f\"   ‚úì {entity_name} IDs are already 0-based sequential - no remapping needed!\")\n",
    "        return df_list, None, False\n",
    "    \n",
    "    # Check if IDs are 1-based sequential (1, 2, 3, ...)\n",
    "    expected_sequential_1based = list(range(1, len(all_ids_sorted) + 1))\n",
    "    is_sequential_1based = (all_ids_sorted == expected_sequential_1based)\n",
    "    \n",
    "    if is_sequential_1based:\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs are 1-based sequential - will remap to 0-based\")\n",
    "    else:\n",
    "        # Calculate gaps\n",
    "        num_gaps = (all_ids_sorted[-1] - all_ids_sorted[0] + 1) - len(all_ids_sorted)\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs have gaps - will remap to 0-based sequential\")\n",
    "        print(f\"   ‚Ä¢ Number of gaps: {num_gaps}\")\n",
    "    \n",
    "    # Create mapping: old_id -> new_idx (0-based)\n",
    "    id_to_idx = {old_id: new_idx for new_idx, old_id in enumerate(all_ids_sorted)}\n",
    "    idx_to_id = {new_idx: old_id for old_id, new_idx in id_to_idx.items()}\n",
    "    \n",
    "    print(f\"\\n   Creating mapping...\")\n",
    "    print(f\"   ‚Ä¢ Example mappings:\")\n",
    "    sample_ids = all_ids_sorted[:5] + all_ids_sorted[-5:]\n",
    "    for old_id in sample_ids[:10]:  # Show first 5 and last 5\n",
    "        print(f\"      {entity_name}_id {old_id} ‚Üí {id_to_idx[old_id]}\")\n",
    "    \n",
    "    # Remap all DataFrames\n",
    "    print(f\"\\n   Remapping {len(df_list)} DataFrames...\")\n",
    "    remapped_dfs = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[id_column] = df_copy[id_column].map(id_to_idx)\n",
    "        remapped_dfs.append(df_copy)\n",
    "        print(f\"   ‚úì Remapped DataFrame {i+1}/{len(df_list)}\")\n",
    "    \n",
    "    return remapped_dfs, (id_to_idx, idx_to_id), True\n",
    "\n",
    "# 1. Remap player IDs\n",
    "print(f\"\\n1Ô∏è‚É£  Processing player IDs...\")\n",
    "player_dfs = [X_train, X_val, X_test, clean_data, player_side_info.reset_index()]\n",
    "remapped_player_dfs, player_mappings, player_remapped = check_and_remap_ids(\n",
    "    player_dfs, 'player_id', 'player'\n",
    ")\n",
    "\n",
    "if player_remapped:\n",
    "    X_train, X_val, X_test, clean_data, player_side_info_remapped = remapped_player_dfs\n",
    "    player_id_to_idx, player_idx_to_id = player_mappings\n",
    "    player_side_info = player_side_info_remapped.set_index('player_id')\n",
    "    print(f\"\\n   ‚úÖ Player ID remapping complete!\")\n",
    "else:\n",
    "    player_id_to_idx, player_idx_to_id = None, None\n",
    "\n",
    "# 2. Remap opening IDs\n",
    "print(f\"\\n2Ô∏è‚É£  Processing opening IDs...\")\n",
    "opening_dfs = [X_train, X_val, X_test, clean_data]\n",
    "remapped_opening_dfs, opening_mappings, opening_remapped = check_and_remap_ids(\n",
    "    opening_dfs, 'opening_id', 'opening'\n",
    ")\n",
    "\n",
    "if opening_remapped:\n",
    "    X_train, X_val, X_test, clean_data = remapped_opening_dfs\n",
    "    opening_id_to_idx, opening_idx_to_id = opening_mappings\n",
    "    print(f\"\\n   ‚úÖ Opening ID remapping complete!\")\n",
    "else:\n",
    "    opening_id_to_idx, opening_idx_to_id = None, None\n",
    "\n",
    "# 3. Spot checks to verify mappings\n",
    "print(f\"\\n3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\")\n",
    "print(f\"   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\")\n",
    "\n",
    "# Sample 10 entries: first, last, and 8 in between\n",
    "print(f\"\\n   Sampling 10 entries from X_train (before remapping was applied)...\")\n",
    "total_rows = len(X_train)\n",
    "# Get indices: first, last, and 8 evenly spaced in between\n",
    "sample_indices = [0]  # First row\n",
    "step = (total_rows - 1) // 9  # Divide remaining rows into 9 parts\n",
    "sample_indices.extend([min(i * step, total_rows - 1) for i in range(1, 9)])\n",
    "sample_indices.append(total_rows - 1)  # Last row\n",
    "\n",
    "print(f\"   ‚Ä¢ Sample indices: {sample_indices}\")\n",
    "\n",
    "# Store samples with their NEW (remapped) IDs\n",
    "samples = []\n",
    "for idx in sample_indices:\n",
    "    row = X_train.iloc[idx]\n",
    "    samples.append({\n",
    "        'index': idx,\n",
    "        'new_player_id': row['player_id'],\n",
    "        'new_opening_id': row['opening_id'],\n",
    "        'confidence': row['confidence']\n",
    "    })\n",
    "\n",
    "print(f\"\\n   Verification checks:\")\n",
    "print(f\"   {'#':<4} {'Row Idx':<10} {'New Player':<12} {'New Opening':<12} {'Confidence':<12} {'Status':<15}\")\n",
    "print(f\"   {'-'*4} {'-'*10} {'-'*12} {'-'*12} {'-'*12} {'-'*15}\")\n",
    "\n",
    "all_checks_passed = True\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    new_player_id = sample['new_player_id']\n",
    "    new_opening_id = sample['new_opening_id']\n",
    "    confidence = sample['confidence']\n",
    "    idx = sample['index']\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: New player ID is valid (0-based sequential)\n",
    "    if player_remapped:\n",
    "        player_valid = 0 <= new_player_id < len(player_id_to_idx)\n",
    "        checks.append((\"player_id\", player_valid))\n",
    "    else:\n",
    "        checks.append((\"player_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 2: New opening ID is valid (0-based sequential)\n",
    "    if opening_remapped:\n",
    "        opening_valid = 0 <= new_opening_id < len(opening_id_to_idx)\n",
    "        checks.append((\"opening_id\", opening_valid))\n",
    "    else:\n",
    "        checks.append((\"opening_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 3: Player exists in player_side_info\n",
    "    player_exists = new_player_id in player_side_info.index\n",
    "    checks.append((\"in_side_info\", player_exists))\n",
    "    \n",
    "    # Check 4: Opening exists in clean_data\n",
    "    opening_exists = new_opening_id in clean_data['opening_id'].values\n",
    "    checks.append((\"in_clean\", opening_exists))\n",
    "    \n",
    "    # All checks must pass\n",
    "    all_pass = all(check[1] for check in checks)\n",
    "    status = \"‚úì PASS\" if all_pass else f\"‚úó FAIL ({','.join([c[0] for c in checks if not c[1]])})\"\n",
    "    \n",
    "    if not all_pass:\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(f\"   {i:<4} {idx:<10} {new_player_id:<12} {new_opening_id:<12} {confidence:<12.4f} {status:<15}\")\n",
    "\n",
    "# Additional verification: check if we can reverse map to original IDs\n",
    "if player_remapped or opening_remapped:\n",
    "    print(f\"\\n   Reverse mapping verification (sample of 3 entries):\")\n",
    "    print(f\"   {'#':<4} {'New‚ÜíOld Player':<30} {'New‚ÜíOld Opening':<30}\")\n",
    "    print(f\"   {'-'*4} {'-'*30} {'-'*30}\")\n",
    "    \n",
    "    for i in [0, len(samples)//2, len(samples)-1]:  # First, middle, last\n",
    "        sample = samples[i]\n",
    "        \n",
    "        # Reverse map player\n",
    "        if player_remapped:\n",
    "            old_player = player_idx_to_id.get(sample['new_player_id'], 'NOT_FOUND')\n",
    "            player_str = f\"{sample['new_player_id']} ‚Üí {old_player}\"\n",
    "        else:\n",
    "            player_str = f\"{sample['new_player_id']} (unchanged)\"\n",
    "        \n",
    "        # Reverse map opening\n",
    "        if opening_remapped:\n",
    "            old_opening = opening_idx_to_id.get(sample['new_opening_id'], 'NOT_FOUND')\n",
    "            opening_str = f\"{sample['new_opening_id']} ‚Üí {old_opening}\"\n",
    "        else:\n",
    "            opening_str = f\"{sample['new_opening_id']} (unchanged)\"\n",
    "        \n",
    "        print(f\"   {i+1:<4} {player_str:<30} {opening_str:<30}\")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(f\"\\n   ‚úÖ All spot checks passed! ID mappings are correct.\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Some spot checks failed - investigate immediately!\")\n",
    "    raise ValueError(\"ID remapping verification failed!\")\n",
    "\n",
    "# 4. Summary\n",
    "print(f\"\\n4Ô∏è‚É£  Summary:\")\n",
    "print(f\"\\n   Player IDs:\")\n",
    "if player_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{player_idx_to_id[0]}, {player_idx_to_id[len(player_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(player_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "    \n",
    "print(f\"\\n   Opening IDs:\")\n",
    "if opening_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{opening_idx_to_id[0]}, {opening_idx_to_id[len(opening_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(opening_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "\n",
    "print(f\"\\n   Updated DataFrames:\")\n",
    "print(f\"   ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ X_val: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ clean_data: {clean_data.shape}\")\n",
    "print(f\"   ‚Ä¢ player_side_info: {player_side_info.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ID REMAPPING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Important:\")\n",
    "print(f\"   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\")\n",
    "print(f\"   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\")\n",
    "print(f\"   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\")\n",
    "print(f\"   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a580c0",
   "metadata": {},
   "source": [
    "## 4. Enumerate Categorical Variables\n",
    "\n",
    "I believe the only variable we need to enumerate here is `eco`. That's the broad categorization of a specific opening.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- One ECO code will have many openings\n",
    "- They're sorted by letter, then further by number. For instance, C21 and C44 are in the `C` family.\n",
    "- Maybe we make this side information?\n",
    "\n",
    "First, let's get some data on ECO codes to help us better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9504a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: ECO CODE STATISTICS\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,897,037\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 6325.4\n",
      "   ‚Ä¢ Median entries per ECO: 823.0\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 201203\n",
      "   ‚Ä¢ Std: 18376.7\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        510,163     17.61%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        1,047,607     36.16%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        987,093     34.07%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D        279,156      9.64%      ‚ñà‚ñà‚ñà\n",
      "   E         73,018      2.52%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00    201,203      6.95%      ‚ñà‚ñà\n",
      "   2      B01    197,244      6.81%      ‚ñà‚ñà\n",
      "   3      A40    125,507      4.33%      ‚ñà\n",
      "   4      C50    101,295      3.50%      ‚ñà\n",
      "   5      C00     87,245      3.01%      \n",
      "   6      C40     81,461      2.81%      \n",
      "   7      B06     73,239      2.53%      \n",
      "   8      C42     65,669      2.27%      \n",
      "   9      C44     60,636      2.09%      \n",
      "   10     B10     56,911      1.96%      \n",
      "   11     C41     55,409      1.91%      \n",
      "   12     B40     52,371      1.81%      \n",
      "   13     B12     48,771      1.68%      \n",
      "   14     A00     48,481      1.67%      \n",
      "   15     C02     48,021      1.66%      \n",
      "   16     D00     41,166      1.42%      \n",
      "   17     B21     37,726      1.30%      \n",
      "   18     A43     35,735      1.23%      \n",
      "   19     B32     33,883      1.17%      \n",
      "   20     A04     33,032      1.14%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      E25          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2      D29          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3      D98          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4      A97          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   5      D68          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   6      E78          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   7      E79          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   8      A63          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   9      A72          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   10     C76          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   11     D49          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   12     C94          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   13     E55          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   14     B69          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   15     D84          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   16     A99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   17     E96          2    ‚ñà‚ñà\n",
      "   18     A93          2    ‚ñà‚ñà\n",
      "   19     E36          1    ‚ñà\n",
      "   20     A98          1    ‚ñà\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   A07, A20, A58, B13, B22, B53, C03, C19, C43, C48, C66, D33, D36, D92, E10, E30, E59, E79, E96, E97\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   ‚Ä¢ Unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,172,777\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 445\n",
      "   ‚Ä¢ Total entries: 434,556\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 440\n",
      "   ‚Ä¢ Total entries: 289,704\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C94    0.6200             5\n",
      "   B77    0.5943           547\n",
      "   D57    0.5921            23\n",
      "   B71    0.5870           228\n",
      "   E96    0.5860             2\n",
      "   B85    0.5844            16\n",
      "   D29    0.5818             8\n",
      "   D49    0.5764             6\n",
      "   E99    0.5733            26\n",
      "   C87    0.5685           184\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   B70    0.4702         4,145\n",
      "   C74    0.4671            23\n",
      "   B58    0.4669            10\n",
      "   A58    0.4620           957\n",
      "   C76    0.4616             6\n",
      "   C38    0.4580         1,659\n",
      "   C99    0.4345            14\n",
      "   E55    0.3855             5\n",
      "   E36    0.3500             1\n",
      "   A98    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   A93    0.0069       0.0829             2\n",
      "   C57    0.0068       0.0824        11,829\n",
      "   E45    0.0068       0.0823            17\n",
      "   C37    0.0044       0.0665         5,927\n",
      "   C51    0.0038       0.0619         3,945\n",
      "   C56    0.0036       0.0600         9,588\n",
      "   C39    0.0036       0.0598         1,447\n",
      "   C31    0.0035       0.0596        10,967\n",
      "   A61    0.0035       0.0591           148\n",
      "   C52    0.0035       0.0590         1,894\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Most common family: B (1,047,607 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (201,203 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: ECO Code Statistics (no mutations, just exploration)\n",
    "\n",
    "print(\"STEP 4: ECO CODE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Missing ECO values: {clean_data['eco'].isna().sum()}\")\n",
    "\n",
    "# ECO value counts\n",
    "eco_counts = clean_data['eco'].value_counts().sort_index()\n",
    "print(f\"\\n2Ô∏è‚É£  Distribution of entries per ECO code:\")\n",
    "print(f\"   ‚Ä¢ Mean entries per ECO: {eco_counts.mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Median entries per ECO: {eco_counts.median():.1f}\")\n",
    "print(f\"   ‚Ä¢ Min entries: {eco_counts.min()}\")\n",
    "print(f\"   ‚Ä¢ Max entries: {eco_counts.max()}\")\n",
    "print(f\"   ‚Ä¢ Std: {eco_counts.std():.1f}\")\n",
    "\n",
    "# ECO by first letter (family)\n",
    "print(f\"\\n3Ô∏è‚É£  ECO families (by first letter):\")\n",
    "eco_families = clean_data['eco'].str[0].value_counts().sort_index()\n",
    "print(f\"\\n   {'Family':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "for family, count in eco_families.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.4)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {family:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Top 20 most common ECO codes\n",
    "print(f\"\\n4Ô∏è‚É£  Top 20 most common ECO codes:\")\n",
    "top_eco = clean_data['eco'].value_counts().head(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*12} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(top_eco.items(), 1):\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.3)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Bottom 20 least common ECO codes\n",
    "print(f\"\\n5Ô∏è‚É£  Bottom 20 least common ECO codes:\")\n",
    "bottom_eco = clean_data['eco'].value_counts().tail(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(bottom_eco.items(), 1):\n",
    "    bar_length = min(count, 30)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {bar}\")\n",
    "\n",
    "# Check for any unusual ECO codes\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of ECO codes:\")\n",
    "sample_eco = clean_data['eco'].drop_duplicates().sample(min(20, clean_data['eco'].nunique()), random_state=42).sort_values()\n",
    "print(f\"   {', '.join(sample_eco.values)}\")\n",
    "\n",
    "# ECO statistics by split\n",
    "print(f\"\\n8Ô∏è‚É£  ECO distribution across splits:\")\n",
    "print(f\"\\n   Train split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_train['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\n   Validation split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_val['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_val):,}\")\n",
    "val_new_eco = set(X_val['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(val_new_eco)}\")\n",
    "\n",
    "print(f\"\\n   Test split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_test['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_test):,}\")\n",
    "test_new_eco = set(X_test['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(test_new_eco)}\")\n",
    "\n",
    "# Average score by ECO code (top 10 and bottom 10)\n",
    "print(f\"\\n9Ô∏è‚É£  Average score by ECO code:\")\n",
    "eco_scores = clean_data.groupby('eco')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\n   Top 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.head(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(f\"\\n   Bottom 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.tail(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# ECO codes with high variance in scores\n",
    "print(f\"\\nüîü  ECO codes with highest score variance (may indicate difficulty):\")\n",
    "eco_variance = clean_data.groupby('eco')['score'].agg(['var', 'std', 'count']).sort_values('var', ascending=False).head(10)\n",
    "print(f\"\\n   {'ECO':<6} {'Variance':<12} {'Std Dev':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_variance.iterrows():\n",
    "    print(f\"   {eco:<6} {row['var']:<12.4f} {row['std']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ECO CODE STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Key takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Most common family: {eco_families.idxmax()} ({eco_families.max():,} entries)\")\n",
    "print(f\"   ‚Ä¢ Most common ECO: {top_eco.index[0]} ({top_eco.iloc[0]:,} entries)\")\n",
    "print(f\"   ‚Ä¢ ECO codes appear in all splits (good for training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbad18",
   "metadata": {},
   "source": [
    "## 4b. Create ECO Side Information\n",
    "\n",
    "**Why ECO is Side Information:**\n",
    "- ECO codes describe **opening characteristics**, not individual player-opening interactions\n",
    "- Similar to how player ratings describe players, ECO describes openings\n",
    "- Each opening has ONE ECO code (not per player-opening pair)\n",
    "\n",
    "**Implementation Strategy:**\n",
    "- Split ECO codes into two categorical features:\n",
    "  - `eco_letter`: A, B, C, D, or E ‚Üí encoded as integers 0-4\n",
    "  - `eco_number`: The numeric part (e.g., \"21\" from \"C21\") ‚Üí encoded as sequential integers\n",
    "- Store in a separate `opening_side_info` lookup table (indexed by opening_id)\n",
    "- Remove `eco` from train/test/val DataFrames (it's not interaction data)\n",
    "- During training, model will lookup opening_id ‚Üí (eco_letter, eco_number)\n",
    "\n",
    "**Why Split ECO into Letter and Number:**\n",
    "- ECO families (A-E) represent fundamentally different opening types:\n",
    "  - **A**: Flank openings (English, R√©ti, Bird's, etc.)\n",
    "  - **B**: Semi-Open games (Sicilian, French, Caro-Kann, etc.)\n",
    "  - **C**: Open games (King's pawn openings, Spanish, Italian, etc.)\n",
    "  - **D**: Closed games (Queen's Gambit variations)\n",
    "  - **E**: Indian defenses (King's Indian, Nimzo-Indian, etc.)\n",
    "- Numbers within each family represent variations (C20-C29, C30-C39, etc.)\n",
    "- Model can learn separate embeddings for family vs variation\n",
    "\n",
    "**Categorical Encoding:**\n",
    "- Both features will be treated as categorical (not ordinal)\n",
    "- Higher numbers don't mean \"better\" openings\n",
    "- Model will learn embedding vectors for each category\n",
    "- This allows the model to capture non-linear relationships between ECO codes and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64124475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4B: CREATE ECO SIDE INFORMATION\n",
      "============================================================\n",
      "   ‚úì Extracted 2,716 unique openings\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['B' 'C' 'A' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (578 openings)\n",
      "      'B' ‚Üí 1 (589 openings)\n",
      "      'C' ‚Üí 2 (894 openings)\n",
      "      'D' ‚Üí 3 (419 openings)\n",
      "      'E' ‚Üí 4 (236 openings)\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 290 openings\n",
      "      '40' (‚Üí 40):  98 openings\n",
      "      '20' (‚Üí 20):  70 openings\n",
      "      '01' (‚Üí  1):  70 openings\n",
      "      '42' (‚Üí 42):  68 openings\n",
      "      '02' (‚Üí  2):  67 openings\n",
      "      '45' (‚Üí 45):  64 openings\n",
      "      '10' (‚Üí 10):  63 openings\n",
      "      '21' (‚Üí 21):  61 openings\n",
      "      '15' (‚Üí 15):  58 openings\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2716, 2)\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ X_train before: (2172777, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (434556, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (289704, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   143          0               4               A04               \n",
      "   1244         2               25              C25               \n",
      "   1711         2               65              C65               \n",
      "   1530         2               44              C44               \n",
      "   399          0               50              A50               \n",
      "   927          1               40              B40               \n",
      "   2333         4               70              E70               \n",
      "   1625         2               52              C52               \n",
      "   1022         1               89              B89               \n",
      "   6            0               0               A00               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            578     21.28%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            589     21.69%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            894     32.92%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            419     15.43%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            236      8.69%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2716, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 2,172,777 rows, 3 features\n",
      "   ‚Ä¢ X_val: 434,556 rows, 3 features\n",
      "   ‚Ä¢ X_test: 289,704 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,716 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n"
     ]
    }
   ],
   "source": [
    "# 4b. Create ECO side information and remove ECO from train/test/val DataFrames\n",
    "\n",
    "# Check if ECO processing has already been done\n",
    "if 'eco' not in X_train.columns and 'opening_side_info' in globals():\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 4B: ECO SIDE INFORMATION CREATION\")\n",
    "    print(\"\\n‚úì ECO column already removed from train/test/val data\")\n",
    "    print(f\"\\nOpening side info shape: {opening_side_info.shape}\")\n",
    "    print(f\"Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "    print(\"opening side info:\", opening_side_info.head().to_string())\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing ECO encoding statistics:\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_letter values: {opening_side_info['eco_letter_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_number values: {opening_side_info['eco_number_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ eco_letter range: [{opening_side_info['eco_letter_cat'].min()}, {opening_side_info['eco_letter_cat'].max()}]\")\n",
    "    print(f\"   ‚Ä¢ eco_number range: [{opening_side_info['eco_number_cat'].min()}, {opening_side_info['eco_number_cat'].max()}]\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing ECO encoding:\")\n",
    "    sample_data = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "    print(sample_data.to_string())\n",
    "else:\n",
    "    def create_eco_side_information(clean_data_df, X_train_df, X_val_df, X_test_df):\n",
    "        \"\"\"\n",
    "        Create ECO side information table and remove ECO from train/test/val DataFrames.\n",
    "        \n",
    "        ECO codes are opening-level features, not player-opening interaction features.\n",
    "        We split each ECO code (e.g., \"C21\") into:\n",
    "        - eco_letter: The letter part (A, B, C, D, or E)\n",
    "        - eco_number: The numeric part (e.g., 21)\n",
    "        \n",
    "        Both are encoded as sequential integers for use as categorical features in embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        clean_data_df : pd.DataFrame\n",
    "            Full cleaned data with opening_id and eco columns\n",
    "        X_train_df, X_val_df, X_test_df : pd.DataFrame\n",
    "            Train/val/test feature DataFrames (will be modified to remove 'eco')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 4B: CREATE ECO SIDE INFORMATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Extract unique opening ‚Üí ECO mappings\n",
    "        opening_eco_map = clean_data_df[['opening_id', 'eco']].drop_duplicates().set_index('opening_id')\n",
    "        print(f\"   ‚úì Extracted {len(opening_eco_map):,} unique openings\")\n",
    "        \n",
    "        # Verify one-to-one mapping\n",
    "        eco_per_opening = clean_data_df.groupby('opening_id')['eco'].nunique()\n",
    "        if (eco_per_opening > 1).any():\n",
    "            problematic = eco_per_opening[eco_per_opening > 1]\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(problematic)} openings have multiple ECO codes!\")\n",
    "            print(f\"   Problematic opening IDs: {problematic.index.tolist()[:10]}...\")\n",
    "        \n",
    "        # Split ECO into letter and number components\n",
    "        opening_eco_map['eco_letter_str'] = opening_eco_map['eco'].str[0]  # First character (A-E)\n",
    "        opening_eco_map['eco_number_str'] = opening_eco_map['eco'].str[1:]  # Remaining characters (numeric)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted letter and number components\")\n",
    "        print(f\"   ‚Ä¢ Unique letters: {opening_eco_map['eco_letter_str'].unique()}\")\n",
    "        print(f\"   ‚Ä¢ Unique numbers: {opening_eco_map['eco_number_str'].nunique()}\")\n",
    "        \n",
    "        # Create encoding mappings for eco_letter (A-E ‚Üí 0-4)\n",
    "        unique_letters = sorted(opening_eco_map['eco_letter_str'].unique())\n",
    "        eco_letter_to_int = {letter: idx for idx, letter in enumerate(unique_letters)}\n",
    "        eco_int_to_letter = {idx: letter for letter, idx in eco_letter_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_letter_cat'] = opening_eco_map['eco_letter_str'].map(eco_letter_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Letter encoding created:\")\n",
    "        for letter, idx in sorted(eco_letter_to_int.items()):\n",
    "            count = (opening_eco_map['eco_letter_str'] == letter).sum()\n",
    "            print(f\"      '{letter}' ‚Üí {idx} ({count:,} openings)\")\n",
    "        \n",
    "        # Create encoding mappings for eco_number (00-99 ‚Üí sequential integers)\n",
    "        unique_numbers = sorted(opening_eco_map['eco_number_str'].unique())\n",
    "        eco_number_to_int = {num: idx for idx, num in enumerate(unique_numbers)}\n",
    "        eco_int_to_number = {idx: num for num, idx in eco_number_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_number_cat'] = opening_eco_map['eco_number_str'].map(eco_number_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Number encoding created:\")\n",
    "        print(f\"      {len(unique_numbers)} unique numbers mapped to [0, {len(unique_numbers)-1}]\")\n",
    "        print(f\"      Range: '{unique_numbers[0]}' ‚Üí 0, ..., '{unique_numbers[-1]}' ‚Üí {len(unique_numbers)-1}\")\n",
    "        \n",
    "        # Show distribution of numbers\n",
    "        print(f\"\\n   Distribution of ECO numbers (top 10):\")\n",
    "        number_counts = opening_eco_map['eco_number_str'].value_counts().head(10)\n",
    "        for num, count in number_counts.items():\n",
    "            encoded = eco_number_to_int[num]\n",
    "            print(f\"      '{num}' (‚Üí {encoded:>2}): {count:>3} openings\")\n",
    "        \n",
    "        # Create final opening_side_info table\n",
    "        # Only keep the encoded categorical columns and rename them for clarity\n",
    "        opening_side_info = opening_eco_map[['eco_letter_cat', 'eco_number_cat']].copy()\n",
    "\n",
    "        print(f\"   ‚úì Created opening_side_info\")\n",
    "        print(f\"      ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"      ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "        # Verify all openings in train/val/test have ECO info\n",
    "        all_openings = set(X_train_df['opening_id'].unique()) | \\\n",
    "                       set(X_val_df['opening_id'].unique()) | \\\n",
    "                       set(X_test_df['opening_id'].unique())\n",
    "        \n",
    "        missing_openings = all_openings - set(opening_side_info.index)\n",
    "        if len(missing_openings) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits are missing ECO info!\")\n",
    "            print(f\"   Missing opening IDs: {sorted(list(missing_openings))[:10]}...\")\n",
    "        \n",
    "        # Remove ECO from train/val/test DataFrames\n",
    "        print(f\"   ‚Ä¢ X_train before: {X_train_df.shape}, columns: {list(X_train_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val before: {X_val_df.shape}, columns: {list(X_val_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test before: {X_test_df.shape}, columns: {list(X_test_df.columns)}\")\n",
    "        \n",
    "        X_train_clean = X_train_df.drop(columns=['eco'])\n",
    "        X_val_clean = X_val_df.drop(columns=['eco'])\n",
    "        X_test_clean = X_test_df.drop(columns=['eco'])\n",
    "        \n",
    "        # Sample data showing the transformation\n",
    "        print(f\"\\n8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\")\n",
    "        sample_openings = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "        \n",
    "        # For display purposes, reconstruct original values from the categorical encodings\n",
    "        print(f\"\\n   {'Opening ID':<12} {'Letter (Cat)':<15} {'Number (Cat)':<15} {'Reconstructed ECO':<18}\")\n",
    "        print(f\"   {'-'*12} {'-'*15} {'-'*15} {'-'*18}\")\n",
    "        for idx, row in sample_openings.iterrows():\n",
    "            letter_str = eco_int_to_letter[row['eco_letter_cat']]\n",
    "            number_str = eco_int_to_number[row['eco_number_cat']]\n",
    "            reconstructed = f\"{letter_str}{number_str}\"\n",
    "            print(f\"   {idx:<12} {row['eco_letter_cat']:<15} {row['eco_number_cat']:<15} {reconstructed:<18}\")\n",
    "        \n",
    "        # Show ECO family distribution\n",
    "        print(f\"\\n9Ô∏è‚É£  ECO family distribution in opening_side_info:\")\n",
    "        letter_dist = opening_side_info['eco_letter_cat'].value_counts().sort_index()\n",
    "        print(f\"\\n   {'Encoded':<8} {'Letter':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "        print(f\"   {'-'*8} {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "        for encoded_val, count in letter_dist.items():\n",
    "            letter = eco_int_to_letter[encoded_val]\n",
    "            pct = 100 * count / len(opening_side_info)\n",
    "            bar_length = int(pct * 0.4)\n",
    "            bar = '‚ñà' * bar_length\n",
    "            print(f\"   {encoded_val:<8} {letter:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "        \n",
    "        print(f\"\\nCreated: opening_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape[0]:,} rows, {X_train_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape[0]:,} rows, {X_val_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape[0]:,} rows, {X_test_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ opening_side_info: {len(opening_side_info):,} openings (one per opening)\")\n",
    "        print(f\"   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\")\n",
    "        print(f\"   ‚Ä¢ eco_letter_to_int: {eco_letter_to_int}\")\n",
    "        print(f\"   ‚Ä¢ eco_number_to_int: (dict with {len(eco_number_to_int)} entries)\")\n",
    "        print(f\"\\n   You'll need them to encode ECO codes for new openings at inference time.\")\n",
    "\n",
    "        return opening_side_info, eco_letter_to_int, eco_number_to_int, X_train_clean, X_val_clean, X_test_clean\n",
    "    \n",
    "    # Call the function\n",
    "    opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test = create_eco_side_information(\n",
    "        clean_data, X_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Create reverse mappings for decoding (needed for verification and debugging)\n",
    "    eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "    eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "    print(f\"\\n‚úì Created reverse mappings for ECO decoding:\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_letter: {len(eco_int_to_letter)} entries\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_number: {len(eco_int_to_number)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICATION: FINAL DATA STRUCTURE\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\n",
      "\n",
      "   X_train:\n",
      "   ‚Ä¢ Shape: (2172777, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ Sample:\n",
      "         player_id  opening_id  confidence\n",
      "1886482      30917        1231    0.893843\n",
      "806613       13111         575    0.166667\n",
      "1389675      22812         773    0.342105\n",
      "\n",
      "   X_val:\n",
      "   ‚Ä¢ Shape: (434556, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "   X_test:\n",
      "   ‚Ä¢ Shape: (289704, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Side Information Tables:\n",
      "\n",
      "   player_side_info (indexed by player_id):\n",
      "   ‚Ä¢ Shape: (48469, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Sample:\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.608709\n",
      "1          1.057978\n",
      "2          0.560573\n",
      "\n",
      "   opening_side_info (indexed by opening_id):\n",
      "   ‚Ä¢ Shape: (2716, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Sample:\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "531                      1               0\n",
      "565                      1               0\n",
      "572                      1               0\n",
      "\n",
      "3Ô∏è‚É£  Encoding Mappings (for inference):\n",
      "\n",
      "   eco_letter_map:\n",
      "      'A' ‚Üí 0\n",
      "      'B' ‚Üí 1\n",
      "      'C' ‚Üí 2\n",
      "      'D' ‚Üí 3\n",
      "      'E' ‚Üí 4\n",
      "\n",
      "   eco_number_map (first 10):\n",
      "      '00' ‚Üí 0\n",
      "      '01' ‚Üí 1\n",
      "      '02' ‚Üí 2\n",
      "      '03' ‚Üí 3\n",
      "      '04' ‚Üí 4\n",
      "      '05' ‚Üí 5\n",
      "      '06' ‚Üí 6\n",
      "      '07' ‚Üí 7\n",
      "      '08' ‚Üí 8\n",
      "      '09' ‚Üí 9\n",
      "      ... (100 total)\n",
      "\n",
      "4Ô∏è‚É£  Example: Lookup flow for a random train sample:\n",
      "\n",
      "   Sample interaction:\n",
      "   ‚Ä¢ player_id: 38004.0\n",
      "   ‚Ä¢ opening_id: 920.0\n",
      "   ‚Ä¢ confidence: 0.5283\n",
      "\n",
      "   Player side info lookup:\n",
      "   ‚Ä¢ rating_z: 0.6408\n",
      "\n",
      "   Opening side info lookup:\n",
      "   ‚Ä¢ eco_letter_cat: 1 (letter: B)\n",
      "   ‚Ä¢ eco_number_cat: 40 (number: 40)\n",
      "\n",
      "============================================================\n",
      "‚úÖ VERIFICATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Ready for PyTorch tensor conversion:\n",
      "   ‚Ä¢ Features: player_id, opening_id, confidence\n",
      "   ‚Ä¢ Target: score (in y_train, y_val, y_test)\n",
      "   ‚Ä¢ Player side info: rating_z\n",
      "   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\n",
      "\n",
      "   All ECO and rating data is now properly separated as side information!\n",
      "\n",
      "   Side info tables are clean - only contain necessary model inputs!\n"
     ]
    }
   ],
   "source": [
    "# Verify final data structure after ECO processing\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION: FINAL DATA STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\")\n",
    "print(f\"\\n   X_train:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_train.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(X_train.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   X_val:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_val.columns)}\")\n",
    "\n",
    "print(f\"\\n   X_test:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_test.columns)}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Side Information Tables:\")\n",
    "\n",
    "print(f\"\\n   player_side_info (indexed by player_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(player_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   opening_side_info (indexed by opening_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(opening_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Encoding Mappings (for inference):\")\n",
    "print(f\"\\n   eco_letter_map:\")\n",
    "for k, v in sorted(eco_letter_map.items()):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "\n",
    "print(f\"\\n   eco_number_map (first 10):\")\n",
    "for i, (k, v) in enumerate(sorted(eco_number_map.items())[:10]):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "print(f\"      ... ({len(eco_number_map)} total)\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Example: Lookup flow for a random train sample:\")\n",
    "sample = X_train.sample(1, random_state=42).iloc[0]\n",
    "player_id = sample['player_id']\n",
    "opening_id = sample['opening_id']\n",
    "\n",
    "print(f\"\\n   Sample interaction:\")\n",
    "print(f\"   ‚Ä¢ player_id: {player_id}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {opening_id}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Player side info lookup:\")\n",
    "player_info = player_side_info.loc[player_id]\n",
    "print(f\"   ‚Ä¢ rating_z: {player_info['rating_z']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Opening side info lookup:\")\n",
    "opening_info = opening_side_info.loc[opening_id]\n",
    "print(f\"   ‚Ä¢ eco_letter_cat: {opening_info['eco_letter_cat']} (letter: {eco_int_to_letter[opening_info['eco_letter_cat']]})\")\n",
    "print(f\"   ‚Ä¢ eco_number_cat: {opening_info['eco_number_cat']} (number: {eco_int_to_number[opening_info['eco_number_cat']]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Ready for PyTorch tensor conversion:\")\n",
    "print(f\"   ‚Ä¢ Features: player_id, opening_id, confidence\")\n",
    "print(f\"   ‚Ä¢ Target: score (in y_train, y_val, y_test)\")\n",
    "print(f\"   ‚Ä¢ Player side info: rating_z\")\n",
    "print(f\"   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\")\n",
    "print(f\"\\n   All ECO and rating data is now properly separated as side information!\")\n",
    "print(f\"\\n   Side info tables are clean - only contain necessary model inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43752aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "============================================================\n",
      "STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\n",
      "============================================================\n",
      "Final DataFrame for correlation created.\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'score', 'confidence', 'rating_z', 'eco_letter_cat', 'eco_number_cat']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRELATIONS WITH TARGET VARIABLE (score)\n",
      "================================================================================\n",
      "eco_letter_cat    0.116288\n",
      "confidence        0.098041\n",
      "eco_number_cat    0.055454\n",
      "rating_z          0.043109\n"
     ]
    }
   ],
   "source": [
    "# Step 4c: Correlation Analysis of Processed Data\n",
    "\n",
    "%pip install seaborn --quiet\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Start with the main interaction data (already has remapped IDs, adjusted score, confidence)\n",
    "corr_df = clean_data[[\"player_id\", \"opening_id\", \"score\", \"confidence\"]].copy()\n",
    "\n",
    "# Merge player side information (rating_z)\n",
    "# player_side_info is indexed by the remapped player_id\n",
    "corr_df = corr_df.merge(\n",
    "    player_side_info[[\"rating_z\"]], left_on=\"player_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# Merge opening side information (eco categories)\n",
    "# opening_side_info is indexed by the remapped opening_id\n",
    "corr_df = corr_df.merge(\n",
    "    opening_side_info[[\"eco_letter_cat\", \"eco_number_cat\"]],\n",
    "    left_on=\"opening_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(f\"Final DataFrame for correlation created.\")\n",
    "print(f\"   ‚Ä¢ Columns: {corr_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "correlation_matrix = corr_df.corr().drop(columns=['player_id', 'opening_id']).drop(index=['player_id', 'opening_id'])\n",
    "\n",
    "\n",
    "# 3. Visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Model Features\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 4. Analyze correlations with the target variable 'score'\n",
    "print(\"CORRELATIONS WITH TARGET VARIABLE (score)\")\n",
    "print(\"=\" * 80)\n",
    "score_correlations = (\n",
    "    correlation_matrix[\"score\"].drop(\"score\").sort_values(ascending=False)\n",
    ")\n",
    "print(score_correlations.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831d7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017515d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\n",
      "====================================================================================================\n",
      "   ‚Ä¢ Converting 84 NEW opening IDs to OLD database IDs for query\n",
      "   ‚Ä¢ Example: NEW ID 920 ‚Üí OLD ID 1167\n",
      "#    Player   Opening   ECO (DB)   Reconstructed Match  Opening Name                                      \n",
      "====================================================================================================\n",
      "1    38004    920       B40        B40           ‚úì      Sicilian Defense: French Variation                \n",
      "2    26015    1057      C00        C00           ‚úì      French Defense: Knight Variation                  \n",
      "3    26663    594       B01        B01           ‚úì      Scandinavian Defense: Main Line                   \n",
      "4    45783    1183      C20        C20           ‚úì      King's Pawn Game: Beyer Gambit                    \n",
      "5    1550     1406      C40        C40           ‚úì      King's Knight Opening                             \n",
      "6    46346    1648      C55        C55           ‚úì      Italian Game: Two Knights Defense, Modern Bis...  \n",
      "7    42680    1060      C00        C00           ‚úì      French Defense: Mediterranean Defense             \n",
      "8    5190     1741      C69        C69           ‚úì      Ruy Lopez: Exchange Variation, Normal Variation   \n",
      "9    44899    2007      D31        D31           ‚úì      Semi-Slav Defense: Accelerated Move Order         \n",
      "10   5329     2461      A40        A40           ‚úì      Queen's Pawn Game: Modern Defense                 \n",
      "11   8086     1298      C31        C31           ‚úì      King's Gambit Declined: Falkbeer Countergambi...  \n",
      "12   47916    1596      C50        C50           ‚úì      Italian Game: Giuoco Pianissimo, Normal           \n",
      "13   20448    1231      C24        C24           ‚úì      Bishop's Opening: Berlin Defense                  \n",
      "14   2818     1294      C30        C30           ‚úì      King's Gambit Declined: Queen's Knight Defense    \n",
      "15   10789    599       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "16   8061     1357      C36        C36           ‚úì      King's Gambit Accepted: Modern Defense            \n",
      "17   43010    688       B07        B07           ‚úì      Pirc Defense: Byrne Variation                     \n",
      "18   40194    893       B32        B32           ‚úì      Sicilian Defense: Kalashnikov Variation           \n",
      "19   46726    767       B17        B17           ‚úì      Caro-Kann Defense: Karpov Variation, Modern V...  \n",
      "20   2662     565       B00        B00           ‚úì      Owen Defense                                      \n",
      "21   45635    42        A00        A00           ‚úì      K√°das Opening                                     \n",
      "22   39056    803       B21        B21           ‚úì      Sicilian Defense: McDonnell Attack                \n",
      "23   28737    1185      C20        C20           ‚úì      King's Pawn Game: King's Head Opening             \n",
      "24   21866    575       B00        B00           ‚úì      St. George Defense                                \n",
      "25   15761    1024      B90        B90           ‚úì      Sicilian Defense: Najdorf Variation               \n",
      "26   16320    1483      C42        C42           ‚úì      Russian Game: Three Knights Game                  \n",
      "27   8871     2467      C54        C54           ‚úì      Italian Game: Classical Variation, Giuoco Pia...  \n",
      "28   44390    1907      D08        D08           ‚úì      Queen's Gambit Declined: Albin Countergambit      \n",
      "29   39933    565       B00        B00           ‚úì      Owen Defense                                      \n",
      "30   47720    398       A50        A50           ‚úì      Indian Defense: Normal Variation                  \n",
      "31   38302    547       B00        B00           ‚úì      Nimzowitsch Defense: Kennedy Variation, de Sm...  \n",
      "32   4765     246       A21        A21           ‚úì      English Opening: King's English Variation, Re...  \n",
      "33   17913    1281      C30        C30           ‚úì      King's Gambit                                     \n",
      "34   30566    1866      D01        D01           ‚úì      Rapport-Jobava System                             \n",
      "35   23946    2660      C42        C42           ‚úì      Petrov's Defense: Three Knights Game              \n",
      "36   34383    1532      C44        C44           ‚úì      Scotch Game: Scotch Gambit, Sarratt Variation     \n",
      "37   7814     1679      C60        C60           ‚úì      Ruy Lopez: Cozio Defense                          \n",
      "38   32292    73        A00        A00           ‚úì      Van Geet Opening                                  \n",
      "39   967      1403      C40        C40           ‚úì      Elephant Gambit: Paulsen Countergambit            \n",
      "40   30518    1057      C00        C00           ‚úì      French Defense: Knight Variation                  \n",
      "41   10012    1482      C42        C42           ‚úì      Russian Game: Stafford Gambit                     \n",
      "42   920      408       A52        A52           ‚úì      Indian Defense: Budapest Defense, Adler Varia...  \n",
      "43   12710    1058      C00        C00           ‚úì      French Defense: La Bourdonnais Variation          \n",
      "44   14644    166       A06        A06           ‚úì      Zukertort Opening: Tennison Gambit                \n",
      "45   34192    1632      C53        C53           ‚úì      Italian Game: Classical Variation                 \n",
      "46   37258    565       B00        B00           ‚úì      Owen Defense                                      \n",
      "47   2133     920       B40        B40           ‚úì      Sicilian Defense: French Variation                \n",
      "48   29229    37        A00        A00           ‚úì      Hungarian Opening: Sicilian Invitation            \n",
      "49   44258    731       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Short V...  \n",
      "50   35590    137       A04        A04           ‚úì      Zukertort Opening: Kingside Fianchetto            \n",
      "51   46433    1183      C20        C20           ‚úì      King's Pawn Game: Beyer Gambit                    \n",
      "52   3321     1024      B90        B90           ‚úì      Sicilian Defense: Najdorf Variation               \n",
      "53   747      531       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "54   25066    835       B23        B23           ‚úì      Sicilian Defense: Closed                          \n",
      "55   17572    563       B00        B00           ‚úì      Nimzowitsch Defense: Williams Variation           \n",
      "56   47890    560       B00        B00           ‚úì      Nimzowitsch Defense: Scandinavian Variation, ...  \n",
      "57   26212    849       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Fianchetto     \n",
      "58   43562    472       A80        A80           ‚úì      Dutch Defense                                     \n",
      "59   45619    2310      E61        E61           ‚úì      King's Indian Defense                             \n",
      "60   26019    1365      C37        C37           ‚úì      King's Gambit Accepted: Muzio Gambit, Sarratt...  \n",
      "61   40063    398       A50        A50           ‚úì      Indian Defense: Normal Variation                  \n",
      "62   10053    1206      C21        C21           ‚úì      Danish Gambit                                     \n",
      "63   28656    565       B00        B00           ‚úì      Owen Defense                                      \n",
      "64   35247    2139      D80        D80           ‚úì      Gr√ºnfeld Defense                                  \n",
      "65   22389    2425      C54        C54           ‚úì      Italian Game: Classical Variation, Giuoco Pia...  \n",
      "66   528      1702      C64        C64           ‚úì      Ruy Lopez: Classical Variation                    \n",
      "67   35571    1578      C48        C48           ‚úì      Four Knights Game: Spanish Variation              \n",
      "68   15511    2483      A53        A53           ‚úì      Old Indian Defense: Czech Variation, with Nc3     \n",
      "69   10384    234       A17        A17           ‚úì      English Opening: Anglo-Indian Defense, Hedgeh...  \n",
      "70   18002    1216      C22        C22           ‚úì      Center Game: Normal Variation                     \n",
      "71   10104    343       A43        A43           ‚úì      Benoni Defense: Old Benoni                        \n",
      "72   7352     1087      C02        C02           ‚úì      French Defense: Advance Variation, Nimzowitsc...  \n",
      "73   29238    1357      C36        C36           ‚úì      King's Gambit Accepted: Modern Defense            \n",
      "74   1740     2196      E11        E11           ‚úì      Bogo-Indian Defense: Gr√ºnfeld Variation           \n",
      "75   2135     598       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "76   2794     688       B07        B07           ‚úì      Pirc Defense: Byrne Variation                     \n",
      "77   26629    2456      C54        C54           ‚úì      Italian Game: Classical Variation, Giuoco Pia...  \n",
      "78   9528     706       B10        B10           ‚úì      Caro-Kann Defense: Accelerated Panov Attack       \n",
      "79   3787     908       B34        B34           ‚úì      Sicilian Defense: Accelerated Dragon, Exchang...  \n",
      "80   5385     1872      D02        D02           ‚úì      Queen's Pawn Game: Krause Variation               \n",
      "81   23758    726       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation              \n",
      "82   1203     920       B40        B40           ‚úì      Sicilian Defense: French Variation                \n",
      "83   25317    789       B20        B20           ‚úì      Sicilian Defense: Mengarini Variation             \n",
      "84   20087    295       A40        A40           ‚úì      English Defense                                   \n",
      "85   26220    918       B40        B40           ‚úì      Sicilian Defense: Four Knights Variation          \n",
      "86   39665    1596      C50        C50           ‚úì      Italian Game: Giuoco Pianissimo, Normal           \n",
      "87   37844    404       A51        A51           ‚úì      Indian Defense: Budapest Defense                  \n",
      "88   32308    1281      C30        C30           ‚úì      King's Gambit                                     \n",
      "89   28405    1065      C00        C00           ‚úì      French Defense: Queen's Knight                    \n",
      "90   21945    625       B02        B02           ‚úì      Alekhine Defense: S√§misch Attack                  \n",
      "91   38605    1655      C57        C57           ‚úì      Italian Game: Two Knights Defense, Fritz Vari...  \n",
      "92   20011    1669      C58        C58           ‚úì      Italian Game: Two Knights Defense, Polerio De...  \n",
      "93   47625    705       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "94   11224    2463      C00        C00           ‚úì      French Defense: Franco-Sicilian Defense           \n",
      "95   27505    1123      C11        C11           ‚úì      French Defense: Classical Variation, Delayed ...  \n",
      "96   46902    1872      D02        D02           ‚úì      Queen's Pawn Game: Krause Variation               \n",
      "97   27456    1556      C46        C46           ‚úì      Three Knights Opening                             \n",
      "98   41672    1024      B90        B90           ‚úì      Sicilian Defense: Najdorf Variation               \n",
      "99   1394     889       B32        B32           ‚úì      Sicilian Defense: Accelerated Dragon              \n",
      "100  21399    1632      C53        C53           ‚úì      Italian Game: Classical Variation                 \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Verification Results:\n",
      "   ‚Ä¢ Total samples: 100\n",
      "   ‚Ä¢ Matches: 100/100 (100.0%)\n",
      "   ‚Ä¢ Mismatches: 0\n",
      "\n",
      " All ECO codes reconstructed correctly\n",
      "   ‚Ä¢ ID remapping preserved all ECO code mappings\n"
     ]
    }
   ],
   "source": [
    "# Verification: Sample player-opening pairs with reconstructed ECO codes and opening names\n",
    "# Doing this to make sure that our ECO encoding/decoding is correct\n",
    "\n",
    "print(\"VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Sample random player-opening pairs from training data\n",
    "sample_size = 100\n",
    "sample_data = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "\n",
    "# Get unique opening IDs from sample (these are NEW remapped IDs)\n",
    "new_opening_ids = sample_data['opening_id'].unique()\n",
    "\n",
    "# Convert NEW opening IDs back to OLD database IDs for query\n",
    "if opening_remapped:\n",
    "    old_opening_ids = [opening_idx_to_id[int(new_id)] for new_id in new_opening_ids]\n",
    "    print(f\"   ‚Ä¢ Converting {len(new_opening_ids)} NEW opening IDs to OLD database IDs for query\")\n",
    "    print(f\"   ‚Ä¢ Example: NEW ID {new_opening_ids[0]} ‚Üí OLD ID {old_opening_ids[0]}\")\n",
    "else:\n",
    "    old_opening_ids = new_opening_ids\n",
    "    print(f\"   ‚Ä¢ No remapping was done - using opening IDs directly\")\n",
    "\n",
    "opening_ids_str = ','.join(map(str, old_opening_ids))\n",
    "\n",
    "# Query database for opening names using OLD IDs\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    opening_query = f\"\"\"\n",
    "        SELECT id, name, eco\n",
    "        FROM opening\n",
    "        WHERE id IN ({opening_ids_str})\n",
    "    \"\"\"\n",
    "    opening_names = pd.DataFrame(con.execute(opening_query).df()).set_index('id')\n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Create reverse mappings for ECO decoding\n",
    "eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "\n",
    "# Build verification table\n",
    "print(f\"{'#':<4} {'Player':<8} {'Opening':<9} {'ECO (DB)':<10} {'Reconstructed':<13} {'Match':<6} {'Opening Name':<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "matches = 0\n",
    "for i, (idx, row) in enumerate(sample_data.iterrows(), 1):\n",
    "    player_id = int(row['player_id'])\n",
    "    new_opening_id = int(row['opening_id'])\n",
    "    \n",
    "    # Convert NEW opening ID to OLD database ID for lookup\n",
    "    if opening_remapped:\n",
    "        old_opening_id = opening_idx_to_id[new_opening_id]\n",
    "    else:\n",
    "        old_opening_id = new_opening_id\n",
    "    \n",
    "    # Lookup opening side info using NEW ID\n",
    "    opening_info = opening_side_info.loc[new_opening_id]\n",
    "    \n",
    "    # Reconstruct ECO from encoded categorical values\n",
    "    eco_letter_encoded = opening_info['eco_letter_cat']\n",
    "    eco_number_encoded = opening_info['eco_number_cat']\n",
    "    \n",
    "    eco_letter_decoded = eco_int_to_letter[eco_letter_encoded]\n",
    "    eco_number_decoded = eco_int_to_number[eco_number_encoded]\n",
    "    \n",
    "    reconstructed_eco = f\"{eco_letter_decoded}{eco_number_decoded}\"\n",
    "    \n",
    "    # Get original ECO from database using OLD ID\n",
    "    db_eco = opening_names.loc[old_opening_id, 'eco']\n",
    "    opening_name = opening_names.loc[old_opening_id, 'name']\n",
    "    \n",
    "    # Check if they match\n",
    "    match = \"‚úì\" if reconstructed_eco == db_eco else \"‚úó\"\n",
    "    if reconstructed_eco == db_eco:\n",
    "        matches += 1\n",
    "    \n",
    "    # Truncate opening name if too long\n",
    "    if len(opening_name) > 48:\n",
    "        opening_name = opening_name[:45] + \"...\"\n",
    "    \n",
    "    # Display using NEW opening ID (what's in the data now)\n",
    "    print(f\"{i:<4} {player_id:<8} {new_opening_id:<9} {db_eco:<10} {reconstructed_eco:<13} {match:<6} {opening_name:<50}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n‚úÖ Verification Results:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(sample_data)}\")\n",
    "print(f\"   ‚Ä¢ Matches: {matches}/{len(sample_data)} ({100*matches/len(sample_data):.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Mismatches: {len(sample_data) - matches}\")\n",
    "\n",
    "if matches == len(sample_data):\n",
    "    print(f\"\\n All ECO codes reconstructed correctly\")\n",
    "    print(f\"   ‚Ä¢ ID remapping preserved all ECO code mappings\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Some ECO codes did not match. Investigate mismatches above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf4878",
   "metadata": {},
   "source": [
    "## 5. Data Verification and Examination\n",
    "We're almost there. Let's examine our data structures to check for any obvious flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train \n",
      "          player_id  opening_id  confidence\n",
      "1886482      30917        1231    0.893843\n",
      "806613       13111         575    0.166667\n",
      "1389675      22812         773    0.342105\n",
      "1286945      21140         894    0.615385\n",
      "2460020      40360        1237    0.468085\n",
      "============================================================\n",
      "X_val \n",
      "          player_id  opening_id  confidence\n",
      "403673        6580        2049    0.305556\n",
      "1076840      17588         569    0.494949\n",
      "1768784      28978          99    0.397590\n",
      "1131516      18505         565    0.568966\n",
      "2099352      34412        1220    0.390244\n",
      "============================================================\n",
      "X_test \n",
      "          player_id  opening_id  confidence\n",
      "1766099      28939        1456    0.650350\n",
      "772009       12540        1632    0.230769\n",
      "2273266      37257        1432    0.180328\n",
      "1280894      21034        2643    0.206349\n",
      "1299871      21363         921    0.572650\n",
      "============================================================\n",
      "y_train \n",
      " 1886482    0.505674\n",
      "806613     0.485887\n",
      "1389675    0.438508\n",
      "1286945    0.502016\n",
      "2460020    0.581100\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_val \n",
      " 403673     0.542673\n",
      "1076840    0.483998\n",
      "1768784    0.536776\n",
      "1131516    0.611447\n",
      "2099352    0.531505\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_test \n",
      " 1766099    0.545516\n",
      "772009     0.578429\n",
      "2273266    0.554022\n",
      "1280894    0.572810\n",
      "1299871    0.512930\n",
      "Name: score, dtype: float64\n",
      "player_side_info \n",
      "            rating_z\n",
      "player_id          \n",
      "0          0.608709\n",
      "1          1.057978\n",
      "2          0.560573\n",
      "3          0.616731\n",
      "4         -2.167135\n",
      "============================================================\n",
      "opening_side_info \n",
      "             eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "531                      1               0\n",
      "565                      1               0\n",
      "572                      1               0\n",
      "579                      1               1\n",
      "589                      1               1\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train \\n\", X_train.head())\n",
    "print(\"=\"*60)\n",
    "print(\"X_val \\n\", X_val.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"X_test \\n\", X_test.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_train \\n\", y_train.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_val \\n\", y_val.head())\n",
    "print(\"=\" * 60) \n",
    "print(\"y_test \\n\", y_test.head())\n",
    "\n",
    "# Now side information\n",
    "print(\"player_side_info \\n\", player_side_info.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"opening_side_info \\n\", opening_side_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52936cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANED SIDE INFORMATION TABLES\n",
      "============================================================\n",
      "\n",
      "üìä player_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (48469, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Index: player_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.608709\n",
      "1          1.057978\n",
      "2          0.560573\n",
      "3          0.616731\n",
      "4         -2.167135\n",
      "\n",
      "üìä opening_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (2716, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Index: opening_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "531                      1               0\n",
      "565                      1               0\n",
      "572                      1               0\n",
      "579                      1               1\n",
      "589                      1               1\n"
     ]
    }
   ],
   "source": [
    "# Final verification: Display cleaned side info tables\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANED SIDE INFORMATION TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä player_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: player_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(player_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\nüìä opening_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: opening_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(opening_side_info.head().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19883cba",
   "metadata": {},
   "source": [
    "## Step 5: Convert to PyTorch Tensors\n",
    "\n",
    "**What are tensors?**\n",
    "Tensors are PyTorch's version of arrays - just multi-dimensional data structures optimized for deep learning. Think of them like fancy NumPy arrays that can run on GPUs.\n",
    "\n",
    "**What we need to convert:**\n",
    "\n",
    "1. **Main features** (X_train, X_val, X_test):\n",
    "   - `player_id` ‚Üí long tensor (integer IDs)\n",
    "   - `opening_id` ‚Üí long tensor (integer IDs)\n",
    "   - `confidence` ‚Üí float tensor (weights for loss function)\n",
    "\n",
    "2. **Targets** (y_train, y_val, y_test):\n",
    "   - `score` ‚Üí float tensor (what we're predicting)\n",
    "\n",
    "3. **Player side info** (player_side_info):\n",
    "   - `rating_z` ‚Üí float tensor (normalized ratings)\n",
    "   - Indexed by player_id for fast lookup\n",
    "\n",
    "4. **Opening side info** (opening_side_info):\n",
    "   - `eco_letter_cat` ‚Üí long tensor (categorical)\n",
    "   - `eco_number_cat` ‚Üí long tensor (categorical)\n",
    "   - Indexed by opening_id for fast lookup\n",
    "\n",
    "**Why these data types?**\n",
    "- `long` (int64): For IDs and categorical features that will be embedded\n",
    "- `float` (float32): For continuous values like scores, confidence, and normalized ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350be514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4c/gt84bby56mqdnzwsg72zvd680000gn/T/ipykernel_11045/1794647647.py\", line 6, in <module>\n",
      "    import torch\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# It says subprocess is unused but complains at me if I don't import it\n",
    "# Something to do with incompatible versions of blah blah blah\n",
    "# Don't really care as long as it works\n",
    "import subprocess\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda01b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: CONVERT TO PYTORCH TENSORS\n",
      "============================================================\n",
      "   ‚Ä¢ CUDA available: False\n",
      "   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\n",
      "\n",
      "0Ô∏è‚É£  Index alignment sanity checks...\n",
      "   ‚úì player_side_info index is sorted\n",
      "   ‚Ä¢ Index range: [0, 48468]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\n",
      "   ‚Ä¢ Index range: [0, 2715]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "\n",
      "   Checking if indices are contiguous 0-based...\n",
      "   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\n",
      "   ‚Ä¢ Training tensors:\n",
      "   ‚Ä¢ player_ids_train: torch.Size([2172777]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_train: torch.Size([2172777]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_train: torch.Size([2172777]), dtype=torch.float32\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([434556]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([434556]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([434556]), dtype=torch.float32\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([289704]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([289704]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([289704]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   ‚Ä¢ scores_train: torch.Size([2172777]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([434556]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([289704]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1004, 1.0000]\n",
      "   ‚Ä¢ Val: [0.1138, 0.7920]\n",
      "   ‚Ä¢ Test: [0.1996, 0.7872]\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([48469]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([48469]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.2674, 4.2430]\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2716]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2716]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2716]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "   ‚Ä¢ Train: 2,172,777 samples\n",
      "   ‚Ä¢ Val: 434,556 samples\n",
      "   ‚Ä¢ Test: 289,704 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 48,469\n",
      "   ‚Ä¢ Openings: 2,716\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 48469 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2716 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 66.53 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Convert all data to PyTorch tensors\n",
    "\n",
    "print(\"STEP 5: CONVERT TO PYTORCH TENSORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\")\n",
    "\n",
    "# 0. Index alignment sanity checks\n",
    "print(f\"\\n0Ô∏è‚É£  Index alignment sanity checks...\")\n",
    "\n",
    "# Check player_side_info is properly indexed\n",
    "player_ids_sorted = sorted(player_side_info.index.values)\n",
    "if player_ids_sorted != list(player_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  player_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì player_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{player_side_info.index.min()}, {player_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {player_side_info.index.dtype}\")\n",
    "\n",
    "# Check opening_side_info is properly indexed\n",
    "opening_ids_sorted = sorted(opening_side_info.index.values)\n",
    "if opening_ids_sorted != list(opening_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì opening_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{opening_side_info.index.min()}, {opening_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {opening_side_info.index.dtype}\")\n",
    "\n",
    "# CRITICAL: Check if indices are contiguous 0-based\n",
    "# If opening_side_info.index = [0, 1, 2, ..., N-1], then we can use opening_id as direct array index\n",
    "# If not (e.g., [10, 15, 23, ...]), we'll need a mapping dictionary\n",
    "print(f\"\\n   Checking if indices are contiguous 0-based...\")\n",
    "player_ids_are_contiguous = (player_side_info.index == range(len(player_side_info))).all()\n",
    "opening_ids_are_contiguous = (opening_side_info.index == range(len(opening_side_info))).all()\n",
    "\n",
    "\n",
    "if not player_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Player IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "if not opening_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "\n",
    "# 1. Convert main features (train/val/test)\n",
    "\n",
    "# Train set\n",
    "player_ids_train = torch.tensor(X_train['player_id'].values, dtype=torch.long)\n",
    "opening_ids_train = torch.tensor(X_train['opening_id'].values, dtype=torch.long)\n",
    "confidence_train = torch.tensor(X_train['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ Training tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_train: {player_ids_train.shape}, dtype={player_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_train: {opening_ids_train.shape}, dtype={opening_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_train: {confidence_train.shape}, dtype={confidence_train.dtype}\")\n",
    "\n",
    "# Validation set\n",
    "player_ids_val = torch.tensor(X_val['player_id'].values, dtype=torch.long)\n",
    "opening_ids_val = torch.tensor(X_val['opening_id'].values, dtype=torch.long)\n",
    "confidence_val = torch.tensor(X_val['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Validation tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_val: {player_ids_val.shape}, dtype={player_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_val: {opening_ids_val.shape}, dtype={opening_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_val: {confidence_val.shape}, dtype={confidence_val.dtype}\")\n",
    "\n",
    "# Test set\n",
    "player_ids_test = torch.tensor(X_test['player_id'].values, dtype=torch.long)\n",
    "opening_ids_test = torch.tensor(X_test['opening_id'].values, dtype=torch.long)\n",
    "confidence_test = torch.tensor(X_test['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Test tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_test: {player_ids_test.shape}, dtype={player_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_test: {opening_ids_test.shape}, dtype={opening_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_test: {confidence_test.shape}, dtype={confidence_test.dtype}\")\n",
    "\n",
    "# 2. Convert targets (scores)\n",
    "print(f\"\\n2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\")\n",
    "\n",
    "scores_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "scores_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "scores_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ scores_train: {scores_train.shape}, dtype={scores_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_val: {scores_val.shape}, dtype={scores_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_test: {scores_test.shape}, dtype={scores_test.dtype}\")\n",
    "\n",
    "print(f\"\\n   Score ranges (sanity check):\")\n",
    "print(f\"   ‚Ä¢ Train: [{scores_train.min():.4f}, {scores_train.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Val: [{scores_val.min():.4f}, {scores_val.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Test: [{scores_test.min():.4f}, {scores_test.max():.4f}]\")\n",
    "\n",
    "# 3. Convert player side information\n",
    "\n",
    "# Create tensor of all player ratings (indexed by player_id)\n",
    "# Since player_side_info is indexed by player_id, we need to ensure coverage\n",
    "player_ratings_tensor = torch.tensor(player_side_info['rating_z'].values, dtype=torch.float32)\n",
    "player_ids_in_side_info = torch.tensor(player_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ player_ratings_tensor: {player_ratings_tensor.shape}, dtype={player_ratings_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ player_ids_in_side_info: {player_ids_in_side_info.shape}, dtype={player_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings_tensor.min():.4f}, {player_ratings_tensor.max():.4f}]\")\n",
    "\n",
    "# Verify all player IDs in train/val/test are covered\n",
    "all_player_ids = torch.cat([player_ids_train, player_ids_val, player_ids_test]).unique()\n",
    "missing_players = set(all_player_ids.tolist()) - set(player_ids_in_side_info.tolist())\n",
    "if len(missing_players) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in splits missing from side_info!\")\n",
    "\n",
    "# 4. Convert opening side information\n",
    "print(f\"\\n4Ô∏è‚É£  Converting opening side information...\")\n",
    "\n",
    "# Verify column names exist (using eco_letter_cat and eco_number_cat consistently)\n",
    "if 'eco_letter_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_letter_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "if 'eco_number_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_number_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "\n",
    "# Create tensors for opening ECO features (indexed by opening_id)\n",
    "opening_eco_letter_tensor = torch.tensor(opening_side_info['eco_letter_cat'].values, dtype=torch.long)\n",
    "opening_eco_number_tensor = torch.tensor(opening_side_info['eco_number_cat'].values, dtype=torch.long)\n",
    "opening_ids_in_side_info = torch.tensor(opening_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ opening_eco_letter_tensor: {opening_eco_letter_tensor.shape}, dtype={opening_eco_letter_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_eco_number_tensor: {opening_eco_number_tensor.shape}, dtype={opening_eco_number_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_in_side_info: {opening_ids_in_side_info.shape}, dtype={opening_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ ECO letter range: [{opening_eco_letter_tensor.min()}, {opening_eco_letter_tensor.max()}]\")\n",
    "print(f\"   ‚Ä¢ ECO number range: [{opening_eco_number_tensor.min()}, {opening_eco_number_tensor.max()}]\")\n",
    "\n",
    "# Verify all opening IDs in train/val/test are covered\n",
    "all_opening_ids = torch.cat([opening_ids_train, opening_ids_val, opening_ids_test]).unique()\n",
    "missing_openings = set(all_opening_ids.tolist()) - set(opening_ids_in_side_info.tolist())\n",
    "if len(missing_openings) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits missing from side_info!\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(f\"\\n5Ô∏è‚É£  Summary statistics:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(scores_train):,} samples\")\n",
    "print(f\"   ‚Ä¢ Val: {len(scores_val):,} samples\")\n",
    "print(f\"   ‚Ä¢ Test: {len(scores_test):,} samples\")\n",
    "\n",
    "print(f\"\\n   Unique entities:\")\n",
    "print(f\"   ‚Ä¢ Players: {len(player_ids_in_side_info):,}\")\n",
    "print(f\"   ‚Ä¢ Openings: {len(opening_ids_in_side_info):,}\")\n",
    "\n",
    "print(f\"\\n   Vocabulary sizes (for embedding layers):\")\n",
    "print(f\"   ‚Ä¢ num_players: {player_ids_in_side_info.max() + 1} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_openings: {opening_ids_in_side_info.max() + 1} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_letters: {opening_eco_letter_tensor.max() + 1} (max eco_letter_cat + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_numbers: {opening_eco_number_tensor.max() + 1} (max eco_number_cat + 1)\")\n",
    "\n",
    "# 6. Memory usage (approximate - using simple calculation)\n",
    "print(f\"\\n6Ô∏è‚É£  Approximate memory usage:\")\n",
    "def tensor_memory_mb(t):\n",
    "    \"\"\"Calculate tensor memory in MB\"\"\"\n",
    "    return (t.element_size() * t.nelement()) / (1024 * 1024)\n",
    "\n",
    "tensors = [\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train,\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val,\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test,\n",
    "    player_ratings_tensor, opening_eco_letter_tensor, opening_eco_number_tensor\n",
    "]\n",
    "total_memory_mb = sum(tensor_memory_mb(t) for t in tensors)\n",
    "print(f\"   ‚Ä¢ Total tensor memory: {total_memory_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TENSOR CONVERSION COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fc308",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup\n",
    "\n",
    "Define all constants, loss functions, optimizer, and helper functions for training.\n",
    "\n",
    "**What we need to set up:**\n",
    "\n",
    "1. **Constants and Hyperparameters:**\n",
    "   - `NUM_FACTORS`: Dimensionality of latent factor embeddings (e.g., 50)\n",
    "   - `LEARNING_RATE`: Step size for SGD optimizer (e.g., 0.01)\n",
    "   - `BATCH_SIZE`: Number of samples per training batch (e.g., 1024)\n",
    "   - `N_EPOCHS`: Number of full passes through training data (e.g., 10)\n",
    "   - Random seeds for reproducibility\n",
    "\n",
    "2. **Loss Functions:**\n",
    "   - MSE (Mean Squared Error): Main training loss\n",
    "   - RMSE (Root Mean Squared Error): Evaluation metric\n",
    "   - Confidence-weighted loss: Down-weight uncertain predictions\n",
    "\n",
    "3. **Optimizer:**\n",
    "   - SGD (Stochastic Gradient Descent) with momentum\n",
    "\n",
    "4. **Helper Functions:**\n",
    "   - `train_one_epoch()`: Train for one epoch\n",
    "   - `evaluate_model()`: Evaluate on validation/test set\n",
    "   - `calculate_rmse()`: Compute RMSE metric\n",
    "   - `save_checkpoint()`: Save model state\n",
    "\n",
    "5. **Logging:**\n",
    "   - Progress bars with ETA\n",
    "   - Loss tracking per epoch\n",
    "   - Model checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d13c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\n",
      "============================================================\n",
      "\n",
      "üìã Hyperparameters:\n",
      "   ‚Ä¢ NUM_FACTORS: 50\n",
      "   ‚Ä¢ LEARNING_RATE: 0.01\n",
      "   ‚Ä¢ MOMENTUM: 0.9\n",
      "   ‚Ä¢ BATCH_SIZE: 1024\n",
      "   ‚Ä¢ N_EPOCHS: 20\n",
      "   ‚Ä¢ WEIGHT_DECAY: 0.0\n",
      "\n",
      "üé≤ Random Seed:\n",
      "   ‚Ä¢ RANDOM_SEED: 42\n",
      "\n",
      "üíª Device Configuration:\n",
      "   ‚Ä¢ DEVICE: cpu\n",
      "\n",
      "üíæ Model Saving:\n",
      "   ‚Ä¢ Save directory: /Users/a/Documents/personalprojects/chess-opening-recommender/data/models\n",
      "   ‚úì Python random seed set to 42\n",
      "   ‚úì NumPy random seed set to 42\n",
      "   ‚úì PyTorch CPU random seed set to 42\n",
      "\n",
      "üìä Vocabulary Sizes (for embedding layers):\n",
      "   ‚Ä¢ NUM_PLAYERS: 48,469 (max player_id + 1)\n",
      "   ‚Ä¢ NUM_OPENINGS: 2,716 (max opening_id + 1)\n",
      "   ‚Ä¢ NUM_ECO_LETTERS: 5 (ECO letter categories)\n",
      "   ‚Ä¢ NUM_ECO_NUMBERS: 100 (ECO number categories)\n",
      "\n",
      "üìà Dataset Statistics:\n",
      "   ‚Ä¢ Train samples: 2,172,777\n",
      "   ‚Ä¢ Validation samples: 434,556\n",
      "   ‚Ä¢ Test samples: 289,704\n",
      "   ‚Ä¢ Total samples: 2,897,037\n",
      "\n",
      "   ‚Ä¢ Batches per epoch: 2,121\n",
      "   ‚Ä¢ Training iterations (total): 42,420\n",
      "\n",
      "============================================================\n",
      "‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\n"
     ]
    }
   ],
   "source": [
    "# Step 6a: Define Hyperparameters and Constants\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HYPERPARAMETERS AND CONSTANTS CONFIG\n",
    "# ========================================\n",
    "\n",
    "NUM_FACTORS = 50  # Dimensionality of latent factor embeddings\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9  # SGD momentum\n",
    "BATCH_SIZE = 1024 \n",
    "N_EPOCHS = 20\n",
    "\n",
    "# Regularization (if needed)\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_SAVE_DIR = Path.cwd().parent / \"data\" / \"models\"  # Saves to projectroot/data/models\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"\\nüìã Hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ NUM_FACTORS: {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ LEARNING_RATE: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ MOMENTUM: {MOMENTUM}\")\n",
    "print(f\"   ‚Ä¢ BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ N_EPOCHS: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ WEIGHT_DECAY: {WEIGHT_DECAY}\")\n",
    "\n",
    "print(f\"\\nüé≤ Random Seed:\")\n",
    "print(f\"   ‚Ä¢ RANDOM_SEED: {RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\nüíª Device Configuration:\")\n",
    "print(f\"   ‚Ä¢ DEVICE: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ‚Ä¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüíæ Model Saving:\")\n",
    "print(f\"   ‚Ä¢ Save directory: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# ========================================\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# ========================================\n",
    "\n",
    "# Python random\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì Python random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì NumPy random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# PyTorch random\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì PyTorch CPU random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU\n",
    "    print(f\"   ‚úì PyTorch GPU random seed set to {RANDOM_SEED}\")\n",
    "    \n",
    "    # Additional CUDA reproducibility settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"   ‚úì CUDA deterministic mode enabled\")\n",
    "\n",
    "# ========================================\n",
    "# CALCULATE VOCABULARY SIZES\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Sizes (for embedding layers):\")\n",
    "\n",
    "# Calculate from our tensors\n",
    "NUM_PLAYERS = int(player_ids_in_side_info.max()) + 1\n",
    "NUM_OPENINGS = int(opening_ids_in_side_info.max()) + 1\n",
    "NUM_ECO_LETTERS = int(opening_eco_letter_tensor.max()) + 1\n",
    "NUM_ECO_NUMBERS = int(opening_eco_number_tensor.max()) + 1\n",
    "\n",
    "print(f\"   ‚Ä¢ NUM_PLAYERS: {NUM_PLAYERS:,} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_OPENINGS: {NUM_OPENINGS:,} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_LETTERS: {NUM_ECO_LETTERS} (ECO letter categories)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_NUMBERS: {NUM_ECO_NUMBERS} (ECO number categories)\")\n",
    "\n",
    "# ========================================\n",
    "# DATASET STATISTICS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Train samples: {len(scores_train):,}\")\n",
    "print(f\"   ‚Ä¢ Validation samples: {len(scores_val):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(scores_test):,}\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(scores_train) + len(scores_val) + len(scores_test):,}\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Batches per epoch: {len(scores_train) // BATCH_SIZE:,}\")\n",
    "print(f\"   ‚Ä¢ Training iterations (total): {(len(scores_train) // BATCH_SIZE) * N_EPOCHS:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a93e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6B: DEFINE LOSS FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "üß™ Testing loss functions with dummy data...\n",
      "\n",
      "   Dummy data:\n",
      "   ‚Ä¢ Predictions: [0.5, 0.6000000238418579, 0.699999988079071, 0.800000011920929]\n",
      "   ‚Ä¢ Targets: [0.550000011920929, 0.6200000047683716, 0.6800000071525574, 0.75]\n",
      "   ‚Ä¢ Confidence: [0.800000011920929, 0.8999999761581421, 0.699999988079071, 1.0]\n",
      "\n",
      "   MSE (unweighted): 0.001450\n",
      "   MSE (weighted): 0.001512\n",
      "\n",
      "   RMSE (unweighted): 0.038079\n",
      "   RMSE (weighted): 0.038881\n",
      "\n",
      "   calculate_rmse() result: 0.038881\n",
      "   ‚úì Returns Python float (not tensor): <class 'float'>\n",
      "\n",
      "============================================================\n",
      "‚úÖ LOSS FUNCTIONS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6b: Define Loss Functions\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "print(\"STEP 6B: DEFINE LOSS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "def mse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss with optional confidence weighting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        Higher confidence = larger loss contribution\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Compute squared error\n",
    "    squared_error = (predictions - targets) ** 2\n",
    "    \n",
    "    # Apply confidence weighting if provided\n",
    "    if confidence_weights is not None:\n",
    "        weighted_squared_error = squared_error * confidence_weights\n",
    "        # Average over all samples (sum of weighted errors / sum of weights)\n",
    "        loss = weighted_squared_error.sum() / confidence_weights.sum()\n",
    "    else:\n",
    "        # Standard MSE (average over all samples)\n",
    "        loss = squared_error.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def rmse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error loss.\n",
    "    \n",
    "    This is the square root of MSE and is our primary evaluation metric.\n",
    "    RMSE is in the same units as the target (score), making it interpretable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar RMSE value\n",
    "    \"\"\"\n",
    "    mse = mse_loss(predictions, targets, confidence_weights)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def calculate_rmse(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Calculate RMSE metric (convenience function for evaluation).\n",
    "    \n",
    "    This is the same as rmse_loss but with clearer naming for evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        RMSE value (Python float, not tensor)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Don't compute gradients for evaluation\n",
    "        rmse = rmse_loss(predictions, targets, confidence_weights)\n",
    "    return rmse.item()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüß™ Testing loss functions with dummy data...\")\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_predictions = torch.tensor([0.5, 0.6, 0.7, 0.8], dtype=torch.float32)\n",
    "dummy_targets = torch.tensor([0.55, 0.62, 0.68, 0.75], dtype=torch.float32)\n",
    "dummy_confidence = torch.tensor([0.8, 0.9, 0.7, 1.0], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Dummy data:\")\n",
    "print(f\"   ‚Ä¢ Predictions: {dummy_predictions.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Targets: {dummy_targets.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Confidence: {dummy_confidence.tolist()}\")\n",
    "\n",
    "# Test MSE without confidence weighting\n",
    "mse_unweighted = mse_loss(dummy_predictions, dummy_targets)\n",
    "print(f\"\\n   MSE (unweighted): {mse_unweighted.item():.6f}\")\n",
    "\n",
    "# Test MSE with confidence weighting\n",
    "mse_weighted = mse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"   MSE (weighted): {mse_weighted.item():.6f}\")\n",
    "\n",
    "# Test RMSE\n",
    "rmse_unweighted = rmse_loss(dummy_predictions, dummy_targets)\n",
    "rmse_weighted = rmse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   RMSE (unweighted): {rmse_unweighted.item():.6f}\")\n",
    "print(f\"   RMSE (weighted): {rmse_weighted.item():.6f}\")\n",
    "\n",
    "# Test calculate_rmse\n",
    "rmse_eval = calculate_rmse(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   calculate_rmse() result: {rmse_eval:.6f}\")\n",
    "print(f\"   ‚úì Returns Python float (not tensor): {type(rmse_eval)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LOSS FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e00b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Creating PyTorch Datasets...\n",
      "   ‚úì Train dataset: 2,172,777 samples\n",
      "   ‚úì Validation dataset: 434,556 samples\n",
      "   ‚úì Test dataset: 289,704 samples\n",
      "\n",
      "2Ô∏è‚É£  Testing dataset access...\n",
      "   Sample from train_dataset[0]:\n",
      "   ‚Ä¢ player_id: 30917\n",
      "   ‚Ä¢ opening_id: 1231\n",
      "   ‚Ä¢ confidence: 0.8938\n",
      "   ‚Ä¢ score: 0.5057\n",
      "\n",
      "3Ô∏è‚É£  Creating PyTorch DataLoaders...\n"
     ]
    }
   ],
   "source": [
    "# Step 6c: Create PyTorch Dataset and DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# CUSTOM DATASET CLASS\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for chess player-opening interactions.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - player_id: Integer ID for embedding lookup\n",
    "    - opening_id: Integer ID for embedding lookup\n",
    "    - confidence: Confidence weight for loss function\n",
    "    - score: Target value (win rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, player_ids, opening_ids, confidence, scores):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (long tensor)\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (long tensor)\n",
    "        confidence : torch.Tensor\n",
    "            Confidence weights (float tensor)\n",
    "        scores : torch.Tensor\n",
    "            Target scores (float tensor)\n",
    "        \"\"\"\n",
    "        self.player_ids = player_ids\n",
    "        self.opening_ids = opening_ids\n",
    "        self.confidence = confidence\n",
    "        self.scores = scores\n",
    "        \n",
    "        # Validate shapes\n",
    "        assert len(player_ids) == len(opening_ids) == len(confidence) == len(scores), \\\n",
    "            \"All tensors must have the same length\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.player_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with keys: 'player_id', 'opening_id', 'confidence', 'score'\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'player_id': self.player_ids[idx],\n",
    "            'opening_id': self.opening_ids[idx],\n",
    "            'confidence': self.confidence[idx],\n",
    "            'score': self.scores[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATASETS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Creating PyTorch Datasets...\")\n",
    "\n",
    "train_dataset = ChessOpeningDataset(\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train\n",
    ")\n",
    "val_dataset = ChessOpeningDataset(\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val\n",
    ")\n",
    "test_dataset = ChessOpeningDataset(\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"   ‚úì Validation dataset: {len(val_dataset):,} samples\")\n",
    "print(f\"   ‚úì Test dataset: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Test dataset access\n",
    "print(f\"\\n2Ô∏è‚É£  Testing dataset access...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"   Sample from train_dataset[0]:\")\n",
    "print(f\"   ‚Ä¢ player_id: {sample['player_id']}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {sample['opening_id']}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "print(f\"   ‚Ä¢ score: {sample['score']:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATALOADERS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Creating PyTorch DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle training data each epoch\n",
    "    num_workers=0,  # Number of subprocesses for data loading (0 = main process)\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle test data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fbb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6D: DEFINE HELPER FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "   format_time() tests:\n",
      "   ‚Ä¢ 30 seconds ‚Üí 30.0s\n",
      "   ‚Ä¢ 90 seconds ‚Üí 1m 30s\n",
      "   ‚Ä¢ 300 seconds ‚Üí 5m 0s\n",
      "   ‚Ä¢ 3661 seconds ‚Üí 1h 1m 1s\n",
      "   ‚Ä¢ 7384 seconds ‚Üí 2h 3m 4s\n",
      "\n",
      "   print_progress() test (simulating batch progress):\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6d: Define Helper Functions for Training\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"STEP 6D: DEFINE HELPER FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Save Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to save\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer state to save\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    train_loss : float\n",
    "        Training loss for this epoch\n",
    "    val_loss : float\n",
    "        Validation loss for this epoch\n",
    "    filepath : Path or str\n",
    "        Where to save the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        \n",
    "        # Save hyperparameters for reproducibility\n",
    "        'hyperparameters': {\n",
    "            'num_factors': NUM_FACTORS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'momentum': MOMENTUM,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'num_players': NUM_PLAYERS,\n",
    "            'num_openings': NUM_OPENINGS,\n",
    "            'num_eco_letters': NUM_ECO_LETTERS,\n",
    "            'num_eco_numbers': NUM_ECO_NUMBERS,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"   Checkpoint saved to: {filepath}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Load Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to load weights into\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to load state into\n",
    "    filepath : Path or str\n",
    "        Where to load the checkpoint from\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        The epoch number from the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"    Checkpoint loaded from: {filepath}\")\n",
    "    print(f\"   ‚Ä¢ Epoch: {epoch}\")\n",
    "    print(f\"   ‚Ä¢ Train loss: {checkpoint['train_loss']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Val loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    return epoch\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Format Time\n",
    "# ========================================\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"\n",
    "    Format seconds as human-readable time string.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seconds : float\n",
    "        Number of seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted time string (e.g., \"1h 23m 45s\")\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Print Progress\n",
    "# ========================================\n",
    "\n",
    "def print_progress(epoch, batch_idx, total_batches, loss, elapsed_time):\n",
    "    \"\"\"\n",
    "    Print training progress with ETA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    batch_idx : int\n",
    "        Current batch index (0-based)\n",
    "    total_batches : int\n",
    "        Total number of batches per epoch\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    elapsed_time : float\n",
    "        Elapsed time in seconds since epoch start\n",
    "    \"\"\"\n",
    "    # Calculate progress\n",
    "    progress_pct = 100.0 * (batch_idx + 1) / total_batches\n",
    "    \n",
    "    # Estimate time remaining\n",
    "    time_per_batch = elapsed_time / (batch_idx + 1)\n",
    "    remaining_batches = total_batches - (batch_idx + 1)\n",
    "    eta_seconds = time_per_batch * remaining_batches\n",
    "    \n",
    "    # Print progress bar\n",
    "    bar_length = 40\n",
    "    filled_length = int(bar_length * (batch_idx + 1) / total_batches)\n",
    "    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)\n",
    "    \n",
    "    print(f\"   Epoch {epoch} [{bar}] {progress_pct:>6.2f}% | \"\n",
    "          f\"Loss: {loss:.6f} | \"\n",
    "          f\"ETA: {format_time(eta_seconds)}\", end='\\r')\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST HELPER FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "# Test format_time\n",
    "print(f\"\\n   format_time() tests:\")\n",
    "test_times = [30, 90, 300, 3661, 7384]\n",
    "for t in test_times:\n",
    "    print(f\"   ‚Ä¢ {t} seconds ‚Üí {format_time(t)}\")\n",
    "\n",
    "# Test print_progress (will overwrite line)\n",
    "print(f\"\\n   print_progress() test (simulating batch progress):\")\n",
    "for i in range(0, 101, 20):\n",
    "    total = 100\n",
    "    loss = 0.05 - i * 0.0003\n",
    "    elapsed = i * 0.5\n",
    "    print_progress(epoch=1, batch_idx=i, total_batches=total, loss=loss, elapsed_time=elapsed)\n",
    "    time.sleep(0.1)  # Brief pause to show animation\n",
    "print()  # New line after progress bar\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HELPER FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\n",
      "============================================================\n",
      " TRAINING AND EVALUATION FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìã Training workflow:\n",
      "   1. Initialize model and optimizer\n",
      "   2. For each epoch:\n",
      "      a. Call train_one_epoch()\n",
      "      b. Call evaluate_model() on validation set\n",
      "      c. Save checkpoint with save_checkpoint()\n",
      "      d. Log metrics\n",
      "   3. After training, evaluate on test set\n"
     ]
    }
   ],
   "source": [
    "# Step 6e: Define Training and Evaluation Functions\n",
    "\n",
    "print(\"STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# TRAINING FUNCTION: Train One Epoch\n",
    "# ========================================\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, device, epoch_num):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to train\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for updating model parameters\n",
    "    device : torch.device\n",
    "        Device to train on (CPU or CUDA)\n",
    "    epoch_num : int\n",
    "        Current epoch number (for logging)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (average_loss, elapsed_time)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move batch to device\n",
    "        player_ids = batch['player_id'].to(device)\n",
    "        opening_ids = batch['opening_id'].to(device)\n",
    "        confidence = batch['confidence'].to(device)\n",
    "        targets = batch['score'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(player_ids, opening_ids)\n",
    "        \n",
    "        # Compute loss with confidence weighting\n",
    "        loss = mse_loss(predictions, targets, confidence)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 10 batches or on last batch\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print_progress(epoch_num, batch_idx, num_batches, loss.item(), elapsed)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    elapsed_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Clear progress line and print summary\n",
    "    print()  # New line after progress bar\n",
    "    \n",
    "    return avg_loss, elapsed_time\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# EVALUATION FUNCTION: Evaluate Model\n",
    "# ========================================\n",
    "\n",
    "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to evaluate\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for evaluation data\n",
    "    device : torch.device\n",
    "        Device to evaluate on (CPU or CUDA)\n",
    "    dataset_name : str\n",
    "        Name of dataset for logging (e.g., \"Validation\", \"Test\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (mse, rmse)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_mse = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in data_loader:\n",
    "            # Move batch to device\n",
    "            player_ids = batch['player_id'].to(device)\n",
    "            opening_ids = batch['opening_id'].to(device)\n",
    "            confidence = batch['confidence'].to(device)\n",
    "            targets = batch['score'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(player_ids, opening_ids)\n",
    "            \n",
    "            # Compute MSE with confidence weighting\n",
    "            batch_mse = mse_loss(predictions, targets, confidence)\n",
    "            \n",
    "            # Track weighted sum of MSE\n",
    "            total_mse += batch_mse.item() * len(targets)\n",
    "            total_samples += len(targets)\n",
    "    \n",
    "    # Calculate average MSE and RMSE\n",
    "    avg_mse = total_mse / total_samples\n",
    "    avg_rmse = avg_mse ** 0.5\n",
    "    \n",
    "    return avg_mse, avg_rmse\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST FUNCTIONS (with dummy model)\n",
    "# ========================================\n",
    "\n",
    "print(\" TRAINING AND EVALUATION FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã Training workflow:\")\n",
    "print(f\"   1. Initialize model and optimizer\")\n",
    "print(f\"   2. For each epoch:\")\n",
    "print(f\"      a. Call train_one_epoch()\")\n",
    "print(f\"      b. Call evaluate_model() on validation set\")\n",
    "print(f\"      c. Save checkpoint with save_checkpoint()\")\n",
    "print(f\"      d. Log metrics\")\n",
    "print(f\"   3. After training, evaluate on test set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6F: DEFINE MODEL ARCHITECTURE\n",
      "============================================================\n",
      "\n",
      "‚úÖ ChessOpeningRecommender model class defined\n",
      "\n",
      "üìä Model Architecture:\n",
      "   Player Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(48469, 50)\n",
      "      ‚Ä¢ Biases: Embedding(48469, 1)\n",
      "      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\n",
      "      ‚Ä¢ Combiner: Linear(51, 50)\n",
      "\n",
      "   Opening Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(2716, 50)\n",
      "      ‚Ä¢ Biases: Embedding(2716, 1)\n",
      "      ‚Ä¢ ECO letter embedding: Embedding(5, 4)\n",
      "      ‚Ä¢ ECO number embedding: Embedding(100, 4)\n",
      "      ‚Ä¢ Combiner: Linear(58, 50)\n",
      "\n",
      "   Prediction:\n",
      "      ‚Ä¢ Dot product of player and opening representations\n",
      "      ‚Ä¢ + player bias + opening bias + global bias\n",
      "      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\n",
      "\n",
      " Model Statistics:\n",
      "   ‚Ä¢ Total trainable parameters: 2,616,406\n",
      "   ‚Ä¢ Player parameters: 2,474,519\n",
      "   ‚Ä¢ Opening parameters: 141,886\n",
      "   ‚Ä¢ Global parameters: 1\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6f: Define Model Architecture\n",
    "\n",
    "print(\"STEP 6F: DEFINE MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Chess Opening Recommender Model\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningRecommender(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for chess opening recommendations.\n",
    "    \n",
    "    The model learns latent factors for players and openings, incorporating\n",
    "    side information:\n",
    "    - Player ratings (normalized)\n",
    "    - Opening ECO codes (letter and number as categorical features)\n",
    "    \n",
    "    Architecture:\n",
    "    - Player embedding: learnable latent factors\n",
    "    - Opening embedding: learnable latent factors\n",
    "    - Player rating: fixed side information (no embedding)\n",
    "    - ECO letter/number: categorical embeddings\n",
    "    \n",
    "    Prediction: dot product of player and opening representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_players, num_openings, num_factors,\n",
    "                 player_ratings, opening_eco_letters, opening_eco_numbers,\n",
    "                 num_eco_letters, num_eco_numbers, eco_embed_dim=4):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_players : int\n",
    "            Total number of unique players\n",
    "        num_openings : int\n",
    "            Total number of unique openings\n",
    "        num_factors : int\n",
    "            Dimensionality of latent factor embeddings\n",
    "        player_ratings : torch.Tensor\n",
    "            Z-score normalized ratings for all players (shape: [num_players])\n",
    "        opening_eco_letters : torch.Tensor\n",
    "            ECO letter categories for all openings (shape: [num_openings])\n",
    "        opening_eco_numbers : torch.Tensor\n",
    "            ECO number categories for all openings (shape: [num_openings])\n",
    "        num_eco_letters : int\n",
    "            Number of unique ECO letter categories\n",
    "        num_eco_numbers : int\n",
    "            Number of unique ECO number categories\n",
    "        eco_embed_dim : int\n",
    "            Dimensionality of ECO categorical embeddings (default: 4)\n",
    "        \"\"\"\n",
    "        super(ChessOpeningRecommender, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_players = num_players\n",
    "        self.num_openings = num_openings\n",
    "        self.num_factors = num_factors\n",
    "        self.eco_embed_dim = eco_embed_dim\n",
    "        \n",
    "        # ========================================\n",
    "        # Player Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.player_factors = torch.nn.Embedding(num_players, num_factors)\n",
    "        \n",
    "        self.player_biases = torch.nn.Embedding(num_players, 1)\n",
    "        \n",
    "        # Player ratings (fixed side information - registered as buffer, not parameter)\n",
    "        self.register_buffer('player_ratings', player_ratings)\n",
    "        \n",
    "        # ========================================\n",
    "        # Opening Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.opening_factors = torch.nn.Embedding(num_openings, num_factors)\n",
    "        \n",
    "        self.opening_biases = torch.nn.Embedding(num_openings, 1)\n",
    "        \n",
    "        # ECO letter and number (fixed side information - registered as buffers)\n",
    "        self.register_buffer('opening_eco_letters', opening_eco_letters)\n",
    "        self.register_buffer('opening_eco_numbers', opening_eco_numbers)\n",
    "        \n",
    "        # ECO embeddings (learnable)\n",
    "        self.eco_letter_embedding = torch.nn.Embedding(num_eco_letters, eco_embed_dim)\n",
    "        self.eco_number_embedding = torch.nn.Embedding(num_eco_numbers, eco_embed_dim)\n",
    "        \n",
    "        # ========================================\n",
    "        # Combination Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Combine player latent factors with rating\n",
    "        # Input: [num_factors + 1] ‚Üí Output: [num_factors]\n",
    "        self.player_combiner = torch.nn.Linear(num_factors + 1, num_factors)\n",
    "        \n",
    "        # Combine opening latent factors with ECO embeddings\n",
    "        # Input: [num_factors + 2*eco_embed_dim] ‚Üí Output: [num_factors]\n",
    "        self.opening_combiner = torch.nn.Linear(num_factors + 2 * eco_embed_dim, num_factors)\n",
    "        \n",
    "        # ========================================\n",
    "        # Global Bias\n",
    "        # ========================================\n",
    "        \n",
    "        # Global bias term (learnable scalar)\n",
    "        self.global_bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # ========================================\n",
    "        # Initialize Weights\n",
    "        # ========================================\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        torch.nn.init.normal_(self.player_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.opening_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_letter_embedding.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_number_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        torch.nn.init.zeros_(self.player_biases.weight)\n",
    "        torch.nn.init.zeros_(self.opening_biases.weight)\n",
    "        \n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        torch.nn.init.xavier_uniform_(self.player_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.player_combiner.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.opening_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.opening_combiner.bias)\n",
    "    \n",
    "    def forward(self, player_ids, opening_ids):\n",
    "        \"\"\"\n",
    "        Forward pass: predict player-opening scores.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (shape: [batch_size])\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (shape: [batch_size])\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Predicted scores (shape: [batch_size])\n",
    "        \"\"\"\n",
    "        # ========================================\n",
    "        # Get Player Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get player latent factors [batch_size, num_factors]\n",
    "        player_embed = self.player_factors(player_ids)\n",
    "        \n",
    "        # Get player ratings [batch_size, 1]\n",
    "        player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "        \n",
    "        # Concatenate player factors with rating [batch_size, num_factors + 1]\n",
    "        player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "        \n",
    "        # Combine into final player representation [batch_size, num_factors]\n",
    "        player_repr = self.player_combiner(player_concat)\n",
    "        \n",
    "        # Get player bias [batch_size]\n",
    "        player_bias = self.player_biases(player_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Get Opening Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get opening latent factors [batch_size, num_factors]\n",
    "        opening_embed = self.opening_factors(opening_ids)\n",
    "        \n",
    "        # Get ECO embeddings\n",
    "        eco_letters = self.opening_eco_letters[opening_ids]  # [batch_size]\n",
    "        eco_numbers = self.opening_eco_numbers[opening_ids]  # [batch_size]\n",
    "        \n",
    "        eco_letter_embed = self.eco_letter_embedding(eco_letters)  # [batch_size, eco_embed_dim]\n",
    "        eco_number_embed = self.eco_number_embedding(eco_numbers)  # [batch_size, eco_embed_dim]\n",
    "        \n",
    "        # Concatenate opening factors with ECO embeddings\n",
    "        # [batch_size, num_factors + 2*eco_embed_dim]\n",
    "        opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "        \n",
    "        # Combine into final opening representation [batch_size, num_factors]\n",
    "        opening_repr = self.opening_combiner(opening_concat)\n",
    "        \n",
    "        # Get opening bias [batch_size]\n",
    "        opening_bias = self.opening_biases(opening_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Compute Prediction\n",
    "        # ========================================\n",
    "        \n",
    "        # Dot product of player and opening representations [batch_size]\n",
    "        interaction = (player_repr * opening_repr).sum(dim=1)\n",
    "        \n",
    "        # Add biases and global bias\n",
    "        prediction = interaction + player_bias + opening_bias + self.global_bias\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] range (since scores are win rates)\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def get_player_embedding(self, player_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific player.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_id : int\n",
    "            Player ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Player representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            player_ids = torch.tensor([player_id], device=self.player_factors.weight.device)\n",
    "            player_embed = self.player_factors(player_ids)\n",
    "            player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "            player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "            player_repr = self.player_combiner(player_concat)\n",
    "            return player_repr.squeeze()\n",
    "    \n",
    "    def get_opening_embedding(self, opening_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific opening.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        opening_id : int\n",
    "            Opening ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Opening representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            opening_ids = torch.tensor([opening_id], device=self.opening_factors.weight.device)\n",
    "            opening_embed = self.opening_factors(opening_ids)\n",
    "            \n",
    "            eco_letters = self.opening_eco_letters[opening_ids]\n",
    "            eco_numbers = self.opening_eco_numbers[opening_ids]\n",
    "            \n",
    "            eco_letter_embed = self.eco_letter_embedding(eco_letters)\n",
    "            eco_number_embed = self.eco_number_embedding(eco_numbers)\n",
    "            \n",
    "            opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "            opening_repr = self.opening_combiner(opening_concat)\n",
    "            return opening_repr.squeeze()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model Summary\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ ChessOpeningRecommender model class defined\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Player Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_PLAYERS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_PLAYERS}, 1)\")\n",
    "print(f\"      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 1}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Opening Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_OPENINGS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_OPENINGS}, 1)\")\n",
    "print(f\"      ‚Ä¢ ECO letter embedding: Embedding({NUM_ECO_LETTERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ ECO number embedding: Embedding({NUM_ECO_NUMBERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 8}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Prediction:\")\n",
    "print(f\"      ‚Ä¢ Dot product of player and opening representations\")\n",
    "print(f\"      ‚Ä¢ + player bias + opening bias + global bias\")\n",
    "print(f\"      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\")\n",
    "\n",
    "# Calculate total parameters\n",
    "player_factor_params = NUM_PLAYERS * NUM_FACTORS\n",
    "player_bias_params = NUM_PLAYERS\n",
    "player_combiner_params = (NUM_FACTORS + 1) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "opening_factor_params = NUM_OPENINGS * NUM_FACTORS\n",
    "opening_bias_params = NUM_OPENINGS\n",
    "eco_letter_params = NUM_ECO_LETTERS * 4\n",
    "eco_number_params = NUM_ECO_NUMBERS * 4\n",
    "opening_combiner_params = (NUM_FACTORS + 8) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "global_bias_params = 1\n",
    "\n",
    "total_params = (player_factor_params + player_bias_params + player_combiner_params +\n",
    "                opening_factor_params + opening_bias_params + \n",
    "                eco_letter_params + eco_number_params + opening_combiner_params +\n",
    "                global_bias_params)\n",
    "\n",
    "print(f\"\\n Model Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total trainable parameters: {total_params:,}\")\n",
    "print(f\"   ‚Ä¢ Player parameters: {player_factor_params + player_bias_params + player_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Opening parameters: {opening_factor_params + opening_bias_params + eco_letter_params + eco_number_params + opening_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Global parameters: {global_bias_params}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e08708d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6.1: CV CONFIGURATION & DATA SUBSETTING\n",
      "============================================================\n",
      "Using a subset of 7,500 players for 5-fold CV.\n",
      "Total interactions in subset: 449,732\n"
     ]
    }
   ],
   "source": [
    "# 6.1 CROSS VALIDATION CONFIGURATION & DATA SUBSETTING\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"STEP 6.1: CV CONFIGURATION & DATA SUBSETTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# 1. Configuration\n",
    "# ========================================\n",
    "NUM_CV_PLAYERS = 7500\n",
    "K_FOLDS = 5\n",
    "CV_EPOCHS = 15 # Number of epochs to train for each fold/config\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"num_factors\": [5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2],\n",
    "    \"batch_size\": [512, 1024, 2048]\n",
    "}\n",
    "\n",
    "# Use a smaller subset of players for faster CV\n",
    "np.random.seed(42)\n",
    "# Get unique player IDs from the full cleaned dataset\n",
    "all_player_ids = clean_data[\"player_id\"].unique()\n",
    "cv_player_ids = np.random.choice(\n",
    "    all_player_ids, size=min(NUM_CV_PLAYERS, len(all_player_ids)), replace=False\n",
    ")\n",
    "\n",
    "# Create a dataframe containing only the interactions from the selected players\n",
    "cv_data = clean_data[clean_data[\"player_id\"].isin(cv_player_ids)].copy()\n",
    "\n",
    "print(\n",
    "    f\"Using a subset of {cv_data['player_id'].nunique():,} players for {K_FOLDS}-fold CV.\"\n",
    ")\n",
    "print(f\"Total interactions in subset: {len(cv_data):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 K-FOLD CROSS-VALIDATION LOOP WITH RANDOM SEARCH\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "print(\"STEP 6.2: K-FOLD CROSS-VALIDATION LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Random Search Configuration\n",
    "# ========================================\n",
    "N_RANDOM_SAMPLES = 20  # Number of random hyperparameter combinations to try\n",
    "random_state = 42\n",
    "\n",
    "# Use sklearn's ParameterSampler for random search\n",
    "param_sampler = ParameterSampler(\n",
    "    param_grid, n_iter=N_RANDOM_SAMPLES, random_state=random_state\n",
    ")\n",
    "\n",
    "print(f\"Using Random Search with {N_RANDOM_SAMPLES} hyperparameter combinations\")\n",
    "print(f\"Total CV runs: {N_RANDOM_SAMPLES * K_FOLDS}\")\n",
    "print()\n",
    "\n",
    "# ========================================\n",
    "# K-Fold Cross-Validation Loop\n",
    "# ========================================\n",
    "\n",
    "# Store results\n",
    "cv_results = []\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Loop through random hyperparameter combinations\n",
    "for config_idx, params in enumerate(param_sampler):\n",
    "    num_factors = params['num_factors']\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    config_str = f\"factors={num_factors}, lr={learning_rate:.5f}, batch={batch_size}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CONFIG {config_idx+1}/{N_RANDOM_SAMPLES}: {config_str}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    fold_metrics = {'rmse': [], 'mae': [], 'time': []}\n",
    "    \n",
    "    # K-Fold CV for this configuration\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(cv_data)):\n",
    "        fold_start_time = time.time()\n",
    "        print(f\"  Fold {fold_idx+1}/{K_FOLDS}...\", end=\" \", flush=True)\n",
    "\n",
    "        # --- Data preparation for this fold ---\n",
    "        train_fold_df = cv_data.iloc[train_idx]\n",
    "        val_fold_df = cv_data.iloc[val_idx]\n",
    "\n",
    "        # --- Convert to Tensors for this fold ---\n",
    "        train_dataset_f = ChessOpeningDataset(\n",
    "            torch.tensor(train_fold_df['player_id'].values, dtype=torch.long),\n",
    "            torch.tensor(train_fold_df['opening_id'].values, dtype=torch.long),\n",
    "            torch.tensor(train_fold_df['confidence'].values, dtype=torch.float32),\n",
    "            torch.tensor(train_fold_df['score'].values, dtype=torch.float32)\n",
    "        )\n",
    "        val_dataset_f = ChessOpeningDataset(\n",
    "            torch.tensor(val_fold_df['player_id'].values, dtype=torch.long),\n",
    "            torch.tensor(val_fold_df['opening_id'].values, dtype=torch.long),\n",
    "            torch.tensor(val_fold_df['confidence'].values, dtype=torch.float32),\n",
    "            torch.tensor(val_fold_df['score'].values, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        train_loader_f = DataLoader(train_dataset_f, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_f = DataLoader(val_dataset_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # --- Model and Optimizer for this fold ---\n",
    "        model_f = ChessOpeningRecommender(\n",
    "            num_players=NUM_PLAYERS, \n",
    "            num_openings=NUM_OPENINGS, \n",
    "            num_factors=num_factors,\n",
    "            player_ratings=player_ratings_tensor, \n",
    "            opening_eco_letters=opening_eco_letter_tensor,\n",
    "            opening_eco_numbers=opening_eco_number_tensor, \n",
    "            num_eco_letters=NUM_ECO_LETTERS,\n",
    "            num_eco_numbers=NUM_ECO_NUMBERS\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        optimizer_f = torch.optim.SGD(model_f.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "        # --- Training loop for this fold (silent mode) ---\n",
    "        model_f.train()\n",
    "        for epoch in range(1, CV_EPOCHS + 1):\n",
    "            for batch in train_loader_f:\n",
    "                optimizer_f.zero_grad()\n",
    "                predictions = model_f(\n",
    "                    batch['player_id'].to(DEVICE), \n",
    "                    batch['opening_id'].to(DEVICE)\n",
    "                )\n",
    "                loss = mse_loss(predictions, batch['score'].to(DEVICE), batch['confidence'].to(DEVICE))\n",
    "                loss.backward()\n",
    "                optimizer_f.step()\n",
    "        \n",
    "        # --- Evaluation for this fold ---\n",
    "        val_mse, val_rmse = evaluate_model(model_f, val_loader_f, DEVICE)\n",
    "        \n",
    "        # Calculate MAE\n",
    "        model_f.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_f:\n",
    "                preds = model_f(batch['player_id'].to(DEVICE), batch['opening_id'].to(DEVICE))\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_targets.append(batch['score'].cpu())\n",
    "        val_mae = torch.nn.functional.l1_loss(torch.cat(all_preds), torch.cat(all_targets)).item()\n",
    "\n",
    "        fold_time = time.time() - fold_start_time\n",
    "        fold_metrics['rmse'].append(val_rmse)\n",
    "        fold_metrics['mae'].append(val_mae)\n",
    "        fold_metrics['time'].append(fold_time)\n",
    "        \n",
    "        print(f\"RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, Time: {fold_time:.1f}s\")\n",
    "\n",
    "    # --- Aggregate metrics for this hyperparameter config ---\n",
    "    avg_rmse = np.mean(fold_metrics['rmse'])\n",
    "    std_rmse = np.std(fold_metrics['rmse'])\n",
    "    avg_mae = np.mean(fold_metrics['mae'])\n",
    "    std_mae = np.std(fold_metrics['mae'])\n",
    "    avg_time = np.mean(fold_metrics['time'])\n",
    "    \n",
    "    cv_results.append({\n",
    "        'num_factors': num_factors,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'mean_rmse': avg_rmse,\n",
    "        'std_rmse': std_rmse,\n",
    "        'mean_mae': avg_mae,\n",
    "        'std_mae': std_mae,\n",
    "        'mean_time_per_fold': avg_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úì Config {config_idx+1} complete: Avg RMSE = {avg_rmse:.4f} ¬± {std_rmse:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ CROSS-VALIDATION LOOP COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03690a7e",
   "metadata": {},
   "source": [
    "## Step 6: Hyperparameter Tuning with K-Fold Cross-Validation\n",
    "\n",
    "Before training the final model, we will perform k-fold cross-validation to find the optimal hyperparameters. This provides a more robust evaluation than a single validation set.\n",
    "\n",
    "**Process:**\n",
    "1.  **Subset Data**: We'll use a smaller, representative sample of players to make the tuning process faster.\n",
    "2.  **K-Fold Split**: The player data will be split into `k` folds. For each fold, we train on `k-1` folds and validate on the remaining one.\n",
    "3.  **Hyperparameter Search**: We will iterate through different combinations of `NUM_FACTORS`, `LEARNING_RATE`, and `BATCH_SIZE`.\n",
    "4.  **Aggregate & Evaluate**: We'll average the performance metrics (like RMSE) across all folds for each hyperparameter combination to determine the best set.\n",
    "5.  **Visualize**: Results will be plotted to visualize the impact of each hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc2c6d",
   "metadata": {},
   "source": [
    "## Step 7: Final Model Training Setup\n",
    "\n",
    "Now we'll set up the training components for the **final model**: the model instance, optimizer, and learning rate scheduler.\n",
    "\n",
    "We'll use the best hyperparameters found during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: TRAINING LOOP\n",
      "============================================================\n",
      "\n",
      "üì¶ Initializing model...\n",
      "‚úÖ Model initialized on cpu\n",
      "   ‚Ä¢ Player embeddings: 48469 √ó 50\n",
      "   ‚Ä¢ Opening embeddings: 2716 √ó 50\n",
      "   ‚Ä¢ Player ratings: 48469 (z-score normalized)\n",
      "   ‚Ä¢ ECO letters: 5 categories\n",
      "   ‚Ä¢ ECO numbers: 100 categories\n",
      "\n",
      "üîß Initializing optimizer...\n",
      "‚úÖ SGD optimizer initialized:\n",
      "   ‚Ä¢ Learning rate: 0.01\n",
      "   ‚Ä¢ Momentum: 0.9\n",
      "   ‚Ä¢ Weight decay: 0.0\n",
      " STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Training configuration:\n",
      "   ‚Ä¢ Epochs: 20\n",
      "   ‚Ä¢ Batch size: 1024\n",
      "   ‚Ä¢ Training samples: 2,172,777\n",
      "   ‚Ä¢ Validation samples: 434,556\n",
      "   ‚Ä¢ Batches per epoch: 2,122\n",
      "TRAINING PROGRESS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/20\n",
      "============================================================\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  26.39% | Loss: 0.001752 | ETA: 37.9ss\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x109189220>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  47.60% | Loss: 0.001627 | ETA: 28.4s\r"
     ]
    }
   ],
   "source": [
    "print(\"STEP 7: FINAL MODEL TRAINING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Hyperparameters for Final Training\n",
    "# ==================================\n",
    "# These would be set by the results from the CV step.\n",
    "# For now, we'll use the initial defaults.\n",
    "NUM_FACTORS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# ==================================\n",
    "# Model Instantiation\n",
    "# ==================================\n",
    "final_model = ChessOpeningRecommender(\n",
    "    num_players=NUM_PLAYERS,\n",
    "    num_openings=NUM_OPENINGS,\n",
    "    num_factors=NUM_FACTORS,\n",
    "    player_ratings=player_ratings_tensor,\n",
    "    opening_eco_letters=opening_eco_letter_tensor,\n",
    "    opening_eco_numbers=opening_eco_number_tensor,\n",
    "    num_eco_letters=NUM_ECO_LETTERS,\n",
    "    num_eco_numbers=NUM_ECO_NUMBERS\n",
    ").to(DEVICE)\n",
    "\n",
    "# ==================================\n",
    "# Optimizer\n",
    "# ==================================\n",
    "# We'll use SGD with momentum, a classic choice for matrix factorization.\n",
    "optimizer = torch.optim.SGD(final_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# ==================================\n",
    "# Learning Rate Scheduler\n",
    "# ==================================\n",
    "# Reduces the learning rate when a metric has stopped improving.\n",
    "# This helps to fine-tune the model in the later stages of training.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',      # The scheduler will step when the quantity monitored has stopped decreasing\n",
    "    factor=0.1,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "    patience=2,      # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=True     # If True, prints a message to stdout for each update\n",
    ")\n",
    "\n",
    "# ==================================\n",
    "# Checkpoint and Model Save Paths\n",
    "# ==================================\n",
    "CHECKPOINT_DIR = \"data/models\"\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.pt\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Final model hyperparameters:\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Num Factors: {NUM_FACTORS}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Num Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"\\nModel, optimizer, and scheduler initialized.\")\n",
    "print(f\"Checkpoints will be saved in: '{CHECKPOINT_DIR}'\")\n",
    "print(f\"Best model will be saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ SETUP COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becba99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0dc479f",
   "metadata": {},
   "source": [
    "## Step 8: Final Model Training Loop\n",
    "\n",
    "Here's the main training loop for the final model. We'll iterate for a specified number of epochs, training the model and evaluating its performance on the validation set periodically. We'll also save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f0315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 8: EVALUATION ON TEST SET\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating model on test set...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Evaluate on Test Set\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Evaluating model on test set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m test_mse, test_rmse = \u001b[43mevaluate_model\u001b[49m(\n\u001b[32m     16\u001b[39m     model=model,\n\u001b[32m     17\u001b[39m     data_loader=test_loader,\n\u001b[32m     18\u001b[39m     device=DEVICE,\n\u001b[32m     19\u001b[39m     dataset_name=\u001b[33m\"\u001b[39m\u001b[33mTest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTEST SET RESULTS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"STEP 8: TRAINING THE FINAL MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Training State Tracking\n",
    "# ==================================\n",
    "history = defaultdict(list)\n",
    "best_val_rmse = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "\n",
    "# ==================================\n",
    "# Main Training Loop\n",
    "# ==================================\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    train_loss, train_rmse = train_one_epoch(final_model, train_loader, optimizer, DEVICE, epoch)\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    val_loss, val_rmse = evaluate_model(final_model, val_loader, DEVICE)\n",
    "    \n",
    "    # --- Learning Rate Scheduler Step ---\n",
    "    scheduler.step(val_rmse)\n",
    "    \n",
    "    # --- Logging ---\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_rmse'].append(train_rmse)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    \n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f} | \"\n",
    "          f\"Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "    # --- Checkpoint Saving ---\n",
    "    # Save a checkpoint every epoch\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch:03d}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_rmse': val_rmse,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Save the best model based on validation RMSE\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        epochs_since_improvement = 0\n",
    "        torch.save(final_model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  -> New best model saved with Val RMSE: {val_rmse:.4f}\")\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "\n",
    "# ==================================\n",
    "# Post-Training Summary\n",
    "# ==================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ FINAL TRAINING COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best Validation RMSE: {best_val_rmse:.4f}\")\n",
    "print(f\"Best model saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(f\"Last checkpoint saved to: '{checkpoint_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68202800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
