{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e163a7",
   "metadata": {},
   "source": [
    "# Notebook 28 ‚Äî Opening Recommender Model: Training Pipeline\n",
    "\n",
    "### 0. Overview and Goals\n",
    "\n",
    "This notebook defines the full pipeline for training the chess opening recommender model.\n",
    "The objective is to predict **player‚Äìopening performance scores** ((wins + (0.5 * draws) / num games)) for openings a player hasn‚Äôt yet played, based on their results in the openings they *have* played.\n",
    "\n",
    "The model will use **matrix factorization** with **stochastic gradient descent (SGD)** to learn latent factors representing player and opening characteristics.\n",
    "All computations will be implemented in **PyTorch**, with data loaded from my local **DuckDB** database.\n",
    "\n",
    "**High-level specs:**\n",
    "- Use only *White* openings initially (we‚Äôll extend to Black later).\n",
    "- Data source: processed player‚Äìopening stats from local DuckDB.\n",
    "- Predict: normalized ‚Äúscore‚Äù = win rate ((wins + 0.5 x draws) / total games).\n",
    "- Filter: only include entries with ‚â• `MIN_GAMES_THRESHOLD` (default = 10).\n",
    "- Ignore: rating differences, time controls, and other metadata for the base model.\n",
    "- Model parameters (to be defined in appropriate places for easy editing):\n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_PLAYERS_TO_PROCESS`\n",
    "- Logging and checkpoints throughout for reproducibility.\n",
    "- All random operations seeded for deterministic runs.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Extraction\n",
    "- Connect to local DuckDB and pull all processed player‚Äìopening statistics.\n",
    "- Verify schema consistency and include row-count sanity checks.\n",
    "- Filter for players with ratings above a minimum threshold (e.g., 1200).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Sanitization & Normalization\n",
    "- Apply confidence weighting using hierarchical Bayesian shrinkage to adjust scores for low-game-count entries.\n",
    "- Normalize player ratings (z-score) for use as side information.\n",
    "- Resequence player and opening IDs to be contiguous integers for embedding layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Splits\n",
    "- Split data into train, validation, and test sets (e.g., 75/15/10).\n",
    "- Ensure splits are handled correctly to avoid data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Enumerate Categorical Variables\n",
    "- Process ECO codes into categorical features (e.g., `eco_letter`, `eco_number`).\n",
    "- Store these as opening-level side information.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Data Structure\n",
    "- Convert the final, processed DataFrames into PyTorch Tensors.\n",
    "- Create custom `Dataset` and `DataLoader` classes for efficient batching.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Cross-Validation & Hyperparameter Tuning\n",
    "- Define ranges for hyperparameters (`NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`).\n",
    "- Perform k-fold cross-validation on a subset of the training data to find the best hyperparameter combination before the final training run.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Final Model Training Setup\n",
    "- Define constants and hyperparameters for the final model (using results from CV).\n",
    "- Instantiate the PyTorch model, optimizer (SGD), and learning rate scheduler.\n",
    "- Implement helper functions for training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Final Model Training Loop\n",
    "- Initialize player and opening embeddings.\n",
    "- Iterate through epochs with mini-batch SGD.\n",
    "- Compute and log training and validation metrics (e.g., RMSE) per epoch.\n",
    "- Save model checkpoints locally.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Evaluation\n",
    "- Evaluate the final trained model on the held-out test set.\n",
    "- Report final metrics (MSE, RMSE) and create visualizations (e.g., predicted vs. actual scores).\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Next Steps\n",
    "- Extend model to include Black openings.\n",
    "- Experiment with more complex architectures or hybrid inputs.\n",
    "- Integrate the trained model into an API for serving recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "- Every random seed and parameter definition will be explicit.\n",
    "- Every major step includes row-count, schema, and type validation.\n",
    "- Model artifacts and logs will be saved locally for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d823",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "Connect to DuckDB and extract all player-opening statistics.\n",
    "Verify schema and perform sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fcea569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA EXTRACTION\n",
      "üìÅ Database exists: True\n",
      "üé® Color filter: White\n",
      "üîí Minimum holdout players: 1,000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd() / 'utils'))\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = Path.cwd().parent / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "COLOR_FILTER = 'w'  # 'w' for white, 'b' for black\n",
    "MIN_HOLDOUT_PLAYERS = 1000  # Minimum number of players to reserve for fold-in verification. These will not be used at all in this notebook for training, test/val or anything else.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: DATA EXTRACTION\")\n",
    "print(f\"üìÅ Database exists: {DB_PATH.exists()}\")\n",
    "print(f\"üé® Color filter: {'White' if COLOR_FILTER == 'w' else 'Black'}\")\n",
    "print(f\"üîí Minimum holdout players: {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9435033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£  Extracting player-opening stats (color: 'w')...\n",
      "   ‚Ä¢ Minimum rating filter: 1200\n",
      "\n",
      "2Ô∏è‚É£  Selecting holdout players for fold-in verification...\n",
      "   ‚Ä¢ Holdout size: 1,000 players minimum\n",
      "   ‚Ä¢ Total eligible players: 49,551\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 48,551\n",
      "   ‚Ä¢ Holdout percentage: 2.0%\n",
      "\n",
      "‚úì Database connection closed\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Query interrupted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Extract data ONLY for training players\u001b[39;00m\n\u001b[32m     52\u001b[39m query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[33m    SELECT \u001b[39m\n\u001b[32m     54\u001b[39m \u001b[33m        pos.player_id,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33m    ORDER BY pos.player_id, pos.opening_id\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m raw_data = pd.DataFrame(\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m.df())\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úì Extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(raw_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Also save holdout player IDs for later use\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Query interrupted"
     ]
    }
   ],
   "source": [
    "# Get our training database\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "MAX_PLAYERS = 50_000\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player-opening stats (color: '{COLOR_FILTER}')...\")\n",
    "\n",
    "    # Extract stats with calculated score and num_games\n",
    "    # Filter by color, minimum rating, and calculate score in the database\n",
    "    MIN_RATING = 1200\n",
    "    print(f\"   ‚Ä¢ Minimum rating filter: {MIN_RATING}\")\n",
    "\n",
    "    print(f\"\\n2Ô∏è‚É£  Selecting holdout players for fold-in verification...\")\n",
    "    print(f\"   ‚Ä¢ Holdout size: {MIN_HOLDOUT_PLAYERS:,} players minimum\")\n",
    "\n",
    "    player_query = f\"\"\"\n",
    "        SELECT DISTINCT p.id as player_id\n",
    "        FROM player p\n",
    "        JOIN player_opening_stats pos ON p.id = pos.player_id\n",
    "        WHERE p.rating >= {MIN_RATING}\n",
    "        AND pos.color = '{COLOR_FILTER}'\n",
    "        LIMIT {MAX_PLAYERS}\n",
    "    \"\"\"\n",
    "\n",
    "    all_eligible_players = pd.DataFrame(con.execute(player_query).df())\n",
    "    total_eligible = len(all_eligible_players)\n",
    "    print(f\"   ‚Ä¢ Total eligible players: {total_eligible:,}\")\n",
    "\n",
    "    if total_eligible < MIN_HOLDOUT_PLAYERS:\n",
    "        raise ValueError(f\"Not enough eligible players ({total_eligible:,}) to create holdout set of {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "    # Randomly sample holdout players (deterministic with seed)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    holdout_player_ids = np.random.choice(\n",
    "        all_eligible_players['player_id'].values,\n",
    "        size=MIN_HOLDOUT_PLAYERS,\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    training_player_ids = set(all_eligible_players['player_id'].values) - set(holdout_player_ids)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Holdout players selected: {len(holdout_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Training players available: {len(training_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Holdout percentage: {100 * len(holdout_player_ids) / total_eligible:.1f}%\")\n",
    "\n",
    "    # SQL-friendly string\n",
    "    training_player_ids_str = ','.join(map(str, training_player_ids))\n",
    "\n",
    "    # Extract data ONLY for training players\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            pos.player_id,\n",
    "            pos.opening_id,\n",
    "            pos.num_wins + pos.num_draws + pos.num_losses as num_games,\n",
    "            (pos.num_wins + (pos.num_draws * 0.5)) / \n",
    "                NULLIF(pos.num_wins + pos.num_draws + pos.num_losses, 0) as score,\n",
    "            o.eco\n",
    "        FROM player_opening_stats pos\n",
    "        JOIN opening o ON pos.opening_id = o.id\n",
    "        JOIN player p ON pos.player_id = p.id\n",
    "        WHERE pos.color = '{COLOR_FILTER}'\n",
    "        AND p.rating >= {MIN_RATING}\n",
    "        AND pos.player_id IN ({training_player_ids_str})\n",
    "        ORDER BY pos.player_id, pos.opening_id\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data = pd.DataFrame(con.execute(query).df())\n",
    "\n",
    "    print(f\"   ‚úì Extracted {len(raw_data):,} rows\")\n",
    "\n",
    "    # Also save holdout player IDs for later use\n",
    "    holdout_players_df = pd.DataFrame({'player_id': holdout_player_ids})\n",
    "\n",
    "    # Schema verification\n",
    "    required_columns = ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
    "\n",
    "    for col in required_columns:\n",
    "        if col not in raw_data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    print(f\"   ‚úì All required columns present: {required_columns}\")\n",
    "\n",
    "    # Data types verification\n",
    "    print(f\"   ‚Ä¢ player_id: {raw_data['player_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ opening_id: {raw_data['opening_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ num_games: {raw_data['num_games'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ score: {raw_data['score'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ eco: {raw_data['eco'].dtype}\")\n",
    "\n",
    "    print(\"\\n6Ô∏è‚É£  Data statistics...\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {len(raw_data):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Total games (sum): {raw_data['num_games'].sum():,}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Player ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['player_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['player_id'].max()}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Opening ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['opening_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['opening_id'].max()}\")\n",
    "\n",
    "    print(f\"\\n   Games per entry:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['num_games'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['num_games'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['num_games'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['num_games'].median():.0f}\")\n",
    "\n",
    "    print(f\"\\n   Score distribution:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['score'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['score'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['score'].mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['score'].median():.4f}\")\n",
    "\n",
    "    print(\"\\n7Ô∏è‚É£  Checking for null values...\")\n",
    "    null_counts = raw_data.isnull().sum()\n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"   ‚úì No null values found\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "\n",
    "    print(\"\\n8Ô∏è‚É£  Sample of extracted data (first 10 rows):\")\n",
    "    print(raw_data.head(10).to_string())\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nData shape: {raw_data.shape}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "\n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"\\n‚úì Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad110b85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "519d8f2f",
   "metadata": {},
   "source": [
    "## Step 2: Data Sanitization & Normalization\n",
    "\n",
    "Filter low-quality data, handle duplicates, and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: DATA SANITIZATION & NORMALIZATION\n",
      "\n",
      "üìä Starting data shape: (11570247, 5)\n",
      "   ‚Ä¢ Rows: 11,570,247\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Before: 11,570,247 rows\n",
      "   ‚Ä¢ After: 2,898,264 rows\n",
      "   ‚Ä¢ Filtered out: 8,671,983 rows (75.0%)\n",
      "   ‚Ä¢ Before: 11,570,247 rows\n",
      "   ‚Ä¢ After: 2,898,264 rows\n",
      "   ‚Ä¢ Filtered out: 8,671,983 rows (75.0%)\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Data statistics:\n",
      "   ‚Ä¢ Total rows: 2,898,264\n",
      "   ‚Ä¢ Unique players: 48,474\n",
      "   ‚Ä¢ Unique openings: 2,714\n",
      "   ‚Ä¢ Total games: 206,346,496\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1067.9\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Data statistics:\n",
      "   ‚Ä¢ Total rows: 2,898,264\n",
      "   ‚Ä¢ Unique players: 48,474\n",
      "   ‚Ä¢ Unique openings: 2,714\n",
      "   ‚Ä¢ Total games: 206,346,496\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1067.9\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5110\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "2638226      44618        2670         20  0.450000  D37\n",
      "320845        5394         838        169  0.588757  B06\n",
      "2891944      49819        3179         40  0.600000  C25\n",
      "2469145      41789         935         17  0.500000  B12\n",
      "1829787      30919         773         13  0.384615  B01\n",
      "412694        6948         242         34  0.500000  A10\n",
      "116815        1982         393         26  0.442308  A40\n",
      "1088877      18357        1135         43  0.534884  B32\n",
      "240409        4038         937         28  0.535714  B12\n",
      "2728911      46299        2019         67  0.552239  C47\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2898264, 5)\n",
      "Data reduction: 75.0%\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5110\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "2638226      44618        2670         20  0.450000  D37\n",
      "320845        5394         838        169  0.588757  B06\n",
      "2891944      49819        3179         40  0.600000  C25\n",
      "2469145      41789         935         17  0.500000  B12\n",
      "1829787      30919         773         13  0.384615  B01\n",
      "412694        6948         242         34  0.500000  A10\n",
      "116815        1982         393         26  0.442308  A40\n",
      "1088877      18357        1135         43  0.534884  B32\n",
      "240409        4038         937         28  0.535714  B12\n",
      "2728911      46299        2019         67  0.552239  C47\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2898264, 5)\n",
      "Data reduction: 75.0%\n"
     ]
    }
   ],
   "source": [
    "# 2a. Filter low-quality data, handle duplicates, and prepare for training.\n",
    "\n",
    "# Configuration\n",
    "MIN_GAMES_THRESHOLD = 10\n",
    "\n",
    "print(\"STEP 2: DATA SANITIZATION & NORMALIZATION\")\n",
    "\n",
    "# Start with raw_data from Step 1\n",
    "print(f\"\\nüìä Starting data shape: {raw_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Rows: {len(raw_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "\n",
    "# 1. Filter by minimum games threshold\n",
    "print(f\"\\n1Ô∏è‚É£  Filtering entries with < {MIN_GAMES_THRESHOLD} games...\")\n",
    "before_filter = len(raw_data)\n",
    "clean_data = raw_data.query(f'num_games >= {MIN_GAMES_THRESHOLD}').copy()\n",
    "num_rows_after_filter = len(clean_data)\n",
    "num_rows_filtered_out = before_filter - num_rows_after_filter\n",
    "\n",
    "print(f\"   ‚Ä¢ Before: {before_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ After: {num_rows_after_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ Filtered out: {num_rows_filtered_out:,} rows ({100*num_rows_filtered_out/before_filter:.1f}%)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "num_duplicates = clean_data.duplicated(subset=['player_id', 'opening_id']).sum()\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {num_duplicates} duplicate player-opening entries\")\n",
    "    dup_mask = clean_data.duplicated(subset=['player_id', 'opening_id'], keep=False)\n",
    "    print(\"\\n   Sample of duplicates:\")\n",
    "    print(clean_data[dup_mask].head(10).to_string())\n",
    "    \n",
    "    # Keep only first occurrence of any duplicate player-opening pair\n",
    "    print(\"\\n   Removing duplicates (keeping first occurrence)...\")\n",
    "    clean_data = pd.DataFrame.drop_duplicates(clean_data, subset=['player_id', 'opening_id'], keep='first')\n",
    "    print(f\"   ‚úì After deduplication: {len(clean_data):,} rows\")\n",
    "\n",
    "# 3. Remove players with no qualifying openings\n",
    "\n",
    "# Note that a few players only play stuff like the Van't Kruijs which we've excluded, so a small numer of players will be excluded here\n",
    "players_before = clean_data['player_id'].nunique()\n",
    "\n",
    "# Count openings per player\n",
    "num_openings_per_player = pd.DataFrame(clean_data.groupby('player_id').size(), columns=['count'])\n",
    "players_with_data = num_openings_per_player[num_openings_per_player['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter\n",
    "clean_data = clean_data[clean_data['player_id'].isin(players_with_data)]\n",
    "players_after = clean_data['player_id'].nunique()\n",
    "\n",
    "# 4. Remove openings with no qualifying players\n",
    "num_openings_before = clean_data['opening_id'].nunique()\n",
    "\n",
    "num_players_per_opening = pd.DataFrame(clean_data.groupby('opening_id').size(), columns=['count'])\n",
    "openings_with_data = num_players_per_opening[num_players_per_opening['count'] > 0].index.tolist()\n",
    "\n",
    "clean_data = clean_data[clean_data['opening_id'].isin(openings_with_data)]\n",
    "openings_after = clean_data['opening_id'].nunique()\n",
    "\n",
    "# 5. Verify no null values using pd.isna()\n",
    "null_counts = pd.DataFrame.isna(clean_data).sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"   ‚úì No null values found\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "    for col, count in null_counts[null_counts > 0].items():\n",
    "        print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    # Drop rows with nulls using pd.DataFrame.dropna()\n",
    "    clean_data = pd.DataFrame.dropna(clean_data)\n",
    "    print(f\"   ‚úì Dropped null rows. New shape: {clean_data.shape}\")\n",
    "\n",
    "# Reset index using pd.DataFrame.reset_index()\n",
    "clean_data = pd.DataFrame.reset_index(clean_data, drop=True)\n",
    "\n",
    "# Final statistics using pd functions\n",
    "print(f\"\\n6Ô∏è‚É£  Data statistics:\")\n",
    "print(f\"   ‚Ä¢ Total rows: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {pd.Series.nunique(clean_data['player_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {pd.Series.nunique(clean_data['opening_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Total games: {pd.Series.sum(clean_data['num_games']):,}\")\n",
    "print(f\"   ‚Ä¢ Avg games per entry: {pd.Series.mean(clean_data['num_games']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg openings per player: {len(clean_data) / pd.Series.nunique(clean_data['player_id']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg players per opening: {len(clean_data) / pd.Series.nunique(clean_data['opening_id']):.1f}\")\n",
    "\n",
    "# Score distribution using pd functions\n",
    "print(f\"\\n   Score statistics:\")\n",
    "print(f\"   ‚Ä¢ Min: {pd.Series.min(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {pd.Series.quantile(clean_data['score'], 0.25):.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {pd.Series.median(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {pd.Series.quantile(clean_data['score'], 0.75):.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {pd.Series.max(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {pd.Series.mean(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {pd.Series.std(clean_data['score']):.4f}\")\n",
    "\n",
    "# Sample of cleaned data using pd.DataFrame.sample()\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of cleaned data (10 random rows):\")\n",
    "print(pd.DataFrame.sample(clean_data, min(10, len(clean_data)), random_state=42).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SANITIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCleaned data shape: {clean_data.shape}\")\n",
    "print(f\"Data reduction: {100 * (1 - len(clean_data)/len(raw_data)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\n",
      "============================================================\n",
      "\n",
      "üìä Global statistics:\n",
      "   ‚Ä¢ Global mean score: 0.5110\n",
      "   ‚Ä¢ Total entries: 2,898,264\n",
      "   ‚Ä¢ Unique openings: 2,714\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1667\n",
      "   ‚Ä¢ 25th percentile: 0.4960\n",
      "   ‚Ä¢ Median: 0.5160\n",
      "   ‚Ä¢ 75th percentile: 0.5363\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0506\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4834\n",
      "   ‚Ä¢ Players per opening (median): 156\n",
      "   ‚Ä¢ Total games range: [10, 5508417]\n",
      "   ‚Ä¢ Players range: [1, 42918]\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1667\n",
      "   ‚Ä¢ 25th percentile: 0.4960\n",
      "   ‚Ä¢ Median: 0.5160\n",
      "   ‚Ä¢ 75th percentile: 0.5363\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0506\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4834\n",
      "   ‚Ä¢ Players per opening (median): 156\n",
      "   ‚Ä¢ Total games range: [10, 5508417]\n",
      "   ‚Ä¢ Players range: [1, 42918]\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,898,264 entries\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Range: [0.1667, 0.9962]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000959\n",
      "   ‚Ä¢ Std adjustment: 0.076782\n",
      "   ‚Ä¢ Max adjustment: 0.457214\n",
      "   ‚Ä¢ Min adjustment: -0.457379\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,898,264 entries\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Range: [0.1667, 0.9962]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000959\n",
      "   ‚Ä¢ Std adjustment: 0.076782\n",
      "   ‚Ä¢ Max adjustment: 0.457214\n",
      "   ‚Ä¢ Min adjustment: -0.457379\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003629\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001781\n",
      "   ‚Ä¢ 75th percentile (n=66 games): avg adjustment = -0.000394\n",
      "   ‚Ä¢ >75th percentile (n>66 games): avg adjustment = -0.001358\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003629\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001781\n",
      "   ‚Ä¢ 75th percentile (n=66 games): avg adjustment = -0.000394\n",
      "   ‚Ä¢ >75th percentile (n>66 games): avg adjustment = -0.001358\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ Median: 0.5118\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   ‚Ä¢ Median: 0.5118\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   Player 13997 | Opening  524 | Games:  16 | Opening mean: 0.4902 | Original: 0.4062 ‚Üí Adjusted: 0.4699 | Diff: +0.0636 | Confidence: 0.242\n",
      "   Player 19885 | Opening 2739 | Games:  10 | Opening mean: 0.5110 | Original: 0.6500 ‚Üí Adjusted: 0.5342 | Diff: -0.1158 | Confidence: 0.167\n",
      "   Player  7820 | Opening 1854 | Games:  12 | Opening mean: 0.5256 | Original: 0.4167 ‚Üí Adjusted: 0.5045 | Diff: +0.0878 | Confidence: 0.194\n",
      "   Player 24051 | Opening  493 | Games:  10 | Opening mean: 0.5298 | Original: 0.9000 ‚Üí Adjusted: 0.5915 | Diff: -0.3085 | Confidence: 0.167\n",
      "   Player  4553 | Opening 1815 | Games:  13 | Opening mean: 0.4960 | Original: 0.4615 ‚Üí Adjusted: 0.4889 | Diff: +0.0274 | Confidence: 0.206\n",
      "   Player 23779 | Opening  772 | Games:  20 | Opening mean: 0.5164 | Original: 0.7500 ‚Üí Adjusted: 0.5832 | Diff: -0.1668 | Confidence: 0.286\n",
      "   Player 48309 | Opening 1167 | Games:  14 | Opening mean: 0.4963 | Original: 0.6071 ‚Üí Adjusted: 0.5206 | Diff: -0.0866 | Confidence: 0.219\n",
      "   Player 12713 | Opening 2587 | Games:  11 | Opening mean: 0.5410 | Original: 0.4091 ‚Üí Adjusted: 0.5172 | Diff: +0.1081 | Confidence: 0.180\n",
      "   Player 13628 | Opening 3117 | Games:  20 | Opening mean: 0.5181 | Original: 0.5000 ‚Üí Adjusted: 0.5129 | Diff: +0.0129 | Confidence: 0.286\n",
      "   Player 20176 | Opening 1725 | Games:  11 | Opening mean: 0.5023 | Original: 0.6364 ‚Üí Adjusted: 0.5265 | Diff: -0.1099 | Confidence: 0.180\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 19611 | Opening 3214 | Games:  67 | Opening mean: 0.5145 | Original: 0.6194 ‚Üí Adjusted: 0.5746 | Diff: -0.0449 | Confidence: 0.573\n",
      "   Player 14889 | Opening 1356 | Games:  62 | Opening mean: 0.5054 | Original: 0.3952 ‚Üí Adjusted: 0.4444 | Diff: +0.0492 | Confidence: 0.554\n",
      "   Player 49096 | Opening  921 | Games:  55 | Opening mean: 0.4681 | Original: 0.4364 ‚Üí Adjusted: 0.4515 | Diff: +0.0151 | Confidence: 0.524\n",
      "   Player 25021 | Opening  768 | Games:  70 | Opening mean: 0.4875 | Original: 0.3429 ‚Üí Adjusted: 0.4031 | Diff: +0.0603 | Confidence: 0.583\n",
      "   Player  6518 | Opening  957 | Games:  79 | Opening mean: 0.5162 | Original: 0.5253 ‚Üí Adjusted: 0.5218 | Diff: -0.0036 | Confidence: 0.612\n",
      "   Player 13949 | Opening  298 | Games:  64 | Opening mean: 0.5318 | Original: 0.5781 ‚Üí Adjusted: 0.5578 | Diff: -0.0203 | Confidence: 0.561\n",
      "   Player 32091 | Opening 3212 | Games:  88 | Opening mean: 0.5149 | Original: 0.5284 ‚Üí Adjusted: 0.5235 | Diff: -0.0049 | Confidence: 0.638\n",
      "   Player 20483 | Opening 3486 | Games:  86 | Opening mean: 0.5588 | Original: 0.3895 ‚Üí Adjusted: 0.4518 | Diff: +0.0622 | Confidence: 0.632\n",
      "   Player 42139 | Opening  838 | Games:  87 | Opening mean: 0.5096 | Original: 0.4253 ‚Üí Adjusted: 0.4561 | Diff: +0.0308 | Confidence: 0.635\n",
      "   Player  8711 | Opening 2687 | Games:  54 | Opening mean: 0.5134 | Original: 0.5741 ‚Üí Adjusted: 0.5449 | Diff: -0.0292 | Confidence: 0.519\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 13997 | Opening  524 | Games:  16 | Opening mean: 0.4902 | Original: 0.4062 ‚Üí Adjusted: 0.4699 | Diff: +0.0636 | Confidence: 0.242\n",
      "   Player 19885 | Opening 2739 | Games:  10 | Opening mean: 0.5110 | Original: 0.6500 ‚Üí Adjusted: 0.5342 | Diff: -0.1158 | Confidence: 0.167\n",
      "   Player  7820 | Opening 1854 | Games:  12 | Opening mean: 0.5256 | Original: 0.4167 ‚Üí Adjusted: 0.5045 | Diff: +0.0878 | Confidence: 0.194\n",
      "   Player 24051 | Opening  493 | Games:  10 | Opening mean: 0.5298 | Original: 0.9000 ‚Üí Adjusted: 0.5915 | Diff: -0.3085 | Confidence: 0.167\n",
      "   Player  4553 | Opening 1815 | Games:  13 | Opening mean: 0.4960 | Original: 0.4615 ‚Üí Adjusted: 0.4889 | Diff: +0.0274 | Confidence: 0.206\n",
      "   Player 23779 | Opening  772 | Games:  20 | Opening mean: 0.5164 | Original: 0.7500 ‚Üí Adjusted: 0.5832 | Diff: -0.1668 | Confidence: 0.286\n",
      "   Player 48309 | Opening 1167 | Games:  14 | Opening mean: 0.4963 | Original: 0.6071 ‚Üí Adjusted: 0.5206 | Diff: -0.0866 | Confidence: 0.219\n",
      "   Player 12713 | Opening 2587 | Games:  11 | Opening mean: 0.5410 | Original: 0.4091 ‚Üí Adjusted: 0.5172 | Diff: +0.1081 | Confidence: 0.180\n",
      "   Player 13628 | Opening 3117 | Games:  20 | Opening mean: 0.5181 | Original: 0.5000 ‚Üí Adjusted: 0.5129 | Diff: +0.0129 | Confidence: 0.286\n",
      "   Player 20176 | Opening 1725 | Games:  11 | Opening mean: 0.5023 | Original: 0.6364 ‚Üí Adjusted: 0.5265 | Diff: -0.1099 | Confidence: 0.180\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 19611 | Opening 3214 | Games:  67 | Opening mean: 0.5145 | Original: 0.6194 ‚Üí Adjusted: 0.5746 | Diff: -0.0449 | Confidence: 0.573\n",
      "   Player 14889 | Opening 1356 | Games:  62 | Opening mean: 0.5054 | Original: 0.3952 ‚Üí Adjusted: 0.4444 | Diff: +0.0492 | Confidence: 0.554\n",
      "   Player 49096 | Opening  921 | Games:  55 | Opening mean: 0.4681 | Original: 0.4364 ‚Üí Adjusted: 0.4515 | Diff: +0.0151 | Confidence: 0.524\n",
      "   Player 25021 | Opening  768 | Games:  70 | Opening mean: 0.4875 | Original: 0.3429 ‚Üí Adjusted: 0.4031 | Diff: +0.0603 | Confidence: 0.583\n",
      "   Player  6518 | Opening  957 | Games:  79 | Opening mean: 0.5162 | Original: 0.5253 ‚Üí Adjusted: 0.5218 | Diff: -0.0036 | Confidence: 0.612\n",
      "   Player 13949 | Opening  298 | Games:  64 | Opening mean: 0.5318 | Original: 0.5781 ‚Üí Adjusted: 0.5578 | Diff: -0.0203 | Confidence: 0.561\n",
      "   Player 32091 | Opening 3212 | Games:  88 | Opening mean: 0.5149 | Original: 0.5284 ‚Üí Adjusted: 0.5235 | Diff: -0.0049 | Confidence: 0.638\n",
      "   Player 20483 | Opening 3486 | Games:  86 | Opening mean: 0.5588 | Original: 0.3895 ‚Üí Adjusted: 0.4518 | Diff: +0.0622 | Confidence: 0.632\n",
      "   Player 42139 | Opening  838 | Games:  87 | Opening mean: 0.5096 | Original: 0.4253 ‚Üí Adjusted: 0.4561 | Diff: +0.0308 | Confidence: 0.635\n",
      "   Player  8711 | Opening 2687 | Games:  54 | Opening mean: 0.5134 | Original: 0.5741 ‚Üí Adjusted: 0.5449 | Diff: -0.0292 | Confidence: 0.519\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 40775 | Opening  751 | Games: 753 | Opening mean: 0.5065 | Original: 0.4880 ‚Üí Adjusted: 0.4892 | Diff: +0.0012 | Confidence: 0.938\n",
      "   Player 31737 | Opening  838 | Games: 269 | Opening mean: 0.5096 | Original: 0.4926 ‚Üí Adjusted: 0.4952 | Diff: +0.0027 | Confidence: 0.843\n",
      "   Player  8901 | Opening 2495 | Games: 666 | Opening mean: 0.5403 | Original: 0.4947 ‚Üí Adjusted: 0.4979 | Diff: +0.0032 | Confidence: 0.930\n",
      "   Player 23881 | Opening 1665 | Games: 671 | Opening mean: 0.5186 | Original: 0.4970 ‚Üí Adjusted: 0.4985 | Diff: +0.0015 | Confidence: 0.931\n",
      "   Player  8495 | Opening   39 | Games: 503 | Opening mean: 0.5038 | Original: 0.5696 ‚Üí Adjusted: 0.5636 | Diff: -0.0059 | Confidence: 0.910\n",
      "   Player 11086 | Opening 1356 | Games: 802 | Opening mean: 0.5054 | Original: 0.5031 ‚Üí Adjusted: 0.5032 | Diff: +0.0001 | Confidence: 0.941\n",
      "   Player 13295 | Opening  910 | Games: 542 | Opening mean: 0.5000 | Original: 0.5175 ‚Üí Adjusted: 0.5161 | Diff: -0.0015 | Confidence: 0.916\n",
      "   Player 27212 | Opening  937 | Games: 264 | Opening mean: 0.4747 | Original: 0.5114 ‚Üí Adjusted: 0.5055 | Diff: -0.0058 | Confidence: 0.841\n",
      "   Player  6829 | Opening   67 | Games: 2339 | Opening mean: 0.5216 | Original: 0.5646 ‚Üí Adjusted: 0.5637 | Diff: -0.0009 | Confidence: 0.979\n",
      "   Player 36250 | Opening  910 | Games: 263 | Opening mean: 0.5000 | Original: 0.5171 ‚Üí Adjusted: 0.5144 | Diff: -0.0027 | Confidence: 0.840\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Player 40775 | Opening  751 | Games: 753 | Opening mean: 0.5065 | Original: 0.4880 ‚Üí Adjusted: 0.4892 | Diff: +0.0012 | Confidence: 0.938\n",
      "   Player 31737 | Opening  838 | Games: 269 | Opening mean: 0.5096 | Original: 0.4926 ‚Üí Adjusted: 0.4952 | Diff: +0.0027 | Confidence: 0.843\n",
      "   Player  8901 | Opening 2495 | Games: 666 | Opening mean: 0.5403 | Original: 0.4947 ‚Üí Adjusted: 0.4979 | Diff: +0.0032 | Confidence: 0.930\n",
      "   Player 23881 | Opening 1665 | Games: 671 | Opening mean: 0.5186 | Original: 0.4970 ‚Üí Adjusted: 0.4985 | Diff: +0.0015 | Confidence: 0.931\n",
      "   Player  8495 | Opening   39 | Games: 503 | Opening mean: 0.5038 | Original: 0.5696 ‚Üí Adjusted: 0.5636 | Diff: -0.0059 | Confidence: 0.910\n",
      "   Player 11086 | Opening 1356 | Games: 802 | Opening mean: 0.5054 | Original: 0.5031 ‚Üí Adjusted: 0.5032 | Diff: +0.0001 | Confidence: 0.941\n",
      "   Player 13295 | Opening  910 | Games: 542 | Opening mean: 0.5000 | Original: 0.5175 ‚Üí Adjusted: 0.5161 | Diff: -0.0015 | Confidence: 0.916\n",
      "   Player 27212 | Opening  937 | Games: 264 | Opening mean: 0.4747 | Original: 0.5114 ‚Üí Adjusted: 0.5055 | Diff: -0.0058 | Confidence: 0.841\n",
      "   Player  6829 | Opening   67 | Games: 2339 | Opening mean: 0.5216 | Original: 0.5646 ‚Üí Adjusted: 0.5637 | Diff: -0.0009 | Confidence: 0.979\n",
      "   Player 36250 | Opening  910 | Games: 263 | Opening mean: 0.5000 | Original: 0.5171 ‚Üí Adjusted: 0.5144 | Diff: -0.0027 | Confidence: 0.840\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4890 vs global) | 1 player entries\n",
      "   Opening 3551 (C19): mean = 0.8000 (+0.2890 vs global) | 1 player entries\n",
      "   Opening 3349 (C48): mean = 0.8000 (+0.2890 vs global) | 1 player entries\n",
      "   Opening 3547 (B18): mean = 0.7929 (+0.2818 vs global) | 2 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4890 vs global) | 1 player entries\n",
      "   Opening 3551 (C19): mean = 0.8000 (+0.2890 vs global) | 1 player entries\n",
      "   Opening 3349 (C48): mean = 0.8000 (+0.2890 vs global) | 1 player entries\n",
      "   Opening 3547 (B18): mean = 0.7929 (+0.2818 vs global) | 2 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 2593 (D26): mean = 0.1667 (-0.3444 vs global) | 1 player entries\n",
      "   Opening  636 (A83): mean = 0.2006 (-0.3104 vs global) | 2 player entries\n",
      "   Opening 3496 (A40): mean = 0.2273 (-0.2838 vs global) | 1 player entries\n",
      "   Opening 1763 (C37): mean = 0.2417 (-0.2694 vs global) | 2 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "   Opening 2593 (D26): mean = 0.1667 (-0.3444 vs global) | 1 player entries\n",
      "   Opening  636 (A83): mean = 0.2006 (-0.3104 vs global) | 2 player entries\n",
      "   Opening 3496 (A40): mean = 0.2273 (-0.2838 vs global) | 1 player entries\n",
      "   Opening 1763 (C37): mean = 0.2417 (-0.2694 vs global) | 2 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 23562 | Opening 3292 (C54) | Games: 12 | Opening mean: 0.7299 | Original: 0.7500 ‚Üí 0.7338\n",
      "      If we'd shrunk to global mean: 0.5573 (would lose +0.1765 of deserved credit)\n",
      "   Player 14997 | Opening 3292 (C54) | Games: 13 | Opening mean: 0.7299 | Original: 0.9231 ‚Üí 0.7697\n",
      "      If we'd shrunk to global mean: 0.5961 (would lose +0.1737 of deserved credit)\n",
      "   Player 12001 | Opening 3292 (C54) | Games: 20 | Opening mean: 0.7299 | Original: 0.7500 ‚Üí 0.7356\n",
      "      If we'd shrunk to global mean: 0.5793 (would lose +0.1563 of deserved credit)\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 23562 | Opening 3292 (C54) | Games: 12 | Opening mean: 0.7299 | Original: 0.7500 ‚Üí 0.7338\n",
      "      If we'd shrunk to global mean: 0.5573 (would lose +0.1765 of deserved credit)\n",
      "   Player 14997 | Opening 3292 (C54) | Games: 13 | Opening mean: 0.7299 | Original: 0.9231 ‚Üí 0.7697\n",
      "      If we'd shrunk to global mean: 0.5961 (would lose +0.1737 of deserved credit)\n",
      "   Player 12001 | Opening 3292 (C54) | Games: 20 | Opening mean: 0.7299 | Original: 0.7500 ‚Üí 0.7356\n",
      "      If we'd shrunk to global mean: 0.5793 (would lose +0.1563 of deserved credit)\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player  1857 | Opening 2293 (C71) | Games: 13 | Opening mean: 0.2665 | Original: 0.1538 ‚Üí 0.2433\n",
      "      If we'd shrunk to global mean: 0.4373 (would unfairly boost by +0.1941)\n",
      "   Player 19116 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3366 | Original: 0.2000 ‚Üí 0.3138\n",
      "      If we'd shrunk to global mean: 0.4592 (would unfairly boost by +0.1454)\n",
      "   Player  4605 | Opening 1779 (C37) | Games: 13 | Opening mean: 0.3366 | Original: 0.3846 ‚Üí 0.3465\n",
      "      If we'd shrunk to global mean: 0.4850 (would unfairly boost by +0.1384)\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (2898264, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player  1857 | Opening 2293 (C71) | Games: 13 | Opening mean: 0.2665 | Original: 0.1538 ‚Üí 0.2433\n",
      "      If we'd shrunk to global mean: 0.4373 (would unfairly boost by +0.1941)\n",
      "   Player 19116 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3366 | Original: 0.2000 ‚Üí 0.3138\n",
      "      If we'd shrunk to global mean: 0.4592 (would unfairly boost by +0.1454)\n",
      "   Player  4605 | Opening 1779 (C37) | Games: 13 | Opening mean: 0.3366 | Original: 0.3846 ‚Üí 0.3465\n",
      "      If we'd shrunk to global mean: 0.4850 (would unfairly boost by +0.1384)\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (2898264, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n"
     ]
    }
   ],
   "source": [
    "# 2b. Apply hierarchical Bayesian shrinkage to adjust scores based on sample size confidence\n",
    "\n",
    "# Check if confidence already exists - if so, skip this processing\n",
    "if 'confidence' in clean_data.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'confidence' column already exists in data\")\n",
    "else:\n",
    "    # Define the processing function\n",
    "    # This is a long function, I recommend you fold it down in your editor\n",
    "    def apply_hierarchical_bayesian_shrinkage(data, k_player=50):\n",
    "        \"\"\"\n",
    "        Apply two-level hierarchical Bayesian shrinkage to adjust scores.\n",
    "        \n",
    "        A lot of our player-opening entries have a small number of games played, because openings are so specific.\n",
    "        This introduces sample size issues.\n",
    "        \n",
    "        We use TWO-LEVEL shrinkage:\n",
    "        Level 1: Calculate opening-specific means (these are our \"ground truth\" for each opening)\n",
    "        Level 2: Shrink individual player-opening scores toward their opening's mean\n",
    "        This is better than shrinking toward global mean because different openings have different baseline win rates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Clean data with columns: player_id, opening_id, score, num_games, eco\n",
    "        k_player : int\n",
    "            Shrinkage constant for player-opening scores (default: 50)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Data with adjusted scores and new 'confidence' column\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Calculate global mean score for comparison\n",
    "        global_mean_score = data[\"score\"].mean()\n",
    "        print(f\"\\nüìä Global statistics:\")\n",
    "        print(f\"   ‚Ä¢ Global mean score: {global_mean_score:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Total entries: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique openings: {data['opening_id'].nunique():,}\")\n",
    "        \n",
    "        # Store original scores for comparison\n",
    "        data = data.copy()\n",
    "        data[\"score_original\"] = data[\"score\"].copy()\n",
    "        \n",
    "        # LEVEL 1: Calculate opening-specific means and statistics        \n",
    "        opening_stats = (\n",
    "            data.groupby(\"opening_id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"score\": \"mean\",\n",
    "                    \"num_games\": \"sum\",\n",
    "                    \"player_id\": \"count\",  # Number of players who played this opening\n",
    "                }\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"score\": \"opening_mean\",\n",
    "                    \"num_games\": \"opening_total_games\",\n",
    "                    \"player_id\": \"opening_num_players\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        # Opening mean statistics\n",
    "        print(f\"\\n   Opening mean score distribution:\")\n",
    "        print(f\"   ‚Ä¢ Min: {opening_stats['opening_mean'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {opening_stats['opening_mean'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {opening_stats['opening_mean'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {opening_stats['opening_mean'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {opening_stats['opening_mean'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {opening_stats['opening_mean'].std():.4f}\")\n",
    "        \n",
    "        # Show distribution of opening sizes\n",
    "        print(f\"\\n   Opening sample size distribution:\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games per opening (median): {opening_stats['opening_total_games'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players per opening (median): {opening_stats['opening_num_players'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games range: [{opening_stats['opening_total_games'].min():.0f}, {opening_stats['opening_total_games'].max():.0f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players range: [{opening_stats['opening_num_players'].min():.0f}, {opening_stats['opening_num_players'].max():.0f}]\"\n",
    "        )\n",
    "        \n",
    "        # Merge opening means back into main dataframe\n",
    "        data = data.merge(\n",
    "            opening_stats[[\"opening_mean\"]], left_on=\"opening_id\", right_index=True, how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # LEVEL 2: Shrink player-opening scores toward opening-specific means\n",
    "        print(\n",
    "            f\"   Formula: adjusted_score = (num_games √ó player_score + {k_player} √ó opening_mean) / (num_games + {k_player})\"\n",
    "        )\n",
    "        \n",
    "        numerator = (data[\"num_games\"] * data[\"score_original\"]) + (\n",
    "            k_player * data[\"opening_mean\"]\n",
    "        )\n",
    "        denominator = data[\"num_games\"] + k_player\n",
    "        data[\"score\"] = numerator / denominator\n",
    "        \n",
    "        print(f\"   ‚úì Scores adjusted for {len(data):,} entries\")\n",
    "        \n",
    "        # Calculate confidence weights (will be used in loss function later)\n",
    "        data[\"confidence\"] = data[\"num_games\"] / (\n",
    "            data[\"num_games\"] + k_player\n",
    "        )\n",
    "        print(f\"   ‚úì Confidence weights calculated\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Range: [{data['confidence'].min():.4f}, {data['confidence'].max():.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # Statistics on the adjustment\n",
    "        score_diff = data[\"score\"] - data[\"score_original\"]\n",
    "        print(f\"\\n4Ô∏è‚É£  Adjustment statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean adjustment: {score_diff.mean():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Std adjustment: {score_diff.std():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Max adjustment: {score_diff.max():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Min adjustment: {score_diff.min():.6f}\")\n",
    "        \n",
    "        # Show distribution of adjustments\n",
    "        print(f\"\\n   Adjustment by num_games quartiles:\")\n",
    "        quartiles = data[\"num_games\"].quantile([0.25, 0.5, 0.75])\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 25th percentile (n={quartiles[0.25]:.0f} games): avg adjustment = {score_diff[data['num_games'] <= quartiles[0.25]].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 50th percentile (n={quartiles[0.5]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.25]) & (data['num_games'] <= quartiles[0.5])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 75th percentile (n={quartiles[0.75]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.5]) & (data['num_games'] <= quartiles[0.75])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ >75th percentile (n>{quartiles[0.75]:.0f} games): avg adjustment = {score_diff[data['num_games'] > quartiles[0.75]].mean():.6f}\"\n",
    "        )\n",
    "        \n",
    "        # New score distribution after adjustment\n",
    "        print(f\"\\n5Ô∏è‚É£  Adjusted score statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {data['score'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {data['score'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {data['score'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {data['score'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {data['score'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {data['score'].mean():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {data['score'].std():.4f}\")\n",
    "        \n",
    "        # Detailed sample showing the effect across different game counts\n",
    "        print(f\"\\n6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\")\n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        low_game_sample = data[\n",
    "            (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in low_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Medium-game entries (50-100 games) - MODERATE shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        med_game_sample = data[\n",
    "            (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in med_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   High-game entries (200+ games) - LOW shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        high_game_sample = data[data[\"num_games\"] >= 200].sample(\n",
    "            min(10, len(data[data[\"num_games\"] >= 200])), random_state=42\n",
    "        )\n",
    "        for idx, row in high_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        # Show extreme cases - comparing to both opening mean AND global mean\n",
    "        print(f\"\\n7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\")\n",
    "        \n",
    "        # Find entries where opening mean differs significantly from global mean\n",
    "        data[\"opening_deviation_from_global\"] = (\n",
    "            data[\"opening_mean\"] - global_mean_score\n",
    "        ).abs()\n",
    "        \n",
    "        print(f\"\\n   Openings with HIGHEST win rates (strong for White):\")\n",
    "        strong_openings = data.nlargest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in strong_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"(+{deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   Openings with LOWEST win rates (weak for White):\")\n",
    "        weak_openings = data.nsmallest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in weak_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"({deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        # Show specific examples where hierarchical shrinkage made a difference\n",
    "        print(f\"\\n8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\")\n",
    "        \n",
    "        # Find entries with strong openings where player did well\n",
    "        strong_opening_ids = data.nlargest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        strong_examples = data[\n",
    "            (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] > 0.6)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] > 0.6)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"\\n   Strong opening + good player performance (shrunk toward HIGH opening mean):\"\n",
    "        )\n",
    "        for idx, row in strong_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would lose {difference:+.4f} of deserved credit)\"\n",
    "            )\n",
    "        \n",
    "        # Find entries with weak openings where player did poorly\n",
    "        weak_opening_ids = data.nsmallest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        weak_examples = data[\n",
    "            (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] < 0.45)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] < 0.45)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Weak opening + poor player performance (shrunk toward LOW opening mean):\")\n",
    "        for idx, row in weak_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would unfairly boost by {-difference:+.4f})\"\n",
    "            )\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        data = data.drop(\n",
    "            columns=[\"score_original\", \"opening_mean\", \"opening_deviation_from_global\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nFinal data shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        print(f\"\\nNew columns added:\")\n",
    "        print(f\"   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\")\n",
    "        print(f\"   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\")\n",
    "        print(f\"\\nKey improvement over simple shrinkage:\")\n",
    "        print(f\"   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\")\n",
    "        print(f\"   ‚Ä¢ Preserves opening difficulty differences\")\n",
    "        print(f\"   ‚Ä¢ More accurate for both strong and weak openings\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    K_PLAYER = 50  # Shrinkage constant for player-opening scores\n",
    "    \n",
    "    clean_data = apply_hierarchical_bayesian_shrinkage(clean_data, k_player=K_PLAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2C: PLAYER RATING STATISTICS\n",
      "============================================================\n",
      "   ‚úì Retrieved ratings for 48,474 players\n",
      "   ‚úì All players have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 48,474\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1765.12\n",
      "   ‚Ä¢ Median: 1762\n",
      "   ‚Ä¢ Std Dev: 249.14\n",
      "\n",
      "5Ô∏è‚É£  Ratings Percentiles\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1359    ‚ñà‚ñà‚ñà\n",
      "      10%          1435    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1494    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1541    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1584    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1623    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1660    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1696    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1729    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1762    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1798    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1829    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1864    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1901    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1936    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1980    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2027    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2088    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2183    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400       3,587      7.40%      ‚ñà‚ñà\n",
      "   1400-1600       9,418     19.43%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800      13,775     28.42%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000      12,895     26.60%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       6,601     13.62%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400       1,847      3.81%      ‚ñà\n",
      "   2400-2600         328      0.68%      \n",
      "   2600-3000          23      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 352\n",
      "   ‚Ä¢ 10th-90th percentile range: 653\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1435):\n",
      "      Player 13470: MaximKasirin - Rating: 1435\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1584):\n",
      "      Player 36058: muki1975 - Rating: 1584\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1762):\n",
      "      Player 4362: Chokaro - Rating: 1762\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1936):\n",
      "      Player 49964: worldchampion007 - Rating: 1936\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2088):\n",
      "      Player 40273: serpantinpsv - Rating: 2088\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "   ‚Ä¢ Total players: 48,474\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1765 ¬± 249\n",
      "   ‚Ä¢ Median: 1762\n",
      "   ‚úì Retrieved ratings for 48,474 players\n",
      "   ‚úì All players have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 48,474\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1765.12\n",
      "   ‚Ä¢ Median: 1762\n",
      "   ‚Ä¢ Std Dev: 249.14\n",
      "\n",
      "5Ô∏è‚É£  Ratings Percentiles\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1359    ‚ñà‚ñà‚ñà\n",
      "      10%          1435    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1494    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1541    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1584    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1623    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1660    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1696    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1729    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1762    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1798    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1829    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1864    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1901    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1936    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1980    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2027    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2088    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2183    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400       3,587      7.40%      ‚ñà‚ñà\n",
      "   1400-1600       9,418     19.43%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800      13,775     28.42%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000      12,895     26.60%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       6,601     13.62%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400       1,847      3.81%      ‚ñà\n",
      "   2400-2600         328      0.68%      \n",
      "   2600-3000          23      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 352\n",
      "   ‚Ä¢ 10th-90th percentile range: 653\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1435):\n",
      "      Player 13470: MaximKasirin - Rating: 1435\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1584):\n",
      "      Player 36058: muki1975 - Rating: 1584\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1762):\n",
      "      Player 4362: Chokaro - Rating: 1762\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1936):\n",
      "      Player 49964: worldchampion007 - Rating: 1936\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2088):\n",
      "      Player 40273: serpantinpsv - Rating: 2088\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "   ‚Ä¢ Total players: 48,474\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1765 ¬± 249\n",
      "   ‚Ä¢ Median: 1762\n"
     ]
    }
   ],
   "source": [
    "# 2c. Gather player rating statistics\n",
    "\n",
    "print(\"STEP 2C: PLAYER RATING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to database and extract player ratings\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:    \n",
    "    unique_player_ids = clean_data['player_id'].unique()\n",
    "    player_ids_str = ','.join(map(str, unique_player_ids))\n",
    "    \n",
    "    # Query to get player ratings\n",
    "    rating_query = f\"\"\"\n",
    "        SELECT \n",
    "            id as player_id,\n",
    "            name,\n",
    "            title,\n",
    "            rating\n",
    "        FROM player\n",
    "        WHERE id IN ({player_ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    player_ratings = pd.DataFrame(con.execute(rating_query).df())\n",
    "    print(f\"   ‚úì Retrieved ratings for {len(player_ratings):,} players\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Check for missing ratings\n",
    "missing_ratings = player_ratings['rating'].isna().sum()\n",
    "if missing_ratings > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: {missing_ratings:,} players have missing ratings\")\n",
    "else:\n",
    "    print(f\"   ‚úì All players have ratings\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Basic rating statistics:\")\n",
    "print(f\"   ‚Ä¢ Count: {player_ratings['rating'].notna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Missing: {player_ratings['rating'].isna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Min: {player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Max: {player_ratings['rating'].max():.0f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {player_ratings['rating'].mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"   ‚Ä¢ Std Dev: {player_ratings['rating'].std():.2f}\")\n",
    "\n",
    "\n",
    "print(f\"\\n5Ô∏è‚É£  Ratings Percentiles\")\n",
    "percentiles = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,\n",
    "               0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "print(f\"\\n   {'Percentile':<12} {'Rating':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*12} {'-'*10} {'-'*40}\")\n",
    "\n",
    "for p in percentiles:\n",
    "    rating_value = player_ratings['rating'].quantile(p)\n",
    "    # Create a simple bar visualization\n",
    "    bar_length = int((rating_value - player_ratings['rating'].min()) / \n",
    "                     (player_ratings['rating'].max() - player_ratings['rating'].min()) * 40)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {p*100:>5.0f}%       {rating_value:>7.0f}    {bar}\")\n",
    "\n",
    "# Rating ranges and counts\n",
    "print(f\"\\n6Ô∏è‚É£  Rating distribution by range:\")\n",
    "rating_ranges = [\n",
    "    (0, 1000), (1000, 1200), (1200, 1400), (1400, 1600), \n",
    "    (1600, 1800), (1800, 2000), (2000, 2200), (2200, 2400), \n",
    "    (2400, 2600), (2600, 3000)\n",
    "]\n",
    "\n",
    "print(f\"\\n   {'Range':<15} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*15} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "\n",
    "for low, high in rating_ranges:\n",
    "    count = len(player_ratings[(player_ratings['rating'] >= low) & (player_ratings['rating'] < high)])\n",
    "    pct = 100 * count / len(player_ratings)\n",
    "    bar_length = int(pct * 0.4)  # Scale for visualization\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {low:>4}-{high:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Interquartile range\n",
    "iqr = player_ratings['rating'].quantile(0.75) - player_ratings['rating'].quantile(0.25)\n",
    "print(f\"\\n7Ô∏è‚É£  Spread statistics:\")\n",
    "print(f\"   ‚Ä¢ Range: {player_ratings['rating'].max() - player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Interquartile Range (IQR): {iqr:.0f}\")\n",
    "print(f\"   ‚Ä¢ 10th-90th percentile range: {player_ratings['rating'].quantile(0.90) - player_ratings['rating'].quantile(0.10):.0f}\")\n",
    "\n",
    "# Sample of players at different rating levels\n",
    "print(f\"\\n9Ô∏è‚É£  Sample players at different rating levels:\")\n",
    "sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "for p in sample_percentiles:\n",
    "    rating_threshold = player_ratings['rating'].quantile(p)\n",
    "    # Get a player near this rating\n",
    "    sample_player = player_ratings.iloc[(player_ratings['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "    print(f\"\\n   ~{p*100:.0f}th percentile (rating ‚âà {rating_threshold:.0f}):\")\n",
    "    for idx, row in sample_player.iterrows():\n",
    "        # print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f} {f'({row['title']})' if pd.notna(row['title']) else ''}\")\n",
    "        title_str = f\" ({row['title']})\" if pd.notna(row['title']) else \"\"\n",
    "        print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f}{title_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ RATING STATISTICS COMPLETE\")\n",
    "print(f\"   ‚Ä¢ Total players: {len(player_ratings):,}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings['rating'].min():.0f}, {player_ratings['rating'].max():.0f}]\")\n",
    "print(f\"   ‚Ä¢ Mean ¬± std: {player_ratings['rating'].mean():.0f} ¬± {player_ratings['rating'].std():.0f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Normalization parameters (calculated from 48,474 players):\n",
      "   ‚Ä¢ Mean: 1765.12\n",
      "   ‚Ä¢ Std Dev: 249.14\n",
      "\n",
      "2Ô∏è‚É£  Normalized rating statistics:\n",
      "   ‚Ä¢ Min: -2.2683\n",
      "   ‚Ä¢ Max: 4.2461\n",
      "   ‚Ä¢ Mean: 0.000000 (should be ~0)\n",
      "   ‚Ä¢ Std: 1.000000 (should be ~1)\n",
      "   ‚Ä¢ Range: [-2.27, 4.25]\n",
      "\n",
      "3Ô∏è‚É£  Sample normalized ratings across skill levels:\n",
      "   ~10th percentile: Player 13057 | Rating: 1435 ‚Üí Z-score: -1.325\n",
      "   ~25th percentile: Player 34968 | Rating: 1584 ‚Üí Z-score: -0.727\n",
      "   ~50th percentile: Player 4227 | Rating: 1762 ‚Üí Z-score: -0.013\n",
      "   ~75th percentile: Player 48438 | Rating: 1936 ‚Üí Z-score:  0.686\n",
      "   ~90th percentile: Player 39045 | Rating: 2088 ‚Üí Z-score:  1.296\n",
      "\n",
      "4Ô∏è‚É£  Interpretation guide:\n",
      "   ‚Ä¢ rating_z ‚âà -2.3: 1200 player (minimum)\n",
      "   ‚Ä¢ rating_z ‚âà -0.7: 1584 player (25th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà  0.0: 1765 player (mean)\n",
      "   ‚Ä¢ rating_z ‚âà 0.7: 1936 player (75th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà 4.2: 2823 player (maximum)\n",
      "\n",
      "5Ô∏è‚É£  Player side information table structure:\n",
      "   ‚Ä¢ Shape: (48474, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'rating', 'rating_z']\n",
      "\n",
      "6Ô∏è‚É£  Sample entries from side information table:\n",
      "   Player  1964 | Rating: 1344 ‚Üí Z-score: -1.690\n",
      "   Player 41801 | Rating: 1824 ‚Üí Z-score:  0.236\n",
      "   Player 11483 | Rating: 1799 ‚Üí Z-score:  0.136\n",
      "   Player 13818 | Rating: 1583 ‚Üí Z-score: -0.731\n",
      "   Player 25684 | Rating: 1317 ‚Üí Z-score: -1.799\n",
      "   Player  8814 | Rating: 1474 ‚Üí Z-score: -1.168\n",
      "   Player 43608 | Rating: 1541 ‚Üí Z-score: -0.900\n",
      "   Player 32578 | Rating: 1710 ‚Üí Z-score: -0.221\n",
      "   Player 31699 | Rating: 1787 ‚Üí Z-score:  0.088\n",
      "   Player  5356 | Rating: 2054 ‚Üí Z-score:  1.160\n",
      "   ‚úì All 48,474 players in clean_data have side information\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING NORMALIZATION COMPLETE\n",
      "\n",
      "Created: player_side_info\n",
      "   ‚Ä¢ Shape: (48474, 1)\n",
      "   ‚Ä¢ Index: player_id\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ clean_data: 2,898,264 rows (player-opening interactions)\n",
      "   ‚Ä¢ player_side_info: 48,474 rows (one per player)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\n",
      "   RATING_MEAN = 1765.12\n",
      "   RATING_STD = 249.14\n",
      "\n",
      "   You'll need them to normalize ratings for new users at inference time.\n"
     ]
    }
   ],
   "source": [
    "# 2d. Normalize player ratings using z-score normalization (for use as side information in MF model)\n",
    "\n",
    "# Check if we've already normalized ratings\n",
    "if 'player_side_info' in globals() and 'rating_z' in player_side_info.columns:\n",
    "    print(\"   SKIPPING STEP 2D: RATING NORMALIZATION\")\n",
    "    print(\"Ratings have already been normalized\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing normalized rating statistics:\")\n",
    "    print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "    print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "\n",
    "else:\n",
    "    def normalize_player_ratings(player_ratings_df):\n",
    "        \"\"\"\n",
    "        Apply z-score normalization to player ratings for use as side information.\n",
    "        \n",
    "        This creates a SEPARATE table of player-level features, NOT merged into clean_data.\n",
    "        Rating is side information - it describes the player, not the player-opening interaction.\n",
    "        \n",
    "        During training, the model will LOOK UP each player's rating_z from this table.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ratings_df : pd.DataFrame\n",
    "            Player ratings with columns: player_id, name, title, rating\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (player_side_info DataFrame, RATING_MEAN, RATING_STD)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calculate normalization parameters\n",
    "        RATING_MEAN = player_ratings_df['rating'].mean()\n",
    "        RATING_STD = player_ratings_df['rating'].std()\n",
    "        \n",
    "        print(f\"\\n1Ô∏è‚É£  Normalization parameters (calculated from {len(player_ratings_df):,} players):\")\n",
    "        print(f\"   ‚Ä¢ Mean: {RATING_MEAN:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Std Dev: {RATING_STD:.2f}\")\n",
    "        \n",
    "        # Create side information table - only keep player_id and rating for now\n",
    "        player_side_info = player_ratings_df[['player_id', 'rating']].copy()\n",
    "        player_side_info['rating_z'] = (player_side_info['rating'] - RATING_MEAN) / RATING_STD\n",
    "        \n",
    "        print(f\"\\n2Ô∏è‚É£  Normalized rating statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "        print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "        print(f\"   ‚Ä¢ Range: [{player_side_info['rating_z'].min():.2f}, {player_side_info['rating_z'].max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\n3Ô∏è‚É£  Sample normalized ratings across skill levels:\")\n",
    "        sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "        for p in sample_percentiles:\n",
    "            rating_threshold = player_side_info['rating'].quantile(p)\n",
    "            sample_player = player_side_info.iloc[(player_side_info['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "            for idx, row in sample_player.iterrows():\n",
    "                print(f\"   ~{p*100:.0f}th percentile: Player {idx} | \"\n",
    "                      f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n4Ô∏è‚É£  Interpretation guide:\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(1200 - RATING_MEAN)/RATING_STD:.1f}: 1200 player (minimum)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.25) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.25):.0f} player (25th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà  0.0: {RATING_MEAN:.0f} player (mean)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.75) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.75):.0f} player (75th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].max() - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].max():.0f} player (maximum)\")\n",
    "        \n",
    "        print(f\"\\n5Ô∏è‚É£  Player side information table structure:\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        # Set player_id as index for fast lookups\n",
    "        player_side_info = player_side_info.set_index('player_id')\n",
    "        \n",
    "        print(f\"\\n6Ô∏è‚É£  Sample entries from side information table:\")\n",
    "        sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "        for idx, row in sample_data.iterrows():\n",
    "            print(f\"   Player {idx:>5} | Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        # Drop rating column - we only need rating_z for the model\n",
    "        player_side_info = player_side_info.drop(columns=['rating'])\n",
    "        \n",
    "        # This is important - make sure every player in clean_data has a rating\n",
    "        missing_players = set(clean_data['player_id'].unique()) - set(player_side_info.index)\n",
    "        if len(missing_players) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in clean_data are missing from side_info!\")\n",
    "            print(f\"   Missing player IDs: {sorted(list(missing_players))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(player_side_info):,} players in clean_data have side information\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ RATING NORMALIZATION COMPLETE\")\n",
    "        print(f\"\\nCreated: player_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: player_id\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ clean_data: {clean_data.shape[0]:,} rows (player-opening interactions)\")\n",
    "        print(f\"   ‚Ä¢ player_side_info: {len(player_side_info):,} rows (one per player)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\")\n",
    "        print(f\"   RATING_MEAN = {RATING_MEAN:.2f}\")\n",
    "        print(f\"   RATING_STD = {RATING_STD:.2f}\")\n",
    "        print(f\"\\n   You'll need them to normalize ratings for new users at inference time.\")\n",
    "        \n",
    "        return player_side_info, RATING_MEAN, RATING_STD\n",
    "    \n",
    "    player_side_info, RATING_MEAN, RATING_STD = normalize_player_ratings(player_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eba9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         player_id  opening_id  num_games     score  eco  confidence\n",
      "2625117      44367        3180         10  0.474844  C50    0.166667\n",
      "888460       14909        2467        873  0.534777  D02    0.945829\n",
      "2353441      39779        2173         23  0.386873  C57    0.315068\n",
      "2565807      43389        3203         38  0.479898  D00    0.431818\n",
      "887425       14890         247         23  0.519816  A10    0.315068\n",
      "2539600      42962         881         12  0.519573  B07    0.193548\n",
      "1212505      20503         137         39  0.481253  A01    0.438202\n",
      "1236749      20940        1310         10  0.524819  B88    0.166667\n",
      "1361305      23038        1877         55  0.464718  C41    0.523810\n",
      "2149075      36337        1643        146  0.480307  C30    0.744898\n",
      "850508       14283        2079         13  0.549273  C51    0.206349\n",
      "2709023      45927        3245         47  0.519748  C50    0.484536\n",
      "991962       16690        2625         42  0.528646  D31    0.456522\n",
      "492661        8277        1010         15  0.530700  B20    0.230769\n",
      "1803405      30462         838        265  0.496762  B06    0.841270\n",
      "1517825      25666         910        104  0.548715  B10    0.675325\n",
      "1524376      25775        3215        122  0.536971  A40    0.709302\n",
      "960155       16132        1615         16  0.528252  C26    0.242424\n",
      "1360225      23023        1385         77  0.553239  C02    0.606299\n",
      "2352728      39766        3033         25  0.448012  E61    0.333333\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac119ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           rating_z\n",
      "player_id          \n",
      "44610     -0.895541\n",
      "42346     -0.755058\n",
      "8089      -1.690272\n",
      "19642     -1.152423\n",
      "8662       1.179591\n",
      "25065     -1.040037\n",
      "41641     -0.261361\n",
      "39911     -1.449444\n",
      "49793     -1.084189\n",
      "34558      0.509287\n"
     ]
    }
   ],
   "source": [
    "print(player_side_info.sample(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30280db",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test/Val splits\n",
    "\n",
    "Here, I split my data and drop columns that are no longer needed. We're very close to being able to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: TRAIN/VALIDATION/TEST SPLIT\n",
      "\n",
      "1Ô∏è‚É£  Preparing data for split...\n",
      "   ‚Ä¢ Features (X): (2898264, 4)\n",
      "   ‚Ä¢ Target (y): (2898264,)\n",
      "   ‚Ä¢ Feature columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ Train: 2,173,697 samples (75.0%)\n",
      "   ‚Ä¢ Val: 434,740 samples (15.0%)\n",
      "   ‚Ä¢ Test: 289,827 samples (10.0%)\n",
      "   ‚Ä¢ Total samples: 2,898,264 (should equal 2,898,264)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 48,421 unique players\n",
      "   ‚Ä¢ Val: 47,524 unique players\n",
      "   ‚Ä¢ Test: 46,540 unique players\n",
      "   ‚Ä¢ Total unique: 48,474 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,686 unique openings\n",
      "   ‚Ä¢ Val: 2,440 unique openings\n",
      "   ‚Ä¢ Test: 2,362 unique openings\n",
      "   ‚Ä¢ Total unique: 2,714 openings\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 38 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 21 (0.9%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 22 (0.0%)\n",
      "   ‚Ä¢ Openings not in train: 9 (0.4%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "   ‚Ä¢ Train: 2,173,697 samples (75.0%)\n",
      "   ‚Ä¢ Val: 434,740 samples (15.0%)\n",
      "   ‚Ä¢ Test: 289,827 samples (10.0%)\n",
      "   ‚Ä¢ Total samples: 2,898,264 (should equal 2,898,264)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 48,421 unique players\n",
      "   ‚Ä¢ Val: 47,524 unique players\n",
      "   ‚Ä¢ Test: 46,540 unique players\n",
      "   ‚Ä¢ Total unique: 48,474 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,686 unique openings\n",
      "   ‚Ä¢ Val: 2,440 unique openings\n",
      "   ‚Ä¢ Test: 2,362 unique openings\n",
      "   ‚Ä¢ Total unique: 2,714 openings\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 38 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 21 (0.9%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 22 (0.0%)\n",
      "   ‚Ä¢ Openings not in train: 9 (0.4%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "\n",
      "   Train y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1351\n",
      "   ‚Ä¢ Max: 0.8000\n",
      "\n",
      "   Test y:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1884\n",
      "   ‚Ä¢ Max: 0.8251\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train confidence:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation confidence:\n",
      "   ‚Ä¢ Mean: 0.4150\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test confidence:\n",
      "   ‚Ä¢ Mean: 0.4150\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 2,173,697 samples (75%)\n",
      "   ‚Ä¢ Validation data: 434,740 samples (15%)\n",
      "   ‚Ä¢ Test data: 289,827 samples (10%)\n",
      "   ‚Ä¢ Player side info: 48,474 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n",
      "\n",
      "   Train y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1351\n",
      "   ‚Ä¢ Max: 0.8000\n",
      "\n",
      "   Test y:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1884\n",
      "   ‚Ä¢ Max: 0.8251\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train confidence:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation confidence:\n",
      "   ‚Ä¢ Mean: 0.4150\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test confidence:\n",
      "   ‚Ä¢ Mean: 0.4150\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 2,173,697 samples (75%)\n",
      "   ‚Ä¢ Validation data: 434,740 samples (15%)\n",
      "   ‚Ä¢ Test data: 289,827 samples (10%)\n",
      "   ‚Ä¢ Player side info: 48,474 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train/Validation/Test Split (75/15/10)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 3: TRAIN/VALIDATION/TEST SPLIT\")\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Preparing data for split...\")\n",
    "\n",
    "# We don't need num_games for modeling because we have the `confidence` based on num_games\n",
    "# Keep: player_id, opening_id, score, eco, confidence\n",
    "X = clean_data[[\"player_id\", \"opening_id\", \"eco\", \"confidence\"]].copy()\n",
    "y = clean_data[\"score\"].copy()\n",
    "\n",
    "print(f\"   ‚Ä¢ Features (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Target (y): {y.shape}\")\n",
    "print(f\"   ‚Ä¢ Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Clean up player_side_info - only keep rating_z\n",
    "# May regret this if we add more side info later and forget we took this step\n",
    "# God help me that would be a nasty bug\n",
    "player_side_info_clean = player_side_info[[\"rating_z\"]].copy()\n",
    "\n",
    "#  Use index-based splitting to avoid DataFrame copies\n",
    "idx = np.arange(len(X))\n",
    "\n",
    "# First split: separate out test set (10%)\n",
    "idx_temp, idx_test = train_test_split(idx, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: split remaining into train (75%) and val (15%)\n",
    "# 15% of original = 15/90 ‚âà 0.1667 of temp\n",
    "idx_train, idx_val = train_test_split(idx_temp, test_size=15/90, random_state=42, shuffle=True)\n",
    "\n",
    "# Create splits using iloc (view, not copy)\n",
    "X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
    "X_test, y_test = X.iloc[idx_test], y.iloc[idx_test]\n",
    "\n",
    "print(f\"   ‚Ä¢ Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Val: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"   ‚Ä¢ Total samples: {total:,} (should equal {len(X):,})\")\n",
    "print(f\"   ‚Ä¢ Train %: {len(X_train)/total*100:.2f}% (target: 75%)\")\n",
    "print(f\"   ‚Ä¢ Val %: {len(X_val)/total*100:.2f}% (target: 15%)\")\n",
    "print(f\"   ‚Ä¢ Test %: {len(X_test)/total*100:.2f}% (target: 10%)\")\n",
    "\n",
    "# Pre-compute unique arrays once\n",
    "players_train = X_train[\"player_id\"].unique()\n",
    "players_val = X_val[\"player_id\"].unique()\n",
    "players_test = X_test[\"player_id\"].unique()\n",
    "\n",
    "openings_train = X_train[\"opening_id\"].unique()\n",
    "openings_val = X_val[\"opening_id\"].unique()\n",
    "openings_test = X_test[\"opening_id\"].unique()\n",
    "\n",
    "print(f\"\\n   Players:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(players_train):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Val: {len(players_val):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Test: {len(players_test):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['player_id'].nunique():,} players\")\n",
    "\n",
    "print(f\"\\n   Openings:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(openings_train):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Val: {len(openings_val):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Test: {len(openings_test):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['opening_id'].nunique():,} openings\")\n",
    "\n",
    "# Use NumPy setdiff1d for cold-start analysis (C-speed)\n",
    "\n",
    "val_cold_players = np.setdiff1d(players_val, players_train, assume_unique=True)\n",
    "val_cold_openings = np.setdiff1d(openings_val, openings_train, assume_unique=True)\n",
    "\n",
    "test_cold_players = np.setdiff1d(players_test, players_train, assume_unique=True)\n",
    "test_cold_openings = np.setdiff1d(openings_test, openings_train, assume_unique=True)\n",
    "\n",
    "print(f\"\\n   Validation set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(val_cold_players):,} ({len(val_cold_players)/len(players_val)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(val_cold_openings):,} ({len(val_cold_openings)/len(openings_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(test_cold_players):,} ({len(test_cold_players)/len(players_test)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(test_cold_openings):,} ({len(test_cold_openings)/len(openings_test)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n7Ô∏è‚É£  Score distribution across splits:\")\n",
    "\n",
    "y_train_stats = y_train.describe()\n",
    "y_val_stats = y_val.describe()\n",
    "y_test_stats = y_test.describe()\n",
    "\n",
    "print(f\"\\n   Train y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_train_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_train_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_train_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_val_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_val_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_val_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_test_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_test_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_test_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n8Ô∏è‚É£  Confidence distribution across splits:\")\n",
    "\n",
    "conf_train_stats = X_train['confidence'].describe()\n",
    "conf_val_stats = X_val['confidence'].describe()\n",
    "conf_test_stats = X_test['confidence'].describe()\n",
    "\n",
    "print(f\"\\n   Train confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_train_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_val_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_test_stats['50%']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SPLIT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   ‚Ä¢ Training data: {len(X_train):,} samples (75%)\")\n",
    "print(f\"   ‚Ä¢ Validation data: {len(X_val):,} samples (15%)\")\n",
    "print(f\"   ‚Ä¢ Test data: {len(X_test):,} samples (10%)\")\n",
    "print(f\"   ‚Ä¢ Player side info: {len(player_side_info_clean):,} players\")\n",
    "print(f\"   ‚Ä¢ Side info columns: {list(player_side_info_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781a382",
   "metadata": {},
   "source": [
    "## Step 3b: Remap Player and Opening IDs to Sequential Integers\n",
    "\n",
    "**Why remap IDs?**\n",
    "- Database IDs may have gaps (e.g., [1, 5, 10, 15, ...]) from deleted entries\n",
    "- Embedding layers need 0-based contiguous indices for efficiency\n",
    "- Remapping saves memory (no unused embedding slots)\n",
    "\n",
    "**Process:**\n",
    "1. Check if IDs are already sequential (0 or 1-based with no gaps)\n",
    "2. If not, create mappings: old_id ‚Üí new_sequential_id\n",
    "3. Remap all DataFrames and side info tables\n",
    "4. Verify mappings with spot checks\n",
    "\n",
    "This ensures embeddings use minimal memory and indices align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4c409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\n",
      "\n",
      "1Ô∏è‚É£  Processing player IDs...\n",
      "Checking player IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique players: 48474\n",
      "   ‚Ä¢ ID range: [1, 50000]\n",
      "   ‚ö†Ô∏è  player IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1526\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      player_id 1 ‚Üí 0\n",
      "      player_id 2 ‚Üí 1\n",
      "      player_id 3 ‚Üí 2\n",
      "      player_id 4 ‚Üí 3\n",
      "      player_id 5 ‚Üí 4\n",
      "      player_id 49996 ‚Üí 48469\n",
      "      player_id 49997 ‚Üí 48470\n",
      "      player_id 49998 ‚Üí 48471\n",
      "      player_id 49999 ‚Üí 48472\n",
      "      player_id 50000 ‚Üí 48473\n",
      "\n",
      "   Remapping 5 DataFrames...\n",
      "Checking player IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique players: 48474\n",
      "   ‚Ä¢ ID range: [1, 50000]\n",
      "   ‚ö†Ô∏è  player IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1526\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      player_id 1 ‚Üí 0\n",
      "      player_id 2 ‚Üí 1\n",
      "      player_id 3 ‚Üí 2\n",
      "      player_id 4 ‚Üí 3\n",
      "      player_id 5 ‚Üí 4\n",
      "      player_id 49996 ‚Üí 48469\n",
      "      player_id 49997 ‚Üí 48470\n",
      "      player_id 49998 ‚Üí 48471\n",
      "      player_id 49999 ‚Üí 48472\n",
      "      player_id 50000 ‚Üí 48473\n",
      "\n",
      "   Remapping 5 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2714\n",
      "   ‚Ä¢ ID range: [2, 3589]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 874\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 3 ‚Üí 1\n",
      "      opening_id 4 ‚Üí 2\n",
      "      opening_id 5 ‚Üí 3\n",
      "      opening_id 6 ‚Üí 4\n",
      "      opening_id 3571 ‚Üí 2709\n",
      "      opening_id 3572 ‚Üí 2710\n",
      "      opening_id 3575 ‚Üí 2711\n",
      "      opening_id 3584 ‚Üí 2712\n",
      "      opening_id 3589 ‚Üí 2713\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2714\n",
      "   ‚Ä¢ ID range: [2, 3589]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 874\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 3 ‚Üí 1\n",
      "      opening_id 4 ‚Üí 2\n",
      "      opening_id 5 ‚Üí 3\n",
      "      opening_id 6 ‚Üí 4\n",
      "      opening_id 3571 ‚Üí 2709\n",
      "      opening_id 3572 ‚Üí 2710\n",
      "      opening_id 3575 ‚Üí 2711\n",
      "      opening_id 3584 ‚Üí 2712\n",
      "      opening_id 3589 ‚Üí 2713\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/4\n",
      "   ‚úì Remapped DataFrame 2/4\n",
      "   ‚úì Remapped DataFrame 3/4\n",
      "   ‚úì Remapped DataFrame 4/4\n",
      "\n",
      "   ‚úÖ Opening ID remapping complete!\n",
      "\n",
      "3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\n",
      "   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\n",
      "\n",
      "   Sampling 10 entries from X_train (before remapping was applied)...\n",
      "   ‚Ä¢ Sample indices: [0, 241521, 483042, 724563, 966084, 1207605, 1449126, 1690647, 1932168, 2173696]\n",
      "\n",
      "   Verification checks:\n",
      "   #    Row Idx    New Player   New Opening  Confidence   Status         \n",
      "   ---- ---------- ------------ ------------ ------------ ---------------\n",
      "   1    0          20824        1934         0.1803       ‚úì PASS         \n",
      "   2    241521     19525        295          0.2958       ‚úì PASS         \n",
      "   3    483042     38914        1888         0.2308       ‚úì PASS         \n",
      "   4    724563     19186        166          0.7992       ‚úì PASS         \n",
      "   5    966084     21488        2019         0.4186       ‚úì PASS         \n",
      "   6    1207605    28199        233          0.7778       ‚úì PASS         \n",
      "   7    1449126    14755        698          0.3056       ‚úì PASS         \n",
      "   8    1690647    42785        783          0.8952       ‚úì PASS         \n",
      "   9    1932168    34249        538          0.2857       ‚úì PASS         \n",
      "   10   2173696    11795        569          0.8221       ‚úì PASS         \n",
      "\n",
      "   Reverse mapping verification (sample of 3 entries):\n",
      "   #    New‚ÜíOld Player                 New‚ÜíOld Opening               \n",
      "   ---- ------------------------------ ------------------------------\n",
      "   1    20824 ‚Üí 21472                  1934 ‚Üí 2540                   \n",
      "   6    28199 ‚Üí 29075                  233 ‚Üí 299                     \n",
      "   10   11795 ‚Üí 12165                  569 ‚Üí 737                     \n",
      "\n",
      "   ‚úÖ All spot checks passed! ID mappings are correct.\n",
      "\n",
      "4Ô∏è‚É£  Summary:\n",
      "\n",
      "   Player IDs:\n",
      "   ‚Ä¢ Original range: [1, 50000]\n",
      "   ‚Ä¢ New range: [0, 48473]\n",
      "   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\n",
      "\n",
      "   Opening IDs:\n",
      "   ‚Ä¢ Original range: [2, 3589]\n",
      "   ‚Ä¢ New range: [0, 2713]\n",
      "   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\n",
      "\n",
      "   Updated DataFrames:\n",
      "   ‚Ä¢ X_train: (2173697, 4)\n",
      "   ‚Ä¢ X_val: (434740, 4)\n",
      "   ‚Ä¢ X_test: (289827, 4)\n",
      "   ‚Ä¢ clean_data: (2898264, 6)\n",
      "   ‚Ä¢ player_side_info: (48474, 1)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ID REMAPPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Important:\n",
      "   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\n",
      "   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\n",
      "   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\n",
      "   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\n",
      "   ‚úì Remapped DataFrame 1/4\n",
      "   ‚úì Remapped DataFrame 2/4\n",
      "   ‚úì Remapped DataFrame 3/4\n",
      "   ‚úì Remapped DataFrame 4/4\n",
      "\n",
      "   ‚úÖ Opening ID remapping complete!\n",
      "\n",
      "3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\n",
      "   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\n",
      "\n",
      "   Sampling 10 entries from X_train (before remapping was applied)...\n",
      "   ‚Ä¢ Sample indices: [0, 241521, 483042, 724563, 966084, 1207605, 1449126, 1690647, 1932168, 2173696]\n",
      "\n",
      "   Verification checks:\n",
      "   #    Row Idx    New Player   New Opening  Confidence   Status         \n",
      "   ---- ---------- ------------ ------------ ------------ ---------------\n",
      "   1    0          20824        1934         0.1803       ‚úì PASS         \n",
      "   2    241521     19525        295          0.2958       ‚úì PASS         \n",
      "   3    483042     38914        1888         0.2308       ‚úì PASS         \n",
      "   4    724563     19186        166          0.7992       ‚úì PASS         \n",
      "   5    966084     21488        2019         0.4186       ‚úì PASS         \n",
      "   6    1207605    28199        233          0.7778       ‚úì PASS         \n",
      "   7    1449126    14755        698          0.3056       ‚úì PASS         \n",
      "   8    1690647    42785        783          0.8952       ‚úì PASS         \n",
      "   9    1932168    34249        538          0.2857       ‚úì PASS         \n",
      "   10   2173696    11795        569          0.8221       ‚úì PASS         \n",
      "\n",
      "   Reverse mapping verification (sample of 3 entries):\n",
      "   #    New‚ÜíOld Player                 New‚ÜíOld Opening               \n",
      "   ---- ------------------------------ ------------------------------\n",
      "   1    20824 ‚Üí 21472                  1934 ‚Üí 2540                   \n",
      "   6    28199 ‚Üí 29075                  233 ‚Üí 299                     \n",
      "   10   11795 ‚Üí 12165                  569 ‚Üí 737                     \n",
      "\n",
      "   ‚úÖ All spot checks passed! ID mappings are correct.\n",
      "\n",
      "4Ô∏è‚É£  Summary:\n",
      "\n",
      "   Player IDs:\n",
      "   ‚Ä¢ Original range: [1, 50000]\n",
      "   ‚Ä¢ New range: [0, 48473]\n",
      "   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\n",
      "\n",
      "   Opening IDs:\n",
      "   ‚Ä¢ Original range: [2, 3589]\n",
      "   ‚Ä¢ New range: [0, 2713]\n",
      "   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\n",
      "\n",
      "   Updated DataFrames:\n",
      "   ‚Ä¢ X_train: (2173697, 4)\n",
      "   ‚Ä¢ X_val: (434740, 4)\n",
      "   ‚Ä¢ X_test: (289827, 4)\n",
      "   ‚Ä¢ clean_data: (2898264, 6)\n",
      "   ‚Ä¢ player_side_info: (48474, 1)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ID REMAPPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Important:\n",
      "   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\n",
      "   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\n",
      "   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\n",
      "   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\n"
     ]
    }
   ],
   "source": [
    "# Step 3b: Remap player and opening IDs to 0-based sequential integers\n",
    "\n",
    "print(\"STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\")\n",
    "\n",
    "def check_and_remap_ids(df_list, id_column, entity_name):\n",
    "    \"\"\"\n",
    "    Check if IDs are sequential starting from 0, and remap if not.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of DataFrames\n",
    "        List of DataFrames containing the ID column to check/remap\n",
    "    id_column : str\n",
    "        Name of the ID column ('player_id' or 'opening_id')\n",
    "    entity_name : str\n",
    "        Name for logging ('player' or 'opening')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (df_list with remapped IDs, id_to_idx mapping dict, needs_remapping bool)\n",
    "    \"\"\"\n",
    "    # Get all unique IDs across all dataframes\n",
    "    all_ids = pd.concat([df[id_column] for df in df_list]).unique()\n",
    "    all_ids_sorted = sorted(all_ids)\n",
    "    \n",
    "    print(f\"Checking {entity_name} IDs...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ‚Ä¢ Total unique {entity_name}s: {len(all_ids_sorted)}\")\n",
    "    print(f\"   ‚Ä¢ ID range: [{all_ids_sorted[0]}, {all_ids_sorted[-1]}]\")\n",
    "    \n",
    "    # Check if IDs are already 0-based sequential (0, 1, 2, ...)\n",
    "    expected_sequential = list(range(len(all_ids_sorted)))\n",
    "    is_sequential = (all_ids_sorted == expected_sequential)\n",
    "    \n",
    "    if is_sequential:\n",
    "        print(f\"   ‚úì {entity_name} IDs are already 0-based sequential - no remapping needed!\")\n",
    "        return df_list, None, False\n",
    "    \n",
    "    # Check if IDs are 1-based sequential (1, 2, 3, ...)\n",
    "    expected_sequential_1based = list(range(1, len(all_ids_sorted) + 1))\n",
    "    is_sequential_1based = (all_ids_sorted == expected_sequential_1based)\n",
    "    \n",
    "    if is_sequential_1based:\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs are 1-based sequential - will remap to 0-based\")\n",
    "    else:\n",
    "        # Calculate gaps\n",
    "        num_gaps = (all_ids_sorted[-1] - all_ids_sorted[0] + 1) - len(all_ids_sorted)\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs have gaps - will remap to 0-based sequential\")\n",
    "        print(f\"   ‚Ä¢ Number of gaps: {num_gaps}\")\n",
    "    \n",
    "    # Create mapping: old_id -> new_idx (0-based)\n",
    "    id_to_idx = {old_id: new_idx for new_idx, old_id in enumerate(all_ids_sorted)}\n",
    "    idx_to_id = {new_idx: old_id for old_id, new_idx in id_to_idx.items()}\n",
    "    \n",
    "    print(f\"\\n   Creating mapping...\")\n",
    "    print(f\"   ‚Ä¢ Example mappings:\")\n",
    "    sample_ids = all_ids_sorted[:5] + all_ids_sorted[-5:]\n",
    "    for old_id in sample_ids[:10]:  # Show first 5 and last 5\n",
    "        print(f\"      {entity_name}_id {old_id} ‚Üí {id_to_idx[old_id]}\")\n",
    "    \n",
    "    # Remap all DataFrames\n",
    "    print(f\"\\n   Remapping {len(df_list)} DataFrames...\")\n",
    "    remapped_dfs = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[id_column] = df_copy[id_column].map(id_to_idx)\n",
    "        remapped_dfs.append(df_copy)\n",
    "        print(f\"   ‚úì Remapped DataFrame {i+1}/{len(df_list)}\")\n",
    "    \n",
    "    return remapped_dfs, (id_to_idx, idx_to_id), True\n",
    "\n",
    "# 1. Remap player IDs\n",
    "print(f\"\\n1Ô∏è‚É£  Processing player IDs...\")\n",
    "player_dfs = [X_train, X_val, X_test, clean_data, player_side_info.reset_index()]\n",
    "remapped_player_dfs, player_mappings, player_remapped = check_and_remap_ids(\n",
    "    player_dfs, 'player_id', 'player'\n",
    ")\n",
    "\n",
    "if player_remapped:\n",
    "    X_train, X_val, X_test, clean_data, player_side_info_remapped = remapped_player_dfs\n",
    "    player_id_to_idx, player_idx_to_id = player_mappings\n",
    "    player_side_info = player_side_info_remapped.set_index('player_id')\n",
    "    print(f\"\\n   ‚úÖ Player ID remapping complete!\")\n",
    "else:\n",
    "    player_id_to_idx, player_idx_to_id = None, None\n",
    "\n",
    "# 2. Remap opening IDs\n",
    "print(f\"\\n2Ô∏è‚É£  Processing opening IDs...\")\n",
    "opening_dfs = [X_train, X_val, X_test, clean_data]\n",
    "remapped_opening_dfs, opening_mappings, opening_remapped = check_and_remap_ids(\n",
    "    opening_dfs, 'opening_id', 'opening'\n",
    ")\n",
    "\n",
    "if opening_remapped:\n",
    "    X_train, X_val, X_test, clean_data = remapped_opening_dfs\n",
    "    opening_id_to_idx, opening_idx_to_id = opening_mappings\n",
    "    print(f\"\\n   ‚úÖ Opening ID remapping complete!\")\n",
    "else:\n",
    "    opening_id_to_idx, opening_idx_to_id = None, None\n",
    "\n",
    "# 3. Spot checks to verify mappings\n",
    "print(f\"\\n3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\")\n",
    "print(f\"   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\")\n",
    "\n",
    "# Sample 10 entries: first, last, and 8 in between\n",
    "print(f\"\\n   Sampling 10 entries from X_train (before remapping was applied)...\")\n",
    "total_rows = len(X_train)\n",
    "# Get indices: first, last, and 8 evenly spaced in between\n",
    "sample_indices = [0]  # First row\n",
    "step = (total_rows - 1) // 9  # Divide remaining rows into 9 parts\n",
    "sample_indices.extend([min(i * step, total_rows - 1) for i in range(1, 9)])\n",
    "sample_indices.append(total_rows - 1)  # Last row\n",
    "\n",
    "print(f\"   ‚Ä¢ Sample indices: {sample_indices}\")\n",
    "\n",
    "# Store samples with their NEW (remapped) IDs\n",
    "samples = []\n",
    "for idx in sample_indices:\n",
    "    row = X_train.iloc[idx]\n",
    "    samples.append({\n",
    "        'index': idx,\n",
    "        'new_player_id': row['player_id'],\n",
    "        'new_opening_id': row['opening_id'],\n",
    "        'confidence': row['confidence']\n",
    "    })\n",
    "\n",
    "print(f\"\\n   Verification checks:\")\n",
    "print(f\"   {'#':<4} {'Row Idx':<10} {'New Player':<12} {'New Opening':<12} {'Confidence':<12} {'Status':<15}\")\n",
    "print(f\"   {'-'*4} {'-'*10} {'-'*12} {'-'*12} {'-'*12} {'-'*15}\")\n",
    "\n",
    "all_checks_passed = True\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    new_player_id = sample['new_player_id']\n",
    "    new_opening_id = sample['new_opening_id']\n",
    "    confidence = sample['confidence']\n",
    "    idx = sample['index']\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: New player ID is valid (0-based sequential)\n",
    "    if player_remapped:\n",
    "        player_valid = 0 <= new_player_id < len(player_id_to_idx)\n",
    "        checks.append((\"player_id\", player_valid))\n",
    "    else:\n",
    "        checks.append((\"player_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 2: New opening ID is valid (0-based sequential)\n",
    "    if opening_remapped:\n",
    "        opening_valid = 0 <= new_opening_id < len(opening_id_to_idx)\n",
    "        checks.append((\"opening_id\", opening_valid))\n",
    "    else:\n",
    "        checks.append((\"opening_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 3: Player exists in player_side_info\n",
    "    player_exists = new_player_id in player_side_info.index\n",
    "    checks.append((\"in_side_info\", player_exists))\n",
    "    \n",
    "    # Check 4: Opening exists in clean_data\n",
    "    opening_exists = new_opening_id in clean_data['opening_id'].values\n",
    "    checks.append((\"in_clean\", opening_exists))\n",
    "    \n",
    "    # All checks must pass\n",
    "    all_pass = all(check[1] for check in checks)\n",
    "    status = \"‚úì PASS\" if all_pass else f\"‚úó FAIL ({','.join([c[0] for c in checks if not c[1]])})\"\n",
    "    \n",
    "    if not all_pass:\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(f\"   {i:<4} {idx:<10} {new_player_id:<12} {new_opening_id:<12} {confidence:<12.4f} {status:<15}\")\n",
    "\n",
    "# Additional verification: check if we can reverse map to original IDs\n",
    "if player_remapped or opening_remapped:\n",
    "    print(f\"\\n   Reverse mapping verification (sample of 3 entries):\")\n",
    "    print(f\"   {'#':<4} {'New‚ÜíOld Player':<30} {'New‚ÜíOld Opening':<30}\")\n",
    "    print(f\"   {'-'*4} {'-'*30} {'-'*30}\")\n",
    "    \n",
    "    for i in [0, len(samples)//2, len(samples)-1]:  # First, middle, last\n",
    "        sample = samples[i]\n",
    "        \n",
    "        # Reverse map player\n",
    "        if player_remapped:\n",
    "            old_player = player_idx_to_id.get(sample['new_player_id'], 'NOT_FOUND')\n",
    "            player_str = f\"{sample['new_player_id']} ‚Üí {old_player}\"\n",
    "        else:\n",
    "            player_str = f\"{sample['new_player_id']} (unchanged)\"\n",
    "        \n",
    "        # Reverse map opening\n",
    "        if opening_remapped:\n",
    "            old_opening = opening_idx_to_id.get(sample['new_opening_id'], 'NOT_FOUND')\n",
    "            opening_str = f\"{sample['new_opening_id']} ‚Üí {old_opening}\"\n",
    "        else:\n",
    "            opening_str = f\"{sample['new_opening_id']} (unchanged)\"\n",
    "        \n",
    "        print(f\"   {i+1:<4} {player_str:<30} {opening_str:<30}\")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(f\"\\n   ‚úÖ All spot checks passed! ID mappings are correct.\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Some spot checks failed - investigate immediately!\")\n",
    "    raise ValueError(\"ID remapping verification failed!\")\n",
    "\n",
    "# 4. Summary\n",
    "print(f\"\\n4Ô∏è‚É£  Summary:\")\n",
    "print(f\"\\n   Player IDs:\")\n",
    "if player_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{player_idx_to_id[0]}, {player_idx_to_id[len(player_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(player_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "    \n",
    "print(f\"\\n   Opening IDs:\")\n",
    "if opening_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{opening_idx_to_id[0]}, {opening_idx_to_id[len(opening_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(opening_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "\n",
    "print(f\"\\n   Updated DataFrames:\")\n",
    "print(f\"   ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ X_val: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ clean_data: {clean_data.shape}\")\n",
    "print(f\"   ‚Ä¢ player_side_info: {player_side_info.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ID REMAPPING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Important:\")\n",
    "print(f\"   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\")\n",
    "print(f\"   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\")\n",
    "print(f\"   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\")\n",
    "print(f\"   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a580c0",
   "metadata": {},
   "source": [
    "## 4. Enumerate Categorical Variables\n",
    "\n",
    "I believe the only variable we need to enumerate here is `eco`. That's the broad categorization of a specific opening.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- One ECO code will have many openings\n",
    "- They're sorted by letter, then further by number. For instance, C21 and C44 are in the `C` family.\n",
    "- Maybe we make this side information?\n",
    "\n",
    "First, let's get some data on ECO codes to help us better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9504a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: ECO CODE STATISTICS\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,898,264\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 6328.1\n",
      "   ‚Ä¢ Median entries per ECO: 825.5\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 201463\n",
      "   ‚Ä¢ Std: 18392.0\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 6328.1\n",
      "   ‚Ä¢ Median entries per ECO: 825.5\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 201463\n",
      "   ‚Ä¢ Std: 18392.0\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        509,704     17.59%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        1,049,271     36.20%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        987,820     34.08%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D        278,545      9.61%      ‚ñà‚ñà‚ñà\n",
      "   E         72,924      2.52%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00    201,463      6.95%      ‚ñà‚ñà\n",
      "   2      B01    197,534      6.82%      ‚ñà‚ñà\n",
      "   3      A40    125,413      4.33%      ‚ñà\n",
      "   4      C50    101,324      3.50%      ‚ñà\n",
      "   5      C00     87,274      3.01%      \n",
      "   6      C40     81,494      2.81%      \n",
      "   7      B06     73,344      2.53%      \n",
      "   8      C42     65,698      2.27%      \n",
      "   9      C44     60,703      2.09%      \n",
      "   10     B10     56,943      1.96%      \n",
      "   11     C41     55,513      1.92%      \n",
      "   12     B40     52,476      1.81%      \n",
      "   13     B12     48,798      1.68%      \n",
      "   14     A00     48,427      1.67%      \n",
      "   15     C02     47,959      1.65%      \n",
      "   16     D00     41,126      1.42%      \n",
      "   17     B21     37,783      1.30%      \n",
      "   18     A43     35,726      1.23%      \n",
      "   19     B32     33,894      1.17%      \n",
      "   20     A04     33,026      1.14%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        509,704     17.59%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        1,049,271     36.20%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        987,820     34.08%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D        278,545      9.61%      ‚ñà‚ñà‚ñà\n",
      "   E         72,924      2.52%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00    201,463      6.95%      ‚ñà‚ñà\n",
      "   2      B01    197,534      6.82%      ‚ñà‚ñà\n",
      "   3      A40    125,413      4.33%      ‚ñà\n",
      "   4      C50    101,324      3.50%      ‚ñà\n",
      "   5      C00     87,274      3.01%      \n",
      "   6      C40     81,494      2.81%      \n",
      "   7      B06     73,344      2.53%      \n",
      "   8      C42     65,698      2.27%      \n",
      "   9      C44     60,703      2.09%      \n",
      "   10     B10     56,943      1.96%      \n",
      "   11     C41     55,513      1.92%      \n",
      "   12     B40     52,476      1.81%      \n",
      "   13     B12     48,798      1.68%      \n",
      "   14     A00     48,427      1.67%      \n",
      "   15     C02     47,959      1.65%      \n",
      "   16     D00     41,126      1.42%      \n",
      "   17     B21     37,783      1.30%      \n",
      "   18     A43     35,726      1.23%      \n",
      "   19     B32     33,894      1.17%      \n",
      "   20     A04     33,026      1.14%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      D62          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2      A97          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3      D98          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4      D68          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   5      D29          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   6      E78          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   7      E79          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   8      A63          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   9      A72          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   10     C76          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   11     D49          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   12     E55          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   13     D84          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   14     A99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   15     B69          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   16     C94          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   17     E96          2    ‚ñà‚ñà\n",
      "   18     A93          2    ‚ñà‚ñà\n",
      "   19     E36          1    ‚ñà\n",
      "   20     A98          1    ‚ñà\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      D62          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2      A97          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3      D98          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4      D68          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   5      D29          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   6      E78          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   7      E79          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   8      A63          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   9      A72          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   10     C76          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   11     D49          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   12     E55          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   13     D84          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   14     A99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   15     B69          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   16     C94          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   17     E96          2    ‚ñà‚ñà\n",
      "   18     A93          2    ‚ñà‚ñà\n",
      "   19     E36          1    ‚ñà\n",
      "   20     A98          1    ‚ñà\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   A07, A14, A20, A24, A58, B13, B22, B53, C03, C43, C48, C66, D36, D74, E10, E30, E33, E59, E79, E96\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   A07, A14, A20, A24, A58, B13, B22, B53, C03, C43, C48, C66, D36, D74, E10, E30, E33, E59, E79, E96\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   ‚Ä¢ Unique ECO codes: 457\n",
      "   ‚Ä¢ Total entries: 2,173,697\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 447\n",
      "   ‚Ä¢ Total entries: 434,740\n",
      "   ‚Ä¢ Unique ECO codes: 457\n",
      "   ‚Ä¢ Total entries: 2,173,697\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 447\n",
      "   ‚Ä¢ Total entries: 434,740\n",
      "   ‚Ä¢ ECO codes not in train: 1\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 444\n",
      "   ‚Ä¢ Total entries: 289,827\n",
      "   ‚Ä¢ ECO codes not in train: 1\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 444\n",
      "   ‚Ä¢ Total entries: 289,827\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C94    0.6193             4\n",
      "   B77    0.5955           550\n",
      "   D57    0.5912            23\n",
      "   B71    0.5870           229\n",
      "   E96    0.5860             2\n",
      "   D29    0.5818             8\n",
      "   B85    0.5817            15\n",
      "   D49    0.5764             6\n",
      "   E99    0.5745            25\n",
      "   C87    0.5694           180\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   A57    0.4709         4,461\n",
      "   B70    0.4703         4,154\n",
      "   C74    0.4671            23\n",
      "   A58    0.4634           949\n",
      "   C76    0.4616             6\n",
      "   C38    0.4581         1,654\n",
      "   C99    0.4370            13\n",
      "   E55    0.3855             5\n",
      "   E36    0.3500             1\n",
      "   A98    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C94    0.6193             4\n",
      "   B77    0.5955           550\n",
      "   D57    0.5912            23\n",
      "   B71    0.5870           229\n",
      "   E96    0.5860             2\n",
      "   D29    0.5818             8\n",
      "   B85    0.5817            15\n",
      "   D49    0.5764             6\n",
      "   E99    0.5745            25\n",
      "   C87    0.5694           180\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   A57    0.4709         4,461\n",
      "   B70    0.4703         4,154\n",
      "   C74    0.4671            23\n",
      "   A58    0.4634           949\n",
      "   C76    0.4616             6\n",
      "   C38    0.4581         1,654\n",
      "   C99    0.4370            13\n",
      "   E55    0.3855             5\n",
      "   E36    0.3500             1\n",
      "   A98    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   E45    0.0085       0.0923            15\n",
      "   A93    0.0069       0.0829             2\n",
      "   C57    0.0068       0.0824        11,893\n",
      "   A97    0.0061       0.0784             9\n",
      "   C37    0.0044       0.0663         5,914\n",
      "   C51    0.0038       0.0615         3,963\n",
      "   A61    0.0037       0.0608           145\n",
      "   C56    0.0036       0.0598         9,593\n",
      "   C31    0.0035       0.0595        10,986\n",
      "   C39    0.0035       0.0595         1,463\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Most common family: B (1,049,271 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (201,463 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   E45    0.0085       0.0923            15\n",
      "   A93    0.0069       0.0829             2\n",
      "   C57    0.0068       0.0824        11,893\n",
      "   A97    0.0061       0.0784             9\n",
      "   C37    0.0044       0.0663         5,914\n",
      "   C51    0.0038       0.0615         3,963\n",
      "   A61    0.0037       0.0608           145\n",
      "   C56    0.0036       0.0598         9,593\n",
      "   C31    0.0035       0.0595        10,986\n",
      "   C39    0.0035       0.0595         1,463\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Most common family: B (1,049,271 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (201,463 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: ECO Code Statistics (no mutations, just exploration)\n",
    "\n",
    "print(\"STEP 4: ECO CODE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Missing ECO values: {clean_data['eco'].isna().sum()}\")\n",
    "\n",
    "# ECO value counts\n",
    "eco_counts = clean_data['eco'].value_counts().sort_index()\n",
    "print(f\"\\n2Ô∏è‚É£  Distribution of entries per ECO code:\")\n",
    "print(f\"   ‚Ä¢ Mean entries per ECO: {eco_counts.mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Median entries per ECO: {eco_counts.median():.1f}\")\n",
    "print(f\"   ‚Ä¢ Min entries: {eco_counts.min()}\")\n",
    "print(f\"   ‚Ä¢ Max entries: {eco_counts.max()}\")\n",
    "print(f\"   ‚Ä¢ Std: {eco_counts.std():.1f}\")\n",
    "\n",
    "# ECO by first letter (family)\n",
    "print(f\"\\n3Ô∏è‚É£  ECO families (by first letter):\")\n",
    "eco_families = clean_data['eco'].str[0].value_counts().sort_index()\n",
    "print(f\"\\n   {'Family':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "for family, count in eco_families.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.4)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {family:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Top 20 most common ECO codes\n",
    "print(f\"\\n4Ô∏è‚É£  Top 20 most common ECO codes:\")\n",
    "top_eco = clean_data['eco'].value_counts().head(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*12} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(top_eco.items(), 1):\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.3)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Bottom 20 least common ECO codes\n",
    "print(f\"\\n5Ô∏è‚É£  Bottom 20 least common ECO codes:\")\n",
    "bottom_eco = clean_data['eco'].value_counts().tail(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(bottom_eco.items(), 1):\n",
    "    bar_length = min(count, 30)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {bar}\")\n",
    "\n",
    "# Check for any unusual ECO codes\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of ECO codes:\")\n",
    "sample_eco = clean_data['eco'].drop_duplicates().sample(min(20, clean_data['eco'].nunique()), random_state=42).sort_values()\n",
    "print(f\"   {', '.join(sample_eco.values)}\")\n",
    "\n",
    "# ECO statistics by split\n",
    "print(f\"\\n8Ô∏è‚É£  ECO distribution across splits:\")\n",
    "print(f\"\\n   Train split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_train['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\n   Validation split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_val['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_val):,}\")\n",
    "val_new_eco = set(X_val['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(val_new_eco)}\")\n",
    "\n",
    "print(f\"\\n   Test split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_test['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_test):,}\")\n",
    "test_new_eco = set(X_test['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(test_new_eco)}\")\n",
    "\n",
    "# Average score by ECO code (top 10 and bottom 10)\n",
    "print(f\"\\n9Ô∏è‚É£  Average score by ECO code:\")\n",
    "eco_scores = clean_data.groupby('eco')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\n   Top 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.head(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(f\"\\n   Bottom 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.tail(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# ECO codes with high variance in scores\n",
    "print(f\"\\nüîü  ECO codes with highest score variance (may indicate difficulty):\")\n",
    "eco_variance = clean_data.groupby('eco')['score'].agg(['var', 'std', 'count']).sort_values('var', ascending=False).head(10)\n",
    "print(f\"\\n   {'ECO':<6} {'Variance':<12} {'Std Dev':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_variance.iterrows():\n",
    "    print(f\"   {eco:<6} {row['var']:<12.4f} {row['std']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ECO CODE STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Key takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Most common family: {eco_families.idxmax()} ({eco_families.max():,} entries)\")\n",
    "print(f\"   ‚Ä¢ Most common ECO: {top_eco.index[0]} ({top_eco.iloc[0]:,} entries)\")\n",
    "print(f\"   ‚Ä¢ ECO codes appear in all splits (good for training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbad18",
   "metadata": {},
   "source": [
    "## 4b. Create ECO Side Information\n",
    "\n",
    "**Why ECO is Side Information:**\n",
    "- ECO codes describe **opening characteristics**, not individual player-opening interactions\n",
    "- Similar to how player ratings describe players, ECO describes openings\n",
    "- Each opening has ONE ECO code (not per player-opening pair)\n",
    "\n",
    "**Implementation Strategy:**\n",
    "- Split ECO codes into two categorical features:\n",
    "  - `eco_letter`: A, B, C, D, or E ‚Üí encoded as integers 0-4\n",
    "  - `eco_number`: The numeric part (e.g., \"21\" from \"C21\") ‚Üí encoded as sequential integers\n",
    "- Store in a separate `opening_side_info` lookup table (indexed by opening_id)\n",
    "- Remove `eco` from train/test/val DataFrames (it's not interaction data)\n",
    "- During training, model will lookup opening_id ‚Üí (eco_letter, eco_number)\n",
    "\n",
    "**Why Split ECO into Letter and Number:**\n",
    "- ECO families (A-E) represent fundamentally different opening types:\n",
    "  - **A**: Flank openings (English, R√©ti, Bird's, etc.)\n",
    "  - **B**: Semi-Open games (Sicilian, French, Caro-Kann, etc.)\n",
    "  - **C**: Open games (King's pawn openings, Spanish, Italian, etc.)\n",
    "  - **D**: Closed games (Queen's Gambit variations)\n",
    "  - **E**: Indian defenses (King's Indian, Nimzo-Indian, etc.)\n",
    "- Numbers within each family represent variations (C20-C29, C30-C39, etc.)\n",
    "- Model can learn separate embeddings for family vs variation\n",
    "\n",
    "**Categorical Encoding:**\n",
    "- Both features will be treated as categorical (not ordinal)\n",
    "- Higher numbers don't mean \"better\" openings\n",
    "- Model will learn embedding vectors for each category\n",
    "- This allows the model to capture non-linear relationships between ECO codes and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64124475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4B: CREATE ECO SIDE INFORMATION\n",
      "============================================================\n",
      "   ‚úì Extracted 2,714 unique openings\n",
      "   ‚úì Extracted 2,714 unique openings\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['B' 'C' 'A' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (578 openings)\n",
      "      'B' ‚Üí 1 (590 openings)\n",
      "      'C' ‚Üí 2 (893 openings)\n",
      "      'D' ‚Üí 3 (419 openings)\n",
      "      'E' ‚Üí 4 (234 openings)\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 290 openings\n",
      "      '40' (‚Üí 40):  98 openings\n",
      "      '20' (‚Üí 20):  70 openings\n",
      "      '01' (‚Üí  1):  70 openings\n",
      "      '02' (‚Üí  2):  67 openings\n",
      "      '42' (‚Üí 42):  67 openings\n",
      "      '45' (‚Üí 45):  64 openings\n",
      "      '10' (‚Üí 10):  62 openings\n",
      "      '21' (‚Üí 21):  61 openings\n",
      "      '15' (‚Üí 15):  58 openings\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2714, 2)\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ X_train before: (2173697, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (434740, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (289827, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   143          0               4               A04               \n",
      "   1416         2               40              C40               \n",
      "   512          0               95              A95               \n",
      "   1473         2               42              C42               \n",
      "   80           0               0               A00               \n",
      "   928          1               40              B40               \n",
      "   2332         4               70              E70               \n",
      "   773          1               17              B17               \n",
      "   2121         3               63              D63               \n",
      "   951          1               50              B50               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            578     21.30%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            590     21.74%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            893     32.90%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            419     15.44%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            234      8.62%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2714, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 2,173,697 rows, 3 features\n",
      "   ‚Ä¢ X_val: 434,740 rows, 3 features\n",
      "   ‚Ä¢ X_test: 289,827 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,714 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['B' 'C' 'A' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (578 openings)\n",
      "      'B' ‚Üí 1 (590 openings)\n",
      "      'C' ‚Üí 2 (893 openings)\n",
      "      'D' ‚Üí 3 (419 openings)\n",
      "      'E' ‚Üí 4 (234 openings)\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 290 openings\n",
      "      '40' (‚Üí 40):  98 openings\n",
      "      '20' (‚Üí 20):  70 openings\n",
      "      '01' (‚Üí  1):  70 openings\n",
      "      '02' (‚Üí  2):  67 openings\n",
      "      '42' (‚Üí 42):  67 openings\n",
      "      '45' (‚Üí 45):  64 openings\n",
      "      '10' (‚Üí 10):  62 openings\n",
      "      '21' (‚Üí 21):  61 openings\n",
      "      '15' (‚Üí 15):  58 openings\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2714, 2)\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ X_train before: (2173697, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (434740, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (289827, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   143          0               4               A04               \n",
      "   1416         2               40              C40               \n",
      "   512          0               95              A95               \n",
      "   1473         2               42              C42               \n",
      "   80           0               0               A00               \n",
      "   928          1               40              B40               \n",
      "   2332         4               70              E70               \n",
      "   773          1               17              B17               \n",
      "   2121         3               63              D63               \n",
      "   951          1               50              B50               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            578     21.30%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            590     21.74%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            893     32.90%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            419     15.44%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            234      8.62%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2714, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 2,173,697 rows, 3 features\n",
      "   ‚Ä¢ X_val: 434,740 rows, 3 features\n",
      "   ‚Ä¢ X_test: 289,827 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,714 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n"
     ]
    }
   ],
   "source": [
    "# 4b. Create ECO side information and remove ECO from train/test/val DataFrames\n",
    "\n",
    "# Check if ECO processing has already been done\n",
    "if 'eco' not in X_train.columns and 'opening_side_info' in globals():\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 4B: ECO SIDE INFORMATION CREATION\")\n",
    "    print(\"\\n‚úì ECO column already removed from train/test/val data\")\n",
    "    print(f\"\\nOpening side info shape: {opening_side_info.shape}\")\n",
    "    print(f\"Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "    print(\"opening side info:\", opening_side_info.head().to_string())\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing ECO encoding statistics:\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_letter values: {opening_side_info['eco_letter_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_number values: {opening_side_info['eco_number_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ eco_letter range: [{opening_side_info['eco_letter_cat'].min()}, {opening_side_info['eco_letter_cat'].max()}]\")\n",
    "    print(f\"   ‚Ä¢ eco_number range: [{opening_side_info['eco_number_cat'].min()}, {opening_side_info['eco_number_cat'].max()}]\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing ECO encoding:\")\n",
    "    sample_data = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "    print(sample_data.to_string())\n",
    "else:\n",
    "    def create_eco_side_information(clean_data_df, X_train_df, X_val_df, X_test_df):\n",
    "        \"\"\"\n",
    "        Create ECO side information table and remove ECO from train/test/val DataFrames.\n",
    "        \n",
    "        ECO codes are opening-level features, not player-opening interaction features.\n",
    "        We split each ECO code (e.g., \"C21\") into:\n",
    "        - eco_letter: The letter part (A, B, C, D, or E)\n",
    "        - eco_number: The numeric part (e.g., 21)\n",
    "        \n",
    "        Both are encoded as sequential integers for use as categorical features in embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        clean_data_df : pd.DataFrame\n",
    "            Full cleaned data with opening_id and eco columns\n",
    "        X_train_df, X_val_df, X_test_df : pd.DataFrame\n",
    "            Train/val/test feature DataFrames (will be modified to remove 'eco')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 4B: CREATE ECO SIDE INFORMATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Extract unique opening ‚Üí ECO mappings\n",
    "        opening_eco_map = clean_data_df[['opening_id', 'eco']].drop_duplicates().set_index('opening_id')\n",
    "        print(f\"   ‚úì Extracted {len(opening_eco_map):,} unique openings\")\n",
    "        \n",
    "        # Verify one-to-one mapping\n",
    "        eco_per_opening = clean_data_df.groupby('opening_id')['eco'].nunique()\n",
    "        if (eco_per_opening > 1).any():\n",
    "            problematic = eco_per_opening[eco_per_opening > 1]\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(problematic)} openings have multiple ECO codes!\")\n",
    "            print(f\"   Problematic opening IDs: {problematic.index.tolist()[:10]}...\")\n",
    "        \n",
    "        # Split ECO into letter and number components\n",
    "        opening_eco_map['eco_letter_str'] = opening_eco_map['eco'].str[0]  # First character (A-E)\n",
    "        opening_eco_map['eco_number_str'] = opening_eco_map['eco'].str[1:]  # Remaining characters (numeric)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted letter and number components\")\n",
    "        print(f\"   ‚Ä¢ Unique letters: {opening_eco_map['eco_letter_str'].unique()}\")\n",
    "        print(f\"   ‚Ä¢ Unique numbers: {opening_eco_map['eco_number_str'].nunique()}\")\n",
    "        \n",
    "        # Create encoding mappings for eco_letter (A-E ‚Üí 0-4)\n",
    "        unique_letters = sorted(opening_eco_map['eco_letter_str'].unique())\n",
    "        eco_letter_to_int = {letter: idx for idx, letter in enumerate(unique_letters)}\n",
    "        eco_int_to_letter = {idx: letter for letter, idx in eco_letter_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_letter_cat'] = opening_eco_map['eco_letter_str'].map(eco_letter_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Letter encoding created:\")\n",
    "        for letter, idx in sorted(eco_letter_to_int.items()):\n",
    "            count = (opening_eco_map['eco_letter_str'] == letter).sum()\n",
    "            print(f\"      '{letter}' ‚Üí {idx} ({count:,} openings)\")\n",
    "        \n",
    "        # Create encoding mappings for eco_number (00-99 ‚Üí sequential integers)\n",
    "        unique_numbers = sorted(opening_eco_map['eco_number_str'].unique())\n",
    "        eco_number_to_int = {num: idx for idx, num in enumerate(unique_numbers)}\n",
    "        eco_int_to_number = {idx: num for num, idx in eco_number_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_number_cat'] = opening_eco_map['eco_number_str'].map(eco_number_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Number encoding created:\")\n",
    "        print(f\"      {len(unique_numbers)} unique numbers mapped to [0, {len(unique_numbers)-1}]\")\n",
    "        print(f\"      Range: '{unique_numbers[0]}' ‚Üí 0, ..., '{unique_numbers[-1]}' ‚Üí {len(unique_numbers)-1}\")\n",
    "        \n",
    "        # Show distribution of numbers\n",
    "        print(f\"\\n   Distribution of ECO numbers (top 10):\")\n",
    "        number_counts = opening_eco_map['eco_number_str'].value_counts().head(10)\n",
    "        for num, count in number_counts.items():\n",
    "            encoded = eco_number_to_int[num]\n",
    "            print(f\"      '{num}' (‚Üí {encoded:>2}): {count:>3} openings\")\n",
    "        \n",
    "        # Create final opening_side_info table\n",
    "        # Only keep the encoded categorical columns and rename them for clarity\n",
    "        opening_side_info = opening_eco_map[['eco_letter_cat', 'eco_number_cat']].copy()\n",
    "\n",
    "        print(f\"   ‚úì Created opening_side_info\")\n",
    "        print(f\"      ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"      ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "        # Verify all openings in train/val/test have ECO info\n",
    "        all_openings = set(X_train_df['opening_id'].unique()) | \\\n",
    "                       set(X_val_df['opening_id'].unique()) | \\\n",
    "                       set(X_test_df['opening_id'].unique())\n",
    "        \n",
    "        missing_openings = all_openings - set(opening_side_info.index)\n",
    "        if len(missing_openings) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits are missing ECO info!\")\n",
    "            print(f\"   Missing opening IDs: {sorted(list(missing_openings))[:10]}...\")\n",
    "        \n",
    "        # Remove ECO from train/val/test DataFrames\n",
    "        print(f\"   ‚Ä¢ X_train before: {X_train_df.shape}, columns: {list(X_train_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val before: {X_val_df.shape}, columns: {list(X_val_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test before: {X_test_df.shape}, columns: {list(X_test_df.columns)}\")\n",
    "        \n",
    "        X_train_clean = X_train_df.drop(columns=['eco'])\n",
    "        X_val_clean = X_val_df.drop(columns=['eco'])\n",
    "        X_test_clean = X_test_df.drop(columns=['eco'])\n",
    "        \n",
    "        # Sample data showing the transformation\n",
    "        print(f\"\\n8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\")\n",
    "        sample_openings = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "        \n",
    "        # For display purposes, reconstruct original values from the categorical encodings\n",
    "        print(f\"\\n   {'Opening ID':<12} {'Letter (Cat)':<15} {'Number (Cat)':<15} {'Reconstructed ECO':<18}\")\n",
    "        print(f\"   {'-'*12} {'-'*15} {'-'*15} {'-'*18}\")\n",
    "        for idx, row in sample_openings.iterrows():\n",
    "            letter_str = eco_int_to_letter[row['eco_letter_cat']]\n",
    "            number_str = eco_int_to_number[row['eco_number_cat']]\n",
    "            reconstructed = f\"{letter_str}{number_str}\"\n",
    "            print(f\"   {idx:<12} {row['eco_letter_cat']:<15} {row['eco_number_cat']:<15} {reconstructed:<18}\")\n",
    "        \n",
    "        # Show ECO family distribution\n",
    "        print(f\"\\n9Ô∏è‚É£  ECO family distribution in opening_side_info:\")\n",
    "        letter_dist = opening_side_info['eco_letter_cat'].value_counts().sort_index()\n",
    "        print(f\"\\n   {'Encoded':<8} {'Letter':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "        print(f\"   {'-'*8} {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "        for encoded_val, count in letter_dist.items():\n",
    "            letter = eco_int_to_letter[encoded_val]\n",
    "            pct = 100 * count / len(opening_side_info)\n",
    "            bar_length = int(pct * 0.4)\n",
    "            bar = '‚ñà' * bar_length\n",
    "            print(f\"   {encoded_val:<8} {letter:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "        \n",
    "        print(f\"\\nCreated: opening_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape[0]:,} rows, {X_train_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape[0]:,} rows, {X_val_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape[0]:,} rows, {X_test_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ opening_side_info: {len(opening_side_info):,} openings (one per opening)\")\n",
    "        print(f\"   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\")\n",
    "        print(f\"   ‚Ä¢ eco_letter_to_int: {eco_letter_to_int}\")\n",
    "        print(f\"   ‚Ä¢ eco_number_to_int: (dict with {len(eco_number_to_int)} entries)\")\n",
    "        print(f\"\\n   You'll need them to encode ECO codes for new openings at inference time.\")\n",
    "\n",
    "        return opening_side_info, eco_letter_to_int, eco_number_to_int, X_train_clean, X_val_clean, X_test_clean\n",
    "    \n",
    "    # Call the function\n",
    "    opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test = create_eco_side_information(\n",
    "        clean_data, X_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Create reverse mappings for decoding (needed for verification and debugging)\n",
    "    eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "    eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "    print(f\"\\n‚úì Created reverse mappings for ECO decoding:\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_letter: {len(eco_int_to_letter)} entries\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_number: {len(eco_int_to_number)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICATION: FINAL DATA STRUCTURE\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\n",
      "\n",
      "   X_train:\n",
      "   ‚Ä¢ Shape: (2173697, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ Sample:\n",
      "         player_id  opening_id  confidence\n",
      "1267073      20824        1934    0.180328\n",
      "1826359      29929         575    0.193548\n",
      "2890885      48268        2666    0.193548\n",
      "\n",
      "   X_val:\n",
      "   ‚Ä¢ Shape: (434740, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "   X_test:\n",
      "   ‚Ä¢ Shape: (289827, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Side Information Tables:\n",
      "\n",
      "   player_side_info (indexed by player_id):\n",
      "   ‚Ä¢ Shape: (48474, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Sample:\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.609632\n",
      "1          1.059177\n",
      "2          0.561467\n",
      "\n",
      "   opening_side_info (indexed by opening_id):\n",
      "   ‚Ä¢ Shape: (2714, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Sample:\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "531                      1               0\n",
      "565                      1               0\n",
      "572                      1               0\n",
      "\n",
      "3Ô∏è‚É£  Encoding Mappings (for inference):\n",
      "\n",
      "   eco_letter_map:\n",
      "      'A' ‚Üí 0\n",
      "      'B' ‚Üí 1\n",
      "      'C' ‚Üí 2\n",
      "      'D' ‚Üí 3\n",
      "      'E' ‚Üí 4\n",
      "\n",
      "   eco_number_map (first 10):\n",
      "      '00' ‚Üí 0\n",
      "      '01' ‚Üí 1\n",
      "      '02' ‚Üí 2\n",
      "      '03' ‚Üí 3\n",
      "      '04' ‚Üí 4\n",
      "      '05' ‚Üí 5\n",
      "      '06' ‚Üí 6\n",
      "      '07' ‚Üí 7\n",
      "      '08' ‚Üí 8\n",
      "      '09' ‚Üí 9\n",
      "      ... (100 total)\n",
      "\n",
      "4Ô∏è‚É£  Example: Lookup flow for a random train sample:\n",
      "\n",
      "   Sample interaction:\n",
      "   ‚Ä¢ player_id: 15899.0\n",
      "   ‚Ä¢ opening_id: 49.0\n",
      "   ‚Ä¢ confidence: 0.2063\n",
      "\n",
      "   Player side info lookup:\n",
      "   ‚Ä¢ rating_z: -0.4420\n",
      "\n",
      "   Opening side info lookup:\n",
      "   ‚Ä¢ eco_letter_cat: 0 (letter: A)\n",
      "   ‚Ä¢ eco_number_cat: 0 (number: 00)\n",
      "\n",
      "============================================================\n",
      "‚úÖ VERIFICATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Ready for PyTorch tensor conversion:\n",
      "   ‚Ä¢ Features: player_id, opening_id, confidence\n",
      "   ‚Ä¢ Target: score (in y_train, y_val, y_test)\n",
      "   ‚Ä¢ Player side info: rating_z\n",
      "   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\n",
      "\n",
      "   All ECO and rating data is now properly separated as side information!\n",
      "\n",
      "   Side info tables are clean - only contain necessary model inputs!\n"
     ]
    }
   ],
   "source": [
    "# Verify final data structure after ECO processing\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION: FINAL DATA STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\")\n",
    "print(f\"\\n   X_train:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_train.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(X_train.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   X_val:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_val.columns)}\")\n",
    "\n",
    "print(f\"\\n   X_test:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_test.columns)}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Side Information Tables:\")\n",
    "\n",
    "print(f\"\\n   player_side_info (indexed by player_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(player_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   opening_side_info (indexed by opening_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(opening_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Encoding Mappings (for inference):\")\n",
    "print(f\"\\n   eco_letter_map:\")\n",
    "for k, v in sorted(eco_letter_map.items()):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "\n",
    "print(f\"\\n   eco_number_map (first 10):\")\n",
    "for i, (k, v) in enumerate(sorted(eco_number_map.items())[:10]):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "print(f\"      ... ({len(eco_number_map)} total)\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Example: Lookup flow for a random train sample:\")\n",
    "sample = X_train.sample(1, random_state=42).iloc[0]\n",
    "player_id = sample['player_id']\n",
    "opening_id = sample['opening_id']\n",
    "\n",
    "print(f\"\\n   Sample interaction:\")\n",
    "print(f\"   ‚Ä¢ player_id: {player_id}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {opening_id}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Player side info lookup:\")\n",
    "player_info = player_side_info.loc[player_id]\n",
    "print(f\"   ‚Ä¢ rating_z: {player_info['rating_z']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Opening side info lookup:\")\n",
    "opening_info = opening_side_info.loc[opening_id]\n",
    "print(f\"   ‚Ä¢ eco_letter_cat: {opening_info['eco_letter_cat']} (letter: {eco_int_to_letter[opening_info['eco_letter_cat']]})\")\n",
    "print(f\"   ‚Ä¢ eco_number_cat: {opening_info['eco_number_cat']} (number: {eco_int_to_number[opening_info['eco_number_cat']]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Ready for PyTorch tensor conversion:\")\n",
    "print(f\"   ‚Ä¢ Features: player_id, opening_id, confidence\")\n",
    "print(f\"   ‚Ä¢ Target: score (in y_train, y_val, y_test)\")\n",
    "print(f\"   ‚Ä¢ Player side info: rating_z\")\n",
    "print(f\"   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\")\n",
    "print(f\"\\n   All ECO and rating data is now properly separated as side information!\")\n",
    "print(f\"\\n   Side info tables are clean - only contain necessary model inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43752aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "============================================================\n",
      "STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\n",
      "============================================================\n",
      "============================================================\n",
      "STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\n",
      "============================================================\n",
      "Final DataFrame for correlation created.\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'score', 'confidence', 'rating_z', 'eco_letter_cat', 'eco_number_cat']\n",
      "Final DataFrame for correlation created.\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'score', 'confidence', 'rating_z', 'eco_letter_cat', 'eco_number_cat']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRELATIONS WITH TARGET VARIABLE (score)\n",
      "================================================================================\n",
      "eco_letter_cat    0.116320\n",
      "confidence        0.097907\n",
      "eco_number_cat    0.055879\n",
      "rating_z          0.043163\n"
     ]
    }
   ],
   "source": [
    "# Step 4c: Correlation Analysis of Processed Data\n",
    "\n",
    "%pip install seaborn --quiet\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Start with the main interaction data (already has remapped IDs, adjusted score, confidence)\n",
    "corr_df = clean_data[[\"player_id\", \"opening_id\", \"score\", \"confidence\"]].copy()\n",
    "\n",
    "# Merge player side information (rating_z)\n",
    "# player_side_info is indexed by the remapped player_id\n",
    "corr_df = corr_df.merge(\n",
    "    player_side_info[[\"rating_z\"]], left_on=\"player_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# Merge opening side information (eco categories)\n",
    "# opening_side_info is indexed by the remapped opening_id\n",
    "corr_df = corr_df.merge(\n",
    "    opening_side_info[[\"eco_letter_cat\", \"eco_number_cat\"]],\n",
    "    left_on=\"opening_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(f\"Final DataFrame for correlation created.\")\n",
    "print(f\"   ‚Ä¢ Columns: {corr_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "correlation_matrix = corr_df.corr().drop(columns=['player_id', 'opening_id']).drop(index=['player_id', 'opening_id'])\n",
    "\n",
    "\n",
    "# 3. Visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Model Features\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 4. Analyze correlations with the target variable 'score'\n",
    "print(\"CORRELATIONS WITH TARGET VARIABLE (score)\")\n",
    "print(\"=\" * 80)\n",
    "score_correlations = (\n",
    "    correlation_matrix[\"score\"].drop(\"score\").sort_values(ascending=False)\n",
    ")\n",
    "print(score_correlations.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831d7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017515d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\n",
      "====================================================================================================\n",
      "   ‚Ä¢ Converting 86 NEW opening IDs to OLD database IDs for query\n",
      "   ‚Ä¢ Example: NEW ID 49 ‚Üí OLD ID 66\n",
      "#    Player   Opening   ECO (DB)   Reconstructed Match  Opening Name                                      \n",
      "====================================================================================================\n",
      "1    15899    49        A00        A00           ‚úì      Mieses Opening: Reversed Rat                      \n",
      "2    33837    535       B00        B00           ‚úì      Nimzowitsch Defense: Franco-Nimzowitsch Varia...  \n",
      "3    36622    1872      D02        D02           ‚úì      Queen's Pawn Game: Krause Variation               \n",
      "4    22389    1156      C15        C15           ‚úì      French Defense: Winawer Variation                 \n",
      "5    20961    2647      A40        A40           ‚úì      Englund Gambit: Hartlaub-Charlick Gambit          \n",
      "6    30694    621       B02        B02           ‚úì      Alekhine Defense: Scandinavian Variation, Ges...  \n",
      "7    31328    1429      C41        C41           ‚úì      Philidor Defense                                  \n",
      "8    34224    1684      C60        C60           ‚úì      Ruy Lopez: N√ºrnberg Variation                     \n",
      "9    2795     2460      A40        A40           ‚úì      Queen's Pawn Game: Modern Defense                 \n",
      "10   30206    1457      C42        C42           ‚úì      Russian Game                                      \n",
      "11   1426     66        A00        A00           ‚úì      Polish Opening: Schiffler-Sokolsky Variation      \n",
      "12   13897    2456      C54        C54           ‚úì      Italian Game: Classical Variation, Greco Gamb...  \n",
      "13   11742    1602      C50        C50           ‚úì      Italian Game: Rousseau Gambit                     \n",
      "14   24984    531       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "15   44252    1597      C50        C50           ‚úì      Italian Game: Giuoco Pianissimo, Normal           \n",
      "16   41342    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "17   139      336       A43        A43           ‚úì      Benoni Defense: Benoni Gambit Accepted            \n",
      "18   25849    533       B00        B00           ‚úì      Nimzowitsch Defense: Declined Variation           \n",
      "19   23034    336       A43        A43           ‚úì      Benoni Defense: Benoni Gambit Accepted            \n",
      "20   11137    571       B00        B00           ‚úì      Rat Defense: Antal Defense                        \n",
      "21   34525    188       A10        A10           ‚úì      English Opening: Anglo-Scandinavian Defense       \n",
      "22   39523    32        A00        A00           ‚úì      Hungarian Opening: Indian Defense                 \n",
      "23   8867     2488      C20        C20           ‚úì      King's Pawn Game: MacLeod Attack                  \n",
      "24   38079    579       B01        B01           ‚úì      Scandinavian Defense                              \n",
      "25   6666     1853      D00        D00           ‚úì      Queen's Pawn Game: Levitsky Attack                \n",
      "26   16016    599       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "27   6010     594       B01        B01           ‚úì      Scandinavian Defense: Main Line                   \n",
      "28   35456    596       B01        B01           ‚úì      Scandinavian Defense: Main Line, Mieses Varia...  \n",
      "29   6238     1515      C44        C44           ‚úì      Scotch Game: Benima Defense                       \n",
      "30   31327    2455      C54        C54           ‚úì      Italian Game: Classical Variation, Giuoco Pia...  \n",
      "31   18976    1881      D05        D05           ‚úì      Queen's Pawn Game: Colle System                   \n",
      "32   28212    1744      C70        C70           ‚úì      Ruy Lopez: Morphy Defense, Caro Variation         \n",
      "33   10078    523       B00        B00           ‚úì      Duras Gambit                                      \n",
      "34   15711    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "35   20430    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "36   16868    1565      C47        C47           ‚úì      Four Knights Game: Italian Variation              \n",
      "37   35947    2501      B21        B21           ‚úì      Sicilian Defense: Smith-Morra Gambit Declined...  \n",
      "38   38950    1487      C43        C43           ‚úì      Russian Game: Modern Attack                       \n",
      "39   4099     729       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Botvinn...  \n",
      "40   22624    1541      C45        C45           ‚úì      Scotch Game: Classical Variation, Intermezzo ...  \n",
      "41   8359     1352      C34        C34           ‚úì      King's Gambit Accepted: MacLeod Defense           \n",
      "42   32218    1566      C47        C47           ‚úì      Four Knights Game: Italian Variation, Noa Gambit  \n",
      "43   10949    1557      C46        C46           ‚úì      Three Knights Opening                             \n",
      "44   24722    2480      D00        D00           ‚úì      Queen's Pawn Game: Accelerated London System,...  \n",
      "45   22328    38        A00        A00           ‚úì      Hungarian Opening: Slav Formation                 \n",
      "46   25888    957       B51        B51           ‚úì      Sicilian Defense: Moscow Variation                \n",
      "47   17402    1058      C00        C00           ‚úì      French Defense: Knight Variation                  \n",
      "48   15308    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "49   46857    585       B01        B01           ‚úì      Scandinavian Defense: Bronstein Variation         \n",
      "50   14304    1059      C00        C00           ‚úì      French Defense: La Bourdonnais Variation          \n",
      "51   37397    599       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "52   11730    295       A40        A40           ‚úì      English Defense                                   \n",
      "53   16551    807       B21        B21           ‚úì      Sicilian Defense: Morphy Gambit, Andreaschek ...  \n",
      "54   28366    1079      C02        C02           ‚úì      French Defense: Advance Variation                 \n",
      "55   29539    1584      C49        C49           ‚úì      Four Knights Game: Double Spanish                 \n",
      "56   25746    599       B01        B01           ‚úì      Scandinavian Defense: Modern Variation            \n",
      "57   24140    2185      E06        E06           ‚úì      Catalan Opening: Closed                           \n",
      "58   877      2424      C54        C54           ‚úì      Italian Game: Classical Variation, Giuoco Pia...  \n",
      "59   41987    1075      C00        C00           ‚úì      Rat Defense: Small Center Defense                 \n",
      "60   10426    1383      C38        C38           ‚úì      King's Gambit Accepted: Traditional Variation     \n",
      "61   21699    302       A40        A40           ‚úì      Englund Gambit Complex: Hartlaub-Charlick Gambit  \n",
      "62   29297    631       B03        B03           ‚úì      Alekhine Defense                                  \n",
      "63   2012     47        A00        A00           ‚úì      Mieses Opening                                    \n",
      "64   6453     1485      C43        C43           ‚úì      Bishop's Opening: Urusov Gambit                   \n",
      "65   42065    1408      C40        C40           ‚úì      King's Pawn Game: Busch-Gass Gambit               \n",
      "66   6954     2063      D43        D43           ‚úì      Semi-Slav Defense                                 \n",
      "67   28445    1481      C42        C42           ‚úì      Russian Game: Nimzowitsch Attack                  \n",
      "68   41878    2454      C56        C56           ‚úì      Italian Game: Two Knights Defense, Open Varia...  \n",
      "69   14422    711       B10        B10           ‚úì      Caro-Kann Defense: Breyer Variation               \n",
      "70   17936    523       B00        B00           ‚úì      Duras Gambit                                      \n",
      "71   34273    1678      C60        C60           ‚úì      Ruy Lopez                                         \n",
      "72   3742     419       A54        A54           ‚úì      Old Indian Defense: Ukrainian Variation           \n",
      "73   45017    1589      C50        C50           ‚úì      Italian Game: Anti-Fried Liver Defense            \n",
      "74   10217    596       B01        B01           ‚úì      Scandinavian Defense: Main Line, Mieses Varia...  \n",
      "75   19408    501       A84        A84           ‚úì      Dutch Defense: Rubinstein Variation               \n",
      "76   720      1272      C28        C28           ‚úì      Vienna Game: Stanley Variation, Three Knights...  \n",
      "77   10995    2049      D37        D37           ‚úì      Queen's Gambit Declined: Three Knights Variation  \n",
      "78   44942    898       B32        B32           ‚úì      Sicilian Defense: Open                            \n",
      "79   2981     2455      C54        C54           ‚úì      Italian Game: Classical Variation, Giuoco Pia...  \n",
      "80   1259     655       B06        B06           ‚úì      Modern Defense: Bishop Attack                     \n",
      "81   13663    579       B01        B01           ‚úì      Scandinavian Defense                              \n",
      "82   34067    565       B00        B00           ‚úì      Owen Defense                                      \n",
      "83   26834    1907      D08        D08           ‚úì      Queen's Gambit Declined: Albin Countergambit      \n",
      "84   46239    653       B06        B06           ‚úì      Modern Defense                                    \n",
      "85   15579    653       B06        B06           ‚úì      Modern Defense                                    \n",
      "86   14237    239       A19        A19           ‚úì      English Opening: Mikenas-Carls, Sicilian          \n",
      "87   3149     598       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "88   7679     584       B01        B01           ‚úì      Scandinavian Defense: Boehnke Gambit              \n",
      "89   16618    1919      D10        D10           ‚úì      Slav Defense: Slav Gambit, Alekhine Attack        \n",
      "90   34855    525       B00        B00           ‚úì      Goldsmith Defense                                 \n",
      "91   22284    1568      C47        C47           ‚úì      Four Knights Game: Scotch Variation Accepted      \n",
      "92   43413    598       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "93   1064     1827      D00        D00           ‚úì      Blackmar-Diemer Gambit: Euwe Defense              \n",
      "94   4031     1238      C25        C25           ‚úì      Vienna Game                                       \n",
      "95   10861    653       B06        B06           ‚úì      Modern Defense                                    \n",
      "96   4456     1529      C44        C44           ‚úì      Scotch Game: Scotch Gambit, Dubois R√©ti Defense   \n",
      "97   33676    1589      C50        C50           ‚úì      Italian Game: Anti-Fried Liver Defense            \n",
      "98   16907    2650      C42        C42           ‚úì      Petrov's Defense                                  \n",
      "99   37084    789       B20        B20           ‚úì      Sicilian Defense: Lasker-Dunne Attack             \n",
      "100  12495    1300      C31        C31           ‚úì      King's Gambit Declined: Falkbeer Countergambi...  \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Verification Results:\n",
      "   ‚Ä¢ Total samples: 100\n",
      "   ‚Ä¢ Matches: 100/100 (100.0%)\n",
      "   ‚Ä¢ Mismatches: 0\n",
      "\n",
      " All ECO codes reconstructed correctly\n",
      "   ‚Ä¢ ID remapping preserved all ECO code mappings\n"
     ]
    }
   ],
   "source": [
    "# Verification: Sample player-opening pairs with reconstructed ECO codes and opening names\n",
    "# Doing this to make sure that our ECO encoding/decoding is correct\n",
    "\n",
    "print(\"VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Sample random player-opening pairs from training data\n",
    "sample_size = 100\n",
    "sample_data = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "\n",
    "# Get unique opening IDs from sample (these are NEW remapped IDs)\n",
    "new_opening_ids = sample_data['opening_id'].unique()\n",
    "\n",
    "# Convert NEW opening IDs back to OLD database IDs for query\n",
    "if opening_remapped:\n",
    "    old_opening_ids = [opening_idx_to_id[int(new_id)] for new_id in new_opening_ids]\n",
    "    print(f\"   ‚Ä¢ Converting {len(new_opening_ids)} NEW opening IDs to OLD database IDs for query\")\n",
    "    print(f\"   ‚Ä¢ Example: NEW ID {new_opening_ids[0]} ‚Üí OLD ID {old_opening_ids[0]}\")\n",
    "else:\n",
    "    old_opening_ids = new_opening_ids\n",
    "    print(f\"   ‚Ä¢ No remapping was done - using opening IDs directly\")\n",
    "\n",
    "opening_ids_str = ','.join(map(str, old_opening_ids))\n",
    "\n",
    "# Query database for opening names using OLD IDs\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    opening_query = f\"\"\"\n",
    "        SELECT id, name, eco\n",
    "        FROM opening\n",
    "        WHERE id IN ({opening_ids_str})\n",
    "    \"\"\"\n",
    "    opening_names = pd.DataFrame(con.execute(opening_query).df()).set_index('id')\n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Create reverse mappings for ECO decoding\n",
    "eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "\n",
    "# Build verification table\n",
    "print(f\"{'#':<4} {'Player':<8} {'Opening':<9} {'ECO (DB)':<10} {'Reconstructed':<13} {'Match':<6} {'Opening Name':<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "matches = 0\n",
    "for i, (idx, row) in enumerate(sample_data.iterrows(), 1):\n",
    "    player_id = int(row['player_id'])\n",
    "    new_opening_id = int(row['opening_id'])\n",
    "    \n",
    "    # Convert NEW opening ID to OLD database ID for lookup\n",
    "    if opening_remapped:\n",
    "        old_opening_id = opening_idx_to_id[new_opening_id]\n",
    "    else:\n",
    "        old_opening_id = new_opening_id\n",
    "    \n",
    "    # Lookup opening side info using NEW ID\n",
    "    opening_info = opening_side_info.loc[new_opening_id]\n",
    "    \n",
    "    # Reconstruct ECO from encoded categorical values\n",
    "    eco_letter_encoded = opening_info['eco_letter_cat']\n",
    "    eco_number_encoded = opening_info['eco_number_cat']\n",
    "    \n",
    "    eco_letter_decoded = eco_int_to_letter[eco_letter_encoded]\n",
    "    eco_number_decoded = eco_int_to_number[eco_number_encoded]\n",
    "    \n",
    "    reconstructed_eco = f\"{eco_letter_decoded}{eco_number_decoded}\"\n",
    "    \n",
    "    # Get original ECO from database using OLD ID\n",
    "    db_eco = opening_names.loc[old_opening_id, 'eco']\n",
    "    opening_name = opening_names.loc[old_opening_id, 'name']\n",
    "    \n",
    "    # Check if they match\n",
    "    match = \"‚úì\" if reconstructed_eco == db_eco else \"‚úó\"\n",
    "    if reconstructed_eco == db_eco:\n",
    "        matches += 1\n",
    "    \n",
    "    # Truncate opening name if too long\n",
    "    if len(opening_name) > 48:\n",
    "        opening_name = opening_name[:45] + \"...\"\n",
    "    \n",
    "    # Display using NEW opening ID (what's in the data now)\n",
    "    print(f\"{i:<4} {player_id:<8} {new_opening_id:<9} {db_eco:<10} {reconstructed_eco:<13} {match:<6} {opening_name:<50}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n‚úÖ Verification Results:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(sample_data)}\")\n",
    "print(f\"   ‚Ä¢ Matches: {matches}/{len(sample_data)} ({100*matches/len(sample_data):.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Mismatches: {len(sample_data) - matches}\")\n",
    "\n",
    "if matches == len(sample_data):\n",
    "    print(f\"\\n All ECO codes reconstructed correctly\")\n",
    "    print(f\"   ‚Ä¢ ID remapping preserved all ECO code mappings\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Some ECO codes did not match. Investigate mismatches above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf4878",
   "metadata": {},
   "source": [
    "## 5. Data Verification and Examination\n",
    "We're almost there. Let's examine our data structures to check for any obvious flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train \n",
      "          player_id  opening_id  confidence\n",
      "1267073      20824        1934    0.180328\n",
      "1826359      29929         575    0.193548\n",
      "2890885      48268        2666    0.193548\n",
      "1510986      24769         783    0.315068\n",
      "2747694      45239        2608    0.206349\n",
      "============================================================\n",
      "X_val \n",
      "          player_id  opening_id  confidence\n",
      "676088       10983         760    0.180328\n",
      "417124        6792        1579    0.484536\n",
      "1982133      32489         364    0.333333\n",
      "1429686      23461        1429    0.879808\n",
      "1706542      27934        1217    0.572650\n",
      "============================================================\n",
      "X_test \n",
      "          player_id  opening_id  confidence\n",
      "2638226      43277        2042    0.285714\n",
      "320845        5232         653    0.771689\n",
      "2891944      48297        2427    0.444444\n",
      "2469145      40522         727    0.253731\n",
      "1829787      29986         599    0.206349\n",
      "============================================================\n",
      "y_train \n",
      " 1267073    0.586818\n",
      "1826359    0.494413\n",
      "2890885    0.515483\n",
      "1510986    0.552534\n",
      "2747694    0.498011\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_val \n",
      " 676088     0.529289\n",
      "417124     0.556793\n",
      "1982133    0.476560\n",
      "1429686    0.555960\n",
      "1706542    0.406074\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_test \n",
      " 2638226    0.505246\n",
      "320845     0.570685\n",
      "2891944    0.560729\n",
      "2469145    0.500042\n",
      "1829787    0.474225\n",
      "Name: score, dtype: float64\n",
      "player_side_info \n",
      "            rating_z\n",
      "player_id          \n",
      "0          0.609632\n",
      "1          1.059177\n",
      "2          0.561467\n",
      "3          0.617660\n",
      "4         -2.167913\n",
      "============================================================\n",
      "opening_side_info \n",
      "             eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "531                      1               0\n",
      "565                      1               0\n",
      "572                      1               0\n",
      "579                      1               1\n",
      "589                      1               1\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train \\n\", X_train.head())\n",
    "print(\"=\"*60)\n",
    "print(\"X_val \\n\", X_val.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"X_test \\n\", X_test.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_train \\n\", y_train.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_val \\n\", y_val.head())\n",
    "print(\"=\" * 60) \n",
    "print(\"y_test \\n\", y_test.head())\n",
    "\n",
    "# Now side information\n",
    "print(\"player_side_info \\n\", player_side_info.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"opening_side_info \\n\", opening_side_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52936cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANED SIDE INFORMATION TABLES\n",
      "============================================================\n",
      "\n",
      "üìä player_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (48474, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Index: player_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.609632\n",
      "1          1.059177\n",
      "2          0.561467\n",
      "3          0.617660\n",
      "4         -2.167913\n",
      "\n",
      "üìä opening_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (2714, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Index: opening_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "531                      1               0\n",
      "565                      1               0\n",
      "572                      1               0\n",
      "579                      1               1\n",
      "589                      1               1\n"
     ]
    }
   ],
   "source": [
    "# Final verification: Display cleaned side info tables\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANED SIDE INFORMATION TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä player_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: player_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(player_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\nüìä opening_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: opening_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(opening_side_info.head().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19883cba",
   "metadata": {},
   "source": [
    "## Step 5: Convert to PyTorch Tensors\n",
    "\n",
    "**What are tensors?**\n",
    "Tensors are PyTorch's version of arrays - just multi-dimensional data structures optimized for deep learning. Think of them like fancy NumPy arrays that can run on GPUs.\n",
    "\n",
    "**What we need to convert:**\n",
    "\n",
    "1. **Main features** (X_train, X_val, X_test):\n",
    "   - `player_id` ‚Üí long tensor (integer IDs)\n",
    "   - `opening_id` ‚Üí long tensor (integer IDs)\n",
    "   - `confidence` ‚Üí float tensor (weights for loss function)\n",
    "\n",
    "2. **Targets** (y_train, y_val, y_test):\n",
    "   - `score` ‚Üí float tensor (what we're predicting)\n",
    "\n",
    "3. **Player side info** (player_side_info):\n",
    "   - `rating_z` ‚Üí float tensor (normalized ratings)\n",
    "   - Indexed by player_id for fast lookup\n",
    "\n",
    "4. **Opening side info** (opening_side_info):\n",
    "   - `eco_letter_cat` ‚Üí long tensor (categorical)\n",
    "   - `eco_number_cat` ‚Üí long tensor (categorical)\n",
    "   - Indexed by opening_id for fast lookup\n",
    "\n",
    "**Why these data types?**\n",
    "- `long` (int64): For IDs and categorical features that will be embedded\n",
    "- `float` (float32): For continuous values like scores, confidence, and normalized ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350be514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4c/gt84bby56mqdnzwsg72zvd680000gn/T/ipykernel_15684/1794647647.py\", line 6, in <module>\n",
      "    import torch\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# It says subprocess is unused but complains at me if I don't import it\n",
    "# Something to do with incompatible versions of blah blah blah\n",
    "# Don't really care as long as it works\n",
    "import subprocess\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda01b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: CONVERT TO PYTORCH TENSORS\n",
      "============================================================\n",
      "   ‚Ä¢ CUDA available: False\n",
      "   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\n",
      "\n",
      "0Ô∏è‚É£  Index alignment sanity checks...\n",
      "   ‚úì player_side_info index is sorted\n",
      "   ‚Ä¢ Index range: [0, 48473]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\n",
      "   ‚Ä¢ Index range: [0, 2713]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "\n",
      "   Checking if indices are contiguous 0-based...\n",
      "   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\n",
      "   ‚Ä¢ Training tensors:\n",
      "   ‚Ä¢ player_ids_train: torch.Size([2173697]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_train: torch.Size([2173697]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_train: torch.Size([2173697]), dtype=torch.float32\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([434740]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([434740]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([434740]), dtype=torch.float32\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([289827]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([289827]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([289827]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   ‚Ä¢ Training tensors:\n",
      "   ‚Ä¢ player_ids_train: torch.Size([2173697]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_train: torch.Size([2173697]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_train: torch.Size([2173697]), dtype=torch.float32\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([434740]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([434740]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([434740]), dtype=torch.float32\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([289827]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([289827]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([289827]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   ‚Ä¢ scores_train: torch.Size([2173697]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([434740]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([289827]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1004, 1.0000]\n",
      "   ‚Ä¢ Val: [0.1351, 0.8000]\n",
      "   ‚Ä¢ Test: [0.1884, 0.8251]\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([48474]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([48474]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.2683, 4.2461]\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2714]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2714]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2714]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "   ‚Ä¢ Train: 2,173,697 samples\n",
      "   ‚Ä¢ Val: 434,740 samples\n",
      "   ‚Ä¢ Test: 289,827 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 48,474\n",
      "   ‚Ä¢ Openings: 2,714\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 48474 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2714 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 66.56 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n",
      "   ‚Ä¢ scores_train: torch.Size([2173697]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([434740]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([289827]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1004, 1.0000]\n",
      "   ‚Ä¢ Val: [0.1351, 0.8000]\n",
      "   ‚Ä¢ Test: [0.1884, 0.8251]\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([48474]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([48474]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.2683, 4.2461]\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2714]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2714]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2714]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "   ‚Ä¢ Train: 2,173,697 samples\n",
      "   ‚Ä¢ Val: 434,740 samples\n",
      "   ‚Ä¢ Test: 289,827 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 48,474\n",
      "   ‚Ä¢ Openings: 2,714\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 48474 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2714 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 66.56 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Convert all data to PyTorch tensors\n",
    "\n",
    "print(\"STEP 5: CONVERT TO PYTORCH TENSORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\")\n",
    "\n",
    "# 0. Index alignment sanity checks\n",
    "print(f\"\\n0Ô∏è‚É£  Index alignment sanity checks...\")\n",
    "\n",
    "# Check player_side_info is properly indexed\n",
    "player_ids_sorted = sorted(player_side_info.index.values)\n",
    "if player_ids_sorted != list(player_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  player_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì player_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{player_side_info.index.min()}, {player_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {player_side_info.index.dtype}\")\n",
    "\n",
    "# Check opening_side_info is properly indexed\n",
    "opening_ids_sorted = sorted(opening_side_info.index.values)\n",
    "if opening_ids_sorted != list(opening_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì opening_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{opening_side_info.index.min()}, {opening_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {opening_side_info.index.dtype}\")\n",
    "\n",
    "# CRITICAL: Check if indices are contiguous 0-based\n",
    "# If opening_side_info.index = [0, 1, 2, ..., N-1], then we can use opening_id as direct array index\n",
    "# If not (e.g., [10, 15, 23, ...]), we'll need a mapping dictionary\n",
    "print(f\"\\n   Checking if indices are contiguous 0-based...\")\n",
    "player_ids_are_contiguous = (player_side_info.index == range(len(player_side_info))).all()\n",
    "opening_ids_are_contiguous = (opening_side_info.index == range(len(opening_side_info))).all()\n",
    "\n",
    "\n",
    "if not player_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Player IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "if not opening_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "\n",
    "# 1. Convert main features (train/val/test)\n",
    "\n",
    "# Train set\n",
    "player_ids_train = torch.tensor(X_train['player_id'].values, dtype=torch.long)\n",
    "opening_ids_train = torch.tensor(X_train['opening_id'].values, dtype=torch.long)\n",
    "confidence_train = torch.tensor(X_train['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ Training tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_train: {player_ids_train.shape}, dtype={player_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_train: {opening_ids_train.shape}, dtype={opening_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_train: {confidence_train.shape}, dtype={confidence_train.dtype}\")\n",
    "\n",
    "# Validation set\n",
    "player_ids_val = torch.tensor(X_val['player_id'].values, dtype=torch.long)\n",
    "opening_ids_val = torch.tensor(X_val['opening_id'].values, dtype=torch.long)\n",
    "confidence_val = torch.tensor(X_val['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Validation tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_val: {player_ids_val.shape}, dtype={player_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_val: {opening_ids_val.shape}, dtype={opening_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_val: {confidence_val.shape}, dtype={confidence_val.dtype}\")\n",
    "\n",
    "# Test set\n",
    "player_ids_test = torch.tensor(X_test['player_id'].values, dtype=torch.long)\n",
    "opening_ids_test = torch.tensor(X_test['opening_id'].values, dtype=torch.long)\n",
    "confidence_test = torch.tensor(X_test['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Test tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_test: {player_ids_test.shape}, dtype={player_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_test: {opening_ids_test.shape}, dtype={opening_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_test: {confidence_test.shape}, dtype={confidence_test.dtype}\")\n",
    "\n",
    "# 2. Convert targets (scores)\n",
    "print(f\"\\n2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\")\n",
    "\n",
    "scores_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "scores_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "scores_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ scores_train: {scores_train.shape}, dtype={scores_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_val: {scores_val.shape}, dtype={scores_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_test: {scores_test.shape}, dtype={scores_test.dtype}\")\n",
    "\n",
    "print(f\"\\n   Score ranges (sanity check):\")\n",
    "print(f\"   ‚Ä¢ Train: [{scores_train.min():.4f}, {scores_train.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Val: [{scores_val.min():.4f}, {scores_val.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Test: [{scores_test.min():.4f}, {scores_test.max():.4f}]\")\n",
    "\n",
    "# 3. Convert player side information\n",
    "\n",
    "# Create tensor of all player ratings (indexed by player_id)\n",
    "# Since player_side_info is indexed by player_id, we need to ensure coverage\n",
    "player_ratings_tensor = torch.tensor(player_side_info['rating_z'].values, dtype=torch.float32)\n",
    "player_ids_in_side_info = torch.tensor(player_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ player_ratings_tensor: {player_ratings_tensor.shape}, dtype={player_ratings_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ player_ids_in_side_info: {player_ids_in_side_info.shape}, dtype={player_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings_tensor.min():.4f}, {player_ratings_tensor.max():.4f}]\")\n",
    "\n",
    "# Verify all player IDs in train/val/test are covered\n",
    "all_player_ids = torch.cat([player_ids_train, player_ids_val, player_ids_test]).unique()\n",
    "missing_players = set(all_player_ids.tolist()) - set(player_ids_in_side_info.tolist())\n",
    "if len(missing_players) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in splits missing from side_info!\")\n",
    "\n",
    "# 4. Convert opening side information\n",
    "print(f\"\\n4Ô∏è‚É£  Converting opening side information...\")\n",
    "\n",
    "# Verify column names exist (using eco_letter_cat and eco_number_cat consistently)\n",
    "if 'eco_letter_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_letter_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "if 'eco_number_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_number_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "\n",
    "# Create tensors for opening ECO features (indexed by opening_id)\n",
    "opening_eco_letter_tensor = torch.tensor(opening_side_info['eco_letter_cat'].values, dtype=torch.long)\n",
    "opening_eco_number_tensor = torch.tensor(opening_side_info['eco_number_cat'].values, dtype=torch.long)\n",
    "opening_ids_in_side_info = torch.tensor(opening_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ opening_eco_letter_tensor: {opening_eco_letter_tensor.shape}, dtype={opening_eco_letter_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_eco_number_tensor: {opening_eco_number_tensor.shape}, dtype={opening_eco_number_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_in_side_info: {opening_ids_in_side_info.shape}, dtype={opening_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ ECO letter range: [{opening_eco_letter_tensor.min()}, {opening_eco_letter_tensor.max()}]\")\n",
    "print(f\"   ‚Ä¢ ECO number range: [{opening_eco_number_tensor.min()}, {opening_eco_number_tensor.max()}]\")\n",
    "\n",
    "# Verify all opening IDs in train/val/test are covered\n",
    "all_opening_ids = torch.cat([opening_ids_train, opening_ids_val, opening_ids_test]).unique()\n",
    "missing_openings = set(all_opening_ids.tolist()) - set(opening_ids_in_side_info.tolist())\n",
    "if len(missing_openings) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits missing from side_info!\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(f\"\\n5Ô∏è‚É£  Summary statistics:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(scores_train):,} samples\")\n",
    "print(f\"   ‚Ä¢ Val: {len(scores_val):,} samples\")\n",
    "print(f\"   ‚Ä¢ Test: {len(scores_test):,} samples\")\n",
    "\n",
    "print(f\"\\n   Unique entities:\")\n",
    "print(f\"   ‚Ä¢ Players: {len(player_ids_in_side_info):,}\")\n",
    "print(f\"   ‚Ä¢ Openings: {len(opening_ids_in_side_info):,}\")\n",
    "\n",
    "print(f\"\\n   Vocabulary sizes (for embedding layers):\")\n",
    "print(f\"   ‚Ä¢ num_players: {player_ids_in_side_info.max() + 1} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_openings: {opening_ids_in_side_info.max() + 1} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_letters: {opening_eco_letter_tensor.max() + 1} (max eco_letter_cat + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_numbers: {opening_eco_number_tensor.max() + 1} (max eco_number_cat + 1)\")\n",
    "\n",
    "# 6. Memory usage (approximate - using simple calculation)\n",
    "print(f\"\\n6Ô∏è‚É£  Approximate memory usage:\")\n",
    "def tensor_memory_mb(t):\n",
    "    \"\"\"Calculate tensor memory in MB\"\"\"\n",
    "    return (t.element_size() * t.nelement()) / (1024 * 1024)\n",
    "\n",
    "tensors = [\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train,\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val,\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test,\n",
    "    player_ratings_tensor, opening_eco_letter_tensor, opening_eco_number_tensor\n",
    "]\n",
    "total_memory_mb = sum(tensor_memory_mb(t) for t in tensors)\n",
    "print(f\"   ‚Ä¢ Total tensor memory: {total_memory_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TENSOR CONVERSION COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fc308",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup\n",
    "\n",
    "Define all constants, loss functions, optimizer, and helper functions for training.\n",
    "\n",
    "**What we need to set up:**\n",
    "\n",
    "1. **Constants and Hyperparameters:**\n",
    "   - `NUM_FACTORS`: Dimensionality of latent factor embeddings (e.g., 50)\n",
    "   - `LEARNING_RATE`: Step size for SGD optimizer (e.g., 0.01)\n",
    "   - `BATCH_SIZE`: Number of samples per training batch (e.g., 1024)\n",
    "   - `N_EPOCHS`: Number of full passes through training data (e.g., 10)\n",
    "   - Random seeds for reproducibility\n",
    "\n",
    "2. **Loss Functions:**\n",
    "   - MSE (Mean Squared Error): Main training loss\n",
    "   - RMSE (Root Mean Squared Error): Evaluation metric\n",
    "   - Confidence-weighted loss: Down-weight uncertain predictions\n",
    "\n",
    "3. **Optimizer:**\n",
    "   - SGD (Stochastic Gradient Descent) with momentum\n",
    "\n",
    "4. **Helper Functions:**\n",
    "   - `train_one_epoch()`: Train for one epoch\n",
    "   - `evaluate_model()`: Evaluate on validation/test set\n",
    "   - `calculate_rmse()`: Compute RMSE metric\n",
    "   - `save_checkpoint()`: Save model state\n",
    "\n",
    "5. **Logging:**\n",
    "   - Progress bars with ETA\n",
    "   - Loss tracking per epoch\n",
    "   - Model checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d13c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\n",
      "============================================================\n",
      "\n",
      "üìã Hyperparameters:\n",
      "   ‚Ä¢ NUM_FACTORS: 50\n",
      "   ‚Ä¢ LEARNING_RATE: 0.01\n",
      "   ‚Ä¢ MOMENTUM: 0.9\n",
      "   ‚Ä¢ BATCH_SIZE: 1024\n",
      "   ‚Ä¢ N_EPOCHS: 20\n",
      "   ‚Ä¢ WEIGHT_DECAY: 0.0\n",
      "\n",
      "üé≤ Random Seed:\n",
      "   ‚Ä¢ RANDOM_SEED: 42\n",
      "\n",
      "üíª Device Configuration:\n",
      "   ‚Ä¢ DEVICE: cpu\n",
      "\n",
      "üíæ Model Saving:\n",
      "   ‚Ä¢ Save directory: /Users/a/Documents/personalprojects/chess-opening-recommender/data/models\n",
      "   ‚úì Python random seed set to 42\n",
      "   ‚úì NumPy random seed set to 42\n",
      "   ‚úì PyTorch CPU random seed set to 42\n",
      "\n",
      "üìä Vocabulary Sizes (for embedding layers):\n",
      "   ‚Ä¢ NUM_PLAYERS: 48,474 (max player_id + 1)\n",
      "   ‚Ä¢ NUM_OPENINGS: 2,714 (max opening_id + 1)\n",
      "   ‚Ä¢ NUM_ECO_LETTERS: 5 (ECO letter categories)\n",
      "   ‚Ä¢ NUM_ECO_NUMBERS: 100 (ECO number categories)\n",
      "\n",
      "üìà Dataset Statistics:\n",
      "   ‚Ä¢ Train samples: 2,173,697\n",
      "   ‚Ä¢ Validation samples: 434,740\n",
      "   ‚Ä¢ Test samples: 289,827\n",
      "   ‚Ä¢ Total samples: 2,898,264\n",
      "\n",
      "   ‚Ä¢ Batches per epoch: 2,122\n",
      "   ‚Ä¢ Training iterations (total): 42,440\n",
      "\n",
      "============================================================\n",
      "‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\n"
     ]
    }
   ],
   "source": [
    "# Step 6a: Define Hyperparameters and Constants\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HYPERPARAMETERS AND CONSTANTS CONFIG\n",
    "# ========================================\n",
    "\n",
    "NUM_FACTORS = 50  # Dimensionality of latent factor embeddings\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9  # SGD momentum\n",
    "BATCH_SIZE = 1024 \n",
    "N_EPOCHS = 20\n",
    "\n",
    "# Regularization (if needed)\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_SAVE_DIR = Path.cwd().parent / \"data\" / \"models\"  # Saves to projectroot/data/models\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"\\nüìã Hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ NUM_FACTORS: {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ LEARNING_RATE: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ MOMENTUM: {MOMENTUM}\")\n",
    "print(f\"   ‚Ä¢ BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ N_EPOCHS: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ WEIGHT_DECAY: {WEIGHT_DECAY}\")\n",
    "\n",
    "print(f\"\\nüé≤ Random Seed:\")\n",
    "print(f\"   ‚Ä¢ RANDOM_SEED: {RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\nüíª Device Configuration:\")\n",
    "print(f\"   ‚Ä¢ DEVICE: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ‚Ä¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüíæ Model Saving:\")\n",
    "print(f\"   ‚Ä¢ Save directory: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# ========================================\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# ========================================\n",
    "\n",
    "# Python random\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì Python random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì NumPy random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# PyTorch random\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì PyTorch CPU random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU\n",
    "    print(f\"   ‚úì PyTorch GPU random seed set to {RANDOM_SEED}\")\n",
    "    \n",
    "    # Additional CUDA reproducibility settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"   ‚úì CUDA deterministic mode enabled\")\n",
    "\n",
    "# ========================================\n",
    "# CALCULATE VOCABULARY SIZES\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Sizes (for embedding layers):\")\n",
    "\n",
    "# Calculate from our tensors\n",
    "NUM_PLAYERS = int(player_ids_in_side_info.max()) + 1\n",
    "NUM_OPENINGS = int(opening_ids_in_side_info.max()) + 1\n",
    "NUM_ECO_LETTERS = int(opening_eco_letter_tensor.max()) + 1\n",
    "NUM_ECO_NUMBERS = int(opening_eco_number_tensor.max()) + 1\n",
    "\n",
    "print(f\"   ‚Ä¢ NUM_PLAYERS: {NUM_PLAYERS:,} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_OPENINGS: {NUM_OPENINGS:,} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_LETTERS: {NUM_ECO_LETTERS} (ECO letter categories)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_NUMBERS: {NUM_ECO_NUMBERS} (ECO number categories)\")\n",
    "\n",
    "# ========================================\n",
    "# DATASET STATISTICS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Train samples: {len(scores_train):,}\")\n",
    "print(f\"   ‚Ä¢ Validation samples: {len(scores_val):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(scores_test):,}\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(scores_train) + len(scores_val) + len(scores_test):,}\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Batches per epoch: {len(scores_train) // BATCH_SIZE:,}\")\n",
    "print(f\"   ‚Ä¢ Training iterations (total): {(len(scores_train) // BATCH_SIZE) * N_EPOCHS:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a93e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6B: DEFINE LOSS FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "üß™ Testing loss functions with dummy data...\n",
      "\n",
      "   Dummy data:\n",
      "   ‚Ä¢ Predictions: [0.5, 0.6000000238418579, 0.699999988079071, 0.800000011920929]\n",
      "   ‚Ä¢ Targets: [0.550000011920929, 0.6200000047683716, 0.6800000071525574, 0.75]\n",
      "   ‚Ä¢ Confidence: [0.800000011920929, 0.8999999761581421, 0.699999988079071, 1.0]\n",
      "\n",
      "   MSE (unweighted): 0.001450\n",
      "   MSE (weighted): 0.001512\n",
      "\n",
      "   RMSE (unweighted): 0.038079\n",
      "   RMSE (weighted): 0.038881\n",
      "\n",
      "   calculate_rmse() result: 0.038881\n",
      "   ‚úì Returns Python float (not tensor): <class 'float'>\n",
      "\n",
      "============================================================\n",
      "‚úÖ LOSS FUNCTIONS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6b: Define Loss Functions\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "print(\"STEP 6B: DEFINE LOSS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "def mse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss with optional confidence weighting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        Higher confidence = larger loss contribution\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Compute squared error\n",
    "    squared_error = (predictions - targets) ** 2\n",
    "    \n",
    "    # Apply confidence weighting if provided\n",
    "    if confidence_weights is not None:\n",
    "        weighted_squared_error = squared_error * confidence_weights\n",
    "        # Average over all samples (sum of weighted errors / sum of weights)\n",
    "        loss = weighted_squared_error.sum() / confidence_weights.sum()\n",
    "    else:\n",
    "        # Standard MSE (average over all samples)\n",
    "        loss = squared_error.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def rmse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error loss.\n",
    "    \n",
    "    This is the square root of MSE and is our primary evaluation metric.\n",
    "    RMSE is in the same units as the target (score), making it interpretable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar RMSE value\n",
    "    \"\"\"\n",
    "    mse = mse_loss(predictions, targets, confidence_weights)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def calculate_rmse(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Calculate RMSE metric (convenience function for evaluation).\n",
    "    \n",
    "    This is the same as rmse_loss but with clearer naming for evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        RMSE value (Python float, not tensor)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Don't compute gradients for evaluation\n",
    "        rmse = rmse_loss(predictions, targets, confidence_weights)\n",
    "    return rmse.item()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüß™ Testing loss functions with dummy data...\")\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_predictions = torch.tensor([0.5, 0.6, 0.7, 0.8], dtype=torch.float32)\n",
    "dummy_targets = torch.tensor([0.55, 0.62, 0.68, 0.75], dtype=torch.float32)\n",
    "dummy_confidence = torch.tensor([0.8, 0.9, 0.7, 1.0], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Dummy data:\")\n",
    "print(f\"   ‚Ä¢ Predictions: {dummy_predictions.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Targets: {dummy_targets.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Confidence: {dummy_confidence.tolist()}\")\n",
    "\n",
    "# Test MSE without confidence weighting\n",
    "mse_unweighted = mse_loss(dummy_predictions, dummy_targets)\n",
    "print(f\"\\n   MSE (unweighted): {mse_unweighted.item():.6f}\")\n",
    "\n",
    "# Test MSE with confidence weighting\n",
    "mse_weighted = mse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"   MSE (weighted): {mse_weighted.item():.6f}\")\n",
    "\n",
    "# Test RMSE\n",
    "rmse_unweighted = rmse_loss(dummy_predictions, dummy_targets)\n",
    "rmse_weighted = rmse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   RMSE (unweighted): {rmse_unweighted.item():.6f}\")\n",
    "print(f\"   RMSE (weighted): {rmse_weighted.item():.6f}\")\n",
    "\n",
    "# Test calculate_rmse\n",
    "rmse_eval = calculate_rmse(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   calculate_rmse() result: {rmse_eval:.6f}\")\n",
    "print(f\"   ‚úì Returns Python float (not tensor): {type(rmse_eval)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LOSS FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e00b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Creating PyTorch Datasets...\n",
      "   ‚úì Train dataset: 2,173,697 samples\n",
      "   ‚úì Validation dataset: 434,740 samples\n",
      "   ‚úì Test dataset: 289,827 samples\n",
      "\n",
      "2Ô∏è‚É£  Testing dataset access...\n",
      "   Sample from train_dataset[0]:\n",
      "   ‚Ä¢ player_id: 20824\n",
      "   ‚Ä¢ opening_id: 1934\n",
      "   ‚Ä¢ confidence: 0.1803\n",
      "   ‚Ä¢ score: 0.5868\n",
      "\n",
      "3Ô∏è‚É£  Creating PyTorch DataLoaders...\n"
     ]
    }
   ],
   "source": [
    "# Step 6c: Create PyTorch Dataset and DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# CUSTOM DATASET CLASS\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for chess player-opening interactions.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - player_id: Integer ID for embedding lookup\n",
    "    - opening_id: Integer ID for embedding lookup\n",
    "    - confidence: Confidence weight for loss function\n",
    "    - score: Target value (win rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, player_ids, opening_ids, confidence, scores):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (long tensor)\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (long tensor)\n",
    "        confidence : torch.Tensor\n",
    "            Confidence weights (float tensor)\n",
    "        scores : torch.Tensor\n",
    "            Target scores (float tensor)\n",
    "        \"\"\"\n",
    "        self.player_ids = player_ids\n",
    "        self.opening_ids = opening_ids\n",
    "        self.confidence = confidence\n",
    "        self.scores = scores\n",
    "        \n",
    "        # Validate shapes\n",
    "        assert len(player_ids) == len(opening_ids) == len(confidence) == len(scores), \\\n",
    "            \"All tensors must have the same length\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.player_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with keys: 'player_id', 'opening_id', 'confidence', 'score'\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'player_id': self.player_ids[idx],\n",
    "            'opening_id': self.opening_ids[idx],\n",
    "            'confidence': self.confidence[idx],\n",
    "            'score': self.scores[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATASETS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Creating PyTorch Datasets...\")\n",
    "\n",
    "train_dataset = ChessOpeningDataset(\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train\n",
    ")\n",
    "val_dataset = ChessOpeningDataset(\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val\n",
    ")\n",
    "test_dataset = ChessOpeningDataset(\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"   ‚úì Validation dataset: {len(val_dataset):,} samples\")\n",
    "print(f\"   ‚úì Test dataset: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Test dataset access\n",
    "print(f\"\\n2Ô∏è‚É£  Testing dataset access...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"   Sample from train_dataset[0]:\")\n",
    "print(f\"   ‚Ä¢ player_id: {sample['player_id']}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {sample['opening_id']}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "print(f\"   ‚Ä¢ score: {sample['score']:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATALOADERS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Creating PyTorch DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle training data each epoch\n",
    "    num_workers=0,  # Number of subprocesses for data loading (0 = main process)\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle test data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fbb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6D: DEFINE HELPER FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "   format_time() tests:\n",
      "   ‚Ä¢ 30 seconds ‚Üí 30.0s\n",
      "   ‚Ä¢ 90 seconds ‚Üí 1m 30s\n",
      "   ‚Ä¢ 300 seconds ‚Üí 5m 0s\n",
      "   ‚Ä¢ 3661 seconds ‚Üí 1h 1m 1s\n",
      "   ‚Ä¢ 7384 seconds ‚Üí 2h 3m 4s\n",
      "\n",
      "   print_progress() test (simulating batch progress):\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6d: Define Helper Functions for Training\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"STEP 6D: DEFINE HELPER FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Save Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to save\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer state to save\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    train_loss : float\n",
    "        Training loss for this epoch\n",
    "    val_loss : float\n",
    "        Validation loss for this epoch\n",
    "    filepath : Path or str\n",
    "        Where to save the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        \n",
    "        # Save hyperparameters for reproducibility\n",
    "        'hyperparameters': {\n",
    "            'num_factors': NUM_FACTORS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'momentum': MOMENTUM,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'num_players': NUM_PLAYERS,\n",
    "            'num_openings': NUM_OPENINGS,\n",
    "            'num_eco_letters': NUM_ECO_LETTERS,\n",
    "            'num_eco_numbers': NUM_ECO_NUMBERS,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"   Checkpoint saved to: {filepath}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Load Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to load weights into\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to load state into\n",
    "    filepath : Path or str\n",
    "        Where to load the checkpoint from\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        The epoch number from the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"    Checkpoint loaded from: {filepath}\")\n",
    "    print(f\"   ‚Ä¢ Epoch: {epoch}\")\n",
    "    print(f\"   ‚Ä¢ Train loss: {checkpoint['train_loss']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Val loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    return epoch\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Format Time\n",
    "# ========================================\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"\n",
    "    Format seconds as human-readable time string.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seconds : float\n",
    "        Number of seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted time string (e.g., \"1h 23m 45s\")\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Print Progress\n",
    "# ========================================\n",
    "\n",
    "def print_progress(epoch, batch_idx, total_batches, loss, elapsed_time):\n",
    "    \"\"\"\n",
    "    Print training progress with ETA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    batch_idx : int\n",
    "        Current batch index (0-based)\n",
    "    total_batches : int\n",
    "        Total number of batches per epoch\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    elapsed_time : float\n",
    "        Elapsed time in seconds since epoch start\n",
    "    \"\"\"\n",
    "    # Calculate progress\n",
    "    progress_pct = 100.0 * (batch_idx + 1) / total_batches\n",
    "    \n",
    "    # Estimate time remaining\n",
    "    time_per_batch = elapsed_time / (batch_idx + 1)\n",
    "    remaining_batches = total_batches - (batch_idx + 1)\n",
    "    eta_seconds = time_per_batch * remaining_batches\n",
    "    \n",
    "    # Print progress bar\n",
    "    bar_length = 40\n",
    "    filled_length = int(bar_length * (batch_idx + 1) / total_batches)\n",
    "    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)\n",
    "    \n",
    "    print(f\"   Epoch {epoch} [{bar}] {progress_pct:>6.2f}% | \"\n",
    "          f\"Loss: {loss:.6f} | \"\n",
    "          f\"ETA: {format_time(eta_seconds)}\", end='\\r')\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST HELPER FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "# Test format_time\n",
    "print(f\"\\n   format_time() tests:\")\n",
    "test_times = [30, 90, 300, 3661, 7384]\n",
    "for t in test_times:\n",
    "    print(f\"   ‚Ä¢ {t} seconds ‚Üí {format_time(t)}\")\n",
    "\n",
    "# Test print_progress (will overwrite line)\n",
    "print(f\"\\n   print_progress() test (simulating batch progress):\")\n",
    "for i in range(0, 101, 20):\n",
    "    total = 100\n",
    "    loss = 0.05 - i * 0.0003\n",
    "    elapsed = i * 0.5\n",
    "    print_progress(epoch=1, batch_idx=i, total_batches=total, loss=loss, elapsed_time=elapsed)\n",
    "    time.sleep(0.1)  # Brief pause to show animation\n",
    "print()  # New line after progress bar\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HELPER FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\n",
      "============================================================\n",
      " TRAINING AND EVALUATION FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìã Training workflow:\n",
      "   1. Initialize model and optimizer\n",
      "   2. For each epoch:\n",
      "      a. Call train_one_epoch()\n",
      "      b. Call evaluate_model() on validation set\n",
      "      c. Save checkpoint with save_checkpoint()\n",
      "      d. Log metrics\n",
      "   3. After training, evaluate on test set\n"
     ]
    }
   ],
   "source": [
    "# Step 6e: Define Training and Evaluation Functions\n",
    "\n",
    "print(\"STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# TRAINING FUNCTION: Train One Epoch\n",
    "# ========================================\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, device, epoch_num):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to train\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for updating model parameters\n",
    "    device : torch.device\n",
    "        Device to train on (CPU or CUDA)\n",
    "    epoch_num : int\n",
    "        Current epoch number (for logging)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (average_loss, elapsed_time)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move batch to device\n",
    "        player_ids = batch['player_id'].to(device)\n",
    "        opening_ids = batch['opening_id'].to(device)\n",
    "        confidence = batch['confidence'].to(device)\n",
    "        targets = batch['score'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(player_ids, opening_ids)\n",
    "        \n",
    "        # Compute loss with confidence weighting\n",
    "        loss = mse_loss(predictions, targets, confidence)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 10 batches or on last batch\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print_progress(epoch_num, batch_idx, num_batches, loss.item(), elapsed)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    elapsed_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Clear progress line and print summary\n",
    "    print()  # New line after progress bar\n",
    "    \n",
    "    return avg_loss, elapsed_time\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# EVALUATION FUNCTION: Evaluate Model\n",
    "# ========================================\n",
    "\n",
    "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to evaluate\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for evaluation data\n",
    "    device : torch.device\n",
    "        Device to evaluate on (CPU or CUDA)\n",
    "    dataset_name : str\n",
    "        Name of dataset for logging (e.g., \"Validation\", \"Test\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (mse, rmse)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_mse = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in data_loader:\n",
    "            # Move batch to device\n",
    "            player_ids = batch['player_id'].to(device)\n",
    "            opening_ids = batch['opening_id'].to(device)\n",
    "            confidence = batch['confidence'].to(device)\n",
    "            targets = batch['score'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(player_ids, opening_ids)\n",
    "            \n",
    "            # Compute MSE with confidence weighting\n",
    "            batch_mse = mse_loss(predictions, targets, confidence)\n",
    "            \n",
    "            # Track weighted sum of MSE\n",
    "            total_mse += batch_mse.item() * len(targets)\n",
    "            total_samples += len(targets)\n",
    "    \n",
    "    # Calculate average MSE and RMSE\n",
    "    avg_mse = total_mse / total_samples\n",
    "    avg_rmse = avg_mse ** 0.5\n",
    "    \n",
    "    return avg_mse, avg_rmse\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST FUNCTIONS (with dummy model)\n",
    "# ========================================\n",
    "\n",
    "print(\" TRAINING AND EVALUATION FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã Training workflow:\")\n",
    "print(f\"   1. Initialize model and optimizer\")\n",
    "print(f\"   2. For each epoch:\")\n",
    "print(f\"      a. Call train_one_epoch()\")\n",
    "print(f\"      b. Call evaluate_model() on validation set\")\n",
    "print(f\"      c. Save checkpoint with save_checkpoint()\")\n",
    "print(f\"      d. Log metrics\")\n",
    "print(f\"   3. After training, evaluate on test set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6F: DEFINE MODEL ARCHITECTURE\n",
      "============================================================\n",
      "\n",
      "‚úÖ ChessOpeningRecommender model class defined\n",
      "\n",
      "üìä Model Architecture:\n",
      "   Player Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(48474, 50)\n",
      "      ‚Ä¢ Biases: Embedding(48474, 1)\n",
      "      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\n",
      "      ‚Ä¢ Combiner: Linear(51, 50)\n",
      "\n",
      "   Opening Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(2714, 50)\n",
      "      ‚Ä¢ Biases: Embedding(2714, 1)\n",
      "      ‚Ä¢ ECO letter embedding: Embedding(5, 4)\n",
      "      ‚Ä¢ ECO number embedding: Embedding(100, 4)\n",
      "      ‚Ä¢ Combiner: Linear(58, 50)\n",
      "\n",
      "   Prediction:\n",
      "      ‚Ä¢ Dot product of player and opening representations\n",
      "      ‚Ä¢ + player bias + opening bias + global bias\n",
      "      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\n",
      "\n",
      " Model Statistics:\n",
      "   ‚Ä¢ Total trainable parameters: 2,616,559\n",
      "   ‚Ä¢ Player parameters: 2,474,774\n",
      "   ‚Ä¢ Opening parameters: 141,784\n",
      "   ‚Ä¢ Global parameters: 1\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6f: Define Model Architecture\n",
    "\n",
    "print(\"STEP 6F: DEFINE MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Chess Opening Recommender Model\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningRecommender(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for chess opening recommendations.\n",
    "    \n",
    "    The model learns latent factors for players and openings, incorporating\n",
    "    side information:\n",
    "    - Player ratings (normalized)\n",
    "    - Opening ECO codes (letter and number as categorical features)\n",
    "    \n",
    "    Architecture:\n",
    "    - Player embedding: learnable latent factors\n",
    "    - Opening embedding: learnable latent factors\n",
    "    - Player rating: fixed side information (no embedding)\n",
    "    - ECO letter/number: categorical embeddings\n",
    "    \n",
    "    Prediction: dot product of player and opening representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_players, num_openings, num_factors,\n",
    "                 player_ratings, opening_eco_letters, opening_eco_numbers,\n",
    "                 num_eco_letters, num_eco_numbers, eco_embed_dim=4):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_players : int\n",
    "            Total number of unique players\n",
    "        num_openings : int\n",
    "            Total number of unique openings\n",
    "        num_factors : int\n",
    "            Dimensionality of latent factor embeddings\n",
    "        player_ratings : torch.Tensor\n",
    "            Z-score normalized ratings for all players (shape: [num_players])\n",
    "        opening_eco_letters : torch.Tensor\n",
    "            ECO letter categories for all openings (shape: [num_openings])\n",
    "        opening_eco_numbers : torch.Tensor\n",
    "            ECO number categories for all openings (shape: [num_openings])\n",
    "        num_eco_letters : int\n",
    "            Number of unique ECO letter categories\n",
    "        num_eco_numbers : int\n",
    "            Number of unique ECO number categories\n",
    "        eco_embed_dim : int\n",
    "            Dimensionality of ECO categorical embeddings (default: 4)\n",
    "        \"\"\"\n",
    "        super(ChessOpeningRecommender, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_players = num_players\n",
    "        self.num_openings = num_openings\n",
    "        self.num_factors = num_factors\n",
    "        self.eco_embed_dim = eco_embed_dim\n",
    "        \n",
    "        # ========================================\n",
    "        # Player Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.player_factors = torch.nn.Embedding(num_players, num_factors)\n",
    "        \n",
    "        self.player_biases = torch.nn.Embedding(num_players, 1)\n",
    "        \n",
    "        # Player ratings (fixed side information - registered as buffer, not parameter)\n",
    "        self.register_buffer('player_ratings', player_ratings)\n",
    "        \n",
    "        # ========================================\n",
    "        # Opening Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.opening_factors = torch.nn.Embedding(num_openings, num_factors)\n",
    "        \n",
    "        self.opening_biases = torch.nn.Embedding(num_openings, 1)\n",
    "        \n",
    "        # ECO letter and number (fixed side information - registered as buffers)\n",
    "        self.register_buffer('opening_eco_letters', opening_eco_letters)\n",
    "        self.register_buffer('opening_eco_numbers', opening_eco_numbers)\n",
    "        \n",
    "        # ECO embeddings (learnable)\n",
    "        self.eco_letter_embedding = torch.nn.Embedding(num_eco_letters, eco_embed_dim)\n",
    "        self.eco_number_embedding = torch.nn.Embedding(num_eco_numbers, eco_embed_dim)\n",
    "        \n",
    "        # ========================================\n",
    "        # Combination Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Combine player latent factors with rating\n",
    "        # Input: [num_factors + 1] ‚Üí Output: [num_factors]\n",
    "        self.player_combiner = torch.nn.Linear(num_factors + 1, num_factors)\n",
    "        \n",
    "        # Combine opening latent factors with ECO embeddings\n",
    "        # Input: [num_factors + 2*eco_embed_dim] ‚Üí Output: [num_factors]\n",
    "        self.opening_combiner = torch.nn.Linear(num_factors + 2 * eco_embed_dim, num_factors)\n",
    "        \n",
    "        # ========================================\n",
    "        # Global Bias\n",
    "        # ========================================\n",
    "        \n",
    "        # Global bias term (learnable scalar)\n",
    "        self.global_bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # ========================================\n",
    "        # Initialize Weights\n",
    "        # ========================================\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        torch.nn.init.normal_(self.player_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.opening_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_letter_embedding.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_number_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        torch.nn.init.zeros_(self.player_biases.weight)\n",
    "        torch.nn.init.zeros_(self.opening_biases.weight)\n",
    "        \n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        torch.nn.init.xavier_uniform_(self.player_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.player_combiner.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.opening_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.opening_combiner.bias)\n",
    "    \n",
    "    def forward(self, player_ids, opening_ids):\n",
    "        \"\"\"\n",
    "        Forward pass: predict player-opening scores.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (shape: [batch_size])\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (shape: [batch_size])\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Predicted scores (shape: [batch_size])\n",
    "        \"\"\"\n",
    "        # ========================================\n",
    "        # Get Player Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get player latent factors [batch_size, num_factors]\n",
    "        player_embed = self.player_factors(player_ids)\n",
    "        \n",
    "        # Get player ratings [batch_size, 1]\n",
    "        player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "        \n",
    "        # Concatenate player factors with rating [batch_size, num_factors + 1]\n",
    "        player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "        \n",
    "        # Combine into final player representation [batch_size, num_factors]\n",
    "        player_repr = self.player_combiner(player_concat)\n",
    "        \n",
    "        # Get player bias [batch_size]\n",
    "        player_bias = self.player_biases(player_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Get Opening Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get opening latent factors [batch_size, num_factors]\n",
    "        opening_embed = self.opening_factors(opening_ids)\n",
    "        \n",
    "        # Get ECO embeddings\n",
    "        eco_letters = self.opening_eco_letters[opening_ids]  # [batch_size]\n",
    "        eco_numbers = self.opening_eco_numbers[opening_ids]  # [batch_size]\n",
    "        \n",
    "        eco_letter_embed = self.eco_letter_embedding(eco_letters)  # [batch_size, eco_embed_dim]\n",
    "        eco_number_embed = self.eco_number_embedding(eco_numbers)  # [batch_size, eco_embed_dim]\n",
    "        \n",
    "        # Concatenate opening factors with ECO embeddings\n",
    "        # [batch_size, num_factors + 2*eco_embed_dim]\n",
    "        opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "        \n",
    "        # Combine into final opening representation [batch_size, num_factors]\n",
    "        opening_repr = self.opening_combiner(opening_concat)\n",
    "        \n",
    "        # Get opening bias [batch_size]\n",
    "        opening_bias = self.opening_biases(opening_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Compute Prediction\n",
    "        # ========================================\n",
    "        \n",
    "        # Dot product of player and opening representations [batch_size]\n",
    "        interaction = (player_repr * opening_repr).sum(dim=1)\n",
    "        \n",
    "        # Add biases and global bias\n",
    "        prediction = interaction + player_bias + opening_bias + self.global_bias\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] range (since scores are win rates)\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def get_player_embedding(self, player_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific player.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_id : int\n",
    "            Player ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Player representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            player_ids = torch.tensor([player_id], device=self.player_factors.weight.device)\n",
    "            player_embed = self.player_factors(player_ids)\n",
    "            player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "            player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "            player_repr = self.player_combiner(player_concat)\n",
    "            return player_repr.squeeze()\n",
    "    \n",
    "    def get_opening_embedding(self, opening_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific opening.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        opening_id : int\n",
    "            Opening ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Opening representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            opening_ids = torch.tensor([opening_id], device=self.opening_factors.weight.device)\n",
    "            opening_embed = self.opening_factors(opening_ids)\n",
    "            \n",
    "            eco_letters = self.opening_eco_letters[opening_ids]\n",
    "            eco_numbers = self.opening_eco_numbers[opening_ids]\n",
    "            \n",
    "            eco_letter_embed = self.eco_letter_embedding(eco_letters)\n",
    "            eco_number_embed = self.eco_number_embedding(eco_numbers)\n",
    "            \n",
    "            opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "            opening_repr = self.opening_combiner(opening_concat)\n",
    "            return opening_repr.squeeze()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model Summary\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ ChessOpeningRecommender model class defined\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Player Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_PLAYERS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_PLAYERS}, 1)\")\n",
    "print(f\"      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 1}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Opening Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_OPENINGS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_OPENINGS}, 1)\")\n",
    "print(f\"      ‚Ä¢ ECO letter embedding: Embedding({NUM_ECO_LETTERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ ECO number embedding: Embedding({NUM_ECO_NUMBERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 8}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Prediction:\")\n",
    "print(f\"      ‚Ä¢ Dot product of player and opening representations\")\n",
    "print(f\"      ‚Ä¢ + player bias + opening bias + global bias\")\n",
    "print(f\"      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\")\n",
    "\n",
    "# Calculate total parameters\n",
    "player_factor_params = NUM_PLAYERS * NUM_FACTORS\n",
    "player_bias_params = NUM_PLAYERS\n",
    "player_combiner_params = (NUM_FACTORS + 1) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "opening_factor_params = NUM_OPENINGS * NUM_FACTORS\n",
    "opening_bias_params = NUM_OPENINGS\n",
    "eco_letter_params = NUM_ECO_LETTERS * 4\n",
    "eco_number_params = NUM_ECO_NUMBERS * 4\n",
    "opening_combiner_params = (NUM_FACTORS + 8) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "global_bias_params = 1\n",
    "\n",
    "total_params = (player_factor_params + player_bias_params + player_combiner_params +\n",
    "                opening_factor_params + opening_bias_params + \n",
    "                eco_letter_params + eco_number_params + opening_combiner_params +\n",
    "                global_bias_params)\n",
    "\n",
    "print(f\"\\n Model Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total trainable parameters: {total_params:,}\")\n",
    "print(f\"   ‚Ä¢ Player parameters: {player_factor_params + player_bias_params + player_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Opening parameters: {opening_factor_params + opening_bias_params + eco_letter_params + eco_number_params + opening_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Global parameters: {global_bias_params}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08708d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6.1: CV CONFIGURATION & DATA SUBSETTING\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     20\u001b[39m param_grid = {\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_factors\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m5\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m100\u001b[39m],\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m1e-4\u001b[39m, \u001b[32m5e-4\u001b[39m, \u001b[32m1e-3\u001b[39m, \u001b[32m5e-3\u001b[39m, \u001b[32m1e-2\u001b[39m, \u001b[32m5e-2\u001b[39m],\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m512\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m2048\u001b[39m]\n\u001b[32m     24\u001b[39m }\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 2. Balanced Player Sampling\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Sample players in a balanced way by rating and number of games\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mnp\u001b[49m.random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Get player-level statistics\u001b[39;00m\n\u001b[32m     33\u001b[39m player_stats = (\n\u001b[32m     34\u001b[39m     clean_data.groupby(\u001b[33m\"\u001b[39m\u001b[33mplayer_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m     .agg({\u001b[33m\"\u001b[39m\u001b[33mnum_games\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m})  \u001b[38;5;66;03m# Sum all games across all openings\u001b[39;00m\n\u001b[32m     36\u001b[39m     .reset_index()\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# 6.1 CROSS VALIDATION CONFIGURATION & DATA SUBSETTING\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"STEP 6.1: CV CONFIGURATION & DATA SUBSETTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# 1. Configuration\n",
    "# ========================================\n",
    "NUM_CV_PLAYERS = 10_000\n",
    "K_FOLDS = 3\n",
    "CV_EPOCHS = 15 # Number of epochs to train for each fold/config\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"num_factors\": [5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2],\n",
    "    \"batch_size\": [512, 1024, 2048]\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# 2. Balanced Player Sampling\n",
    "# ========================================\n",
    "# Sample players in a balanced way by rating and number of games\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get player-level statistics\n",
    "player_stats = (\n",
    "    clean_data.groupby(\"player_id\")\n",
    "    .agg({\"num_games\": \"sum\"})  # Sum all games across all openings\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge with player_side_info to get rating_z\n",
    "player_stats = player_stats.merge(\n",
    "    player_side_info.reset_index()[[\"player_id\", \"rating_z\"]],\n",
    "    on=\"player_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Create stratification bins\n",
    "# Bin by rating (quartiles)\n",
    "player_stats['rating_bin'] = pd.qcut(player_stats['rating_z'], q=4, labels=['low', 'med_low', 'med_high', 'high'], duplicates='drop')\n",
    "\n",
    "# Bin by number of games (quartiles)\n",
    "player_stats['games_bin'] = pd.qcut(player_stats['num_games'], q=4, labels=['few', 'some', 'many', 'lots'], duplicates='drop')\n",
    "\n",
    "# Create combined stratification key\n",
    "player_stats['strata'] = player_stats['rating_bin'].astype(str) + '_' + player_stats['games_bin'].astype(str)\n",
    "\n",
    "print(f\"Total players: {len(player_stats):,}\")\n",
    "print(f\"Unique strata: {player_stats['strata'].nunique()}\")\n",
    "print(\"\\nStrata distribution:\")\n",
    "print(player_stats['strata'].value_counts().sort_index())\n",
    "\n",
    "# Sample proportionally from each stratum\n",
    "cv_player_ids = []\n",
    "for stratum in player_stats['strata'].unique():\n",
    "    stratum_players = player_stats[player_stats['strata'] == stratum]['player_id'].values\n",
    "    n_sample = max(1, int(len(stratum_players) * NUM_CV_PLAYERS / len(player_stats)))\n",
    "    n_sample = min(n_sample, len(stratum_players))  # Don't sample more than available\n",
    "    sampled = np.random.choice(stratum_players, size=n_sample, replace=False)\n",
    "    cv_player_ids.extend(sampled)\n",
    "\n",
    "cv_player_ids = np.array(cv_player_ids)\n",
    "print(f\"\\nSampled {len(cv_player_ids):,} players for CV (target was {NUM_CV_PLAYERS:,})\")\n",
    "\n",
    "# Create a dataframe containing only the interactions from the selected players\n",
    "cv_data = clean_data[clean_data[\"player_id\"].isin(cv_player_ids)].copy()\n",
    "\n",
    "# Verify balanced distribution\n",
    "cv_player_stats = player_stats[player_stats['player_id'].isin(cv_player_ids)]\n",
    "print(f\"\\nCV subset player statistics:\")\n",
    "print(f\"  Players: {len(cv_player_ids):,}\")\n",
    "print(f\"  Interactions: {len(cv_data):,}\")\n",
    "print(f\"  Rating range: {cv_player_stats['rating_z'].min():.0f} - {cv_player_stats['rating_z'].max():.0f}\")\n",
    "print(f\"  Rating mean: {cv_player_stats['rating_z'].mean():.1f} (full data: {player_stats['rating_z'].mean():.1f})\")\n",
    "print(f\"  Games range: {cv_player_stats['num_games'].min()} - {cv_player_stats['num_games'].max()}\")\n",
    "print(f\"  Games mean: {cv_player_stats['num_games'].mean():.1f} (full data: {player_stats['num_games'].mean():.1f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ BALANCED SAMPLING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6.2: K-FOLD CROSS-VALIDATION LOOP\n",
      "============================================================\n",
      "Using Random Search with 15 hyperparameter combinations\n",
      "Total CV runs: 75\n",
      "\n",
      "\n",
      "============================================================\n",
      "CONFIG 1/15: factors=5, lr=0.00100, batch=1024\n",
      "============================================================\n",
      "  Fold 1/5... RMSE: 0.0415, MAE: 0.0322, Time: 83.0s\n",
      "  Fold 2/5... RMSE: 0.0416, MAE: 0.0322, Time: 81.1s\n",
      "  Fold 3/5... RMSE: 0.0417, MAE: 0.0323, Time: 87.5s\n",
      "  Fold 4/5... RMSE: 0.0414, MAE: 0.0322, Time: 92.7s\n",
      "  Fold 5/5... RMSE: 0.0415, MAE: 0.0322, Time: 83.0s\n",
      "  ‚úì Config 1 complete: Avg RMSE = 0.0415 ¬± 0.0001\n",
      "\n",
      "============================================================\n",
      "CONFIG 2/15: factors=20, lr=0.01000, batch=512\n",
      "============================================================\n",
      "  Fold 1/5... RMSE: 0.0397, MAE: 0.0308, Time: 112.1s\n",
      "  Fold 2/5... RMSE: 0.0397, MAE: 0.0308, Time: 922.8s\n",
      "  Fold 3/5... RMSE: 0.0399, MAE: 0.0309, Time: 136.6s\n",
      "  Fold 4/5... RMSE: 0.0397, MAE: 0.0309, Time: 119.2s\n",
      "  Fold 5/5... RMSE: 0.0397, MAE: 0.0308, Time: 110.8s\n",
      "  ‚úì Config 2 complete: Avg RMSE = 0.0397 ¬± 0.0001\n",
      "\n",
      "============================================================\n",
      "CONFIG 3/15: factors=5, lr=0.05000, batch=1024\n",
      "============================================================\n",
      "  Fold 1/5... RMSE: 0.0384, MAE: 0.0297, Time: 76.6s\n",
      "  Fold 2/5... RMSE: 0.0385, MAE: 0.0298, Time: 80.7s\n",
      "  Fold 3/5... RMSE: 0.0387, MAE: 0.0299, Time: 86.3s\n",
      "  Fold 4/5... RMSE: 0.0384, MAE: 0.0298, Time: 103.1s\n",
      "  Fold 5/5... RMSE: 0.0384, MAE: 0.0298, Time: 89.3s\n",
      "  ‚úì Config 3 complete: Avg RMSE = 0.0385 ¬± 0.0001\n",
      "\n",
      "============================================================\n",
      "CONFIG 4/15: factors=5, lr=0.00100, batch=2048\n",
      "============================================================\n",
      "  Fold 1/5... RMSE: 0.0415, MAE: 0.0322, Time: 118.1s\n",
      "  Fold 2/5... RMSE: 0.0416, MAE: 0.0322, Time: 100.8s\n",
      "  Fold 3/5... RMSE: 0.0418, MAE: 0.0323, Time: 95.8s\n",
      "  Fold 4/5... RMSE: 0.0415, MAE: 0.0323, Time: 95.1s\n",
      "  Fold 5/5... RMSE: 0.0416, MAE: 0.0323, Time: 90.7s\n",
      "  ‚úì Config 4 complete: Avg RMSE = 0.0416 ¬± 0.0001\n",
      "\n",
      "============================================================\n",
      "CONFIG 5/15: factors=5, lr=0.00010, batch=512\n",
      "============================================================\n",
      "  Fold 1/5... RMSE: 0.0418, MAE: 0.0322, Time: 94.8s\n",
      "  Fold 2/5... RMSE: 0.0419, MAE: 0.0323, Time: 91.5s\n",
      "  Fold 3/5... RMSE: 0.0420, MAE: 0.0324, Time: 76.3s\n",
      "  Fold 4/5... RMSE: 0.0417, MAE: 0.0323, Time: 88.0s\n",
      "  Fold 5/5... RMSE: 0.0418, MAE: 0.0323, Time: 78.1s\n",
      "  ‚úì Config 5 complete: Avg RMSE = 0.0419 ¬± 0.0001\n",
      "\n",
      "============================================================\n",
      "CONFIG 6/15: factors=10, lr=0.05000, batch=512\n",
      "============================================================\n",
      "  Fold 1/5... RMSE: 0.0374, MAE: 0.0289, Time: 95.5s\n",
      "  Fold 2/5... RMSE: 0.0374, MAE: 0.0289, Time: 105.7s\n",
      "  Fold 3/5... RMSE: 0.0376, MAE: 0.0290, Time: 94.7s\n",
      "  Fold 4/5... RMSE: 0.0373, MAE: 0.0289, Time: 102.3s\n",
      "  Fold 5/5... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     95\u001b[39m         loss = mse_loss(predictions, batch[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE), batch[\u001b[33m'\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m'\u001b[39m].to(DEVICE))\n\u001b[32m     96\u001b[39m         loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m         \u001b[43moptimizer_f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# --- Evaluation for this fold ---\u001b[39;00m\n\u001b[32m    100\u001b[39m val_mse, val_rmse = evaluate_model(model_f, val_loader_f, DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    380\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    382\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    383\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m'\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     75\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/optim/sgd.py:75\u001b[39m, in \u001b[36mSGD.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m     71\u001b[39m momentum_buffer_list = []\n\u001b[32m     73\u001b[39m has_sparse_grad = \u001b[38;5;28mself\u001b[39m._init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmomentum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdampening\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnesterov\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/optim/sgd.py:185\u001b[39m, in \u001b[36msgd\u001b[39m\u001b[34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[39m\n\u001b[32m     92\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m     95\u001b[39m SGD.\u001b[34m__doc__\u001b[39m = \u001b[33mr\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mImplements stochastic gradient descent (optionally with momentum).\u001b[39m\n\u001b[32m     96\u001b[39m \n\u001b[32m     97\u001b[39m \u001b[33m    .. math::\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m \n\u001b[32m    182\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msgd\u001b[39m(params: List[Tensor],\n\u001b[32m    186\u001b[39m         d_p_list: List[Tensor],\n\u001b[32m    187\u001b[39m         momentum_buffer_list: List[Optional[Tensor]],\n\u001b[32m    188\u001b[39m         \u001b[38;5;66;03m# kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\u001b[39;00m\n\u001b[32m    189\u001b[39m         \u001b[38;5;66;03m# setting this as kwarg for now as functional API is compiled by torch/distributed/optim\u001b[39;00m\n\u001b[32m    190\u001b[39m         has_sparse_grad: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    191\u001b[39m         foreach: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    192\u001b[39m         *,\n\u001b[32m    193\u001b[39m         weight_decay: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m    194\u001b[39m         momentum: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m    195\u001b[39m         lr: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m    196\u001b[39m         dampening: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m    197\u001b[39m         nesterov: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    198\u001b[39m         maximize: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m    199\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Functional API that performs SGD algorithm computation.\u001b[39;00m\n\u001b[32m    200\u001b[39m \n\u001b[32m    201\u001b[39m \u001b[33;03m    See :class:`~torch.optim.SGD` for details.\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    205\u001b[39m         \u001b[38;5;66;03m# why must we be explicit about an if statement for torch.jit.is_scripting here?\u001b[39;00m\n\u001b[32m    206\u001b[39m         \u001b[38;5;66;03m# because JIT can't handle Optionals nor fancy conditionals when scripting\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 6.2 K-FOLD CROSS-VALIDATION LOOP WITH RANDOM SEARCH\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "print(\"STEP 6.2: K-FOLD CROSS-VALIDATION LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Random Search Configuration\n",
    "# ========================================\n",
    "N_RANDOM_SAMPLES = 25  # Number of random hyperparameter combinations to try\n",
    "random_state = 42\n",
    "\n",
    "# Use sklearn's ParameterSampler for random search\n",
    "param_sampler = ParameterSampler(\n",
    "    param_grid, n_iter=N_RANDOM_SAMPLES, random_state=random_state\n",
    ")\n",
    "\n",
    "print(f\"Using Random Search with {N_RANDOM_SAMPLES} hyperparameter combinations\")\n",
    "print(f\"Total CV runs: {N_RANDOM_SAMPLES * K_FOLDS}\")\n",
    "print()\n",
    "\n",
    "# ========================================\n",
    "# K-Fold Cross-Validation Loop\n",
    "# ========================================\n",
    "\n",
    "# Store results\n",
    "cv_results = []\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Loop through random hyperparameter combinations\n",
    "for config_idx, params in enumerate(param_sampler):\n",
    "    num_factors = params['num_factors']\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    config_str = f\"factors={num_factors}, lr={learning_rate:.5f}, batch={batch_size}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CONFIG {config_idx+1}/{N_RANDOM_SAMPLES}: {config_str}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    fold_metrics = {'rmse': [], 'mae': [], 'time': []}\n",
    "    \n",
    "    # K-Fold CV for this configuration\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(cv_data)):\n",
    "        fold_start_time = time.time()\n",
    "        print(f\"  Fold {fold_idx+1}/{K_FOLDS}...\", end=\" \", flush=True)\n",
    "\n",
    "        # --- Data preparation for this fold ---\n",
    "        train_fold_df = cv_data.iloc[train_idx]\n",
    "        val_fold_df = cv_data.iloc[val_idx]\n",
    "\n",
    "        # --- Convert to Tensors for this fold ---\n",
    "        train_dataset_f = ChessOpeningDataset(\n",
    "            torch.tensor(train_fold_df['player_id'].values, dtype=torch.long),\n",
    "            torch.tensor(train_fold_df['opening_id'].values, dtype=torch.long),\n",
    "            torch.tensor(train_fold_df['confidence'].values, dtype=torch.float32),\n",
    "            torch.tensor(train_fold_df['score'].values, dtype=torch.float32)\n",
    "        )\n",
    "        val_dataset_f = ChessOpeningDataset(\n",
    "            torch.tensor(val_fold_df['player_id'].values, dtype=torch.long),\n",
    "            torch.tensor(val_fold_df['opening_id'].values, dtype=torch.long),\n",
    "            torch.tensor(val_fold_df['confidence'].values, dtype=torch.float32),\n",
    "            torch.tensor(val_fold_df['score'].values, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        train_loader_f = DataLoader(train_dataset_f, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_f = DataLoader(val_dataset_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # --- Model and Optimizer for this fold ---\n",
    "        model_f = ChessOpeningRecommender(\n",
    "            num_players=NUM_PLAYERS, \n",
    "            num_openings=NUM_OPENINGS, \n",
    "            num_factors=num_factors,\n",
    "            player_ratings=player_ratings_tensor, \n",
    "            opening_eco_letters=opening_eco_letter_tensor,\n",
    "            opening_eco_numbers=opening_eco_number_tensor, \n",
    "            num_eco_letters=NUM_ECO_LETTERS,\n",
    "            num_eco_numbers=NUM_ECO_NUMBERS\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        optimizer_f = torch.optim.SGD(model_f.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "        # --- Training loop for this fold (silent mode) ---\n",
    "        model_f.train()\n",
    "        for epoch in range(1, CV_EPOCHS + 1):\n",
    "            for batch in train_loader_f:\n",
    "                optimizer_f.zero_grad()\n",
    "                predictions = model_f(\n",
    "                    batch['player_id'].to(DEVICE), \n",
    "                    batch['opening_id'].to(DEVICE)\n",
    "                )\n",
    "                loss = mse_loss(predictions, batch['score'].to(DEVICE), batch['confidence'].to(DEVICE))\n",
    "                loss.backward()\n",
    "                optimizer_f.step()\n",
    "        \n",
    "        # --- Evaluation for this fold ---\n",
    "        val_mse, val_rmse = evaluate_model(model_f, val_loader_f, DEVICE)\n",
    "        \n",
    "        # Calculate MAE\n",
    "        model_f.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_f:\n",
    "                preds = model_f(batch['player_id'].to(DEVICE), batch['opening_id'].to(DEVICE))\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_targets.append(batch['score'].cpu())\n",
    "        val_mae = torch.nn.functional.l1_loss(torch.cat(all_preds), torch.cat(all_targets)).item()\n",
    "\n",
    "        fold_time = time.time() - fold_start_time\n",
    "        fold_metrics['rmse'].append(val_rmse)\n",
    "        fold_metrics['mae'].append(val_mae)\n",
    "        fold_metrics['time'].append(fold_time)\n",
    "        \n",
    "        print(f\"RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, Time: {fold_time:.1f}s\")\n",
    "\n",
    "    # --- Aggregate metrics for this hyperparameter config ---\n",
    "    avg_rmse = np.mean(fold_metrics['rmse'])\n",
    "    std_rmse = np.std(fold_metrics['rmse'])\n",
    "    avg_mae = np.mean(fold_metrics['mae'])\n",
    "    std_mae = np.std(fold_metrics['mae'])\n",
    "    avg_time = np.mean(fold_metrics['time'])\n",
    "    \n",
    "    cv_results.append({\n",
    "        'num_factors': num_factors,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'mean_rmse': avg_rmse,\n",
    "        'std_rmse': std_rmse,\n",
    "        'mean_mae': avg_mae,\n",
    "        'std_mae': std_mae,\n",
    "        'mean_time_per_fold': avg_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úì Config {config_idx+1} complete: Avg RMSE = {avg_rmse:.4f} ¬± {std_rmse:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ CROSS-VALIDATION LOOP COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03690a7e",
   "metadata": {},
   "source": [
    "## Step 6: Hyperparameter Tuning with K-Fold Cross-Validation\n",
    "\n",
    "Before training the final model, we will perform k-fold cross-validation to find the optimal hyperparameters. This provides a more robust evaluation than a single validation set.\n",
    "\n",
    "**Process:**\n",
    "1.  **Subset Data**: We'll use a smaller, representative sample of players to make the tuning process faster.\n",
    "2.  **K-Fold Split**: The player data will be split into `k` folds. For each fold, we train on `k-1` folds and validate on the remaining one.\n",
    "3.  **Hyperparameter Search**: We will iterate through different combinations of `NUM_FACTORS`, `LEARNING_RATE`, and `BATCH_SIZE`.\n",
    "4.  **Aggregate & Evaluate**: We'll average the performance metrics (like RMSE) across all folds for each hyperparameter combination to determine the best set.\n",
    "5.  **Visualize**: Results will be plotted to visualize the impact of each hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc2c6d",
   "metadata": {},
   "source": [
    "## Step 7: Final Model Training Setup\n",
    "\n",
    "Now we'll set up the training components for the **final model**: the model instance, optimizer, and learning rate scheduler.\n",
    "\n",
    "We'll use the best hyperparameters found during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 7: FINAL MODEL TRAINING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Hyperparameters for Final Training\n",
    "# ==================================\n",
    "# These would be set by the results from the CV step.\n",
    "# For now, we'll use the initial defaults.\n",
    "NUM_FACTORS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# ==================================\n",
    "# Model Instantiation\n",
    "# ==================================\n",
    "final_model = ChessOpeningRecommender(\n",
    "    num_players=NUM_PLAYERS,\n",
    "    num_openings=NUM_OPENINGS,\n",
    "    num_factors=NUM_FACTORS,\n",
    "    player_ratings=player_ratings_tensor,\n",
    "    opening_eco_letters=opening_eco_letter_tensor,\n",
    "    opening_eco_numbers=opening_eco_number_tensor,\n",
    "    num_eco_letters=NUM_ECO_LETTERS,\n",
    "    num_eco_numbers=NUM_ECO_NUMBERS\n",
    ").to(DEVICE)\n",
    "\n",
    "# ==================================\n",
    "# Optimizer\n",
    "# ==================================\n",
    "# We'll use SGD with momentum, a classic choice for matrix factorization.\n",
    "optimizer = torch.optim.SGD(final_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# ==================================\n",
    "# Learning Rate Scheduler\n",
    "# ==================================\n",
    "# Reduces the learning rate when a metric has stopped improving.\n",
    "# This helps to fine-tune the model in the later stages of training.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',      # The scheduler will step when the quantity monitored has stopped decreasing\n",
    "    factor=0.1,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "    patience=2,      # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=True     # If True, prints a message to stdout for each update\n",
    ")\n",
    "\n",
    "# ==================================\n",
    "# Checkpoint and Model Save Paths\n",
    "# ==================================\n",
    "CHECKPOINT_DIR = \"data/models\"\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.pt\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Final model hyperparameters:\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Num Factors: {NUM_FACTORS}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Num Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"\\nModel, optimizer, and scheduler initialized.\")\n",
    "print(f\"Checkpoints will be saved in: '{CHECKPOINT_DIR}'\")\n",
    "print(f\"Best model will be saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ SETUP COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becba99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0dc479f",
   "metadata": {},
   "source": [
    "## Step 8: Final Model Training Loop\n",
    "\n",
    "Here's the main training loop for the final model. We'll iterate for a specified number of epochs, training the model and evaluating its performance on the validation set periodically. We'll also save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"STEP 8: TRAINING THE FINAL MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Training State Tracking\n",
    "# ==================================\n",
    "history = defaultdict(list)\n",
    "best_val_rmse = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "\n",
    "# ==================================\n",
    "# Main Training Loop\n",
    "# ==================================\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    train_loss, train_rmse = train_one_epoch(final_model, train_loader, optimizer, DEVICE, epoch)\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    val_loss, val_rmse = evaluate_model(final_model, val_loader, DEVICE)\n",
    "    \n",
    "    # --- Learning Rate Scheduler Step ---\n",
    "    scheduler.step(val_rmse)\n",
    "    \n",
    "    # --- Logging ---\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_rmse'].append(train_rmse)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    \n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f} | \"\n",
    "          f\"Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "    # --- Checkpoint Saving ---\n",
    "    # Save a checkpoint every epoch\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch:03d}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_rmse': val_rmse,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Save the best model based on validation RMSE\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        epochs_since_improvement = 0\n",
    "        torch.save(final_model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  -> New best model saved with Val RMSE: {val_rmse:.4f}\")\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "\n",
    "# ==================================\n",
    "# Post-Training Summary\n",
    "# ==================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ FINAL TRAINING COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best Validation RMSE: {best_val_rmse:.4f}\")\n",
    "print(f\"Best model saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(f\"Last checkpoint saved to: '{checkpoint_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68202800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
