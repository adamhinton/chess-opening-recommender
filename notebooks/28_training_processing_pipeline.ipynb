{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e163a7",
   "metadata": {},
   "source": [
    "# Notebook 26 ‚Äî Opening Recommender Model: Training Pipeline\n",
    "\n",
    "### 0. Overview and Goals\n",
    "\n",
    "This notebook defines the full pipeline for training the chess opening recommender model.  \n",
    "The objective is to predict **player‚Äìopening performance scores** ((wins + (0.5 * draws) / num games)) for openings a player hasn‚Äôt yet played, based on their results in the openings they *have* played.  \n",
    "\n",
    "The model will use **matrix factorization** with **stochastic gradient descent (SGD)** to learn latent factors representing player and opening characteristics.  \n",
    "All computations will be implemented in **PyTorch**, with data loaded from my local **DuckDB** database.\n",
    "\n",
    "**High-level specs:**\n",
    "- Use only *White* openings initially (we‚Äôll extend to Black later).  \n",
    "- Data source: processed player‚Äìopening stats from local DuckDB.  \n",
    "- Predict: normalized ‚Äúscore‚Äù = win rate ((wins + 0.5 x draws) / total games).  \n",
    "- Filter: only include entries with ‚â• `MIN_GAMES_THRESHOLD` (default = 50).  \n",
    "- Ignore: rating differences, time controls, and other metadata.  \n",
    "- Model parameters (to be defined in appropriate places for easy editing):  \n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_PLAYERS_TO_PROCESS`  \n",
    "- Logging and checkpoints throughout for reproducibility.  \n",
    "- All random operations seeded for deterministic runs.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Extraction\n",
    "- Connect to local DuckDB\n",
    "- Pull all processed player‚Äìopening statistics from\n",
    "- Verify schema consistency:  \n",
    "  - Required columns: `player_id`, `opening_id`, `eco`, `num_games`, `wins`, `draws`, `losses`.  \n",
    "- Include a row-count sanity check.\n",
    "- Only players with ratings above 1200\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Sanitization & Normalization\n",
    "- Optionally normalize scores if needed for MF convergence.  \n",
    "- Drop players with no qualifying openings and openings with no qualifying players.  \n",
    "  - I believe there shouldn't be any but we'll double check.\n",
    "- Resequence player_id and opening_id to be sequential integers - right now there are gaps because of entries we deleted from the DB \n",
    "- Check for sparsity consistency (no implicit zeros yet).  \n",
    "- Note that this data has already been split in to white and black games further up the pipeline\n",
    "\n",
    "### Data Quality\n",
    "- Drop entries with fewer than `MIN_GAMES_THRESHOLD` games\n",
    "- Handle any duplicate `(player_id, opening_id)` combinations\n",
    "- Remove players with no qualifying openings\n",
    "- Remove openings with no qualifying players\n",
    "- Verify no null values remain\n",
    "\n",
    "### ECO Codes\n",
    "- Keep ECO codes for later categorical encoding (Step 4)\n",
    "- ECO will be used as opening side information (similar to rating for players)\n",
    "\n",
    "### Confidence Weighting\n",
    "- Use `MIN_GAMES_THRESHOLD = 10` to keep more data\n",
    "- Add a **confidence weight** column: `confidence = num_games / (num_games + K)` where K ‚âà 50\n",
    "- This weight will be used in the loss function to down-weight uncertain predictions\n",
    "- High-game-count entries ‚Üí high confidence ‚Üí larger loss impact\n",
    "- Low-game-count entries ‚Üí low confidence ‚Üí smaller loss impact\n",
    "\n",
    "### Player Rating (Side Information)\n",
    "- **Player ratings are side information** - they describe player characteristics, not individual player-opening interactions\n",
    "- Ratings will be stored separately and joined to player embeddings during training\n",
    "- We'll **normalize ratings** (likely z-score normalization) to avoid scaling issues with the embedding layer\n",
    "- Rating normalization will be done once after extraction, not per-row\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Splits\n",
    "- Split into train/test/val sets.  \n",
    "- Ensure every player and every opening appears at least once in the training data.  \n",
    "- Strategy:  \n",
    "  - Sample unique players and openings to guarantee coverage in train.  \n",
    "  - Remaining data ‚Üí stratified random split into train/test.  \n",
    "  - Deduplicate and merge unique IDs back into train if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Enumerate Categorical Variables\n",
    "- Enumerate `eco` (if included) as an integer categorical variable.  \n",
    "- Confirm all columns are numeric and compatible with PyTorch tensors.  \n",
    "- Verify no missing or out-of-range IDs.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Data Structure\n",
    "- Each row: one `(player_id, opening_id, score)` record.\n",
    "- Include other fields- eco, num games etc\n",
    "- Convert DataFrame to PyTorch tensors (`torch.long` for IDs, `torch.float` for scores).  \n",
    "- Log dataset shapes and sparsity metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Training Setup\n",
    "Define constants:\n",
    "- `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_FACTORS`  \n",
    "- Loss functions: MSE and RMSE  \n",
    "- Activation: sigmoid or none (depending on score normalization)  \n",
    "- Optimizer: SGD  \n",
    "- Figure out if there's anything else we need to design or specify\n",
    "\n",
    "Implement helper functions:\n",
    "- `train_one_epoch()`\n",
    "- `evaluate_model()`\n",
    "- `calculate_rmse()`\n",
    "- `save_checkpoint()`  \n",
    "\n",
    "Ensure detailed logging, ETA reporting, and reproducible random seeds.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Training Loop\n",
    "- Initialize player and opening embeddings.  \n",
    "- Iterate through epochs with mini-batch SGD (`BATCH_SIZE = 1024`).  \n",
    "- Compute and log MSE/RMSE per epoch.  \n",
    "- Save model checkpoints locally after each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Evaluation\n",
    "- Evaluate on test set.  \n",
    "- Report MSE, RMSE, and visual diagnostics (predicted vs actual score).  \n",
    "- Inspect a few player and opening latent factors for sanity.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Cross-Validation & Hyperparameter Tuning\n",
    "- Define ranges for:  \n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`  \n",
    "- Perform small-scale grid or random search for best configuration.  \n",
    "- Compare validation RMSE across runs.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Next Steps\n",
    "- Extend model to include Black openings.  \n",
    "- Experiment with hybrid inputs (player rating, ECO grouping).  \n",
    "- Consider implicit feedback handling (unplayed openings as zeros).  \n",
    "- Integrate trained model into API for recommendation output.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**  \n",
    "- Every random seed and parameter definition will be explicit.  \n",
    "- Every major step includes row-count, schema, and type validation.  \n",
    "- Model artifacts and logs will be saved locally for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d823",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "Connect to DuckDB and extract all player-opening statistics.\n",
    "Verify schema and perform sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcea569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA EXTRACTION\n",
      "============================================================\n",
      "\n",
      "üìÅ Database: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "üìÅ Database exists: True\n",
      "üé® Color filter: White\n",
      "üîí Minimum holdout players: 1,000\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path.cwd() / 'utils'))\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = Path.cwd().parent / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "COLOR_FILTER = 'w'  # 'w' for white, 'b' for black\n",
    "MIN_HOLDOUT_PLAYERS = 1000  # Minimum number of players to reserve for fold-in verification. These will not be used at all in this notebook for training, test/val or anything else.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: DATA EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìÅ Database: {DB_PATH}\")\n",
    "print(f\"üìÅ Database exists: {DB_PATH.exists()}\")\n",
    "print(f\"üé® Color filter: {'White' if COLOR_FILTER == 'w' else 'Black'}\")\n",
    "print(f\"üîí Minimum holdout players: {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9435033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£  Extracting player-opening statistics (color: 'w')...\n",
      "   ‚Ä¢ Minimum rating filter: 1200\n",
      "\n",
      "2Ô∏è‚É£  Selecting holdout players for fold-in verification...\n",
      "   ‚Ä¢ Holdout size: 1,000 players minimum\n",
      "   ‚Ä¢ Total eligible players: 49,551\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 48,551\n",
      "   ‚Ä¢ Holdout percentage: 2.0%\n",
      "   ‚Ä¢ Total eligible players: 49,551\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 48,551\n",
      "   ‚Ä¢ Holdout percentage: 2.0%\n",
      "\n",
      "3Ô∏è‚É£  Extracting training data (excluding holdout players)...\n",
      "\n",
      "3Ô∏è‚É£  Extracting training data (excluding holdout players)...\n",
      "   ‚úì Extracted 11,560,642 rows\n",
      "\n",
      "   üíæ Saved holdout_players_df with 1,000 player IDs\n",
      "   ‚Ä¢ These players are COMPLETELY UNSEEN by the training process\n",
      "   ‚Ä¢ Use them later for fold-in verification\n",
      "\n",
      "4Ô∏è‚É£  Verifying schema...\n",
      "   ‚úì All required columns present: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "5Ô∏è‚É£  Checking data types...\n",
      "   ‚Ä¢ player_id: int32\n",
      "   ‚Ä¢ opening_id: int32\n",
      "   ‚Ä¢ num_games: int32\n",
      "   ‚Ä¢ score: float64\n",
      "   ‚Ä¢ eco: object\n",
      "\n",
      "6Ô∏è‚É£  Data statistics...\n",
      "   ‚Ä¢ Total rows: 11,560,642\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "   ‚Ä¢ Total games (sum): 228,796,788\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 50000\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 13462\n",
      "   ‚Ä¢ Mean: 19.8\n",
      "   ‚úì Extracted 11,560,642 rows\n",
      "\n",
      "   üíæ Saved holdout_players_df with 1,000 player IDs\n",
      "   ‚Ä¢ These players are COMPLETELY UNSEEN by the training process\n",
      "   ‚Ä¢ Use them later for fold-in verification\n",
      "\n",
      "4Ô∏è‚É£  Verifying schema...\n",
      "   ‚úì All required columns present: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "5Ô∏è‚É£  Checking data types...\n",
      "   ‚Ä¢ player_id: int32\n",
      "   ‚Ä¢ opening_id: int32\n",
      "   ‚Ä¢ num_games: int32\n",
      "   ‚Ä¢ score: float64\n",
      "   ‚Ä¢ eco: object\n",
      "\n",
      "6Ô∏è‚É£  Data statistics...\n",
      "   ‚Ä¢ Total rows: 11,560,642\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "   ‚Ä¢ Total games (sum): 228,796,788\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 50000\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 13462\n",
      "   ‚Ä¢ Mean: 19.8\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5007\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5007\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0          1          39          6  0.666667  A00\n",
      "1          1          49          2  0.500000  A00\n",
      "2          1          53          1  0.000000  A00\n",
      "3          1         182          1  0.000000  A04\n",
      "4          1         187          1  1.000000  A04\n",
      "5          1         671          3  1.000000  B00\n",
      "6          1         675          3  0.000000  B00\n",
      "7          1         677          6  0.333333  B00\n",
      "8          1         688         25  0.620000  B00\n",
      "9          1         717          1  1.000000  B00\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (11560642, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: 1,000 players held out for fold-in verification\n",
      "   ‚Ä¢ Access via: holdout_players_df\n",
      "   ‚Ä¢ These players will NOT appear in any training, validation, or test splits\n",
      "\n",
      "‚úì Database connection closed\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0          1          39          6  0.666667  A00\n",
      "1          1          49          2  0.500000  A00\n",
      "2          1          53          1  0.000000  A00\n",
      "3          1         182          1  0.000000  A04\n",
      "4          1         187          1  1.000000  A04\n",
      "5          1         671          3  1.000000  B00\n",
      "6          1         675          3  0.000000  B00\n",
      "7          1         677          6  0.333333  B00\n",
      "8          1         688         25  0.620000  B00\n",
      "9          1         717          1  1.000000  B00\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (11560642, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: 1,000 players held out for fold-in verification\n",
      "   ‚Ä¢ Access via: holdout_players_df\n",
      "   ‚Ä¢ These players will NOT appear in any training, validation, or test splits\n",
      "\n",
      "‚úì Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# Connect to DuckDB and extract player-opening statistics\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player-opening statistics (color: '{COLOR_FILTER}')...\")\n",
    "    \n",
    "    # Extract stats with calculated score and num_games\n",
    "    # Filter by color, minimum rating, and calculate score in the database\n",
    "    MIN_RATING = 1200\n",
    "    print(f\"   ‚Ä¢ Minimum rating filter: {MIN_RATING}\")\n",
    "    \n",
    "    # First, get all eligible players and randomly select holdout set\n",
    "    print(f\"\\n2Ô∏è‚É£  Selecting holdout players for fold-in verification...\")\n",
    "    print(f\"   ‚Ä¢ Holdout size: {MIN_HOLDOUT_PLAYERS:,} players minimum\")\n",
    "    \n",
    "    # Get all players with sufficient data\n",
    "    player_query = f\"\"\"\n",
    "        SELECT DISTINCT p.id as player_id\n",
    "        FROM player p\n",
    "        JOIN player_opening_stats pos ON p.id = pos.player_id\n",
    "        WHERE p.rating >= {MIN_RATING}\n",
    "        AND pos.color = '{COLOR_FILTER}'\n",
    "    \"\"\"\n",
    "    \n",
    "    all_eligible_players = pd.DataFrame(con.execute(player_query).df())\n",
    "    total_eligible = len(all_eligible_players)\n",
    "    print(f\"   ‚Ä¢ Total eligible players: {total_eligible:,}\")\n",
    "    \n",
    "    if total_eligible < MIN_HOLDOUT_PLAYERS:\n",
    "        raise ValueError(f\"Not enough eligible players ({total_eligible:,}) to create holdout set of {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "    \n",
    "    # Randomly sample holdout players (deterministic with seed)\n",
    "    import numpy as np\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    holdout_player_ids = np.random.choice(\n",
    "        all_eligible_players['player_id'].values,\n",
    "        size=MIN_HOLDOUT_PLAYERS,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    training_player_ids = set(all_eligible_players['player_id'].values) - set(holdout_player_ids)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Holdout players selected: {len(holdout_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Training players available: {len(training_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Holdout percentage: {100 * len(holdout_player_ids) / total_eligible:.1f}%\")\n",
    "    \n",
    "    # Convert training player IDs to SQL-friendly string\n",
    "    training_player_ids_str = ','.join(map(str, training_player_ids))\n",
    "    \n",
    "    # Extract data ONLY for training players\n",
    "    print(f\"\\n3Ô∏è‚É£  Extracting training data (excluding holdout players)...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            pos.player_id,\n",
    "            pos.opening_id,\n",
    "            pos.num_wins + pos.num_draws + pos.num_losses as num_games,\n",
    "            (pos.num_wins + (pos.num_draws * 0.5)) / \n",
    "                NULLIF(pos.num_wins + pos.num_draws + pos.num_losses, 0) as score,\n",
    "            o.eco\n",
    "        FROM player_opening_stats pos\n",
    "        JOIN opening o ON pos.opening_id = o.id\n",
    "        JOIN player p ON pos.player_id = p.id\n",
    "        WHERE pos.color = '{COLOR_FILTER}'\n",
    "        AND p.rating >= {MIN_RATING}\n",
    "        AND pos.player_id IN ({training_player_ids_str})\n",
    "        ORDER BY pos.player_id, pos.opening_id\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_data = pd.DataFrame(con.execute(query).df())\n",
    "    \n",
    "    print(f\"   ‚úì Extracted {len(raw_data):,} rows\")\n",
    "    \n",
    "    # Also save holdout player IDs for later use\n",
    "    holdout_players_df = pd.DataFrame({'player_id': holdout_player_ids})\n",
    "    print(f\"\\n   üíæ Saved holdout_players_df with {len(holdout_players_df):,} player IDs\")\n",
    "    print(f\"   ‚Ä¢ These players are COMPLETELY UNSEEN by the training process\")\n",
    "    print(f\"   ‚Ä¢ Use them later for fold-in verification\")\n",
    "    \n",
    "    # Schema verification\n",
    "    print(\"\\n4Ô∏è‚É£  Verifying schema...\")\n",
    "    required_columns = ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
    "    \n",
    "    for col in required_columns:\n",
    "        if col not in raw_data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    print(f\"   ‚úì All required columns present: {required_columns}\")\n",
    "    \n",
    "    # Data types verification\n",
    "    print(\"\\n5Ô∏è‚É£  Checking data types...\")\n",
    "    print(f\"   ‚Ä¢ player_id: {raw_data['player_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ opening_id: {raw_data['opening_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ num_games: {raw_data['num_games'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ score: {raw_data['score'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ eco: {raw_data['eco'].dtype}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n6Ô∏è‚É£  Data statistics...\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {len(raw_data):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Total games (sum): {raw_data['num_games'].sum():,}\")\n",
    "    \n",
    "    # Player ID range\n",
    "    print(f\"\\n   Player ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['player_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['player_id'].max()}\")\n",
    "    \n",
    "    # Opening ID range\n",
    "    print(f\"\\n   Opening ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['opening_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['opening_id'].max()}\")\n",
    "    \n",
    "    # Games per entry statistics\n",
    "    print(f\"\\n   Games per entry:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['num_games'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['num_games'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['num_games'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['num_games'].median():.0f}\")\n",
    "    \n",
    "    # Score statistics\n",
    "    print(f\"\\n   Score distribution:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['score'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['score'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['score'].mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['score'].median():.4f}\")\n",
    "    \n",
    "    # Check for null values\n",
    "    print(\"\\n7Ô∏è‚É£  Checking for null values...\")\n",
    "    null_counts = raw_data.isnull().sum()\n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"   ‚úì No null values found\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(\"\\n8Ô∏è‚É£  Sample of extracted data (first 10 rows):\")\n",
    "    print(raw_data.head(10).to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nData shape: {raw_data.shape}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "    print(f\"\\n‚ö†Ô∏è  IMPORTANT: {len(holdout_player_ids):,} players held out for fold-in verification\")\n",
    "    print(f\"   ‚Ä¢ Access via: holdout_players_df\")\n",
    "    print(f\"   ‚Ä¢ These players will NOT appear in any training, validation, or test splits\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"\\n‚úì Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad110b85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "519d8f2f",
   "metadata": {},
   "source": [
    "## Step 2: Data Sanitization & Normalization\n",
    "\n",
    "Filter low-quality data, handle duplicates, and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74f0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: DATA SANITIZATION & NORMALIZATION\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ MIN_GAMES_THRESHOLD: 10\n",
      "\n",
      "üìä Starting data shape: (11560642, 5)\n",
      "   ‚Ä¢ Rows: 11,560,642\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Before: 11,560,642 rows\n",
      "   ‚Ä¢ After: 2,896,295 rows\n",
      "   ‚Ä¢ Filtered out: 8,664,347 rows (74.9%)\n",
      "\n",
      "2Ô∏è‚É£  Checking for duplicate (player_id, opening_id) combinations...\n",
      "   ‚Ä¢ Before: 11,560,642 rows\n",
      "   ‚Ä¢ After: 2,896,295 rows\n",
      "   ‚Ä¢ Filtered out: 8,664,347 rows (74.9%)\n",
      "\n",
      "2Ô∏è‚É£  Checking for duplicate (player_id, opening_id) combinations...\n",
      "   ‚úì No duplicates found\n",
      "\n",
      "3Ô∏è‚É£  Removing players with no qualifying openings...\n",
      "   ‚úì No duplicates found\n",
      "\n",
      "3Ô∏è‚É£  Removing players with no qualifying openings...\n",
      "   ‚Ä¢ Players before: 48,469\n",
      "   ‚Ä¢ Players after: 48,469\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "4Ô∏è‚É£  Removing openings with no qualifying players...\n",
      "   ‚Ä¢ Openings before: 2,714\n",
      "   ‚Ä¢ Openings after: 2,714\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "5Ô∏è‚É£  Verifying no null values...\n",
      "   ‚Ä¢ Players before: 48,469\n",
      "   ‚Ä¢ Players after: 48,469\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "4Ô∏è‚É£  Removing openings with no qualifying players...\n",
      "   ‚Ä¢ Openings before: 2,714\n",
      "   ‚Ä¢ Openings after: 2,714\n",
      "   ‚Ä¢ Removed: 0\n",
      "\n",
      "5Ô∏è‚É£  Verifying no null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Final data statistics:\n",
      "   ‚Ä¢ Total rows: 2,896,295\n",
      "   ‚Ä¢ Unique players: 48,469\n",
      "   ‚Ä¢ Unique openings: 2,714\n",
      "   ‚Ä¢ Total games: 206,215,295\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1067.2\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Final data statistics:\n",
      "   ‚Ä¢ Total rows: 2,896,295\n",
      "   ‚Ä¢ Unique players: 48,469\n",
      "   ‚Ä¢ Unique openings: 2,714\n",
      "   ‚Ä¢ Total games: 206,215,295\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1067.2\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5111\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "20929          370        2058         82  0.689024  C50\n",
      "2110167      35669        1821         72  0.513889  C40\n",
      "753302       12646        3180        152  0.470395  C50\n",
      "1897554      32072        2052         56  0.544643  C50\n",
      "2389705      40414        2891         11  0.363636  E11\n",
      "559443        9428        2548         22  0.386364  D17\n",
      "1626418      27518        2110         26  0.653846  C53\n",
      "1621363      27430         794         41  0.512195  B02\n",
      "2693734      45684        1385        102  0.480392  C02\n",
      "2168988      36708        3250         12  0.541667  B06\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2896295, 5)\n",
      "Data reduction: 74.9%\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5111\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "20929          370        2058         82  0.689024  C50\n",
      "2110167      35669        1821         72  0.513889  C40\n",
      "753302       12646        3180        152  0.470395  C50\n",
      "1897554      32072        2052         56  0.544643  C50\n",
      "2389705      40414        2891         11  0.363636  E11\n",
      "559443        9428        2548         22  0.386364  D17\n",
      "1626418      27518        2110         26  0.653846  C53\n",
      "1621363      27430         794         41  0.512195  B02\n",
      "2693734      45684        1385        102  0.480392  C02\n",
      "2168988      36708        3250         12  0.541667  B06\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2896295, 5)\n",
      "Data reduction: 74.9%\n"
     ]
    }
   ],
   "source": [
    "# 2a. Filter low-quality data, handle duplicates, and prepare for training.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "MIN_GAMES_THRESHOLD = 10\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: DATA SANITIZATION & NORMALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   ‚Ä¢ MIN_GAMES_THRESHOLD: {MIN_GAMES_THRESHOLD}\")\n",
    "\n",
    "# Start with raw_data from Step 1\n",
    "print(f\"\\nüìä Starting data shape: {raw_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Rows: {len(raw_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "\n",
    "# 1. Filter by minimum games threshold\n",
    "print(f\"\\n1Ô∏è‚É£  Filtering entries with < {MIN_GAMES_THRESHOLD} games...\")\n",
    "before_filter = len(raw_data)\n",
    "clean_data = raw_data.query(f'num_games >= {MIN_GAMES_THRESHOLD}').copy()\n",
    "num_rows_after_filter = len(clean_data)\n",
    "num_rows_filtered_out = before_filter - num_rows_after_filter\n",
    "\n",
    "print(f\"   ‚Ä¢ Before: {before_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ After: {num_rows_after_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ Filtered out: {num_rows_filtered_out:,} rows ({100*num_rows_filtered_out/before_filter:.1f}%)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "print(f\"\\n2Ô∏è‚É£  Checking for duplicate (player_id, opening_id) combinations...\")\n",
    "num_duplicates = clean_data.duplicated(subset=['player_id', 'opening_id']).sum()\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {num_duplicates} duplicate entries\")\n",
    "    dup_mask = clean_data.duplicated(subset=['player_id', 'opening_id'], keep=False)\n",
    "    print(\"\\n   Sample of duplicates:\")\n",
    "    print(clean_data[dup_mask].head(10).to_string())\n",
    "    \n",
    "    # Keep only first occurrence of any duplicate player-opening pair\n",
    "    print(\"\\n   Removing duplicates (keeping first occurrence)...\")\n",
    "    clean_data = pd.DataFrame.drop_duplicates(clean_data, subset=['player_id', 'opening_id'], keep='first')\n",
    "    print(f\"   ‚úì After deduplication: {len(clean_data):,} rows\")\n",
    "else:\n",
    "    print(f\"   ‚úì No duplicates found\")\n",
    "\n",
    "# 3. Remove players with no qualifying openings\n",
    "print(f\"\\n3Ô∏è‚É£  Removing players with no qualifying openings...\") # Note that a few players only play stuff like the Van't Kruijs which we've excluded, so a small numer of players will be excluded here\n",
    "players_before = clean_data['player_id'].nunique()\n",
    "\n",
    "# Count openings per player\n",
    "num_openings_per_player = pd.DataFrame(clean_data.groupby('player_id').size(), columns=['count'])\n",
    "players_with_data = num_openings_per_player[num_openings_per_player['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter\n",
    "clean_data = clean_data[clean_data['player_id'].isin(players_with_data)]\n",
    "players_after = clean_data['player_id'].nunique()\n",
    "\n",
    "print(f\"   ‚Ä¢ Players before: {players_before:,}\")\n",
    "print(f\"   ‚Ä¢ Players after: {players_after:,}\")\n",
    "print(f\"   ‚Ä¢ Removed: {players_before - players_after}\")\n",
    "\n",
    "# 4. Remove openings with no qualifying players\n",
    "print(f\"\\n4Ô∏è‚É£  Removing openings with no qualifying players...\")\n",
    "num_openings_before = clean_data['opening_id'].nunique()\n",
    "\n",
    "# Use pd.DataFrame.groupby() to count players per opening\n",
    "num_players_per_opening = pd.DataFrame(clean_data.groupby('opening_id').size(), columns=['count'])\n",
    "openings_with_data = num_players_per_opening[num_players_per_opening['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter using pd.DataFrame.isin()\n",
    "clean_data = clean_data[clean_data['opening_id'].isin(openings_with_data)]\n",
    "openings_after = clean_data['opening_id'].nunique()\n",
    "\n",
    "print(f\"   ‚Ä¢ Openings before: {num_openings_before:,}\")\n",
    "print(f\"   ‚Ä¢ Openings after: {openings_after:,}\")\n",
    "print(f\"   ‚Ä¢ Removed: {num_openings_before - openings_after}\")\n",
    "\n",
    "# 5. Verify no null values using pd.isna()\n",
    "print(f\"\\n5Ô∏è‚É£  Verifying no null values...\")\n",
    "null_counts = pd.DataFrame.isna(clean_data).sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"   ‚úì No null values found\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "    for col, count in null_counts[null_counts > 0].items():\n",
    "        print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    # Drop rows with nulls using pd.DataFrame.dropna()\n",
    "    clean_data = pd.DataFrame.dropna(clean_data)\n",
    "    print(f\"   ‚úì Dropped null rows. New shape: {clean_data.shape}\")\n",
    "\n",
    "# TODO: Add confidence weighting column\n",
    "# TODO: Extract and normalize player ratings (side information)\n",
    "\n",
    "# Reset index using pd.DataFrame.reset_index()\n",
    "clean_data = pd.DataFrame.reset_index(clean_data, drop=True)\n",
    "\n",
    "# Final statistics using pd functions\n",
    "print(f\"\\n6Ô∏è‚É£  Final data statistics:\")\n",
    "print(f\"   ‚Ä¢ Total rows: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {pd.Series.nunique(clean_data['player_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {pd.Series.nunique(clean_data['opening_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Total games: {pd.Series.sum(clean_data['num_games']):,}\")\n",
    "print(f\"   ‚Ä¢ Avg games per entry: {pd.Series.mean(clean_data['num_games']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg openings per player: {len(clean_data) / pd.Series.nunique(clean_data['player_id']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg players per opening: {len(clean_data) / pd.Series.nunique(clean_data['opening_id']):.1f}\")\n",
    "\n",
    "# Score distribution using pd functions\n",
    "print(f\"\\n   Score statistics:\")\n",
    "print(f\"   ‚Ä¢ Min: {pd.Series.min(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {pd.Series.quantile(clean_data['score'], 0.25):.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {pd.Series.median(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {pd.Series.quantile(clean_data['score'], 0.75):.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {pd.Series.max(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {pd.Series.mean(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {pd.Series.std(clean_data['score']):.4f}\")\n",
    "\n",
    "# Sample of cleaned data using pd.DataFrame.sample()\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of cleaned data (10 random rows):\")\n",
    "print(pd.DataFrame.sample(clean_data, min(10, len(clean_data)), random_state=42).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SANITIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCleaned data shape: {clean_data.shape}\")\n",
    "print(f\"Data reduction: {100 * (1 - len(clean_data)/len(raw_data)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e671b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ K_PLAYER (shrinkage constant): 50\n",
      "   ‚Ä¢ Method: Two-level empirical Bayes shrinkage\n",
      "   ‚Ä¢ Level 1: Calculate opening-specific means\n",
      "   ‚Ä¢ Level 2: Shrink player scores toward opening means\n",
      "\n",
      "üìä Global statistics:\n",
      "   ‚Ä¢ Global mean score: 0.5111\n",
      "   ‚Ä¢ Total entries: 2,896,295\n",
      "   ‚Ä¢ Unique openings: 2,714\n",
      "\n",
      "1Ô∏è‚É£  LEVEL 1: Calculating opening-specific means...\n",
      "   ‚úì Calculated means for 2,714 openings\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1500\n",
      "   ‚Ä¢ 25th percentile: 0.4960\n",
      "   ‚Ä¢ Median: 0.5164\n",
      "   ‚Ä¢ 75th percentile: 0.5366\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0508\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4780\n",
      "   ‚Ä¢ Players per opening (median): 157\n",
      "   ‚Ä¢ Total games range: [10, 5496308]\n",
      "   ‚Ä¢ Players range: [1, 42901]\n",
      "   ‚úì Calculated means for 2,714 openings\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1500\n",
      "   ‚Ä¢ 25th percentile: 0.4960\n",
      "   ‚Ä¢ Median: 0.5164\n",
      "   ‚Ä¢ 75th percentile: 0.5366\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0508\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4780\n",
      "   ‚Ä¢ Players per opening (median): 157\n",
      "   ‚Ä¢ Total games range: [10, 5496308]\n",
      "   ‚Ä¢ Players range: [1, 42901]\n",
      "\n",
      "2Ô∏è‚É£  LEVEL 2: Shrinking player scores toward opening means...\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,896,295 entries\n",
      "\n",
      "3Ô∏è‚É£  Calculating confidence weights...\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Formula: confidence = num_games / (num_games + 50)\n",
      "   ‚Ä¢ Range: [0.1667, 0.9963]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000961\n",
      "   ‚Ä¢ Std adjustment: 0.076770\n",
      "   ‚Ä¢ Max adjustment: 0.457516\n",
      "   ‚Ä¢ Min adjustment: -0.457512\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003650\n",
      "\n",
      "2Ô∏è‚É£  LEVEL 2: Shrinking player scores toward opening means...\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,896,295 entries\n",
      "\n",
      "3Ô∏è‚É£  Calculating confidence weights...\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Formula: confidence = num_games / (num_games + 50)\n",
      "   ‚Ä¢ Range: [0.1667, 0.9963]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000961\n",
      "   ‚Ä¢ Std adjustment: 0.076770\n",
      "   ‚Ä¢ Max adjustment: 0.457516\n",
      "   ‚Ä¢ Min adjustment: -0.457512\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003650\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001796\n",
      "   ‚Ä¢ 75th percentile (n=66 games): avg adjustment = -0.000418\n",
      "   ‚Ä¢ >75th percentile (n>66 games): avg adjustment = -0.001363\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ Median: 0.5118\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001796\n",
      "   ‚Ä¢ 75th percentile (n=66 games): avg adjustment = -0.000418\n",
      "   ‚Ä¢ >75th percentile (n>66 games): avg adjustment = -0.001363\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ Median: 0.5118\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   Player 43954 | Opening  708 | Games:  13 | Opening mean: 0.4788 | Original: 0.5000 ‚Üí Adjusted: 0.4831 | Diff: -0.0169 | Confidence: 0.206\n",
      "   Player  6018 | Opening 1357 | Games:  13 | Opening mean: 0.4984 | Original: 0.4615 ‚Üí Adjusted: 0.4908 | Diff: +0.0293 | Confidence: 0.206\n",
      "   Player 45268 | Opening 1135 | Games:  12 | Opening mean: 0.5645 | Original: 0.5000 ‚Üí Adjusted: 0.5520 | Diff: +0.0520 | Confidence: 0.194\n",
      "   Player 49212 | Opening 2060 | Games:  17 | Opening mean: 0.5072 | Original: 0.4706 ‚Üí Adjusted: 0.4979 | Diff: +0.0273 | Confidence: 0.254\n",
      "   Player 25718 | Opening 2941 | Games:  11 | Opening mean: 0.5225 | Original: 0.4091 ‚Üí Adjusted: 0.5020 | Diff: +0.0929 | Confidence: 0.180\n",
      "   Player  1726 | Opening 1591 | Games:  10 | Opening mean: 0.5110 | Original: 0.3500 ‚Üí Adjusted: 0.4842 | Diff: +0.1342 | Confidence: 0.167\n",
      "   Player 32444 | Opening 1875 | Games:  10 | Opening mean: 0.5083 | Original: 0.4000 ‚Üí Adjusted: 0.4902 | Diff: +0.0902 | Confidence: 0.167\n",
      "   Player  4690 | Opening 1914 | Games:  13 | Opening mean: 0.5186 | Original: 0.4615 ‚Üí Adjusted: 0.5069 | Diff: +0.0453 | Confidence: 0.206\n",
      "   Player 33413 | Opening  491 | Games:  10 | Opening mean: 0.5354 | Original: 0.7500 ‚Üí Adjusted: 0.5712 | Diff: -0.1788 | Confidence: 0.167\n",
      "   Player  2164 | Opening 2527 | Games:  11 | Opening mean: 0.5309 | Original: 0.5909 ‚Üí Adjusted: 0.5417 | Diff: -0.0492 | Confidence: 0.180\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 37041 | Opening 2006 | Games:  88 | Opening mean: 0.5139 | Original: 0.5568 ‚Üí Adjusted: 0.5413 | Diff: -0.0155 | Confidence: 0.638\n",
      "   Player 33602 | Opening  133 | Games:  54 | Opening mean: 0.5015 | Original: 0.5093 ‚Üí Adjusted: 0.5055 | Diff: -0.0037 | Confidence: 0.519\n",
      "   Player 42533 | Opening 2651 | Games:  53 | Opening mean: 0.5203 | Original: 0.6132 ‚Üí Adjusted: 0.5681 | Diff: -0.0451 | Confidence: 0.515\n",
      "   Player  8908 | Opening  854 | Games:  73 | Opening mean: 0.5366 | Original: 0.5616 ‚Üí Adjusted: 0.5515 | Diff: -0.0102 | Confidence: 0.593\n",
      "   Player 19885 | Opening  387 | Games:  70 | Opening mean: 0.4830 | Original: 0.5286 ‚Üí Adjusted: 0.5096 | Diff: -0.0190 | Confidence: 0.583\n",
      "   Player 46963 | Opening  911 | Games:  57 | Opening mean: 0.5230 | Original: 0.3772 ‚Üí Adjusted: 0.4453 | Diff: +0.0681 | Confidence: 0.533\n",
      "   Player 19695 | Opening 1977 | Games:  61 | Opening mean: 0.5400 | Original: 0.4918 ‚Üí Adjusted: 0.5135 | Diff: +0.0217 | Confidence: 0.550\n",
      "   Player 31518 | Opening  737 | Games:  83 | Opening mean: 0.5084 | Original: 0.4699 ‚Üí Adjusted: 0.4844 | Diff: +0.0145 | Confidence: 0.624\n",
      "   Player 18996 | Opening  768 | Games:  95 | Opening mean: 0.4875 | Original: 0.4789 ‚Üí Adjusted: 0.4819 | Diff: +0.0029 | Confidence: 0.655\n",
      "   Player 14260 | Opening 1194 | Games:  53 | Opening mean: 0.4953 | Original: 0.3962 ‚Üí Adjusted: 0.4443 | Diff: +0.0481 | Confidence: 0.515\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 43954 | Opening  708 | Games:  13 | Opening mean: 0.4788 | Original: 0.5000 ‚Üí Adjusted: 0.4831 | Diff: -0.0169 | Confidence: 0.206\n",
      "   Player  6018 | Opening 1357 | Games:  13 | Opening mean: 0.4984 | Original: 0.4615 ‚Üí Adjusted: 0.4908 | Diff: +0.0293 | Confidence: 0.206\n",
      "   Player 45268 | Opening 1135 | Games:  12 | Opening mean: 0.5645 | Original: 0.5000 ‚Üí Adjusted: 0.5520 | Diff: +0.0520 | Confidence: 0.194\n",
      "   Player 49212 | Opening 2060 | Games:  17 | Opening mean: 0.5072 | Original: 0.4706 ‚Üí Adjusted: 0.4979 | Diff: +0.0273 | Confidence: 0.254\n",
      "   Player 25718 | Opening 2941 | Games:  11 | Opening mean: 0.5225 | Original: 0.4091 ‚Üí Adjusted: 0.5020 | Diff: +0.0929 | Confidence: 0.180\n",
      "   Player  1726 | Opening 1591 | Games:  10 | Opening mean: 0.5110 | Original: 0.3500 ‚Üí Adjusted: 0.4842 | Diff: +0.1342 | Confidence: 0.167\n",
      "   Player 32444 | Opening 1875 | Games:  10 | Opening mean: 0.5083 | Original: 0.4000 ‚Üí Adjusted: 0.4902 | Diff: +0.0902 | Confidence: 0.167\n",
      "   Player  4690 | Opening 1914 | Games:  13 | Opening mean: 0.5186 | Original: 0.4615 ‚Üí Adjusted: 0.5069 | Diff: +0.0453 | Confidence: 0.206\n",
      "   Player 33413 | Opening  491 | Games:  10 | Opening mean: 0.5354 | Original: 0.7500 ‚Üí Adjusted: 0.5712 | Diff: -0.1788 | Confidence: 0.167\n",
      "   Player  2164 | Opening 2527 | Games:  11 | Opening mean: 0.5309 | Original: 0.5909 ‚Üí Adjusted: 0.5417 | Diff: -0.0492 | Confidence: 0.180\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 37041 | Opening 2006 | Games:  88 | Opening mean: 0.5139 | Original: 0.5568 ‚Üí Adjusted: 0.5413 | Diff: -0.0155 | Confidence: 0.638\n",
      "   Player 33602 | Opening  133 | Games:  54 | Opening mean: 0.5015 | Original: 0.5093 ‚Üí Adjusted: 0.5055 | Diff: -0.0037 | Confidence: 0.519\n",
      "   Player 42533 | Opening 2651 | Games:  53 | Opening mean: 0.5203 | Original: 0.6132 ‚Üí Adjusted: 0.5681 | Diff: -0.0451 | Confidence: 0.515\n",
      "   Player  8908 | Opening  854 | Games:  73 | Opening mean: 0.5366 | Original: 0.5616 ‚Üí Adjusted: 0.5515 | Diff: -0.0102 | Confidence: 0.593\n",
      "   Player 19885 | Opening  387 | Games:  70 | Opening mean: 0.4830 | Original: 0.5286 ‚Üí Adjusted: 0.5096 | Diff: -0.0190 | Confidence: 0.583\n",
      "   Player 46963 | Opening  911 | Games:  57 | Opening mean: 0.5230 | Original: 0.3772 ‚Üí Adjusted: 0.4453 | Diff: +0.0681 | Confidence: 0.533\n",
      "   Player 19695 | Opening 1977 | Games:  61 | Opening mean: 0.5400 | Original: 0.4918 ‚Üí Adjusted: 0.5135 | Diff: +0.0217 | Confidence: 0.550\n",
      "   Player 31518 | Opening  737 | Games:  83 | Opening mean: 0.5084 | Original: 0.4699 ‚Üí Adjusted: 0.4844 | Diff: +0.0145 | Confidence: 0.624\n",
      "   Player 18996 | Opening  768 | Games:  95 | Opening mean: 0.4875 | Original: 0.4789 ‚Üí Adjusted: 0.4819 | Diff: +0.0029 | Confidence: 0.655\n",
      "   Player 14260 | Opening 1194 | Games:  53 | Opening mean: 0.4953 | Original: 0.3962 ‚Üí Adjusted: 0.4443 | Diff: +0.0481 | Confidence: 0.515\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 24984 | Opening 2754 | Games: 349 | Opening mean: 0.5157 | Original: 0.5759 ‚Üí Adjusted: 0.5684 | Diff: -0.0076 | Confidence: 0.875\n",
      "   Player 28502 | Opening 1356 | Games: 347 | Opening mean: 0.5054 | Original: 0.5072 ‚Üí Adjusted: 0.5070 | Diff: -0.0002 | Confidence: 0.874\n",
      "   Player 12502 | Opening 2521 | Games: 1211 | Opening mean: 0.5193 | Original: 0.5454 ‚Üí Adjusted: 0.5444 | Diff: -0.0010 | Confidence: 0.960\n",
      "   Player 19441 | Opening 1854 | Games: 265 | Opening mean: 0.5255 | Original: 0.4283 ‚Üí Adjusted: 0.4437 | Diff: +0.0154 | Confidence: 0.841\n",
      "   Player  8955 | Opening  493 | Games: 635 | Opening mean: 0.5299 | Original: 0.4906 ‚Üí Adjusted: 0.4934 | Diff: +0.0029 | Confidence: 0.927\n",
      "   Player 43069 | Opening 3248 | Games: 202 | Opening mean: 0.5947 | Original: 0.6312 ‚Üí Adjusted: 0.6239 | Diff: -0.0072 | Confidence: 0.802\n",
      "   Player 30451 | Opening  910 | Games: 225 | Opening mean: 0.5002 | Original: 0.4733 ‚Üí Adjusted: 0.4782 | Diff: +0.0049 | Confidence: 0.818\n",
      "   Player  3831 | Opening 2445 | Games: 235 | Opening mean: 0.5181 | Original: 0.4830 ‚Üí Adjusted: 0.4891 | Diff: +0.0062 | Confidence: 0.825\n",
      "   Player 19978 | Opening 1365 | Games: 285 | Opening mean: 0.5066 | Original: 0.5491 ‚Üí Adjusted: 0.5428 | Diff: -0.0063 | Confidence: 0.851\n",
      "   Player 30743 | Opening  309 | Games: 201 | Opening mean: 0.5171 | Original: 0.5199 ‚Üí Adjusted: 0.5194 | Diff: -0.0006 | Confidence: 0.801\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Player 24984 | Opening 2754 | Games: 349 | Opening mean: 0.5157 | Original: 0.5759 ‚Üí Adjusted: 0.5684 | Diff: -0.0076 | Confidence: 0.875\n",
      "   Player 28502 | Opening 1356 | Games: 347 | Opening mean: 0.5054 | Original: 0.5072 ‚Üí Adjusted: 0.5070 | Diff: -0.0002 | Confidence: 0.874\n",
      "   Player 12502 | Opening 2521 | Games: 1211 | Opening mean: 0.5193 | Original: 0.5454 ‚Üí Adjusted: 0.5444 | Diff: -0.0010 | Confidence: 0.960\n",
      "   Player 19441 | Opening 1854 | Games: 265 | Opening mean: 0.5255 | Original: 0.4283 ‚Üí Adjusted: 0.4437 | Diff: +0.0154 | Confidence: 0.841\n",
      "   Player  8955 | Opening  493 | Games: 635 | Opening mean: 0.5299 | Original: 0.4906 ‚Üí Adjusted: 0.4934 | Diff: +0.0029 | Confidence: 0.927\n",
      "   Player 43069 | Opening 3248 | Games: 202 | Opening mean: 0.5947 | Original: 0.6312 ‚Üí Adjusted: 0.6239 | Diff: -0.0072 | Confidence: 0.802\n",
      "   Player 30451 | Opening  910 | Games: 225 | Opening mean: 0.5002 | Original: 0.4733 ‚Üí Adjusted: 0.4782 | Diff: +0.0049 | Confidence: 0.818\n",
      "   Player  3831 | Opening 2445 | Games: 235 | Opening mean: 0.5181 | Original: 0.4830 ‚Üí Adjusted: 0.4891 | Diff: +0.0062 | Confidence: 0.825\n",
      "   Player 19978 | Opening 1365 | Games: 285 | Opening mean: 0.5066 | Original: 0.5491 ‚Üí Adjusted: 0.5428 | Diff: -0.0063 | Confidence: 0.851\n",
      "   Player 30743 | Opening  309 | Games: 201 | Opening mean: 0.5171 | Original: 0.5199 ‚Üí Adjusted: 0.5194 | Diff: -0.0006 | Confidence: 0.801\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4889 vs global) | 1 player entries\n",
      "   Opening 3551 (C19): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2881 (E10): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2046 (C49): mean = 0.7857 (+0.2746 vs global) | 1 player entries\n",
      "   Opening 1112 (B28): mean = 0.7750 (+0.2639 vs global) | 3 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4889 vs global) | 1 player entries\n",
      "   Opening 3551 (C19): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2881 (E10): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2046 (C49): mean = 0.7857 (+0.2746 vs global) | 1 player entries\n",
      "   Opening 1112 (B28): mean = 0.7750 (+0.2639 vs global) | 3 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 1763 (C37): mean = 0.1500 (-0.3611 vs global) | 1 player entries\n",
      "   Opening 2593 (D26): mean = 0.1667 (-0.3444 vs global) | 1 player entries\n",
      "   Opening  636 (A83): mean = 0.2006 (-0.3104 vs global) | 2 player entries\n",
      "   Opening 3496 (A40): mean = 0.2273 (-0.2838 vs global) | 1 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "   Opening 1763 (C37): mean = 0.1500 (-0.3611 vs global) | 1 player entries\n",
      "   Opening 2593 (D26): mean = 0.1667 (-0.3444 vs global) | 1 player entries\n",
      "   Opening  636 (A83): mean = 0.2006 (-0.3104 vs global) | 2 player entries\n",
      "   Opening 3496 (A40): mean = 0.2273 (-0.2838 vs global) | 1 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 35201 | Opening 3292 (C54) | Games: 19 | Opening mean: 0.7277 | Original: 0.8947 ‚Üí 0.7737\n",
      "      If we'd shrunk to global mean: 0.6167 (would lose +0.1569 of deserved credit)\n",
      "   Player 22984 | Opening 3292 (C54) | Games: 10 | Opening mean: 0.7277 | Original: 0.7000 ‚Üí 0.7231\n",
      "      If we'd shrunk to global mean: 0.5426 (would lose +0.1805 of deserved credit)\n",
      "   Player 23562 | Opening 3292 (C54) | Games: 12 | Opening mean: 0.7277 | Original: 0.7500 ‚Üí 0.7320\n",
      "      If we'd shrunk to global mean: 0.5573 (would lose +0.1747 of deserved credit)\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 35201 | Opening 3292 (C54) | Games: 19 | Opening mean: 0.7277 | Original: 0.8947 ‚Üí 0.7737\n",
      "      If we'd shrunk to global mean: 0.6167 (would lose +0.1569 of deserved credit)\n",
      "   Player 22984 | Opening 3292 (C54) | Games: 10 | Opening mean: 0.7277 | Original: 0.7000 ‚Üí 0.7231\n",
      "      If we'd shrunk to global mean: 0.5426 (would lose +0.1805 of deserved credit)\n",
      "   Player 23562 | Opening 3292 (C54) | Games: 12 | Opening mean: 0.7277 | Original: 0.7500 ‚Üí 0.7320\n",
      "      If we'd shrunk to global mean: 0.5573 (would lose +0.1747 of deserved credit)\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player  1857 | Opening 2293 (C71) | Games: 13 | Opening mean: 0.2575 | Original: 0.1538 ‚Üí 0.2361\n",
      "      If we'd shrunk to global mean: 0.4374 (would unfairly boost by +0.2013)\n",
      "   Player 40878 | Opening  668 (A98) | Games: 12 | Opening mean: 0.2500 | Original: 0.2500 ‚Üí 0.2500\n",
      "      If we'd shrunk to global mean: 0.4605 (would unfairly boost by +0.2105)\n",
      "   Player  8240 | Opening 1779 (C37) | Games: 16 | Opening mean: 0.3347 | Original: 0.2812 ‚Üí 0.3217\n",
      "      If we'd shrunk to global mean: 0.4554 (would unfairly boost by +0.1336)\n",
      "\n",
      "9Ô∏è‚É£  Cleaning up...\n",
      "   ‚úì Removed temporary columns\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (2896295, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player  1857 | Opening 2293 (C71) | Games: 13 | Opening mean: 0.2575 | Original: 0.1538 ‚Üí 0.2361\n",
      "      If we'd shrunk to global mean: 0.4374 (would unfairly boost by +0.2013)\n",
      "   Player 40878 | Opening  668 (A98) | Games: 12 | Opening mean: 0.2500 | Original: 0.2500 ‚Üí 0.2500\n",
      "      If we'd shrunk to global mean: 0.4605 (would unfairly boost by +0.2105)\n",
      "   Player  8240 | Opening 1779 (C37) | Games: 16 | Opening mean: 0.3347 | Original: 0.2812 ‚Üí 0.3217\n",
      "      If we'd shrunk to global mean: 0.4554 (would unfairly boost by +0.1336)\n",
      "\n",
      "9Ô∏è‚É£  Cleaning up...\n",
      "   ‚úì Removed temporary columns\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (2896295, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n"
     ]
    }
   ],
   "source": [
    "# 2b. Apply hierarchical Bayesian shrinkage to adjust scores based on sample size confidence\n",
    "\n",
    "# Check if confidence already exists - if so, skip this processing\n",
    "if 'confidence' in clean_data.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'confidence' column already exists in data\")\n",
    "    print(\"   This indicates hierarchical Bayesian processing has already been applied.\")\n",
    "    print(f\"\\nCurrent data shape: {clean_data.shape}\")\n",
    "    print(f\"Confidence range: [{clean_data['confidence'].min():.4f}, {clean_data['confidence'].max():.4f}]\")\n",
    "else:\n",
    "    # Define the processing function\n",
    "    # This is a long function, I recommend you fold it down in your editor\n",
    "    def apply_hierarchical_bayesian_shrinkage(data, k_player=50):\n",
    "        \"\"\"\n",
    "        Apply two-level hierarchical Bayesian shrinkage to adjust scores.\n",
    "        \n",
    "        A lot of our player-opening entries have a small number of games played, because openings are so specific.\n",
    "        This introduces sample size issues.\n",
    "        \n",
    "        We use TWO-LEVEL shrinkage:\n",
    "        Level 1: Calculate opening-specific means (these are our \"ground truth\" for each opening)\n",
    "        Level 2: Shrink individual player-opening scores toward their opening's mean\n",
    "        This is better than shrinking toward global mean because different openings have different baseline win rates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Clean data with columns: player_id, opening_id, score, num_games, eco\n",
    "        k_player : int\n",
    "            Shrinkage constant for player-opening scores (default: 50)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Data with adjusted scores and new 'confidence' column\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "        print(f\"   ‚Ä¢ K_PLAYER (shrinkage constant): {k_player}\")\n",
    "        print(f\"   ‚Ä¢ Method: Two-level empirical Bayes shrinkage\")\n",
    "        print(f\"   ‚Ä¢ Level 1: Calculate opening-specific means\")\n",
    "        print(f\"   ‚Ä¢ Level 2: Shrink player scores toward opening means\")\n",
    "        \n",
    "        # Calculate global mean score for comparison\n",
    "        global_mean_score = data[\"score\"].mean()\n",
    "        print(f\"\\nüìä Global statistics:\")\n",
    "        print(f\"   ‚Ä¢ Global mean score: {global_mean_score:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Total entries: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique openings: {data['opening_id'].nunique():,}\")\n",
    "        \n",
    "        # Store original scores for comparison\n",
    "        data = data.copy()  # Best practice: work on a copy\n",
    "        data[\"score_original\"] = data[\"score\"].copy()\n",
    "        \n",
    "        # LEVEL 1: Calculate opening-specific means and statistics\n",
    "        print(f\"\\n1Ô∏è‚É£  LEVEL 1: Calculating opening-specific means...\")\n",
    "        \n",
    "        opening_stats = (\n",
    "            data.groupby(\"opening_id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"score\": \"mean\",\n",
    "                    \"num_games\": \"sum\",\n",
    "                    \"player_id\": \"count\",  # Number of players who played this opening\n",
    "                }\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"score\": \"opening_mean\",\n",
    "                    \"num_games\": \"opening_total_games\",\n",
    "                    \"player_id\": \"opening_num_players\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úì Calculated means for {len(opening_stats):,} openings\")\n",
    "        \n",
    "        # Opening mean statistics\n",
    "        print(f\"\\n   Opening mean score distribution:\")\n",
    "        print(f\"   ‚Ä¢ Min: {opening_stats['opening_mean'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {opening_stats['opening_mean'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {opening_stats['opening_mean'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {opening_stats['opening_mean'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {opening_stats['opening_mean'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {opening_stats['opening_mean'].std():.4f}\")\n",
    "        \n",
    "        # Show distribution of opening sizes\n",
    "        print(f\"\\n   Opening sample size distribution:\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games per opening (median): {opening_stats['opening_total_games'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players per opening (median): {opening_stats['opening_num_players'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games range: [{opening_stats['opening_total_games'].min():.0f}, {opening_stats['opening_total_games'].max():.0f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players range: [{opening_stats['opening_num_players'].min():.0f}, {opening_stats['opening_num_players'].max():.0f}]\"\n",
    "        )\n",
    "        \n",
    "        # Merge opening means back into main dataframe\n",
    "        data = data.merge(\n",
    "            opening_stats[[\"opening_mean\"]], left_on=\"opening_id\", right_index=True, how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # LEVEL 2: Shrink player-opening scores toward opening-specific means\n",
    "        print(f\"\\n2Ô∏è‚É£  LEVEL 2: Shrinking player scores toward opening means...\")\n",
    "        print(\n",
    "            f\"   Formula: adjusted_score = (num_games √ó player_score + {k_player} √ó opening_mean) / (num_games + {k_player})\"\n",
    "        )\n",
    "        \n",
    "        numerator = (data[\"num_games\"] * data[\"score_original\"]) + (\n",
    "            k_player * data[\"opening_mean\"]\n",
    "        )\n",
    "        denominator = data[\"num_games\"] + k_player\n",
    "        data[\"score\"] = numerator / denominator\n",
    "        \n",
    "        print(f\"   ‚úì Scores adjusted for {len(data):,} entries\")\n",
    "        \n",
    "        # Calculate confidence weights (will be used in loss function later)\n",
    "        print(f\"\\n3Ô∏è‚É£  Calculating confidence weights...\")\n",
    "        data[\"confidence\"] = data[\"num_games\"] / (\n",
    "            data[\"num_games\"] + k_player\n",
    "        )\n",
    "        print(f\"   ‚úì Confidence weights calculated\")\n",
    "        print(f\"   ‚Ä¢ Formula: confidence = num_games / (num_games + {k_player})\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Range: [{data['confidence'].min():.4f}, {data['confidence'].max():.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # Statistics on the adjustment\n",
    "        score_diff = data[\"score\"] - data[\"score_original\"]\n",
    "        print(f\"\\n4Ô∏è‚É£  Adjustment statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean adjustment: {score_diff.mean():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Std adjustment: {score_diff.std():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Max adjustment: {score_diff.max():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Min adjustment: {score_diff.min():.6f}\")\n",
    "        \n",
    "        # Show distribution of adjustments\n",
    "        print(f\"\\n   Adjustment by num_games quartiles:\")\n",
    "        quartiles = data[\"num_games\"].quantile([0.25, 0.5, 0.75])\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 25th percentile (n={quartiles[0.25]:.0f} games): avg adjustment = {score_diff[data['num_games'] <= quartiles[0.25]].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 50th percentile (n={quartiles[0.5]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.25]) & (data['num_games'] <= quartiles[0.5])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 75th percentile (n={quartiles[0.75]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.5]) & (data['num_games'] <= quartiles[0.75])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ >75th percentile (n>{quartiles[0.75]:.0f} games): avg adjustment = {score_diff[data['num_games'] > quartiles[0.75]].mean():.6f}\"\n",
    "        )\n",
    "        \n",
    "        # New score distribution after adjustment\n",
    "        print(f\"\\n5Ô∏è‚É£  Adjusted score statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {data['score'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {data['score'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {data['score'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {data['score'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {data['score'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {data['score'].mean():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {data['score'].std():.4f}\")\n",
    "        \n",
    "        # Detailed sample showing the effect across different game counts\n",
    "        print(f\"\\n6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\")\n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        low_game_sample = data[\n",
    "            (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in low_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Medium-game entries (50-100 games) - MODERATE shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        med_game_sample = data[\n",
    "            (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in med_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   High-game entries (200+ games) - LOW shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        high_game_sample = data[data[\"num_games\"] >= 200].sample(\n",
    "            min(10, len(data[data[\"num_games\"] >= 200])), random_state=42\n",
    "        )\n",
    "        for idx, row in high_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        # Show extreme cases - comparing to both opening mean AND global mean\n",
    "        print(f\"\\n7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\")\n",
    "        \n",
    "        # Find entries where opening mean differs significantly from global mean\n",
    "        data[\"opening_deviation_from_global\"] = (\n",
    "            data[\"opening_mean\"] - global_mean_score\n",
    "        ).abs()\n",
    "        \n",
    "        print(f\"\\n   Openings with HIGHEST win rates (strong for White):\")\n",
    "        strong_openings = data.nlargest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in strong_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"(+{deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   Openings with LOWEST win rates (weak for White):\")\n",
    "        weak_openings = data.nsmallest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in weak_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"({deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        # Show specific examples where hierarchical shrinkage made a difference\n",
    "        print(f\"\\n8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\")\n",
    "        \n",
    "        # Find entries with strong openings where player did well\n",
    "        strong_opening_ids = data.nlargest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        strong_examples = data[\n",
    "            (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] > 0.6)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] > 0.6)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"\\n   Strong opening + good player performance (shrunk toward HIGH opening mean):\"\n",
    "        )\n",
    "        for idx, row in strong_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would lose {difference:+.4f} of deserved credit)\"\n",
    "            )\n",
    "        \n",
    "        # Find entries with weak openings where player did poorly\n",
    "        weak_opening_ids = data.nsmallest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        weak_examples = data[\n",
    "            (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] < 0.45)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] < 0.45)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Weak opening + poor player performance (shrunk toward LOW opening mean):\")\n",
    "        for idx, row in weak_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would unfairly boost by {-difference:+.4f})\"\n",
    "            )\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        print(f\"\\n9Ô∏è‚É£  Cleaning up...\")\n",
    "        data = data.drop(\n",
    "            columns=[\"score_original\", \"opening_mean\", \"opening_deviation_from_global\"]\n",
    "        )\n",
    "        print(f\"   ‚úì Removed temporary columns\")\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nFinal data shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        print(f\"\\nNew columns added:\")\n",
    "        print(f\"   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\")\n",
    "        print(f\"   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\")\n",
    "        print(f\"\\nKey improvement over simple shrinkage:\")\n",
    "        print(f\"   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\")\n",
    "        print(f\"   ‚Ä¢ Preserves opening difficulty differences\")\n",
    "        print(f\"   ‚Ä¢ More accurate for both strong and weak openings\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Configuration for Bayesian shrinkage\n",
    "    K_PLAYER = 50  # Shrinkage constant for player-opening scores\n",
    "    \n",
    "    # Call the function\n",
    "    clean_data = apply_hierarchical_bayesian_shrinkage(clean_data, k_player=K_PLAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73319d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         player_id  opening_id  num_games     score  eco  confidence\n",
      "1736188      29355        1510         78  0.547014  C20    0.609375\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2C: PLAYER RATING STATISTICS\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Extracting player ratings from database...\n",
      "   ‚úì Retrieved ratings for 48,469 players\n",
      "   ‚úì Database connection closed\n",
      "\n",
      "2Ô∏è‚É£  Merging ratings with clean_data...\n",
      "   ‚úì Merged successfully\n",
      "   ‚úì All entries have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 48,469\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1765.19\n",
      "   ‚Ä¢ Median: 1762\n",
      "   ‚Ä¢ Std Dev: 249.25\n",
      "\n",
      "4Ô∏è‚É£  Quartile statistics:\n",
      "   ‚Ä¢ 25th percentile: 1584\n",
      "   ‚Ä¢ 50th percentile (median): 1762\n",
      "   ‚Ä¢ 75th percentile: 1936\n",
      "\n",
      "5Ô∏è‚É£  Detailed percentile distribution (5% increments):\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1359    ‚ñà‚ñà‚ñà\n",
      "      10%          1435    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1494    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1541    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1584    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1623    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1660    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1696    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1729    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1762    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1798    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1829    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1864    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1901    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1936    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1980    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2027    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2089    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2183    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400       3,583      7.39%      ‚ñà‚ñà\n",
      "   1400-1600       9,418     19.43%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800      13,767     28.40%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000      12,894     26.60%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       6,607     13.63%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400       1,848      3.81%      ‚ñà\n",
      "   2400-2600         327      0.67%      \n",
      "   2600-3000          25      0.05%      \n",
      "   ‚úì Retrieved ratings for 48,469 players\n",
      "   ‚úì Database connection closed\n",
      "\n",
      "2Ô∏è‚É£  Merging ratings with clean_data...\n",
      "   ‚úì Merged successfully\n",
      "   ‚úì All entries have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 48,469\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1765.19\n",
      "   ‚Ä¢ Median: 1762\n",
      "   ‚Ä¢ Std Dev: 249.25\n",
      "\n",
      "4Ô∏è‚É£  Quartile statistics:\n",
      "   ‚Ä¢ 25th percentile: 1584\n",
      "   ‚Ä¢ 50th percentile (median): 1762\n",
      "   ‚Ä¢ 75th percentile: 1936\n",
      "\n",
      "5Ô∏è‚É£  Detailed percentile distribution (5% increments):\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1359    ‚ñà‚ñà‚ñà\n",
      "      10%          1435    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1494    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1541    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1584    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1623    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1660    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1696    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1729    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1762    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1798    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1829    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1864    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1901    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1936    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1980    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2027    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2089    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2183    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400       3,583      7.39%      ‚ñà‚ñà\n",
      "   1400-1600       9,418     19.43%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800      13,767     28.40%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000      12,894     26.60%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       6,607     13.63%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400       1,848      3.81%      ‚ñà\n",
      "   2400-2600         327      0.67%      \n",
      "   2600-3000          25      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 352\n",
      "   ‚Ä¢ 10th-90th percentile range: 654\n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 352\n",
      "   ‚Ä¢ 10th-90th percentile range: 654\n",
      "\n",
      "8Ô∏è‚É£  Distribution shape:\n",
      "   ‚Ä¢ Skewness: 0.1653 (right-skewed)\n",
      "   ‚Ä¢ Kurtosis: -0.3061 (light-tailed)\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1435):\n",
      "      Player 10964: KasparWD - Rating: 1435\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1584):\n",
      "      Player 6808: Evgenij2710 - Rating: 1584\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1762):\n",
      "      Player 48550: PENATUA_BALANCHINE - Rating: 1762\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1936):\n",
      "      Player 805: AkramNaji - Rating: 1936\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2089):\n",
      "      Player 39999: schieferberg - Rating: 2089\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Key takeaways:\n",
      "   ‚Ä¢ Total players: 48,469\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1765 ¬± 249\n",
      "   ‚Ä¢ Median: 1762\n",
      "\n",
      "   Next steps: Normalize ratings for model input\n",
      "\n",
      "8Ô∏è‚É£  Distribution shape:\n",
      "   ‚Ä¢ Skewness: 0.1653 (right-skewed)\n",
      "   ‚Ä¢ Kurtosis: -0.3061 (light-tailed)\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1435):\n",
      "      Player 10964: KasparWD - Rating: 1435\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1584):\n",
      "      Player 6808: Evgenij2710 - Rating: 1584\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1762):\n",
      "      Player 48550: PENATUA_BALANCHINE - Rating: 1762\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1936):\n",
      "      Player 805: AkramNaji - Rating: 1936\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2089):\n",
      "      Player 39999: schieferberg - Rating: 2089\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "Key takeaways:\n",
      "   ‚Ä¢ Total players: 48,469\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1765 ¬± 249\n",
      "   ‚Ä¢ Median: 1762\n",
      "\n",
      "   Next steps: Normalize ratings for model input\n"
     ]
    }
   ],
   "source": [
    "# 2c. Gather player rating statistics (no mutation, just exploration)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2C: PLAYER RATING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to database and extract player ratings\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player ratings from database...\")\n",
    "    \n",
    "    # Get unique player IDs from our clean_data\n",
    "    unique_player_ids = clean_data['player_id'].unique()\n",
    "    player_ids_str = ','.join(map(str, unique_player_ids))\n",
    "    \n",
    "    # Query to get player ratings\n",
    "    rating_query = f\"\"\"\n",
    "        SELECT \n",
    "            id as player_id,\n",
    "            name,\n",
    "            title,\n",
    "            rating\n",
    "        FROM player\n",
    "        WHERE id IN ({player_ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    player_ratings = pd.DataFrame(con.execute(rating_query).df())\n",
    "    print(f\"   ‚úì Retrieved ratings for {len(player_ratings):,} players\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"   ‚úì Database connection closed\")\n",
    "\n",
    "# Merge ratings into clean_data for analysis\n",
    "print(f\"\\n2Ô∏è‚É£  Merging ratings with clean_data...\")\n",
    "clean_data_with_ratings = clean_data.merge(player_ratings[['player_id', 'rating']], on='player_id', how='left')\n",
    "print(f\"   ‚úì Merged successfully\")\n",
    "\n",
    "# Check for missing ratings\n",
    "num_missing_ratings = clean_data_with_ratings['rating'].isna().sum()\n",
    "if num_missing_ratings > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  {num_missing_ratings:,} entries ({100*num_missing_ratings/len(clean_data_with_ratings):.2f}%) have missing ratings\")\n",
    "else:\n",
    "    print(f\"   ‚úì All entries have ratings\")\n",
    "\n",
    "# Basic rating statistics\n",
    "print(f\"\\n3Ô∏è‚É£  Basic rating statistics:\")\n",
    "print(f\"   ‚Ä¢ Count: {player_ratings['rating'].notna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Missing: {player_ratings['rating'].isna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Min: {player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Max: {player_ratings['rating'].max():.0f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {player_ratings['rating'].mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"   ‚Ä¢ Std Dev: {player_ratings['rating'].std():.2f}\")\n",
    "\n",
    "# Quartile statistics\n",
    "print(f\"\\n4Ô∏è‚É£  Quartile statistics:\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {player_ratings['rating'].quantile(0.25):.0f}\")\n",
    "print(f\"   ‚Ä¢ 50th percentile (median): {player_ratings['rating'].quantile(0.50):.0f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {player_ratings['rating'].quantile(0.75):.0f}\")\n",
    "\n",
    "# Granular percentile statistics (5% increments)\n",
    "print(f\"\\n5Ô∏è‚É£  Detailed percentile distribution (5% increments):\")\n",
    "percentiles = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,\n",
    "               0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "print(f\"\\n   {'Percentile':<12} {'Rating':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*12} {'-'*10} {'-'*40}\")\n",
    "\n",
    "for p in percentiles:\n",
    "    rating_value = player_ratings['rating'].quantile(p)\n",
    "    # Create a simple bar visualization\n",
    "    bar_length = int((rating_value - player_ratings['rating'].min()) / \n",
    "                     (player_ratings['rating'].max() - player_ratings['rating'].min()) * 40)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {p*100:>5.0f}%       {rating_value:>7.0f}    {bar}\")\n",
    "\n",
    "# Rating ranges and counts\n",
    "print(f\"\\n6Ô∏è‚É£  Rating distribution by range:\")\n",
    "rating_ranges = [\n",
    "    (0, 1000), (1000, 1200), (1200, 1400), (1400, 1600), \n",
    "    (1600, 1800), (1800, 2000), (2000, 2200), (2200, 2400), \n",
    "    (2400, 2600), (2600, 3000)\n",
    "]\n",
    "\n",
    "print(f\"\\n   {'Range':<15} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*15} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "\n",
    "for low, high in rating_ranges:\n",
    "    count = len(player_ratings[(player_ratings['rating'] >= low) & (player_ratings['rating'] < high)])\n",
    "    pct = 100 * count / len(player_ratings)\n",
    "    bar_length = int(pct * 0.4)  # Scale for visualization\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {low:>4}-{high:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Interquartile range\n",
    "iqr = player_ratings['rating'].quantile(0.75) - player_ratings['rating'].quantile(0.25)\n",
    "print(f\"\\n7Ô∏è‚É£  Spread statistics:\")\n",
    "print(f\"   ‚Ä¢ Range: {player_ratings['rating'].max() - player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Interquartile Range (IQR): {iqr:.0f}\")\n",
    "print(f\"   ‚Ä¢ 10th-90th percentile range: {player_ratings['rating'].quantile(0.90) - player_ratings['rating'].quantile(0.10):.0f}\")\n",
    "\n",
    "# Skewness and kurtosis if available\n",
    "try:\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    skewness = skew(player_ratings['rating'].dropna())\n",
    "    kurt = kurtosis(player_ratings['rating'].dropna())\n",
    "    print(f\"\\n8Ô∏è‚É£  Distribution shape:\")\n",
    "    print(f\"   ‚Ä¢ Skewness: {skewness:.4f} {'(right-skewed)' if skewness > 0 else '(left-skewed)' if skewness < 0 else '(symmetric)'}\")\n",
    "    print(f\"   ‚Ä¢ Kurtosis: {kurt:.4f} {'(heavy-tailed)' if kurt > 0 else '(light-tailed)' if kurt < 0 else '(normal)'}\")\n",
    "except ImportError:\n",
    "    print(f\"\\n8Ô∏è‚É£  Distribution shape:\")\n",
    "    print(f\"   ‚Ä¢ scipy not available for skewness/kurtosis calculation\")\n",
    "\n",
    "# Sample of players at different rating levels\n",
    "print(f\"\\n9Ô∏è‚É£  Sample players at different rating levels:\")\n",
    "sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "for p in sample_percentiles:\n",
    "    rating_threshold = player_ratings['rating'].quantile(p)\n",
    "    # Get a player near this rating\n",
    "    sample_player = player_ratings.iloc[(player_ratings['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "    print(f\"\\n   ~{p*100:.0f}th percentile (rating ‚âà {rating_threshold:.0f}):\")\n",
    "    for idx, row in sample_player.iterrows():\n",
    "        # print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f} {f'({row['title']})' if pd.notna(row['title']) else ''}\")\n",
    "        title_str = f\" ({row['title']})\" if pd.notna(row['title']) else \"\"\n",
    "        print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f}{title_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ RATING STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nKey takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total players: {len(player_ratings):,}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings['rating'].min():.0f}, {player_ratings['rating'].max():.0f}]\")\n",
    "print(f\"   ‚Ä¢ Mean ¬± std: {player_ratings['rating'].mean():.0f} ¬± {player_ratings['rating'].std():.0f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"\\n   Next steps: Normalize ratings for model input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ab6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Normalization strategy: Z-score\n",
      "   ‚Ä¢ Formula: (rating - mean) / std\n",
      "   ‚Ä¢ Purpose: Scale ratings for use as side information in MF model\n",
      "   ‚Ä¢ Storage: SEPARATE lookup table, NOT merged into clean_data\n",
      "   ‚Ä¢ Usage: Model will lookup player_id ‚Üí rating_z during training\n",
      "\n",
      "1Ô∏è‚É£  Normalization parameters (calculated from 48,469 players):\n",
      "   ‚Ä¢ Mean: 1765.19\n",
      "   ‚Ä¢ Std Dev: 249.25\n",
      "\n",
      "2Ô∏è‚É£  Normalized rating statistics:\n",
      "   ‚Ä¢ Min: -2.2675\n",
      "   ‚Ä¢ Max: 4.2439\n",
      "   ‚Ä¢ Mean: 0.000000 (should be ~0)\n",
      "   ‚Ä¢ Std: 1.000000 (should be ~1)\n",
      "   ‚Ä¢ Range: [-2.27, 4.24]\n",
      "\n",
      "3Ô∏è‚É£  Sample normalized ratings across skill levels:\n",
      "   ~10th percentile: Player 10609 | Rating: 1435 ‚Üí Z-score: -1.325\n",
      "   ~25th percentile: Player 6588 | Rating: 1584 ‚Üí Z-score: -0.727\n",
      "   ~50th percentile: Player 47073 | Rating: 1762 ‚Üí Z-score: -0.013\n",
      "   ~75th percentile: Player 778 | Rating: 1936 ‚Üí Z-score:  0.685\n",
      "   ~90th percentile: Player 38783 | Rating: 2089 ‚Üí Z-score:  1.299\n",
      "\n",
      "4Ô∏è‚É£  Interpretation guide:\n",
      "   ‚Ä¢ rating_z ‚âà -2.3: 1200 player (minimum)\n",
      "   ‚Ä¢ rating_z ‚âà -0.7: 1584 player (25th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà  0.0: 1765 player (mean)\n",
      "   ‚Ä¢ rating_z ‚âà 0.7: 1936 player (75th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà 4.2: 2823 player (maximum)\n",
      "\n",
      "5Ô∏è‚É£  Side information table structure:\n",
      "   ‚Ä¢ Shape: (48469, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'rating', 'rating_z']\n",
      "   ‚Ä¢ Indexing: Setting player_id as index for O(1) lookups\n",
      "\n",
      "6Ô∏è‚É£  Sample entries from side information table:\n",
      "   Player  1963 | Rating: 1422 ‚Üí Z-score: -1.377\n",
      "   Player 37692 | Rating: 1937 ‚Üí Z-score:  0.689\n",
      "   Player 46251 | Rating: 1801 ‚Üí Z-score:  0.144\n",
      "   Player 48519 | Rating: 1402 ‚Üí Z-score: -1.457\n",
      "   Player 23810 | Rating: 1527 ‚Üí Z-score: -0.956\n",
      "   Player 26430 | Rating: 1252 ‚Üí Z-score: -2.059\n",
      "   Player 29445 | Rating: 1748 ‚Üí Z-score: -0.069\n",
      "   Player 17355 | Rating: 1572 ‚Üí Z-score: -0.775\n",
      "   Player 37093 | Rating: 1245 ‚Üí Z-score: -2.087\n",
      "   Player  5375 | Rating: 2012 ‚Üí Z-score:  0.990\n",
      "\n",
      "7Ô∏è‚É£  Removing unnecessary columns...\n",
      "   ‚úì Dropped 'rating' column (only keeping 'rating_z')\n",
      "   ‚Ä¢ Final columns: ['rating_z']\n",
      "\n",
      "8Ô∏è‚É£  Verifying all clean_data players have ratings:\n",
      "   ‚úì All 48,469 players in clean_data have side information\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING NORMALIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Created: player_side_info\n",
      "   ‚Ä¢ Shape: (48469, 1)\n",
      "   ‚Ä¢ Index: player_id\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ clean_data: 2,896,295 rows (player-opening interactions)\n",
      "   ‚Ä¢ player_side_info: 48,469 rows (one per player)\n",
      "   ‚Ä¢ Rating storage: ONE value per player (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\n",
      "   RATING_MEAN = 1765.19\n",
      "   RATING_STD = 249.25\n",
      "\n",
      "   You'll need them to normalize ratings for new users at inference time.\n"
     ]
    }
   ],
   "source": [
    "# 2d. Normalize player ratings using z-score normalization (for use as side information in MF model)\n",
    "\n",
    "# Check if we've already created the player_side_info table\n",
    "if 'player_side_info' in globals() and 'rating_z' in player_side_info.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2D: RATING NORMALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'player_side_info' table already exists\")\n",
    "    print(\"   This indicates rating normalization has already been applied.\")\n",
    "    print(f\"\\nPlayer side info shape: {player_side_info.shape}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing normalized rating statistics:\")\n",
    "    print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "    print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing normalized ratings:\")\n",
    "    sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "    for idx, row in sample_data.iterrows():\n",
    "        print(f\"   Player {idx:>5} | {row['name']:<20} | \"\n",
    "              f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "else:\n",
    "    def normalize_player_ratings(player_ratings_df):\n",
    "        \"\"\"\n",
    "        Apply z-score normalization to player ratings for use as side information.\n",
    "        \n",
    "        This creates a SEPARATE table of player-level features, NOT merged into clean_data.\n",
    "        Rating is side information - it describes the player, not the player-opening interaction.\n",
    "        \n",
    "        During training, the model will LOOK UP each player's rating_z from this table.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ratings_df : pd.DataFrame\n",
    "            Player ratings with columns: player_id, name, title, rating\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (player_side_info DataFrame, RATING_MEAN, RATING_STD)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Normalization strategy: Z-score\")\n",
    "        print(f\"   ‚Ä¢ Formula: (rating - mean) / std\")\n",
    "        print(f\"   ‚Ä¢ Purpose: Scale ratings for use as side information in MF model\")\n",
    "        print(f\"   ‚Ä¢ Storage: SEPARATE lookup table, NOT merged into clean_data\")\n",
    "        print(f\"   ‚Ä¢ Usage: Model will lookup player_id ‚Üí rating_z during training\")\n",
    "        \n",
    "        # Calculate normalization parameters\n",
    "        RATING_MEAN = player_ratings_df['rating'].mean()\n",
    "        RATING_STD = player_ratings_df['rating'].std()\n",
    "        \n",
    "        print(f\"\\n1Ô∏è‚É£  Normalization parameters (calculated from {len(player_ratings_df):,} players):\")\n",
    "        print(f\"   ‚Ä¢ Mean: {RATING_MEAN:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Std Dev: {RATING_STD:.2f}\")\n",
    "        \n",
    "        # Create side information table - only keep player_id and rating for now\n",
    "        player_side_info = player_ratings_df[['player_id', 'rating']].copy()\n",
    "        player_side_info['rating_z'] = (player_side_info['rating'] - RATING_MEAN) / RATING_STD\n",
    "        \n",
    "        print(f\"\\n2Ô∏è‚É£  Normalized rating statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "        print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "        print(f\"   ‚Ä¢ Range: [{player_side_info['rating_z'].min():.2f}, {player_side_info['rating_z'].max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\n3Ô∏è‚É£  Sample normalized ratings across skill levels:\")\n",
    "        sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "        for p in sample_percentiles:\n",
    "            rating_threshold = player_side_info['rating'].quantile(p)\n",
    "            sample_player = player_side_info.iloc[(player_side_info['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "            for idx, row in sample_player.iterrows():\n",
    "                print(f\"   ~{p*100:.0f}th percentile: Player {idx} | \"\n",
    "                      f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n4Ô∏è‚É£  Interpretation guide:\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(1200 - RATING_MEAN)/RATING_STD:.1f}: 1200 player (minimum)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.25) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.25):.0f} player (25th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà  0.0: {RATING_MEAN:.0f} player (mean)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.75) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.75):.0f} player (75th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].max() - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].max():.0f} player (maximum)\")\n",
    "        \n",
    "        print(f\"\\n5Ô∏è‚É£  Side information table structure:\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        print(f\"   ‚Ä¢ Indexing: Setting player_id as index for O(1) lookups\")\n",
    "        \n",
    "        # Set player_id as index for fast lookups\n",
    "        player_side_info = player_side_info.set_index('player_id')\n",
    "        \n",
    "        print(f\"\\n6Ô∏è‚É£  Sample entries from side information table:\")\n",
    "        sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "        for idx, row in sample_data.iterrows():\n",
    "            print(f\"   Player {idx:>5} | Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n7Ô∏è‚É£  Removing unnecessary columns...\")\n",
    "        # Drop rating column - we only need rating_z for the model\n",
    "        player_side_info = player_side_info.drop(columns=['rating'])\n",
    "        print(f\"   ‚úì Dropped 'rating' column (only keeping 'rating_z')\")\n",
    "        print(f\"   ‚Ä¢ Final columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\n8Ô∏è‚É£  Verifying all clean_data players have ratings:\")\n",
    "        # This is important - make sure every player in clean_data has a rating\n",
    "        missing_players = set(clean_data['player_id'].unique()) - set(player_side_info.index)\n",
    "        if len(missing_players) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in clean_data are missing from side_info!\")\n",
    "            print(f\"   Missing player IDs: {sorted(list(missing_players))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(player_side_info):,} players in clean_data have side information\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ RATING NORMALIZATION COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nCreated: player_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: player_id\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ clean_data: {clean_data.shape[0]:,} rows (player-opening interactions)\")\n",
    "        print(f\"   ‚Ä¢ player_side_info: {len(player_side_info):,} rows (one per player)\")\n",
    "        print(f\"   ‚Ä¢ Rating storage: ONE value per player (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\")\n",
    "        print(f\"   RATING_MEAN = {RATING_MEAN:.2f}\")\n",
    "        print(f\"   RATING_STD = {RATING_STD:.2f}\")\n",
    "        print(f\"\\n   You'll need them to normalize ratings for new users at inference time.\")\n",
    "        \n",
    "        return player_side_info, RATING_MEAN, RATING_STD\n",
    "    \n",
    "    # Call the function\n",
    "    player_side_info, RATING_MEAN, RATING_STD = normalize_player_ratings(player_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14eba9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         player_id  opening_id  num_games     score  eco  confidence\n",
      "1020279      17213         386         14  0.538483  A40    0.218750\n",
      "2028224      34282        1371         72  0.561939  C00    0.590164\n",
      "903334       15209        1486         41  0.543695  C16    0.450549\n",
      "2393174      40474        1117         11  0.548971  B28    0.180328\n",
      "1937884      32747         768         17  0.513046  B01    0.253731\n",
      "1623346      27464         773         59  0.508008  B01    0.541284\n",
      "2824585      48243        1436         21  0.544964  C11    0.295775\n",
      "1258122      21359         762         12  0.504713  B01    0.193548\n",
      "2118088      35808         773         59  0.489660  B01    0.541284\n",
      "711687       11956        1479         31  0.476379  C15    0.382716\n",
      "1023366      17269        3180         25  0.493149  C50    0.333333\n",
      "174790        2953         910         73  0.443181  B10    0.593496\n",
      "568354        9574         881         21  0.467836  B07    0.295775\n",
      "1934033      32677        3211         16  0.574622  B15    0.242424\n",
      "1560287      26402        3204         13  0.521690  B01    0.206349\n",
      "2878228      49493        2521         21  0.485421  D10    0.295775\n",
      "2226719      37658         737         49  0.468882  B00    0.494949\n",
      "2156147      36486         838         90  0.485564  B06    0.642857\n",
      "567388        9557        2495         44  0.532012  D06    0.468085\n",
      "1800459      30445         247         93  0.531216  A10    0.650350\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac119ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           rating_z\n",
      "player_id          \n",
      "26308     -0.598549\n",
      "2632      -1.268555\n",
      "39249     -1.031846\n",
      "20732     -0.678790\n",
      "39552      2.550878\n",
      "14136     -1.003762\n",
      "16625      0.348285\n",
      "12210     -0.072976\n",
      "11925      0.195829\n",
      "33203     -0.161240\n"
     ]
    }
   ],
   "source": [
    "print(player_side_info.sample(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30280db",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test/Val splits\n",
    "\n",
    "Here, I split my data and drop columns that are no longer needed. We're very close to being able to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf3849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: TRAIN/VALIDATION/TEST SPLIT\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Configuration:\n",
      "   ‚Ä¢ Train: 75%\n",
      "   ‚Ä¢ Validation: 15%\n",
      "   ‚Ä¢ Test: 10%\n",
      "   ‚Ä¢ Random seed: 42 (for reproducibility)\n",
      "\n",
      "1Ô∏è‚É£  Preparing data for split...\n",
      "   ‚Ä¢ Features (X): (2896295, 4)\n",
      "   ‚Ä¢ Target (y): (2896295,)\n",
      "   ‚Ä¢ Feature columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Cleaning player side information...\n",
      "   ‚Ä¢ Original player_side_info shape: (48469, 1)\n",
      "   ‚Ä¢ Cleaned player_side_info shape: (48469, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "3Ô∏è‚É£  Splitting data (optimized approach)...\n",
      "   ‚Ä¢ Train: 2,172,220 samples (75.0%)\n",
      "   ‚Ä¢ Validation: 434,445 samples (15.0%)\n",
      "   ‚Ä¢ Test: 289,630 samples (10.0%)\n",
      "\n",
      "4Ô∏è‚É£  Verification:\n",
      "   ‚Ä¢ Total samples: 2,896,295 (should equal 2,896,295)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "5Ô∏è‚É£  Computing coverage statistics (cached)...\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 48,415 unique players\n",
      "   ‚Ä¢ Val: 47,519 unique players\n",
      "   ‚Ä¢ Test: 46,509 unique players\n",
      "   ‚Ä¢ Total unique: 48,469 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,680 unique openings\n",
      "   ‚Ä¢ Val: 2,447 unique openings\n",
      "   ‚Ä¢ Test: 2,366 unique openings\n",
      "   ‚Ä¢ Total unique: 2,714 openings\n",
      "\n",
      "6Ô∏è‚É£  Cold start analysis (vectorized)...\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 37 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 23 (0.9%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 22 (0.0%)\n",
      "   ‚Ä¢ Openings not in train: 13 (0.5%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "   ‚Ä¢ Train: 2,172,220 samples (75.0%)\n",
      "   ‚Ä¢ Validation: 434,445 samples (15.0%)\n",
      "   ‚Ä¢ Test: 289,630 samples (10.0%)\n",
      "\n",
      "4Ô∏è‚É£  Verification:\n",
      "   ‚Ä¢ Total samples: 2,896,295 (should equal 2,896,295)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "5Ô∏è‚É£  Computing coverage statistics (cached)...\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 48,415 unique players\n",
      "   ‚Ä¢ Val: 47,519 unique players\n",
      "   ‚Ä¢ Test: 46,509 unique players\n",
      "   ‚Ä¢ Total unique: 48,469 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,680 unique openings\n",
      "   ‚Ä¢ Val: 2,447 unique openings\n",
      "   ‚Ä¢ Test: 2,366 unique openings\n",
      "   ‚Ä¢ Total unique: 2,714 openings\n",
      "\n",
      "6Ô∏è‚É£  Cold start analysis (vectorized)...\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 37 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 23 (0.9%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 22 (0.0%)\n",
      "   ‚Ä¢ Openings not in train: 13 (0.5%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.5121\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1500\n",
      "   ‚Ä¢ Max: 0.7916\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1629\n",
      "   ‚Ä¢ Max: 0.7679\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.4150\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.4145\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 2,172,220 samples (75%)\n",
      "   ‚Ä¢ Validation data: 434,445 samples (15%)\n",
      "   ‚Ä¢ Test data: 289,630 samples (10%)\n",
      "   ‚Ä¢ Player side info: 48,469 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n",
      "\n",
      "üì¶ Available datasets:\n",
      "   ‚Ä¢ X_train, y_train - Training features and targets\n",
      "   ‚Ä¢ X_val, y_val - Validation features and targets\n",
      "   ‚Ä¢ X_test, y_test - Test features and targets\n",
      "   ‚Ä¢ player_side_info_clean - Player ratings (indexed by player_id)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as categorical features\n",
      "   ‚Ä¢ Convert to PyTorch tensors\n",
      "   ‚Ä¢ Build matrix factorization model with side information\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.5121\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1500\n",
      "   ‚Ä¢ Max: 0.7916\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1629\n",
      "   ‚Ä¢ Max: 0.7679\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation:\n",
      "   ‚Ä¢ Mean: 0.4150\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test:\n",
      "   ‚Ä¢ Mean: 0.4145\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 2,172,220 samples (75%)\n",
      "   ‚Ä¢ Validation data: 434,445 samples (15%)\n",
      "   ‚Ä¢ Test data: 289,630 samples (10%)\n",
      "   ‚Ä¢ Player side info: 48,469 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n",
      "\n",
      "üì¶ Available datasets:\n",
      "   ‚Ä¢ X_train, y_train - Training features and targets\n",
      "   ‚Ä¢ X_val, y_val - Validation features and targets\n",
      "   ‚Ä¢ X_test, y_test - Test features and targets\n",
      "   ‚Ä¢ player_side_info_clean - Player ratings (indexed by player_id)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as categorical features\n",
      "   ‚Ä¢ Convert to PyTorch tensors\n",
      "   ‚Ä¢ Build matrix factorization model with side information\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train/Validation/Test Split (75/15/10) - OPTIMIZED\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: TRAIN/VALIDATION/TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   ‚Ä¢ Train: 75%\")\n",
    "print(f\"   ‚Ä¢ Validation: 15%\")\n",
    "print(f\"   ‚Ä¢ Test: 10%\")\n",
    "print(f\"   ‚Ä¢ Random seed: 42 (for reproducibility)\")\n",
    "\n",
    "# Prepare the data\n",
    "print(f\"\\n1Ô∏è‚É£  Preparing data for split...\")\n",
    "\n",
    "# Drop num_games from clean_data - we don't need it for training\n",
    "# Keep: player_id, opening_id, score, eco, confidence\n",
    "X = clean_data[[\"player_id\", \"opening_id\", \"eco\", \"confidence\"]].copy()\n",
    "y = clean_data[\"score\"].copy()\n",
    "\n",
    "print(f\"   ‚Ä¢ Features (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Target (y): {y.shape}\")\n",
    "print(f\"   ‚Ä¢ Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Clean up player_side_info - only keep rating_z\n",
    "print(f\"\\n2Ô∏è‚É£  Cleaning player side information...\")\n",
    "player_side_info_clean = player_side_info[[\"rating_z\"]].copy()\n",
    "print(f\"   ‚Ä¢ Original player_side_info shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Cleaned player_side_info shape: {player_side_info_clean.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info_clean.columns)}\")\n",
    "\n",
    "# OPTIMIZED: Use index-based splitting to avoid DataFrame copies\n",
    "print(f\"\\n3Ô∏è‚É£  Splitting data (optimized approach)...\")\n",
    "idx = np.arange(len(X))\n",
    "\n",
    "# First split: separate out test set (10%)\n",
    "idx_temp, idx_test = train_test_split(idx, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: split remaining into train (75%) and val (15%)\n",
    "# 15% of original = 15/90 ‚âà 0.1667 of temp\n",
    "idx_train, idx_val = train_test_split(idx_temp, test_size=15/90, random_state=42, shuffle=True)\n",
    "\n",
    "# Create splits using iloc (view, not copy)\n",
    "X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
    "X_test, y_test = X.iloc[idx_test], y.iloc[idx_test]\n",
    "\n",
    "print(f\"   ‚Ä¢ Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Validation: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split\n",
    "print(f\"\\n4Ô∏è‚É£  Verification:\")\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"   ‚Ä¢ Total samples: {total:,} (should equal {len(X):,})\")\n",
    "print(f\"   ‚Ä¢ Train %: {len(X_train)/total*100:.2f}% (target: 75%)\")\n",
    "print(f\"   ‚Ä¢ Val %: {len(X_val)/total*100:.2f}% (target: 15%)\")\n",
    "print(f\"   ‚Ä¢ Test %: {len(X_test)/total*100:.2f}% (target: 10%)\")\n",
    "\n",
    "# OPTIMIZED: Pre-compute unique arrays once\n",
    "print(f\"\\n5Ô∏è‚É£  Computing coverage statistics (cached)...\")\n",
    "players_train = X_train[\"player_id\"].unique()\n",
    "players_val = X_val[\"player_id\"].unique()\n",
    "players_test = X_test[\"player_id\"].unique()\n",
    "\n",
    "openings_train = X_train[\"opening_id\"].unique()\n",
    "openings_val = X_val[\"opening_id\"].unique()\n",
    "openings_test = X_test[\"opening_id\"].unique()\n",
    "\n",
    "print(f\"\\n   Players:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(players_train):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Val: {len(players_val):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Test: {len(players_test):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['player_id'].nunique():,} players\")\n",
    "\n",
    "print(f\"\\n   Openings:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(openings_train):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Val: {len(openings_val):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Test: {len(openings_test):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['opening_id'].nunique():,} openings\")\n",
    "\n",
    "# OPTIMIZED: Use NumPy setdiff1d for cold-start analysis (C-speed)\n",
    "print(f\"\\n6Ô∏è‚É£  Cold start analysis (vectorized)...\")\n",
    "\n",
    "val_cold_players = np.setdiff1d(players_val, players_train, assume_unique=True)\n",
    "val_cold_openings = np.setdiff1d(openings_val, openings_train, assume_unique=True)\n",
    "\n",
    "test_cold_players = np.setdiff1d(players_test, players_train, assume_unique=True)\n",
    "test_cold_openings = np.setdiff1d(openings_test, openings_train, assume_unique=True)\n",
    "\n",
    "print(f\"\\n   Validation set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(val_cold_players):,} ({len(val_cold_players)/len(players_val)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(val_cold_openings):,} ({len(val_cold_openings)/len(openings_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(test_cold_players):,} ({len(test_cold_players)/len(players_test)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(test_cold_openings):,} ({len(test_cold_openings)/len(openings_test)*100:.1f}%)\")\n",
    "\n",
    "# OPTIMIZED: Compute stats in one pass using describe()\n",
    "print(f\"\\n7Ô∏è‚É£  Score distribution across splits:\")\n",
    "\n",
    "y_train_stats = y_train.describe()\n",
    "y_val_stats = y_val.describe()\n",
    "y_test_stats = y_test.describe()\n",
    "\n",
    "print(f\"\\n   Train:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_train_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_train_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_train_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_val_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_val_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_val_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_test_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_test_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_test_stats['max']:.4f}\")\n",
    "\n",
    "# OPTIMIZED: Compute confidence stats in one pass\n",
    "print(f\"\\n8Ô∏è‚É£  Confidence distribution across splits:\")\n",
    "\n",
    "conf_train_stats = X_train['confidence'].describe()\n",
    "conf_val_stats = X_val['confidence'].describe()\n",
    "conf_test_stats = X_test['confidence'].describe()\n",
    "\n",
    "print(f\"\\n   Train:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_train_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_val_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_test_stats['50%']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SPLIT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   ‚Ä¢ Training data: {len(X_train):,} samples (75%)\")\n",
    "print(f\"   ‚Ä¢ Validation data: {len(X_val):,} samples (15%)\")\n",
    "print(f\"   ‚Ä¢ Test data: {len(X_test):,} samples (10%)\")\n",
    "print(f\"   ‚Ä¢ Player side info: {len(player_side_info_clean):,} players\")\n",
    "print(f\"   ‚Ä¢ Side info columns: {list(player_side_info_clean.columns)}\")\n",
    "\n",
    "print(f\"\\nüì¶ Available datasets:\")\n",
    "print(f\"   ‚Ä¢ X_train, y_train - Training features and targets\")\n",
    "print(f\"   ‚Ä¢ X_val, y_val - Validation features and targets\")\n",
    "print(f\"   ‚Ä¢ X_test, y_test - Test features and targets\")\n",
    "print(f\"   ‚Ä¢ player_side_info_clean - Player ratings (indexed by player_id)\")\n",
    "\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"   ‚Ä¢ Enumerate ECO codes as categorical features\")\n",
    "print(f\"   ‚Ä¢ Convert to PyTorch tensors\")\n",
    "print(f\"   ‚Ä¢ Build matrix factorization model with side information\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a580c0",
   "metadata": {},
   "source": [
    "## 4. Enumerate Categorical Variables\n",
    "\n",
    "I believe the only variable we need to enumerate here is `eco`. That's the broad categorization of a specific opening.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- One ECO code will have many openings\n",
    "- They're sorted by letter, then further by number. For instance, C21 and C44 are in the `C` family.\n",
    "- Maybe we make this side information?\n",
    "\n",
    "First, let's get some data on ECO codes to help us better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81224c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9504a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: ECO CODE STATISTICS\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Overall ECO statistics:\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,896,295\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,896,295\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 6323.8\n",
      "   ‚Ä¢ Median entries per ECO: 821.5\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 201215\n",
      "   ‚Ä¢ Std: 18379.4\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 6323.8\n",
      "   ‚Ä¢ Median entries per ECO: 821.5\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 201215\n",
      "   ‚Ä¢ Std: 18379.4\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        509,577     17.59%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        1,048,206     36.19%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        986,823     34.07%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D        278,911      9.63%      ‚ñà‚ñà‚ñà\n",
      "   E         72,778      2.51%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00    201,215      6.95%      ‚ñà‚ñà\n",
      "   2      B01    197,407      6.82%      ‚ñà‚ñà\n",
      "   3      A40    125,430      4.33%      ‚ñà\n",
      "   4      C50    101,397      3.50%      ‚ñà\n",
      "   5      C00     87,181      3.01%      \n",
      "   6      C40     81,405      2.81%      \n",
      "   7      B06     73,332      2.53%      \n",
      "   8      C42     65,645      2.27%      \n",
      "   9      C44     60,590      2.09%      \n",
      "   10     B10     56,896      1.96%      \n",
      "   11     C41     55,392      1.91%      \n",
      "   12     B40     52,301      1.81%      \n",
      "   13     B12     48,881      1.69%      \n",
      "   14     A00     48,283      1.67%      \n",
      "   15     C02     48,035      1.66%      \n",
      "   16     D00     41,339      1.43%      \n",
      "   17     B21     37,792      1.30%      \n",
      "   18     A43     35,759      1.23%      \n",
      "   19     B32     33,868      1.17%      \n",
      "   20     A04     32,890      1.14%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        509,577     17.59%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        1,048,206     36.19%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        986,823     34.07%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D        278,911      9.63%      ‚ñà‚ñà‚ñà\n",
      "   E         72,778      2.51%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00    201,215      6.95%      ‚ñà‚ñà\n",
      "   2      B01    197,407      6.82%      ‚ñà‚ñà\n",
      "   3      A40    125,430      4.33%      ‚ñà\n",
      "   4      C50    101,397      3.50%      ‚ñà\n",
      "   5      C00     87,181      3.01%      \n",
      "   6      C40     81,405      2.81%      \n",
      "   7      B06     73,332      2.53%      \n",
      "   8      C42     65,645      2.27%      \n",
      "   9      C44     60,590      2.09%      \n",
      "   10     B10     56,896      1.96%      \n",
      "   11     C41     55,392      1.91%      \n",
      "   12     B40     52,301      1.81%      \n",
      "   13     B12     48,881      1.69%      \n",
      "   14     A00     48,283      1.67%      \n",
      "   15     C02     48,035      1.66%      \n",
      "   16     D00     41,339      1.43%      \n",
      "   17     B21     37,792      1.30%      \n",
      "   18     A43     35,759      1.23%      \n",
      "   19     B32     33,868      1.17%      \n",
      "   20     A04     32,890      1.14%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      A97          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2      D62          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3      D98          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4      D29          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   5      E78          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   6      E79          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   7      D68          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   8      A72          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   9      A63          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   10     C76          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   11     D49          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   12     C94          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   13     D84          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   14     E55          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   15     B69          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   16     A99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   17     E96          2    ‚ñà‚ñà\n",
      "   18     A93          2    ‚ñà‚ñà\n",
      "   19     E36          1    ‚ñà\n",
      "   20     A98          1    ‚ñà\n",
      "\n",
      "6Ô∏è‚É£  ECO code format analysis:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      A97          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2      D62          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3      D98          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4      D29          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   5      E78          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   6      E79          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   7      D68          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   8      A72          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   9      A63          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   10     C76          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   11     D49          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   12     C94          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   13     D84          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   14     E55          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   15     B69          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   16     A99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   17     E96          2    ‚ñà‚ñà\n",
      "   18     A93          2    ‚ñà‚ñà\n",
      "   19     E36          1    ‚ñà\n",
      "   20     A98          1    ‚ñà\n",
      "\n",
      "6Ô∏è‚É£  ECO code format analysis:\n",
      "   ‚Ä¢ ECO code lengths:\n",
      "      3 characters: 2,896,295 (100.00%)\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   ‚Ä¢ ECO code lengths:\n",
      "      3 characters: 2,896,295 (100.00%)\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   A07, A20, A24, A71, B13, B22, B53, C03, C19, C43, C48, C66, C99, E10, E30, E40, E48, E71, E74, E79\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   A07, A20, A24, A71, B13, B22, B53, C03, C19, C43, C48, C66, C99, E10, E30, E40, E48, E71, E74, E79\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   ‚Ä¢ Unique ECO codes: 457\n",
      "   ‚Ä¢ Total entries: 2,172,220\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 448\n",
      "   ‚Ä¢ Total entries: 434,445\n",
      "   ‚Ä¢ Unique ECO codes: 457\n",
      "   ‚Ä¢ Total entries: 2,172,220\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 448\n",
      "   ‚Ä¢ Total entries: 434,445\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 440\n",
      "   ‚Ä¢ Total entries: 289,630\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 440\n",
      "   ‚Ä¢ Total entries: 289,630\n",
      "   ‚Ä¢ ECO codes not in train: 1\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "   ‚Ä¢ ECO codes not in train: 1\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C94    0.6200             5\n",
      "   B77    0.5945           551\n",
      "   D57    0.5862            24\n",
      "   E96    0.5860             2\n",
      "   B71    0.5859           229\n",
      "   B85    0.5844            16\n",
      "   D29    0.5818             8\n",
      "   D49    0.5764             6\n",
      "   E99    0.5733            26\n",
      "   C87    0.5711           179\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C74    0.4701            22\n",
      "   B70    0.4699         4,144\n",
      "   B58    0.4669            10\n",
      "   A58    0.4627           957\n",
      "   C76    0.4616             6\n",
      "   C38    0.4576         1,653\n",
      "   C99    0.4265            13\n",
      "   E55    0.3855             5\n",
      "   E36    0.3500             1\n",
      "   A98    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C94    0.6200             5\n",
      "   B77    0.5945           551\n",
      "   D57    0.5862            24\n",
      "   E96    0.5860             2\n",
      "   B71    0.5859           229\n",
      "   B85    0.5844            16\n",
      "   D29    0.5818             8\n",
      "   D49    0.5764             6\n",
      "   E99    0.5733            26\n",
      "   C87    0.5711           179\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C74    0.4701            22\n",
      "   B70    0.4699         4,144\n",
      "   B58    0.4669            10\n",
      "   A58    0.4627           957\n",
      "   C76    0.4616             6\n",
      "   C38    0.4576         1,653\n",
      "   C99    0.4265            13\n",
      "   E55    0.3855             5\n",
      "   E36    0.3500             1\n",
      "   A98    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   A93    0.0069       0.0829             2\n",
      "   C57    0.0068       0.0822        11,877\n",
      "   E45    0.0067       0.0820            16\n",
      "   A97    0.0061       0.0784             9\n",
      "   C37    0.0044       0.0662         5,923\n",
      "   C51    0.0038       0.0617         3,964\n",
      "   C39    0.0037       0.0608         1,457\n",
      "   C56    0.0036       0.0599         9,592\n",
      "   C31    0.0036       0.0598        10,961\n",
      "   C52    0.0035       0.0591         1,899\n",
      "\n",
      "1Ô∏è‚É£1Ô∏è‚É£  Openings per ECO code:\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   A93    0.0069       0.0829             2\n",
      "   C57    0.0068       0.0822        11,877\n",
      "   E45    0.0067       0.0820            16\n",
      "   A97    0.0061       0.0784             9\n",
      "   C37    0.0044       0.0662         5,923\n",
      "   C51    0.0038       0.0617         3,964\n",
      "   C39    0.0037       0.0608         1,457\n",
      "   C56    0.0036       0.0599         9,592\n",
      "   C31    0.0036       0.0598        10,961\n",
      "   C52    0.0035       0.0591         1,899\n",
      "\n",
      "1Ô∏è‚É£1Ô∏è‚É£  Openings per ECO code:\n",
      "   ‚Ä¢ Mean openings per ECO: 6.5\n",
      "   ‚Ä¢ Median openings per ECO: 3.0\n",
      "   ‚Ä¢ Max openings per ECO: 107\n",
      "   ‚Ä¢ Min openings per ECO: 1\n",
      "\n",
      "   Top 10 ECO codes by number of openings:\n",
      "\n",
      "   ECO    # Openings  \n",
      "   ------ ------------\n",
      "   A00           107\n",
      "   B00            87\n",
      "   D00            86\n",
      "   C42            59\n",
      "   A40            55\n",
      "   B01            45\n",
      "   C44            45\n",
      "   C00            42\n",
      "   B06            40\n",
      "   C33            38\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Mean openings per ECO: 6.5\n",
      "   ‚Ä¢ Median openings per ECO: 3.0\n",
      "   ‚Ä¢ Max openings per ECO: 107\n",
      "   ‚Ä¢ Min openings per ECO: 1\n",
      "\n",
      "   Top 10 ECO codes by number of openings:\n",
      "\n",
      "   ECO    # Openings  \n",
      "   ------ ------------\n",
      "   A00           107\n",
      "   B00            87\n",
      "   D00            86\n",
      "   C42            59\n",
      "   A40            55\n",
      "   B01            45\n",
      "   C44            45\n",
      "   C00            42\n",
      "   B06            40\n",
      "   C33            38\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Most common family: B (1,048,206 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (201,215 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as integers for categorical encoding\n",
      "   ‚Ä¢ Consider ECO as opening-level side information (similar to player ratings)\n",
      "   ‚Ä¢ Verify all ECO codes in validation/test exist in training set\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Most common family: B (1,048,206 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (201,215 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n",
      "\n",
      "üí° Next steps:\n",
      "   ‚Ä¢ Enumerate ECO codes as integers for categorical encoding\n",
      "   ‚Ä¢ Consider ECO as opening-level side information (similar to player ratings)\n",
      "   ‚Ä¢ Verify all ECO codes in validation/test exist in training set\n"
     ]
    }
   ],
   "source": [
    "# Step 4: ECO Code Statistics (no mutations, just exploration)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: ECO CODE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic ECO statistics across all data\n",
    "print(f\"\\n1Ô∏è‚É£  Overall ECO statistics:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Missing ECO values: {clean_data['eco'].isna().sum()}\")\n",
    "\n",
    "# ECO value counts\n",
    "eco_counts = clean_data['eco'].value_counts().sort_index()\n",
    "print(f\"\\n2Ô∏è‚É£  Distribution of entries per ECO code:\")\n",
    "print(f\"   ‚Ä¢ Mean entries per ECO: {eco_counts.mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Median entries per ECO: {eco_counts.median():.1f}\")\n",
    "print(f\"   ‚Ä¢ Min entries: {eco_counts.min()}\")\n",
    "print(f\"   ‚Ä¢ Max entries: {eco_counts.max()}\")\n",
    "print(f\"   ‚Ä¢ Std: {eco_counts.std():.1f}\")\n",
    "\n",
    "# ECO by first letter (family)\n",
    "print(f\"\\n3Ô∏è‚É£  ECO families (by first letter):\")\n",
    "eco_families = clean_data['eco'].str[0].value_counts().sort_index()\n",
    "print(f\"\\n   {'Family':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "for family, count in eco_families.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.4)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {family:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Top 20 most common ECO codes\n",
    "print(f\"\\n4Ô∏è‚É£  Top 20 most common ECO codes:\")\n",
    "top_eco = clean_data['eco'].value_counts().head(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*12} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(top_eco.items(), 1):\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.3)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Bottom 20 least common ECO codes\n",
    "print(f\"\\n5Ô∏è‚É£  Bottom 20 least common ECO codes:\")\n",
    "bottom_eco = clean_data['eco'].value_counts().tail(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(bottom_eco.items(), 1):\n",
    "    bar_length = min(count, 30)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {bar}\")\n",
    "\n",
    "# ECO code format analysis\n",
    "print(f\"\\n6Ô∏è‚É£  ECO code format analysis:\")\n",
    "eco_lengths = clean_data['eco'].str.len().value_counts().sort_index()\n",
    "print(f\"   ‚Ä¢ ECO code lengths:\")\n",
    "for length, count in eco_lengths.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    print(f\"      {length} characters: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Check for any unusual ECO codes\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of ECO codes:\")\n",
    "sample_eco = clean_data['eco'].drop_duplicates().sample(min(20, clean_data['eco'].nunique()), random_state=42).sort_values()\n",
    "print(f\"   {', '.join(sample_eco.values)}\")\n",
    "\n",
    "# ECO statistics by split\n",
    "print(f\"\\n8Ô∏è‚É£  ECO distribution across splits:\")\n",
    "print(f\"\\n   Train split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_train['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\n   Validation split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_val['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_val):,}\")\n",
    "val_new_eco = set(X_val['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(val_new_eco)}\")\n",
    "\n",
    "print(f\"\\n   Test split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_test['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_test):,}\")\n",
    "test_new_eco = set(X_test['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(test_new_eco)}\")\n",
    "\n",
    "# Average score by ECO code (top 10 and bottom 10)\n",
    "print(f\"\\n9Ô∏è‚É£  Average score by ECO code:\")\n",
    "eco_scores = clean_data.groupby('eco')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\n   Top 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.head(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(f\"\\n   Bottom 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.tail(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# ECO codes with high variance in scores\n",
    "print(f\"\\nüîü  ECO codes with highest score variance (may indicate difficulty):\")\n",
    "eco_variance = clean_data.groupby('eco')['score'].agg(['var', 'std', 'count']).sort_values('var', ascending=False).head(10)\n",
    "print(f\"\\n   {'ECO':<6} {'Variance':<12} {'Std Dev':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_variance.iterrows():\n",
    "    print(f\"   {eco:<6} {row['var']:<12.4f} {row['std']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# Number of openings per ECO code\n",
    "print(f\"\\n1Ô∏è‚É£1Ô∏è‚É£  Openings per ECO code:\")\n",
    "# Connect to database to get opening counts\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    eco_opening_query = \"\"\"\n",
    "        SELECT eco, COUNT(DISTINCT id) as num_openings\n",
    "        FROM opening\n",
    "        GROUP BY eco\n",
    "        ORDER BY num_openings DESC\n",
    "    \"\"\"\n",
    "    eco_opening_counts = pd.DataFrame(con.execute(eco_opening_query).df())\n",
    "    \n",
    "    # Filter to only ECO codes in our data\n",
    "    eco_opening_counts = eco_opening_counts[eco_opening_counts['eco'].isin(clean_data['eco'].unique())]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Mean openings per ECO: {eco_opening_counts['num_openings'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median openings per ECO: {eco_opening_counts['num_openings'].median():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Max openings per ECO: {eco_opening_counts['num_openings'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Min openings per ECO: {eco_opening_counts['num_openings'].min()}\")\n",
    "    \n",
    "    print(f\"\\n   Top 10 ECO codes by number of openings:\")\n",
    "    print(f\"\\n   {'ECO':<6} {'# Openings':<12}\")\n",
    "    print(f\"   {'-'*6} {'-'*12}\")\n",
    "    for idx, row in eco_opening_counts.head(10).iterrows():\n",
    "        print(f\"   {row['eco']:<6} {int(row['num_openings']):>10}\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ECO CODE STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Key takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Most common family: {eco_families.idxmax()} ({eco_families.max():,} entries)\")\n",
    "print(f\"   ‚Ä¢ Most common ECO: {top_eco.index[0]} ({top_eco.iloc[0]:,} entries)\")\n",
    "print(f\"   ‚Ä¢ ECO codes appear in all splits (good for training)\")\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"   ‚Ä¢ Enumerate ECO codes as integers for categorical encoding\")\n",
    "print(f\"   ‚Ä¢ Consider ECO as opening-level side information (similar to player ratings)\")\n",
    "print(f\"   ‚Ä¢ Verify all ECO codes in validation/test exist in training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbad18",
   "metadata": {},
   "source": [
    "## 4b. Create ECO Side Information\n",
    "\n",
    "**Why ECO is Side Information:**\n",
    "- ECO codes describe **opening characteristics**, not individual player-opening interactions\n",
    "- Similar to how player ratings describe players, ECO describes openings\n",
    "- Each opening has ONE ECO code (not per player-opening pair)\n",
    "\n",
    "**Implementation Strategy:**\n",
    "- Split ECO codes into two categorical features:\n",
    "  - `eco_letter`: A, B, C, D, or E ‚Üí encoded as integers 0-4\n",
    "  - `eco_number`: The numeric part (e.g., \"21\" from \"C21\") ‚Üí encoded as sequential integers\n",
    "- Store in a separate `opening_side_info` lookup table (indexed by opening_id)\n",
    "- Remove `eco` from train/test/val DataFrames (it's not interaction data)\n",
    "- During training, model will lookup opening_id ‚Üí (eco_letter, eco_number)\n",
    "\n",
    "**Why Split ECO into Letter and Number:**\n",
    "- ECO families (A-E) represent fundamentally different opening types:\n",
    "  - **A**: Flank openings (English, R√©ti, Bird's, etc.)\n",
    "  - **B**: Semi-Open games (Sicilian, French, Caro-Kann, etc.)\n",
    "  - **C**: Open games (King's pawn openings, Spanish, Italian, etc.)\n",
    "  - **D**: Closed games (Queen's Gambit variations)\n",
    "  - **E**: Indian defenses (King's Indian, Nimzo-Indian, etc.)\n",
    "- Numbers within each family represent variations (C20-C29, C30-C39, etc.)\n",
    "- Model can learn separate embeddings for family vs variation\n",
    "\n",
    "**Categorical Encoding:**\n",
    "- Both features will be treated as categorical (not ordinal)\n",
    "- Higher numbers don't mean \"better\" openings\n",
    "- Model will learn embedding vectors for each category\n",
    "- This allows the model to capture non-linear relationships between ECO codes and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64124475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4B: CREATE ECO SIDE INFORMATION\n",
      "============================================================\n",
      "\n",
      "‚öôÔ∏è  Strategy:\n",
      "   ‚Ä¢ Extract unique opening_id ‚Üí eco mappings from clean_data\n",
      "   ‚Ä¢ Split ECO codes: 'C21' ‚Üí letter='C', number='21'\n",
      "   ‚Ä¢ Encode as sequential integers (categorical, not ordinal)\n",
      "   ‚Ä¢ Store in opening_side_info lookup table\n",
      "   ‚Ä¢ Remove 'eco' column from X_train, X_val, X_test\n",
      "\n",
      "1Ô∏è‚É£  Extracting unique opening-ECO mappings...\n",
      "   ‚úì Extracted 2,714 unique openings\n",
      "   ‚úì Extracted 2,714 unique openings\n",
      "   ‚úì Verified: Each opening has exactly one ECO code (good!)\n",
      "\n",
      "2Ô∏è‚É£  Splitting ECO codes into letter and number components...\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['B' 'C' 'A' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "\n",
      "3Ô∏è‚É£  Encoding ECO letters as categorical integers...\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (578 openings)\n",
      "      'B' ‚Üí 1 (590 openings)\n",
      "      'C' ‚Üí 2 (891 openings)\n",
      "      'D' ‚Üí 3 (419 openings)\n",
      "      'E' ‚Üí 4 (236 openings)\n",
      "\n",
      "4Ô∏è‚É£  Encoding ECO numbers as categorical integers...\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 290 openings\n",
      "      '40' (‚Üí 40):  98 openings\n",
      "      '20' (‚Üí 20):  70 openings\n",
      "      '01' (‚Üí  1):  70 openings\n",
      "      '02' (‚Üí  2):  67 openings\n",
      "      '42' (‚Üí 42):  67 openings\n",
      "      '45' (‚Üí 45):  64 openings\n",
      "      '10' (‚Üí 10):  63 openings\n",
      "      '21' (‚Üí 21):  61 openings\n",
      "      '15' (‚Üí 15):  57 openings\n",
      "\n",
      "5Ô∏è‚É£  Creating opening_side_info lookup table...\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2714, 2)\n",
      "      ‚Ä¢ Index: opening_id\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "      ‚Ä¢ eco_letter_cat: Categorical encoding of ECO letter (A-E ‚Üí 0-4)\n",
      "      ‚Ä¢ eco_number_cat: Categorical encoding of ECO number\n",
      "\n",
      "6Ô∏è‚É£  Verifying coverage of train/val/test openings...\n",
      "   ‚úì All 2,714 openings in train/val/test have ECO side information\n",
      "\n",
      "7Ô∏è‚É£  Removing 'eco' column from train/val/test DataFrames...\n",
      "   ‚Ä¢ X_train before: (2172220, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (434445, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (289630, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "   After removing 'eco':\n",
      "   ‚Ä¢ X_train: (2172220, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_val: (434445, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_test: (289630, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   188          0               4               A04               \n",
      "   234          0               9               A09               \n",
      "   661          0               95              A95               \n",
      "   2272         2               68              C68               \n",
      "   3476         2               42              C42               \n",
      "   1175         1               40              B40               \n",
      "   3061         4               70              E70               \n",
      "   567          0               58              A58               \n",
      "   1855         2               41              C41               \n",
      "   465          0               45              A45               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            578     21.30%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            590     21.74%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            891     32.83%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            419     15.44%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            236      8.70%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO SIDE INFORMATION CREATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2714, 2)\n",
      "   ‚Ä¢ Index: opening_id (for O(1) lookups)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 2,172,220 rows, 3 features\n",
      "   ‚Ä¢ X_val: 434,445 rows, 3 features\n",
      "   ‚Ä¢ X_test: 289,630 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,714 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "üí° Model usage:\n",
      "   During training, for each (player_id, opening_id) pair:\n",
      "   1. Lookup opening_id ‚Üí opening_side_info[opening_id]\n",
      "   2. Get eco_letter_cat and eco_number_cat (already encoded as integers)\n",
      "   3. Feed into categorical embedding layers\n",
      "   4. Combine with opening latent factors\n",
      "\n",
      "üßπ Final cleanup:\n",
      "   ‚Ä¢ Kept only encoded categorical columns: eco_letter_cat, eco_number_cat\n",
      "   ‚Ä¢ Removed raw ECO strings (eco, eco_letter_str, eco_number_str)\n",
      "   ‚Ä¢ Column names clearly indicate categorical encoding (_cat suffix)\n",
      "   ‚úì Verified: Each opening has exactly one ECO code (good!)\n",
      "\n",
      "2Ô∏è‚É£  Splitting ECO codes into letter and number components...\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['B' 'C' 'A' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "\n",
      "3Ô∏è‚É£  Encoding ECO letters as categorical integers...\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (578 openings)\n",
      "      'B' ‚Üí 1 (590 openings)\n",
      "      'C' ‚Üí 2 (891 openings)\n",
      "      'D' ‚Üí 3 (419 openings)\n",
      "      'E' ‚Üí 4 (236 openings)\n",
      "\n",
      "4Ô∏è‚É£  Encoding ECO numbers as categorical integers...\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 290 openings\n",
      "      '40' (‚Üí 40):  98 openings\n",
      "      '20' (‚Üí 20):  70 openings\n",
      "      '01' (‚Üí  1):  70 openings\n",
      "      '02' (‚Üí  2):  67 openings\n",
      "      '42' (‚Üí 42):  67 openings\n",
      "      '45' (‚Üí 45):  64 openings\n",
      "      '10' (‚Üí 10):  63 openings\n",
      "      '21' (‚Üí 21):  61 openings\n",
      "      '15' (‚Üí 15):  57 openings\n",
      "\n",
      "5Ô∏è‚É£  Creating opening_side_info lookup table...\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2714, 2)\n",
      "      ‚Ä¢ Index: opening_id\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "      ‚Ä¢ eco_letter_cat: Categorical encoding of ECO letter (A-E ‚Üí 0-4)\n",
      "      ‚Ä¢ eco_number_cat: Categorical encoding of ECO number\n",
      "\n",
      "6Ô∏è‚É£  Verifying coverage of train/val/test openings...\n",
      "   ‚úì All 2,714 openings in train/val/test have ECO side information\n",
      "\n",
      "7Ô∏è‚É£  Removing 'eco' column from train/val/test DataFrames...\n",
      "   ‚Ä¢ X_train before: (2172220, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (434445, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (289630, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "   After removing 'eco':\n",
      "   ‚Ä¢ X_train: (2172220, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_val: (434445, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ X_test: (289630, 3), columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   188          0               4               A04               \n",
      "   234          0               9               A09               \n",
      "   661          0               95              A95               \n",
      "   2272         2               68              C68               \n",
      "   3476         2               42              C42               \n",
      "   1175         1               40              B40               \n",
      "   3061         4               70              E70               \n",
      "   567          0               58              A58               \n",
      "   1855         2               41              C41               \n",
      "   465          0               45              A45               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            578     21.30%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            590     21.74%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            891     32.83%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            419     15.44%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            236      8.70%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO SIDE INFORMATION CREATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2714, 2)\n",
      "   ‚Ä¢ Index: opening_id (for O(1) lookups)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 2,172,220 rows, 3 features\n",
      "   ‚Ä¢ X_val: 434,445 rows, 3 features\n",
      "   ‚Ä¢ X_test: 289,630 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,714 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "üí° Model usage:\n",
      "   During training, for each (player_id, opening_id) pair:\n",
      "   1. Lookup opening_id ‚Üí opening_side_info[opening_id]\n",
      "   2. Get eco_letter_cat and eco_number_cat (already encoded as integers)\n",
      "   3. Feed into categorical embedding layers\n",
      "   4. Combine with opening latent factors\n",
      "\n",
      "üßπ Final cleanup:\n",
      "   ‚Ä¢ Kept only encoded categorical columns: eco_letter_cat, eco_number_cat\n",
      "   ‚Ä¢ Removed raw ECO strings (eco, eco_letter_str, eco_number_str)\n",
      "   ‚Ä¢ Column names clearly indicate categorical encoding (_cat suffix)\n"
     ]
    }
   ],
   "source": [
    "# 4b. Create ECO side information and remove ECO from train/test/val DataFrames\n",
    "\n",
    "# Check if ECO processing has already been done\n",
    "if 'eco' not in X_train.columns and 'opening_side_info' in globals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 4B: ECO SIDE INFORMATION CREATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì ECO column already removed from train/test/val data\")\n",
    "    print(\"‚úì 'opening_side_info' table already exists\")\n",
    "    print(f\"\\nOpening side info shape: {opening_side_info.shape}\")\n",
    "    print(f\"Columns: {list(opening_side_info.columns)}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing ECO encoding statistics:\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_letter values: {opening_side_info['eco_letter'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_number values: {opening_side_info['eco_number'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ eco_letter range: [{opening_side_info['eco_letter'].min()}, {opening_side_info['eco_letter'].max()}]\")\n",
    "    print(f\"   ‚Ä¢ eco_number range: [{opening_side_info['eco_number'].min()}, {opening_side_info['eco_number'].max()}]\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing ECO encoding:\")\n",
    "    sample_data = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "    for idx, row in sample_data.iterrows():\n",
    "        print(f\"   Opening {idx:>4} | ECO: {row['eco']:>3} ‚Üí Letter: {row['eco_letter']} ({row['eco_letter_str']}), Number: {row['eco_number']:>2} ({row['eco_number_str']:>2})\")\n",
    "else:\n",
    "    def create_eco_side_information(clean_data_df, X_train_df, X_val_df, X_test_df):\n",
    "        \"\"\"\n",
    "        Create ECO side information table and remove ECO from train/test/val DataFrames.\n",
    "        \n",
    "        ECO codes are opening-level features, not player-opening interaction features.\n",
    "        We split each ECO code (e.g., \"C21\") into:\n",
    "        - eco_letter: The letter part (A, B, C, D, or E)\n",
    "        - eco_number: The numeric part (e.g., 21)\n",
    "        \n",
    "        Both are encoded as sequential integers for use as categorical features in embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        clean_data_df : pd.DataFrame\n",
    "            Full cleaned data with opening_id and eco columns\n",
    "        X_train_df, X_val_df, X_test_df : pd.DataFrame\n",
    "            Train/val/test feature DataFrames (will be modified to remove 'eco')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 4B: CREATE ECO SIDE INFORMATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Strategy:\")\n",
    "        print(f\"   ‚Ä¢ Extract unique opening_id ‚Üí eco mappings from clean_data\")\n",
    "        print(f\"   ‚Ä¢ Split ECO codes: 'C21' ‚Üí letter='C', number='21'\")\n",
    "        print(f\"   ‚Ä¢ Encode as sequential integers (categorical, not ordinal)\")\n",
    "        print(f\"   ‚Ä¢ Store in opening_side_info lookup table\")\n",
    "        print(f\"   ‚Ä¢ Remove 'eco' column from X_train, X_val, X_test\")\n",
    "        \n",
    "        # Extract unique opening ‚Üí ECO mappings\n",
    "        print(f\"\\n1Ô∏è‚É£  Extracting unique opening-ECO mappings...\")\n",
    "        opening_eco_map = clean_data_df[['opening_id', 'eco']].drop_duplicates().set_index('opening_id')\n",
    "        print(f\"   ‚úì Extracted {len(opening_eco_map):,} unique openings\")\n",
    "        \n",
    "        # Verify one-to-one mapping\n",
    "        eco_per_opening = clean_data_df.groupby('opening_id')['eco'].nunique()\n",
    "        if (eco_per_opening > 1).any():\n",
    "            problematic = eco_per_opening[eco_per_opening > 1]\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(problematic)} openings have multiple ECO codes!\")\n",
    "            print(f\"   Problematic opening IDs: {problematic.index.tolist()[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì Verified: Each opening has exactly one ECO code (good!)\")\n",
    "        \n",
    "        # Split ECO into letter and number components\n",
    "        print(f\"\\n2Ô∏è‚É£  Splitting ECO codes into letter and number components...\")\n",
    "        opening_eco_map['eco_letter_str'] = opening_eco_map['eco'].str[0]  # First character (A-E)\n",
    "        opening_eco_map['eco_number_str'] = opening_eco_map['eco'].str[1:]  # Remaining characters (numeric)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted letter and number components\")\n",
    "        print(f\"   ‚Ä¢ Unique letters: {opening_eco_map['eco_letter_str'].unique()}\")\n",
    "        print(f\"   ‚Ä¢ Unique numbers: {opening_eco_map['eco_number_str'].nunique()}\")\n",
    "        \n",
    "        # Create encoding mappings for eco_letter (A-E ‚Üí 0-4)\n",
    "        print(f\"\\n3Ô∏è‚É£  Encoding ECO letters as categorical integers...\")\n",
    "        unique_letters = sorted(opening_eco_map['eco_letter_str'].unique())\n",
    "        eco_letter_to_int = {letter: idx for idx, letter in enumerate(unique_letters)}\n",
    "        eco_int_to_letter = {idx: letter for letter, idx in eco_letter_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_letter'] = opening_eco_map['eco_letter_str'].map(eco_letter_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Letter encoding created:\")\n",
    "        for letter, idx in sorted(eco_letter_to_int.items()):\n",
    "            count = (opening_eco_map['eco_letter_str'] == letter).sum()\n",
    "            print(f\"      '{letter}' ‚Üí {idx} ({count:,} openings)\")\n",
    "        \n",
    "        # Create encoding mappings for eco_number (00-99 ‚Üí sequential integers)\n",
    "        print(f\"\\n4Ô∏è‚É£  Encoding ECO numbers as categorical integers...\")\n",
    "        unique_numbers = sorted(opening_eco_map['eco_number_str'].unique())\n",
    "        eco_number_to_int = {num: idx for idx, num in enumerate(unique_numbers)}\n",
    "        eco_int_to_number = {idx: num for num, idx in eco_number_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_number'] = opening_eco_map['eco_number_str'].map(eco_number_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Number encoding created:\")\n",
    "        print(f\"      {len(unique_numbers)} unique numbers mapped to [0, {len(unique_numbers)-1}]\")\n",
    "        print(f\"      Range: '{unique_numbers[0]}' ‚Üí 0, ..., '{unique_numbers[-1]}' ‚Üí {len(unique_numbers)-1}\")\n",
    "        \n",
    "        # Show distribution of numbers\n",
    "        print(f\"\\n   Distribution of ECO numbers (top 10):\")\n",
    "        number_counts = opening_eco_map['eco_number_str'].value_counts().head(10)\n",
    "        for num, count in number_counts.items():\n",
    "            encoded = eco_number_to_int[num]\n",
    "            print(f\"      '{num}' (‚Üí {encoded:>2}): {count:>3} openings\")\n",
    "        \n",
    "        # Create final opening_side_info table\n",
    "        print(f\"\\n5Ô∏è‚É£  Creating opening_side_info lookup table...\")\n",
    "        # Only keep the encoded categorical columns and rename them for clarity\n",
    "        opening_side_info = opening_eco_map[['eco_letter', 'eco_number']].copy()\n",
    "        opening_side_info = opening_side_info.rename(columns={\n",
    "            'eco_letter': 'eco_letter_cat',  # _cat suffix indicates categorical encoding\n",
    "            'eco_number': 'eco_number_cat'\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úì Created opening_side_info\")\n",
    "        print(f\"      ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"      ‚Ä¢ Index: opening_id\")\n",
    "        print(f\"      ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        print(f\"      ‚Ä¢ eco_letter_cat: Categorical encoding of ECO letter (A-E ‚Üí 0-4)\")\n",
    "        print(f\"      ‚Ä¢ eco_number_cat: Categorical encoding of ECO number\")\n",
    "        \n",
    "        # Verify all openings in train/val/test have ECO info\n",
    "        print(f\"\\n6Ô∏è‚É£  Verifying coverage of train/val/test openings...\")\n",
    "        all_openings = set(X_train_df['opening_id'].unique()) | \\\n",
    "                       set(X_val_df['opening_id'].unique()) | \\\n",
    "                       set(X_test_df['opening_id'].unique())\n",
    "        \n",
    "        missing_openings = all_openings - set(opening_side_info.index)\n",
    "        if len(missing_openings) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits are missing ECO info!\")\n",
    "            print(f\"   Missing opening IDs: {sorted(list(missing_openings))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(all_openings):,} openings in train/val/test have ECO side information\")\n",
    "        \n",
    "        # Remove ECO from train/val/test DataFrames\n",
    "        print(f\"\\n7Ô∏è‚É£  Removing 'eco' column from train/val/test DataFrames...\")\n",
    "        print(f\"   ‚Ä¢ X_train before: {X_train_df.shape}, columns: {list(X_train_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val before: {X_val_df.shape}, columns: {list(X_val_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test before: {X_test_df.shape}, columns: {list(X_test_df.columns)}\")\n",
    "        \n",
    "        X_train_clean = X_train_df.drop(columns=['eco'])\n",
    "        X_val_clean = X_val_df.drop(columns=['eco'])\n",
    "        X_test_clean = X_test_df.drop(columns=['eco'])\n",
    "        \n",
    "        print(f\"\\n   After removing 'eco':\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape}, columns: {list(X_train_clean.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape}, columns: {list(X_val_clean.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape}, columns: {list(X_test_clean.columns)}\")\n",
    "        \n",
    "        # Sample data showing the transformation\n",
    "        print(f\"\\n8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\")\n",
    "        sample_openings = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "        \n",
    "        # For display purposes, reconstruct original values from the categorical encodings\n",
    "        print(f\"\\n   {'Opening ID':<12} {'Letter (Cat)':<15} {'Number (Cat)':<15} {'Reconstructed ECO':<18}\")\n",
    "        print(f\"   {'-'*12} {'-'*15} {'-'*15} {'-'*18}\")\n",
    "        for idx, row in sample_openings.iterrows():\n",
    "            letter_str = eco_int_to_letter[row['eco_letter_cat']]\n",
    "            number_str = eco_int_to_number[row['eco_number_cat']]\n",
    "            reconstructed = f\"{letter_str}{number_str}\"\n",
    "            print(f\"   {idx:<12} {row['eco_letter_cat']:<15} {row['eco_number_cat']:<15} {reconstructed:<18}\")\n",
    "        \n",
    "        # Show ECO family distribution\n",
    "        print(f\"\\n9Ô∏è‚É£  ECO family distribution in opening_side_info:\")\n",
    "        letter_dist = opening_side_info['eco_letter_cat'].value_counts().sort_index()\n",
    "        print(f\"\\n   {'Encoded':<8} {'Letter':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "        print(f\"   {'-'*8} {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "        for encoded_val, count in letter_dist.items():\n",
    "            letter = eco_int_to_letter[encoded_val]\n",
    "            pct = 100 * count / len(opening_side_info)\n",
    "            bar_length = int(pct * 0.4)\n",
    "            bar = '‚ñà' * bar_length\n",
    "            print(f\"   {encoded_val:<8} {letter:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ ECO SIDE INFORMATION CREATION COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nCreated: opening_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: opening_id (for O(1) lookups)\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape[0]:,} rows, {X_train_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape[0]:,} rows, {X_val_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape[0]:,} rows, {X_test_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ opening_side_info: {len(opening_side_info):,} openings (one per opening)\")\n",
    "        print(f\"   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\")\n",
    "        print(f\"   ‚Ä¢ eco_letter_to_int: {eco_letter_to_int}\")\n",
    "        print(f\"   ‚Ä¢ eco_number_to_int: (dict with {len(eco_number_to_int)} entries)\")\n",
    "        print(f\"\\n   You'll need them to encode ECO codes for new openings at inference time.\")\n",
    "        \n",
    "        print(f\"\\nüí° Model usage:\")\n",
    "        print(f\"   During training, for each (player_id, opening_id) pair:\")\n",
    "        print(f\"   1. Lookup opening_id ‚Üí opening_side_info[opening_id]\")\n",
    "        print(f\"   2. Get eco_letter_cat and eco_number_cat (already encoded as integers)\")\n",
    "        print(f\"   3. Feed into categorical embedding layers\")\n",
    "        print(f\"   4. Combine with opening latent factors\")\n",
    "        \n",
    "        print(f\"\\nüßπ Final cleanup:\")\n",
    "        print(f\"   ‚Ä¢ Kept only encoded categorical columns: eco_letter_cat, eco_number_cat\")\n",
    "        print(f\"   ‚Ä¢ Removed raw ECO strings (eco, eco_letter_str, eco_number_str)\")\n",
    "        print(f\"   ‚Ä¢ Column names clearly indicate categorical encoding (_cat suffix)\")\n",
    "        \n",
    "        return opening_side_info, eco_letter_to_int, eco_number_to_int, X_train_clean, X_val_clean, X_test_clean\n",
    "    \n",
    "    # Call the function\n",
    "    opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test = create_eco_side_information(\n",
    "        clean_data, X_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Create reverse mappings for decoding (needed for verification and debugging)\n",
    "    eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "    eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "    print(f\"\\n‚úì Created reverse mappings for ECO decoding:\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_letter: {len(eco_int_to_letter)} entries\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_number: {len(eco_int_to_number)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4b4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICATION: FINAL DATA STRUCTURE\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\n",
      "\n",
      "   X_train:\n",
      "   ‚Ä¢ Shape: (2172220, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ Sample:\n",
      "         player_id  opening_id  confidence\n",
      "383888        6489        1314    0.484536\n",
      "1185183      20079        1141    0.358974\n",
      "2381369      40268        3482    0.242424\n",
      "\n",
      "   X_val:\n",
      "   ‚Ä¢ Shape: (434445, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "   X_test:\n",
      "   ‚Ä¢ Shape: (289630, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Side Information Tables:\n",
      "\n",
      "   player_side_info (indexed by player_id):\n",
      "   ‚Ä¢ Shape: (48469, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Sample:\n",
      "           rating_z\n",
      "player_id          \n",
      "1          0.609066\n",
      "2          1.058411\n",
      "3          0.560921\n",
      "\n",
      "   opening_side_info (indexed by opening_id):\n",
      "   ‚Ä¢ Shape: (2714, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Sample:\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "688                      1               0\n",
      "730                      1               0\n",
      "741                      1               0\n",
      "\n",
      "3Ô∏è‚É£  Encoding Mappings (for inference):\n",
      "\n",
      "   eco_letter_map:\n",
      "      'A' ‚Üí 0\n",
      "      'B' ‚Üí 1\n",
      "      'C' ‚Üí 2\n",
      "      'D' ‚Üí 3\n",
      "      'E' ‚Üí 4\n",
      "\n",
      "   eco_number_map (first 10):\n",
      "      '00' ‚Üí 0\n",
      "      '01' ‚Üí 1\n",
      "      '02' ‚Üí 2\n",
      "      '03' ‚Üí 3\n",
      "      '04' ‚Üí 4\n",
      "      '05' ‚Üí 5\n",
      "      '06' ‚Üí 6\n",
      "      '07' ‚Üí 7\n",
      "      '08' ‚Üí 8\n",
      "      '09' ‚Üí 9\n",
      "      ... (100 total)\n",
      "\n",
      "4Ô∏è‚É£  Example: Lookup flow for a random train sample:\n",
      "\n",
      "   Sample interaction:\n",
      "   ‚Ä¢ player_id: 3689.0\n",
      "   ‚Ä¢ opening_id: 838.0\n",
      "   ‚Ä¢ confidence: 0.4048\n",
      "\n",
      "   Player side info lookup:\n",
      "   ‚Ä¢ rating_z: 1.1547\n",
      "\n",
      "   Opening side info lookup:\n",
      "\n",
      "   Sample interaction:\n",
      "   ‚Ä¢ player_id: 3689.0\n",
      "   ‚Ä¢ opening_id: 838.0\n",
      "   ‚Ä¢ confidence: 0.4048\n",
      "\n",
      "   Player side info lookup:\n",
      "   ‚Ä¢ rating_z: 1.1547\n",
      "\n",
      "   Opening side info lookup:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eco_int_to_letter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m   Opening side info lookup:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m opening_info = opening_side_info.loc[opening_id]\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚Ä¢ eco_letter_cat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopening_info[\u001b[33m'\u001b[39m\u001b[33meco_letter_cat\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (letter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43meco_int_to_letter\u001b[49m[opening_info[\u001b[33m'\u001b[39m\u001b[33meco_letter_cat\u001b[39m\u001b[33m'\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚Ä¢ eco_number_cat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopening_info[\u001b[33m'\u001b[39m\u001b[33meco_number_cat\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meco_int_to_number[opening_info[\u001b[33m'\u001b[39m\u001b[33meco_number_cat\u001b[39m\u001b[33m'\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'eco_int_to_letter' is not defined"
     ]
    }
   ],
   "source": [
    "# Verify final data structure after ECO processing\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION: FINAL DATA STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\")\n",
    "print(f\"\\n   X_train:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_train.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(X_train.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   X_val:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_val.columns)}\")\n",
    "\n",
    "print(f\"\\n   X_test:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_test.columns)}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Side Information Tables:\")\n",
    "\n",
    "print(f\"\\n   player_side_info (indexed by player_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(player_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   opening_side_info (indexed by opening_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(opening_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Encoding Mappings (for inference):\")\n",
    "print(f\"\\n   eco_letter_map:\")\n",
    "for k, v in sorted(eco_letter_map.items()):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "\n",
    "print(f\"\\n   eco_number_map (first 10):\")\n",
    "for i, (k, v) in enumerate(sorted(eco_number_map.items())[:10]):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "print(f\"      ... ({len(eco_number_map)} total)\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Example: Lookup flow for a random train sample:\")\n",
    "sample = X_train.sample(1, random_state=42).iloc[0]\n",
    "player_id = sample['player_id']\n",
    "opening_id = sample['opening_id']\n",
    "\n",
    "print(f\"\\n   Sample interaction:\")\n",
    "print(f\"   ‚Ä¢ player_id: {player_id}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {opening_id}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Player side info lookup:\")\n",
    "player_info = player_side_info.loc[player_id]\n",
    "print(f\"   ‚Ä¢ rating_z: {player_info['rating_z']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Opening side info lookup:\")\n",
    "opening_info = opening_side_info.loc[opening_id]\n",
    "print(f\"   ‚Ä¢ eco_letter_cat: {opening_info['eco_letter_cat']} (letter: {eco_int_to_letter[opening_info['eco_letter_cat']]})\")\n",
    "print(f\"   ‚Ä¢ eco_number_cat: {opening_info['eco_number_cat']} (number: {eco_int_to_number[opening_info['eco_number_cat']]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Ready for PyTorch tensor conversion:\")\n",
    "print(f\"   ‚Ä¢ Features: player_id, opening_id, confidence\")\n",
    "print(f\"   ‚Ä¢ Target: score (in y_train, y_val, y_test)\")\n",
    "print(f\"   ‚Ä¢ Player side info: rating_z\")\n",
    "print(f\"   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\")\n",
    "print(f\"\\n   All ECO and rating data is now properly separated as side information!\")\n",
    "print(f\"\\n   Side info tables are clean - only contain necessary model inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017515d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Sample 100 player-opening pairs with reconstructed ECO codes and opening names\n",
    "# Doing this to make sure that our ECO encoding/decoding is correct\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Sample 100 random player-opening pairs from training data\n",
    "sample_size = 100\n",
    "sample_data = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "\n",
    "print(f\"\\nSampling {len(sample_data)} player-opening pairs for verification...\\n\")\n",
    "\n",
    "# Get unique opening IDs from sample\n",
    "opening_ids = sample_data['opening_id'].unique()\n",
    "opening_ids_str = ','.join(map(str, opening_ids.astype(int)))\n",
    "\n",
    "# Query database for opening names\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    opening_query = f\"\"\"\n",
    "        SELECT id, name, eco\n",
    "        FROM opening\n",
    "        WHERE id IN ({opening_ids_str})\n",
    "    \"\"\"\n",
    "    opening_names = pd.DataFrame(con.execute(opening_query).df()).set_index('id')\n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Create reverse mappings for ECO decoding\n",
    "eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "\n",
    "# Build verification table\n",
    "print(f\"{'#':<4} {'Player':<8} {'Opening':<9} {'ECO (DB)':<10} {'Reconstructed':<13} {'Match':<6} {'Opening Name':<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "matches = 0\n",
    "for i, (idx, row) in enumerate(sample_data.iterrows(), 1):\n",
    "    player_id = int(row['player_id'])\n",
    "    opening_id = int(row['opening_id'])\n",
    "    \n",
    "    # Lookup opening side info\n",
    "    opening_info = opening_side_info.loc[opening_id]\n",
    "    \n",
    "    # Reconstruct ECO from encoded categorical values\n",
    "    eco_letter_encoded = opening_info['eco_letter_cat']\n",
    "    eco_number_encoded = opening_info['eco_number_cat']\n",
    "    \n",
    "    eco_letter_decoded = eco_int_to_letter[eco_letter_encoded]\n",
    "    eco_number_decoded = eco_int_to_number[eco_number_encoded]\n",
    "    \n",
    "    reconstructed_eco = f\"{eco_letter_decoded}{eco_number_decoded}\"\n",
    "    \n",
    "    # Get original ECO from database\n",
    "    db_eco = opening_names.loc[opening_id, 'eco']\n",
    "    opening_name = opening_names.loc[opening_id, 'name']\n",
    "    \n",
    "    # Check if they match\n",
    "    match = \"‚úì\" if reconstructed_eco == db_eco else \"‚úó\"\n",
    "    if reconstructed_eco == db_eco:\n",
    "        matches += 1\n",
    "    \n",
    "    # Truncate opening name if too long\n",
    "    if len(opening_name) > 48:\n",
    "        opening_name = opening_name[:45] + \"...\"\n",
    "    \n",
    "    print(f\"{i:<4} {player_id:<8} {opening_id:<9} {db_eco:<10} {reconstructed_eco:<13} {match:<6} {opening_name:<50}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n‚úÖ Verification Results:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(sample_data)}\")\n",
    "print(f\"   ‚Ä¢ Matches: {matches}/{len(sample_data)} ({100*matches/len(sample_data):.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Mismatches: {len(sample_data) - matches}\")\n",
    "\n",
    "if matches == len(sample_data):\n",
    "    print(f\"\\nüéâ Perfect! All ECO codes reconstructed correctly!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Some ECO codes did not match. Investigate mismatches above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf4878",
   "metadata": {},
   "source": [
    "## 5. Data Verification and Examination\n",
    "We're almost there. Let's examine our data structures to check for any obvious flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"X_train \\n\", X_train.head())\n",
    "print(\"=\"*60)\n",
    "print(\"X_val \\n\", X_val.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"X_test \\n\", X_test.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_train \\n\", y_train.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_val \\n\", y_val.head())\n",
    "print(\"=\" * 60) \n",
    "print(\"y_test \\n\", y_test.head())\n",
    "\n",
    "# Now side information\n",
    "print(\"player_side_info \\n\", player_side_info.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"opening_side_info \\n\", opening_side_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52936cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification: Display cleaned side info tables\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANED SIDE INFORMATION TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä player_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: player_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(player_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\nüìä opening_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: opening_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(opening_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\n‚úÖ Both side info tables contain ONLY the necessary model inputs:\")\n",
    "print(f\"   ‚Ä¢ player_side_info: rating_z (normalized rating)\")\n",
    "print(f\"   ‚Ä¢ opening_side_info: eco_letter_cat, eco_number_cat (categorical encodings)\")\n",
    "print(f\"   ‚Ä¢ No unnecessary columns (names, titles, raw strings, etc.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
