{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e163a7",
   "metadata": {},
   "source": [
    "# Notebook 28 ‚Äî Opening Recommender Model: Training Pipeline\n",
    "\n",
    "### 0. Overview and Goals\n",
    "\n",
    "This notebook defines the full pipeline for training the chess opening recommender model.\n",
    "The objective is to predict **player‚Äìopening performance scores** ((wins + (0.5 * draws) / num games)) for openings a player hasn‚Äôt yet played, based on their results in the openings they *have* played.\n",
    "\n",
    "The model will use **matrix factorization** with **stochastic gradient descent (SGD)** to learn latent factors representing player and opening characteristics.\n",
    "All computations will be implemented in **PyTorch**, with data loaded from my local **DuckDB** database.\n",
    "\n",
    "**High-level specs:**\n",
    "- Use only *White* openings initially (we‚Äôll extend to Black later).\n",
    "- Data source: processed player‚Äìopening stats from local DuckDB.\n",
    "- Predict: normalized ‚Äúscore‚Äù = win rate ((wins + 0.5 x draws) / total games).\n",
    "- Filter: only include entries with ‚â• `MIN_GAMES_THRESHOLD` (default = 10).\n",
    "- Ignore: rating differences, time controls, and other metadata for the base model.\n",
    "- Model parameters (to be defined in appropriate places for easy editing):\n",
    "  - `NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`, `N_EPOCHS`, `NUM_PLAYERS_TO_PROCESS`\n",
    "- Logging and checkpoints throughout for reproducibility.\n",
    "- All random operations seeded for deterministic runs.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Extraction\n",
    "- Connect to local DuckDB and pull all processed player‚Äìopening statistics.\n",
    "- Verify schema consistency and include row-count sanity checks.\n",
    "- Filter for players with ratings above a minimum threshold (e.g., 1200).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Sanitization & Normalization\n",
    "- Apply confidence weighting using hierarchical Bayesian shrinkage to adjust scores for low-game-count entries.\n",
    "- Normalize player ratings (z-score) for use as side information.\n",
    "- Resequence player and opening IDs to be contiguous integers for embedding layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Splits\n",
    "- Split data into train, validation, and test sets (e.g., 75/15/10).\n",
    "- Ensure splits are handled correctly to avoid data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Enumerate Categorical Variables\n",
    "- Process ECO codes into categorical features (e.g., `eco_letter`, `eco_number`).\n",
    "- Store these as opening-level side information.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training Data Structure\n",
    "- Convert the final, processed DataFrames into PyTorch Tensors.\n",
    "- Create custom `Dataset` and `DataLoader` classes for efficient batching.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Cross-Validation & Hyperparameter Tuning\n",
    "- Define ranges for hyperparameters (`NUM_FACTORS`, `LEARNING_RATE`, `BATCH_SIZE`).\n",
    "- Perform k-fold cross-validation on a subset of the training data to find the best hyperparameter combination before the final training run.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Final Model Training Setup\n",
    "- Define constants and hyperparameters for the final model (using results from CV).\n",
    "- Instantiate the PyTorch model, optimizer (SGD), and learning rate scheduler.\n",
    "- Implement helper functions for training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Final Model Training Loop\n",
    "- Initialize player and opening embeddings.\n",
    "- Iterate through epochs with mini-batch SGD.\n",
    "- Compute and log training and validation metrics (e.g., RMSE) per epoch.\n",
    "- Save model checkpoints locally.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Evaluation\n",
    "- Evaluate the final trained model on the held-out test set.\n",
    "- Report final metrics (MSE, RMSE) and create visualizations (e.g., predicted vs. actual scores).\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Next Steps\n",
    "- Extend model to include Black openings.\n",
    "- Experiment with more complex architectures or hybrid inputs.\n",
    "- Integrate the trained model into an API for serving recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "- Every random seed and parameter definition will be explicit.\n",
    "- Every major step includes row-count, schema, and type validation.\n",
    "- Model artifacts and logs will be saved locally for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6d823",
   "metadata": {},
   "source": [
    "## Step 1: Data Extraction\n",
    "\n",
    "Connect to DuckDB and extract all player-opening statistics.\n",
    "Verify schema and perform sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcea569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA EXTRACTION\n",
      "üìÅ Database exists: True\n",
      "üé® Color filter: White\n",
      "üîí Minimum holdout players: 1,000\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd() / 'utils'))\n",
    "from database.db_utils import get_db_connection\n",
    "\n",
    "# Configuration\n",
    "DB_PATH = Path.cwd().parent / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "COLOR_FILTER = 'w'  # 'w' for white, 'b' for black\n",
    "MIN_HOLDOUT_PLAYERS = 1000  # Minimum number of players to reserve for fold-in verification. These will not be used at all in this notebook for training, test/val or anything else.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: DATA EXTRACTION\")\n",
    "print(f\"üìÅ Database exists: {DB_PATH.exists()}\")\n",
    "print(f\"üé® Color filter: {'White' if COLOR_FILTER == 'w' else 'Black'}\")\n",
    "print(f\"üîí Minimum holdout players: {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9435033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£  Extracting player-opening stats (color: 'w')...\n",
      "   ‚Ä¢ Minimum rating filter: 1200\n",
      "\n",
      "2Ô∏è‚É£  Selecting holdout players for fold-in verification...\n",
      "   ‚Ä¢ Holdout size: 1,000 players minimum\n",
      "   ‚Ä¢ Total eligible players: 49,551\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 48,551\n",
      "   ‚Ä¢ Holdout percentage: 2.0%\n",
      "   ‚Ä¢ Total eligible players: 49,551\n",
      "   ‚Ä¢ Holdout players selected: 1,000\n",
      "   ‚Ä¢ Training players available: 48,551\n",
      "   ‚Ä¢ Holdout percentage: 2.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a266a5de998411a80bb3ff9b79dd49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Extracted 11,567,358 rows\n",
      "   ‚úì All required columns present: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "   ‚Ä¢ player_id: int32\n",
      "   ‚Ä¢ opening_id: int32\n",
      "   ‚Ä¢ num_games: int32\n",
      "   ‚Ä¢ score: float64\n",
      "   ‚Ä¢ eco: object\n",
      "\n",
      "6Ô∏è‚É£  Data statistics...\n",
      "   ‚Ä¢ Total rows: 11,567,358\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "   ‚Ä¢ Total games (sum): 228,918,732\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 50000\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 13462\n",
      "   ‚Ä¢ Mean: 19.8\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "   ‚Ä¢ Total games (sum): 228,918,732\n",
      "\n",
      "   Player ID range:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 50000\n",
      "\n",
      "   Opening ID range:\n",
      "   ‚Ä¢ Min: 2\n",
      "   ‚Ä¢ Max: 3589\n",
      "\n",
      "   Games per entry:\n",
      "   ‚Ä¢ Min: 1\n",
      "   ‚Ä¢ Max: 13462\n",
      "   ‚Ä¢ Mean: 19.8\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5007\n",
      "   ‚Ä¢ Median: 3\n",
      "\n",
      "   Score distribution:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5007\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚Ä¢ Median: 0.5000\n",
      "\n",
      "7Ô∏è‚É£  Checking for null values...\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0          1          39          6  0.666667  A00\n",
      "1          1          49          2  0.500000  A00\n",
      "2          1          53          1  0.000000  A00\n",
      "3          1         182          1  0.000000  A04\n",
      "4          1         187          1  1.000000  A04\n",
      "5          1         671          3  1.000000  B00\n",
      "6          1         675          3  0.000000  B00\n",
      "7          1         677          6  0.333333  B00\n",
      "8          1         688         25  0.620000  B00\n",
      "9          1         717          1  1.000000  B00\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (11567358, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚úì Database connection closed\n",
      "   ‚úì No null values found\n",
      "\n",
      "8Ô∏è‚É£  Sample of extracted data (first 10 rows):\n",
      "   player_id  opening_id  num_games     score  eco\n",
      "0          1          39          6  0.666667  A00\n",
      "1          1          49          2  0.500000  A00\n",
      "2          1          53          1  0.000000  A00\n",
      "3          1         182          1  0.000000  A04\n",
      "4          1         187          1  1.000000  A04\n",
      "5          1         671          3  1.000000  B00\n",
      "6          1         675          3  0.000000  B00\n",
      "7          1         677          6  0.333333  B00\n",
      "8          1         688         25  0.620000  B00\n",
      "9          1         717          1  1.000000  B00\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data shape: (11567358, 5)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
      "\n",
      "‚úì Database connection closed\n"
     ]
    }
   ],
   "source": [
    "# Get our training database\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "MAX_PLAYERS = 50_000\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    print(f\"\\n1Ô∏è‚É£  Extracting player-opening stats (color: '{COLOR_FILTER}')...\")\n",
    "\n",
    "    # Extract stats with calculated score and num_games\n",
    "    # Filter by color, minimum rating, and calculate score in the database\n",
    "    MIN_RATING = 1200\n",
    "    print(f\"   ‚Ä¢ Minimum rating filter: {MIN_RATING}\")\n",
    "\n",
    "    print(f\"\\n2Ô∏è‚É£  Selecting holdout players for fold-in verification...\")\n",
    "    print(f\"   ‚Ä¢ Holdout size: {MIN_HOLDOUT_PLAYERS:,} players minimum\")\n",
    "\n",
    "    player_query = f\"\"\"\n",
    "        SELECT DISTINCT p.id as player_id\n",
    "        FROM player p\n",
    "        JOIN player_opening_stats pos ON p.id = pos.player_id\n",
    "        WHERE p.rating >= {MIN_RATING}\n",
    "        AND pos.color = '{COLOR_FILTER}'\n",
    "        LIMIT {MAX_PLAYERS}\n",
    "    \"\"\"\n",
    "\n",
    "    all_eligible_players = pd.DataFrame(con.execute(player_query).df())\n",
    "    total_eligible = len(all_eligible_players)\n",
    "    print(f\"   ‚Ä¢ Total eligible players: {total_eligible:,}\")\n",
    "\n",
    "    if total_eligible < MIN_HOLDOUT_PLAYERS:\n",
    "        raise ValueError(f\"Not enough eligible players ({total_eligible:,}) to create holdout set of {MIN_HOLDOUT_PLAYERS:,}\")\n",
    "\n",
    "    # Randomly sample holdout players (deterministic with seed)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    holdout_player_ids = np.random.choice(\n",
    "        all_eligible_players['player_id'].values,\n",
    "        size=MIN_HOLDOUT_PLAYERS,\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    training_player_ids = set(all_eligible_players['player_id'].values) - set(holdout_player_ids)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Holdout players selected: {len(holdout_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Training players available: {len(training_player_ids):,}\")\n",
    "    print(f\"   ‚Ä¢ Holdout percentage: {100 * len(holdout_player_ids) / total_eligible:.1f}%\")\n",
    "\n",
    "    # SQL-friendly string\n",
    "    training_player_ids_str = ','.join(map(str, training_player_ids))\n",
    "\n",
    "    # Extract data ONLY for training players\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            pos.player_id,\n",
    "            pos.opening_id,\n",
    "            pos.num_wins + pos.num_draws + pos.num_losses as num_games,\n",
    "            (pos.num_wins + (pos.num_draws * 0.5)) / \n",
    "                NULLIF(pos.num_wins + pos.num_draws + pos.num_losses, 0) as score,\n",
    "            o.eco\n",
    "        FROM player_opening_stats pos\n",
    "        JOIN opening o ON pos.opening_id = o.id\n",
    "        JOIN player p ON pos.player_id = p.id\n",
    "        WHERE pos.color = '{COLOR_FILTER}'\n",
    "        AND p.rating >= {MIN_RATING}\n",
    "        AND pos.player_id IN ({training_player_ids_str})\n",
    "        ORDER BY pos.player_id, pos.opening_id\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data = pd.DataFrame(con.execute(query).df())\n",
    "\n",
    "    print(f\"   ‚úì Extracted {len(raw_data):,} rows\")\n",
    "\n",
    "    # Also save holdout player IDs for later use\n",
    "    holdout_players_df = pd.DataFrame({'player_id': holdout_player_ids})\n",
    "\n",
    "    # Schema verification\n",
    "    required_columns = ['player_id', 'opening_id', 'num_games', 'score', 'eco']\n",
    "\n",
    "    for col in required_columns:\n",
    "        if col not in raw_data.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    print(f\"   ‚úì All required columns present: {required_columns}\")\n",
    "\n",
    "    # Data types verification\n",
    "    print(f\"   ‚Ä¢ player_id: {raw_data['player_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ opening_id: {raw_data['opening_id'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ num_games: {raw_data['num_games'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ score: {raw_data['score'].dtype}\")\n",
    "    print(f\"   ‚Ä¢ eco: {raw_data['eco'].dtype}\")\n",
    "\n",
    "    print(\"\\n6Ô∏è‚É£  Data statistics...\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {len(raw_data):,}\")\n",
    "    print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Total games (sum): {raw_data['num_games'].sum():,}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Player ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['player_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['player_id'].max()}\")\n",
    "\n",
    "    # These won't be sequential; that's OK, we'll sequentialize them later\n",
    "    print(f\"\\n   Opening ID range:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['opening_id'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['opening_id'].max()}\")\n",
    "\n",
    "    print(f\"\\n   Games per entry:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['num_games'].min()}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['num_games'].max()}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['num_games'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['num_games'].median():.0f}\")\n",
    "\n",
    "    print(f\"\\n   Score distribution:\")\n",
    "    print(f\"   ‚Ä¢ Min: {raw_data['score'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {raw_data['score'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {raw_data['score'].mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Median: {raw_data['score'].median():.4f}\")\n",
    "\n",
    "    print(\"\\n7Ô∏è‚É£  Checking for null values...\")\n",
    "    null_counts = raw_data.isnull().sum()\n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"   ‚úì No null values found\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "\n",
    "    print(\"\\n8Ô∏è‚É£  Sample of extracted data (first 10 rows):\")\n",
    "    print(raw_data.head(10).to_string())\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ DATA EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nData shape: {raw_data.shape}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "\n",
    "finally:\n",
    "    con.close()\n",
    "    print(\"\\n‚úì Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad110b85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "519d8f2f",
   "metadata": {},
   "source": [
    "## Step 2: Data Sanitization & Normalization\n",
    "\n",
    "Filter low-quality data, handle duplicates, and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74f0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: DATA SANITIZATION & NORMALIZATION\n",
      "\n",
      "üìä Starting data shape: (11567358, 5)\n",
      "   ‚Ä¢ Rows: 11,567,358\n",
      "   ‚Ä¢ Unique players: 48,551\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Unique openings: 2,991\n",
      "\n",
      "1Ô∏è‚É£  Filtering entries with < 10 games...\n",
      "   ‚Ä¢ Before: 11,567,358 rows\n",
      "   ‚Ä¢ After: 2,897,700 rows\n",
      "   ‚Ä¢ Filtered out: 8,669,658 rows (74.9%)\n",
      "   ‚Ä¢ Before: 11,567,358 rows\n",
      "   ‚Ä¢ After: 2,897,700 rows\n",
      "   ‚Ä¢ Filtered out: 8,669,658 rows (74.9%)\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Data statistics:\n",
      "   ‚Ä¢ Total rows: 2,897,700\n",
      "   ‚Ä¢ Unique players: 48,468\n",
      "   ‚Ä¢ Unique openings: 2,717\n",
      "   ‚Ä¢ Total games: 206,320,314\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚úì No null values found\n",
      "\n",
      "6Ô∏è‚É£  Data statistics:\n",
      "   ‚Ä¢ Total rows: 2,897,700\n",
      "   ‚Ä¢ Unique players: 48,468\n",
      "   ‚Ä¢ Unique openings: 2,717\n",
      "   ‚Ä¢ Total games: 206,320,314\n",
      "   ‚Ä¢ Avg games per entry: 71.2\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1066.5\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Avg openings per player: 59.8\n",
      "   ‚Ä¢ Avg players per opening: 1066.5\n",
      "\n",
      "   Score statistics:\n",
      "   ‚Ä¢ Min: 0.0000\n",
      "   ‚Ä¢ 25th percentile: 0.4500\n",
      "   ‚Ä¢ Median: 0.5125\n",
      "   ‚Ä¢ 75th percentile: 0.5750\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5111\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "1400658      23714        1385        128  0.449219  C02\n",
      "2321194      39231        2187         91  0.560440  C58\n",
      "2710107      45961        1622         29  0.568966  C26\n",
      "1204342      20380         218         37  0.459459  A06\n",
      "419673        7053        1582        217  0.467742  C24\n",
      "2379207      40223        3204         58  0.508621  B01\n",
      "2120216      35840        1424         14  0.500000  C10\n",
      "887519       14890        1854         21  0.380952  C41\n",
      "2891967      49837         299         31  0.725806  A16\n",
      "637048       10683         353         41  0.500000  A34\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2897700, 5)\n",
      "Data reduction: 74.9%\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5111\n",
      "   ‚Ä¢ Std: 0.1083\n",
      "\n",
      "7Ô∏è‚É£  Sample of cleaned data (10 random rows):\n",
      "         player_id  opening_id  num_games     score  eco\n",
      "1400658      23714        1385        128  0.449219  C02\n",
      "2321194      39231        2187         91  0.560440  C58\n",
      "2710107      45961        1622         29  0.568966  C26\n",
      "1204342      20380         218         37  0.459459  A06\n",
      "419673        7053        1582        217  0.467742  C24\n",
      "2379207      40223        3204         58  0.508621  B01\n",
      "2120216      35840        1424         14  0.500000  C10\n",
      "887519       14890        1854         21  0.380952  C41\n",
      "2891967      49837         299         31  0.725806  A16\n",
      "637048       10683         353         41  0.500000  A34\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SANITIZATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Cleaned data shape: (2897700, 5)\n",
      "Data reduction: 74.9%\n"
     ]
    }
   ],
   "source": [
    "# 2a. Filter low-quality data, handle duplicates, and prepare for training.\n",
    "\n",
    "# Configuration\n",
    "MIN_GAMES_THRESHOLD = 10\n",
    "\n",
    "print(\"STEP 2: DATA SANITIZATION & NORMALIZATION\")\n",
    "\n",
    "# Start with raw_data from Step 1\n",
    "print(f\"\\nüìä Starting data shape: {raw_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Rows: {len(raw_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {raw_data['player_id'].nunique():,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {raw_data['opening_id'].nunique():,}\")\n",
    "\n",
    "# 1. Filter by minimum games threshold\n",
    "print(f\"\\n1Ô∏è‚É£  Filtering entries with < {MIN_GAMES_THRESHOLD} games...\")\n",
    "before_filter = len(raw_data)\n",
    "clean_data = raw_data.query(f'num_games >= {MIN_GAMES_THRESHOLD}').copy()\n",
    "num_rows_after_filter = len(clean_data)\n",
    "num_rows_filtered_out = before_filter - num_rows_after_filter\n",
    "\n",
    "print(f\"   ‚Ä¢ Before: {before_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ After: {num_rows_after_filter:,} rows\")\n",
    "print(f\"   ‚Ä¢ Filtered out: {num_rows_filtered_out:,} rows ({100*num_rows_filtered_out/before_filter:.1f}%)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "num_duplicates = clean_data.duplicated(subset=['player_id', 'opening_id']).sum()\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {num_duplicates} duplicate player-opening entries\")\n",
    "    dup_mask = clean_data.duplicated(subset=['player_id', 'opening_id'], keep=False)\n",
    "    print(\"\\n   Sample of duplicates:\")\n",
    "    print(clean_data[dup_mask].head(10).to_string())\n",
    "    \n",
    "    # Keep only first occurrence of any duplicate player-opening pair\n",
    "    print(\"\\n   Removing duplicates (keeping first occurrence)...\")\n",
    "    clean_data = pd.DataFrame.drop_duplicates(clean_data, subset=['player_id', 'opening_id'], keep='first')\n",
    "    print(f\"   ‚úì After deduplication: {len(clean_data):,} rows\")\n",
    "\n",
    "# 3. Remove players with no qualifying openings\n",
    "\n",
    "# Note that a few players only play stuff like the Van't Kruijs which we've excluded, so a small numer of players will be excluded here\n",
    "players_before = clean_data['player_id'].nunique()\n",
    "\n",
    "# Count openings per player\n",
    "num_openings_per_player = pd.DataFrame(clean_data.groupby('player_id').size(), columns=['count'])\n",
    "players_with_data = num_openings_per_player[num_openings_per_player['count'] > 0].index.tolist()\n",
    "\n",
    "# Filter\n",
    "clean_data = clean_data[clean_data['player_id'].isin(players_with_data)]\n",
    "players_after = clean_data['player_id'].nunique()\n",
    "\n",
    "# 4. Remove openings with no qualifying players\n",
    "num_openings_before = clean_data['opening_id'].nunique()\n",
    "\n",
    "num_players_per_opening = pd.DataFrame(clean_data.groupby('opening_id').size(), columns=['count'])\n",
    "openings_with_data = num_players_per_opening[num_players_per_opening['count'] > 0].index.tolist()\n",
    "\n",
    "clean_data = clean_data[clean_data['opening_id'].isin(openings_with_data)]\n",
    "openings_after = clean_data['opening_id'].nunique()\n",
    "\n",
    "# 5. Verify no null values using pd.isna()\n",
    "null_counts = pd.DataFrame.isna(clean_data).sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"   ‚úì No null values found\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Found null values:\")\n",
    "    for col, count in null_counts[null_counts > 0].items():\n",
    "        print(f\"      ‚Ä¢ {col}: {count} nulls\")\n",
    "    # Drop rows with nulls using pd.DataFrame.dropna()\n",
    "    clean_data = pd.DataFrame.dropna(clean_data)\n",
    "    print(f\"   ‚úì Dropped null rows. New shape: {clean_data.shape}\")\n",
    "\n",
    "# Reset index using pd.DataFrame.reset_index()\n",
    "clean_data = pd.DataFrame.reset_index(clean_data, drop=True)\n",
    "\n",
    "# Final statistics using pd functions\n",
    "print(f\"\\n6Ô∏è‚É£  Data statistics:\")\n",
    "print(f\"   ‚Ä¢ Total rows: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Unique players: {pd.Series.nunique(clean_data['player_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Unique openings: {pd.Series.nunique(clean_data['opening_id']):,}\")\n",
    "print(f\"   ‚Ä¢ Total games: {pd.Series.sum(clean_data['num_games']):,}\")\n",
    "print(f\"   ‚Ä¢ Avg games per entry: {pd.Series.mean(clean_data['num_games']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg openings per player: {len(clean_data) / pd.Series.nunique(clean_data['player_id']):.1f}\")\n",
    "print(f\"   ‚Ä¢ Avg players per opening: {len(clean_data) / pd.Series.nunique(clean_data['opening_id']):.1f}\")\n",
    "\n",
    "# Score distribution using pd functions\n",
    "print(f\"\\n   Score statistics:\")\n",
    "print(f\"   ‚Ä¢ Min: {pd.Series.min(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 25th percentile: {pd.Series.quantile(clean_data['score'], 0.25):.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {pd.Series.median(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ 75th percentile: {pd.Series.quantile(clean_data['score'], 0.75):.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {pd.Series.max(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {pd.Series.mean(clean_data['score']):.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {pd.Series.std(clean_data['score']):.4f}\")\n",
    "\n",
    "# Sample of cleaned data using pd.DataFrame.sample()\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of cleaned data (10 random rows):\")\n",
    "print(pd.DataFrame.sample(clean_data, min(10, len(clean_data)), random_state=42).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SANITIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCleaned data shape: {clean_data.shape}\")\n",
    "print(f\"Data reduction: {100 * (1 - len(clean_data)/len(raw_data)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e671b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\n",
      "============================================================\n",
      "\n",
      "üìä Global statistics:\n",
      "   ‚Ä¢ Global mean score: 0.5111\n",
      "   ‚Ä¢ Total entries: 2,897,700\n",
      "   ‚Ä¢ Unique openings: 2,717\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1667\n",
      "   ‚Ä¢ 25th percentile: 0.4964\n",
      "   ‚Ä¢ Median: 0.5161\n",
      "   ‚Ä¢ 75th percentile: 0.5361\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0505\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4743\n",
      "   ‚Ä¢ Players per opening (median): 156\n",
      "   ‚Ä¢ Total games range: [10, 5508743]\n",
      "   ‚Ä¢ Players range: [1, 42902]\n",
      "\n",
      "   Opening mean score distribution:\n",
      "   ‚Ä¢ Min: 0.1667\n",
      "   ‚Ä¢ 25th percentile: 0.4964\n",
      "   ‚Ä¢ Median: 0.5161\n",
      "   ‚Ä¢ 75th percentile: 0.5361\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Std: 0.0505\n",
      "\n",
      "   Opening sample size distribution:\n",
      "   ‚Ä¢ Total games per opening (median): 4743\n",
      "   ‚Ä¢ Players per opening (median): 156\n",
      "   ‚Ä¢ Total games range: [10, 5508743]\n",
      "   ‚Ä¢ Players range: [1, 42902]\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,897,700 entries\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Range: [0.1667, 0.9963]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000959\n",
      "   ‚Ä¢ Std adjustment: 0.076769\n",
      "   ‚Ä¢ Max adjustment: 0.457238\n",
      "   ‚Ä¢ Min adjustment: -0.457458\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   Formula: adjusted_score = (num_games √ó player_score + 50 √ó opening_mean) / (num_games + 50)\n",
      "   ‚úì Scores adjusted for 2,897,700 entries\n",
      "   ‚úì Confidence weights calculated\n",
      "   ‚Ä¢ Range: [0.1667, 0.9963]\n",
      "\n",
      "4Ô∏è‚É£  Adjustment statistics:\n",
      "   ‚Ä¢ Mean adjustment: 0.000959\n",
      "   ‚Ä¢ Std adjustment: 0.076769\n",
      "   ‚Ä¢ Max adjustment: 0.457238\n",
      "   ‚Ä¢ Min adjustment: -0.457458\n",
      "\n",
      "   Adjustment by num_games quartiles:\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003626\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001799\n",
      "   ‚Ä¢ 75th percentile (n=65 games): avg adjustment = -0.000393\n",
      "   ‚Ä¢ >75th percentile (n>65 games): avg adjustment = -0.001361\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile (n=15 games): avg adjustment = 0.003626\n",
      "   ‚Ä¢ 50th percentile (n=27 games): avg adjustment = 0.001799\n",
      "   ‚Ä¢ 75th percentile (n=65 games): avg adjustment = -0.000393\n",
      "   ‚Ä¢ >75th percentile (n>65 games): avg adjustment = -0.001361\n",
      "\n",
      "5Ô∏è‚É£  Adjusted score statistics:\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ Median: 0.5118\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   ‚Ä¢ 25th percentile: 0.4850\n",
      "   ‚Ä¢ Median: 0.5118\n",
      "   ‚Ä¢ 75th percentile: 0.5385\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "\n",
      "6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\n",
      "   ========================================================================================================================\n",
      "   Player 21931 | Opening  765 | Games:  14 | Opening mean: 0.4628 | Original: 0.5714 ‚Üí Adjusted: 0.4866 | Diff: -0.0848 | Confidence: 0.219\n",
      "   Player 35046 | Opening 2764 | Games:  10 | Opening mean: 0.5727 | Original: 0.2500 ‚Üí Adjusted: 0.5190 | Diff: +0.2690 | Confidence: 0.167\n",
      "   Player 19157 | Opening 1385 | Games:  12 | Opening mean: 0.4852 | Original: 0.2083 ‚Üí Adjusted: 0.4316 | Diff: +0.2233 | Confidence: 0.194\n",
      "   Player 42984 | Opening 2058 | Games:  18 | Opening mean: 0.5218 | Original: 0.5278 ‚Üí Adjusted: 0.5233 | Diff: -0.0044 | Confidence: 0.265\n",
      "   Player 11294 | Opening  336 | Games:  20 | Opening mean: 0.5175 | Original: 0.4500 ‚Üí Adjusted: 0.4982 | Diff: +0.0482 | Confidence: 0.286\n",
      "   Player  6897 | Opening 1229 | Games:  18 | Opening mean: 0.4955 | Original: 0.4444 ‚Üí Adjusted: 0.4820 | Diff: +0.0375 | Confidence: 0.265\n",
      "   Player 37716 | Opening  744 | Games:  15 | Opening mean: 0.5128 | Original: 0.4333 ‚Üí Adjusted: 0.4944 | Diff: +0.0611 | Confidence: 0.231\n",
      "   Player 14294 | Opening 2396 | Games:  12 | Opening mean: 0.5212 | Original: 0.4167 ‚Üí Adjusted: 0.5010 | Diff: +0.0843 | Confidence: 0.194\n",
      "   Player 45684 | Opening 2490 | Games:  19 | Opening mean: 0.5398 | Original: 0.5789 ‚Üí Adjusted: 0.5506 | Diff: -0.0283 | Confidence: 0.275\n",
      "   Player 42044 | Opening 1014 | Games:  10 | Opening mean: 0.4949 | Original: 0.6500 ‚Üí Adjusted: 0.5208 | Diff: -0.1292 | Confidence: 0.167\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 36174 | Opening 1190 | Games:  62 | Opening mean: 0.4727 | Original: 0.5161 ‚Üí Adjusted: 0.4967 | Diff: -0.0194 | Confidence: 0.554\n",
      "   Player 21512 | Opening  603 | Games:  54 | Opening mean: 0.5179 | Original: 0.5278 ‚Üí Adjusted: 0.5230 | Diff: -0.0048 | Confidence: 0.519\n",
      "   Player  2854 | Opening 2232 | Games:  56 | Opening mean: 0.5241 | Original: 0.4821 ‚Üí Adjusted: 0.5019 | Diff: +0.0198 | Confidence: 0.528\n",
      "   Player 35728 | Opening  376 | Games:  66 | Opening mean: 0.5178 | Original: 0.5833 ‚Üí Adjusted: 0.5551 | Diff: -0.0282 | Confidence: 0.569\n",
      "   Player 24933 | Opening  838 | Games:  74 | Opening mean: 0.5096 | Original: 0.4932 ‚Üí Adjusted: 0.4999 | Diff: +0.0066 | Confidence: 0.597\n",
      "   Player  6745 | Opening 1440 | Games:  56 | Opening mean: 0.5002 | Original: 0.4732 ‚Üí Adjusted: 0.4860 | Diff: +0.0127 | Confidence: 0.528\n",
      "   Player  9059 | Opening  737 | Games:  95 | Opening mean: 0.5084 | Original: 0.5263 ‚Üí Adjusted: 0.5201 | Diff: -0.0062 | Confidence: 0.655\n",
      "   Player 14655 | Opening 2829 | Games:  80 | Opening mean: 0.5083 | Original: 0.5188 ‚Üí Adjusted: 0.5147 | Diff: -0.0040 | Confidence: 0.615\n",
      "   Player 35612 | Opening 3242 | Games:  85 | Opening mean: 0.5526 | Original: 0.6176 ‚Üí Adjusted: 0.5935 | Diff: -0.0241 | Confidence: 0.630\n",
      "   Player 37440 | Opening 2477 | Games:  84 | Opening mean: 0.5094 | Original: 0.5238 ‚Üí Adjusted: 0.5184 | Diff: -0.0054 | Confidence: 0.627\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 21931 | Opening  765 | Games:  14 | Opening mean: 0.4628 | Original: 0.5714 ‚Üí Adjusted: 0.4866 | Diff: -0.0848 | Confidence: 0.219\n",
      "   Player 35046 | Opening 2764 | Games:  10 | Opening mean: 0.5727 | Original: 0.2500 ‚Üí Adjusted: 0.5190 | Diff: +0.2690 | Confidence: 0.167\n",
      "   Player 19157 | Opening 1385 | Games:  12 | Opening mean: 0.4852 | Original: 0.2083 ‚Üí Adjusted: 0.4316 | Diff: +0.2233 | Confidence: 0.194\n",
      "   Player 42984 | Opening 2058 | Games:  18 | Opening mean: 0.5218 | Original: 0.5278 ‚Üí Adjusted: 0.5233 | Diff: -0.0044 | Confidence: 0.265\n",
      "   Player 11294 | Opening  336 | Games:  20 | Opening mean: 0.5175 | Original: 0.4500 ‚Üí Adjusted: 0.4982 | Diff: +0.0482 | Confidence: 0.286\n",
      "   Player  6897 | Opening 1229 | Games:  18 | Opening mean: 0.4955 | Original: 0.4444 ‚Üí Adjusted: 0.4820 | Diff: +0.0375 | Confidence: 0.265\n",
      "   Player 37716 | Opening  744 | Games:  15 | Opening mean: 0.5128 | Original: 0.4333 ‚Üí Adjusted: 0.4944 | Diff: +0.0611 | Confidence: 0.231\n",
      "   Player 14294 | Opening 2396 | Games:  12 | Opening mean: 0.5212 | Original: 0.4167 ‚Üí Adjusted: 0.5010 | Diff: +0.0843 | Confidence: 0.194\n",
      "   Player 45684 | Opening 2490 | Games:  19 | Opening mean: 0.5398 | Original: 0.5789 ‚Üí Adjusted: 0.5506 | Diff: -0.0283 | Confidence: 0.275\n",
      "   Player 42044 | Opening 1014 | Games:  10 | Opening mean: 0.4949 | Original: 0.6500 ‚Üí Adjusted: 0.5208 | Diff: -0.1292 | Confidence: 0.167\n",
      "\n",
      "   ========================================================================================================================\n",
      "   Medium-game entries (50-100 games) - MODERATE shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 36174 | Opening 1190 | Games:  62 | Opening mean: 0.4727 | Original: 0.5161 ‚Üí Adjusted: 0.4967 | Diff: -0.0194 | Confidence: 0.554\n",
      "   Player 21512 | Opening  603 | Games:  54 | Opening mean: 0.5179 | Original: 0.5278 ‚Üí Adjusted: 0.5230 | Diff: -0.0048 | Confidence: 0.519\n",
      "   Player  2854 | Opening 2232 | Games:  56 | Opening mean: 0.5241 | Original: 0.4821 ‚Üí Adjusted: 0.5019 | Diff: +0.0198 | Confidence: 0.528\n",
      "   Player 35728 | Opening  376 | Games:  66 | Opening mean: 0.5178 | Original: 0.5833 ‚Üí Adjusted: 0.5551 | Diff: -0.0282 | Confidence: 0.569\n",
      "   Player 24933 | Opening  838 | Games:  74 | Opening mean: 0.5096 | Original: 0.4932 ‚Üí Adjusted: 0.4999 | Diff: +0.0066 | Confidence: 0.597\n",
      "   Player  6745 | Opening 1440 | Games:  56 | Opening mean: 0.5002 | Original: 0.4732 ‚Üí Adjusted: 0.4860 | Diff: +0.0127 | Confidence: 0.528\n",
      "   Player  9059 | Opening  737 | Games:  95 | Opening mean: 0.5084 | Original: 0.5263 ‚Üí Adjusted: 0.5201 | Diff: -0.0062 | Confidence: 0.655\n",
      "   Player 14655 | Opening 2829 | Games:  80 | Opening mean: 0.5083 | Original: 0.5188 ‚Üí Adjusted: 0.5147 | Diff: -0.0040 | Confidence: 0.615\n",
      "   Player 35612 | Opening 3242 | Games:  85 | Opening mean: 0.5526 | Original: 0.6176 ‚Üí Adjusted: 0.5935 | Diff: -0.0241 | Confidence: 0.630\n",
      "   Player 37440 | Opening 2477 | Games:  84 | Opening mean: 0.5094 | Original: 0.5238 ‚Üí Adjusted: 0.5184 | Diff: -0.0054 | Confidence: 0.627\n",
      "\n",
      "   ========================================================================================================================\n",
      "   High-game entries (200+ games) - LOW shrinkage:\n",
      "   ========================================================================================================================\n",
      "   Player 12999 | Opening  376 | Games: 228 | Opening mean: 0.5178 | Original: 0.5110 ‚Üí Adjusted: 0.5122 | Diff: +0.0012 | Confidence: 0.820\n",
      "   Player 22124 | Opening 1584 | Games: 438 | Opening mean: 0.5327 | Original: 0.5445 ‚Üí Adjusted: 0.5433 | Diff: -0.0012 | Confidence: 0.898\n",
      "   Player  6697 | Opening  383 | Games: 438 | Opening mean: 0.4931 | Original: 0.4806 ‚Üí Adjusted: 0.4819 | Diff: +0.0013 | Confidence: 0.898\n",
      "   Player 43122 | Opening 2187 | Games: 304 | Opening mean: 0.4771 | Original: 0.4638 ‚Üí Adjusted: 0.4657 | Diff: +0.0019 | Confidence: 0.859\n",
      "   Player 45054 | Opening  138 | Games: 2218 | Opening mean: 0.5055 | Original: 0.5000 ‚Üí Adjusted: 0.5001 | Diff: +0.0001 | Confidence: 0.978\n",
      "   Player 34831 | Opening  480 | Games: 316 | Opening mean: 0.5069 | Original: 0.5032 ‚Üí Adjusted: 0.5037 | Diff: +0.0005 | Confidence: 0.863\n",
      "   Player 27309 | Opening 1615 | Games: 284 | Opening mean: 0.5081 | Original: 0.5387 ‚Üí Adjusted: 0.5341 | Diff: -0.0046 | Confidence: 0.850\n",
      "   Player 42398 | Opening 2456 | Games: 255 | Opening mean: 0.5166 | Original: 0.4961 ‚Üí Adjusted: 0.4994 | Diff: +0.0034 | Confidence: 0.836\n",
      "   Player 42258 | Opening  772 | Games: 660 | Opening mean: 0.5164 | Original: 0.5114 ‚Üí Adjusted: 0.5117 | Diff: +0.0004 | Confidence: 0.930\n",
      "   Player 40578 | Opening  976 | Games: 374 | Opening mean: 0.5124 | Original: 0.4960 ‚Üí Adjusted: 0.4979 | Diff: +0.0019 | Confidence: 0.882\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Player 12999 | Opening  376 | Games: 228 | Opening mean: 0.5178 | Original: 0.5110 ‚Üí Adjusted: 0.5122 | Diff: +0.0012 | Confidence: 0.820\n",
      "   Player 22124 | Opening 1584 | Games: 438 | Opening mean: 0.5327 | Original: 0.5445 ‚Üí Adjusted: 0.5433 | Diff: -0.0012 | Confidence: 0.898\n",
      "   Player  6697 | Opening  383 | Games: 438 | Opening mean: 0.4931 | Original: 0.4806 ‚Üí Adjusted: 0.4819 | Diff: +0.0013 | Confidence: 0.898\n",
      "   Player 43122 | Opening 2187 | Games: 304 | Opening mean: 0.4771 | Original: 0.4638 ‚Üí Adjusted: 0.4657 | Diff: +0.0019 | Confidence: 0.859\n",
      "   Player 45054 | Opening  138 | Games: 2218 | Opening mean: 0.5055 | Original: 0.5000 ‚Üí Adjusted: 0.5001 | Diff: +0.0001 | Confidence: 0.978\n",
      "   Player 34831 | Opening  480 | Games: 316 | Opening mean: 0.5069 | Original: 0.5032 ‚Üí Adjusted: 0.5037 | Diff: +0.0005 | Confidence: 0.863\n",
      "   Player 27309 | Opening 1615 | Games: 284 | Opening mean: 0.5081 | Original: 0.5387 ‚Üí Adjusted: 0.5341 | Diff: -0.0046 | Confidence: 0.850\n",
      "   Player 42398 | Opening 2456 | Games: 255 | Opening mean: 0.5166 | Original: 0.4961 ‚Üí Adjusted: 0.4994 | Diff: +0.0034 | Confidence: 0.836\n",
      "   Player 42258 | Opening  772 | Games: 660 | Opening mean: 0.5164 | Original: 0.5114 ‚Üí Adjusted: 0.5117 | Diff: +0.0004 | Confidence: 0.930\n",
      "   Player 40578 | Opening  976 | Games: 374 | Opening mean: 0.5124 | Original: 0.4960 ‚Üí Adjusted: 0.4979 | Diff: +0.0019 | Confidence: 0.882\n",
      "\n",
      "7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\n",
      "\n",
      "   Openings with HIGHEST win rates (strong for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4889 vs global) | 1 player entries\n",
      "   Opening 3551 (C19): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2881 (E10): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2046 (C49): mean = 0.7857 (+0.2747 vs global) | 1 player entries\n",
      "   Opening 1659 (C30): mean = 0.7692 (+0.2582 vs global) | 1 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 1811 (C39): mean = 1.0000 (+0.4889 vs global) | 1 player entries\n",
      "   Opening 3551 (C19): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2881 (E10): mean = 0.8000 (+0.2889 vs global) | 1 player entries\n",
      "   Opening 2046 (C49): mean = 0.7857 (+0.2747 vs global) | 1 player entries\n",
      "   Opening 1659 (C30): mean = 0.7692 (+0.2582 vs global) | 1 player entries\n",
      "\n",
      "   Openings with LOWEST win rates (weak for White):\n",
      "   Opening 2593 (D26): mean = 0.1667 (-0.3444 vs global) | 1 player entries\n",
      "   Opening  636 (A83): mean = 0.2006 (-0.3104 vs global) | 2 player entries\n",
      "   Opening 3496 (A40): mean = 0.2273 (-0.2838 vs global) | 1 player entries\n",
      "   Opening 1763 (C37): mean = 0.2417 (-0.2694 vs global) | 2 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "   Opening 2593 (D26): mean = 0.1667 (-0.3444 vs global) | 1 player entries\n",
      "   Opening  636 (A83): mean = 0.2006 (-0.3104 vs global) | 2 player entries\n",
      "   Opening 3496 (A40): mean = 0.2273 (-0.2838 vs global) | 1 player entries\n",
      "   Opening 1763 (C37): mean = 0.2417 (-0.2694 vs global) | 2 player entries\n",
      "\n",
      "8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 39505 | Opening 3292 (C54) | Games: 10 | Opening mean: 0.7314 | Original: 0.7000 ‚Üí 0.7261\n",
      "      If we'd shrunk to global mean: 0.5425 (would lose +0.1836 of deserved credit)\n",
      "   Player 23562 | Opening 3292 (C54) | Games: 12 | Opening mean: 0.7314 | Original: 0.7500 ‚Üí 0.7350\n",
      "      If we'd shrunk to global mean: 0.5573 (would lose +0.1777 of deserved credit)\n",
      "   Player 23721 | Opening 3292 (C54) | Games: 11 | Opening mean: 0.7314 | Original: 0.6364 ‚Üí 0.7142\n",
      "      If we'd shrunk to global mean: 0.5337 (would lose +0.1806 of deserved credit)\n",
      "\n",
      "   Strong opening + good player performance (shrunk toward HIGH opening mean):\n",
      "   Player 39505 | Opening 3292 (C54) | Games: 10 | Opening mean: 0.7314 | Original: 0.7000 ‚Üí 0.7261\n",
      "      If we'd shrunk to global mean: 0.5425 (would lose +0.1836 of deserved credit)\n",
      "   Player 23562 | Opening 3292 (C54) | Games: 12 | Opening mean: 0.7314 | Original: 0.7500 ‚Üí 0.7350\n",
      "      If we'd shrunk to global mean: 0.5573 (would lose +0.1777 of deserved credit)\n",
      "   Player 23721 | Opening 3292 (C54) | Games: 11 | Opening mean: 0.7314 | Original: 0.6364 ‚Üí 0.7142\n",
      "      If we'd shrunk to global mean: 0.5337 (would lose +0.1806 of deserved credit)\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player  1857 | Opening 2293 (C71) | Games: 13 | Opening mean: 0.2665 | Original: 0.1538 ‚Üí 0.2433\n",
      "      If we'd shrunk to global mean: 0.4373 (would unfairly boost by +0.1941)\n",
      "   Player 40878 | Opening  443 (A43) | Games: 11 | Opening mean: 0.3182 | Original: 0.3182 ‚Üí 0.3182\n",
      "      If we'd shrunk to global mean: 0.4763 (would unfairly boost by +0.1581)\n",
      "   Player  8251 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3370 | Original: 0.3000 ‚Üí 0.3309\n",
      "      If we'd shrunk to global mean: 0.4759 (would unfairly boost by +0.1450)\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (2897700, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n",
      "\n",
      "   Weak opening + poor player performance (shrunk toward LOW opening mean):\n",
      "   Player  1857 | Opening 2293 (C71) | Games: 13 | Opening mean: 0.2665 | Original: 0.1538 ‚Üí 0.2433\n",
      "      If we'd shrunk to global mean: 0.4373 (would unfairly boost by +0.1941)\n",
      "   Player 40878 | Opening  443 (A43) | Games: 11 | Opening mean: 0.3182 | Original: 0.3182 ‚Üí 0.3182\n",
      "      If we'd shrunk to global mean: 0.4763 (would unfairly boost by +0.1581)\n",
      "   Player  8251 | Opening 1779 (C37) | Games: 10 | Opening mean: 0.3370 | Original: 0.3000 ‚Üí 0.3309\n",
      "      If we'd shrunk to global mean: 0.4759 (would unfairly boost by +0.1450)\n",
      "\n",
      "============================================================\n",
      "‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final data shape: (2897700, 6)\n",
      "Columns: ['player_id', 'opening_id', 'num_games', 'score', 'eco', 'confidence']\n",
      "\n",
      "New columns added:\n",
      "   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\n",
      "   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\n",
      "\n",
      "Key improvement over simple shrinkage:\n",
      "   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\n",
      "   ‚Ä¢ Preserves opening difficulty differences\n",
      "   ‚Ä¢ More accurate for both strong and weak openings\n"
     ]
    }
   ],
   "source": [
    "# 2b. Apply hierarchical Bayesian shrinkage to adjust scores based on sample size confidence\n",
    "\n",
    "# Check if confidence already exists - if so, skip this processing\n",
    "if 'confidence' in clean_data.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n‚úì 'confidence' column already exists in data\")\n",
    "else:\n",
    "    # Define the processing function\n",
    "    # This is a long function, I recommend you fold it down in your editor\n",
    "    def apply_hierarchical_bayesian_shrinkage(data, k_player=50):\n",
    "        \"\"\"\n",
    "        Apply two-level hierarchical Bayesian shrinkage to adjust scores.\n",
    "        \n",
    "        A lot of our player-opening entries have a small number of games played, because openings are so specific.\n",
    "        This introduces sample size issues.\n",
    "        \n",
    "        We use TWO-LEVEL shrinkage:\n",
    "        Level 1: Calculate opening-specific means (these are our \"ground truth\" for each opening)\n",
    "        Level 2: Shrink individual player-opening scores toward their opening's mean\n",
    "        This is better than shrinking toward global mean because different openings have different baseline win rates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Clean data with columns: player_id, opening_id, score, num_games, eco\n",
    "        k_player : int\n",
    "            Shrinkage constant for player-opening scores (default: 50)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Data with adjusted scores and new 'confidence' column\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2B: HIERARCHICAL BAYESIAN SCORE ADJUSTMENT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Calculate global mean score for comparison\n",
    "        global_mean_score = data[\"score\"].mean()\n",
    "        print(f\"\\nüìä Global statistics:\")\n",
    "        print(f\"   ‚Ä¢ Global mean score: {global_mean_score:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Total entries: {len(data):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique openings: {data['opening_id'].nunique():,}\")\n",
    "        \n",
    "        # Store original scores for comparison\n",
    "        data = data.copy()\n",
    "        data[\"score_original\"] = data[\"score\"].copy()\n",
    "        \n",
    "        # LEVEL 1: Calculate opening-specific means and statistics        \n",
    "        opening_stats = (\n",
    "            data.groupby(\"opening_id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"score\": \"mean\",\n",
    "                    \"num_games\": \"sum\",\n",
    "                    \"player_id\": \"count\",  # Number of players who played this opening\n",
    "                }\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\n",
    "                    \"score\": \"opening_mean\",\n",
    "                    \"num_games\": \"opening_total_games\",\n",
    "                    \"player_id\": \"opening_num_players\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        # Opening mean statistics\n",
    "        print(f\"\\n   Opening mean score distribution:\")\n",
    "        print(f\"   ‚Ä¢ Min: {opening_stats['opening_mean'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {opening_stats['opening_mean'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {opening_stats['opening_mean'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {opening_stats['opening_mean'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {opening_stats['opening_mean'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {opening_stats['opening_mean'].std():.4f}\")\n",
    "        \n",
    "        # Show distribution of opening sizes\n",
    "        print(f\"\\n   Opening sample size distribution:\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games per opening (median): {opening_stats['opening_total_games'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players per opening (median): {opening_stats['opening_num_players'].median():.0f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Total games range: [{opening_stats['opening_total_games'].min():.0f}, {opening_stats['opening_total_games'].max():.0f}]\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Players range: [{opening_stats['opening_num_players'].min():.0f}, {opening_stats['opening_num_players'].max():.0f}]\"\n",
    "        )\n",
    "        \n",
    "        # Merge opening means back into main dataframe\n",
    "        data = data.merge(\n",
    "            opening_stats[[\"opening_mean\"]], left_on=\"opening_id\", right_index=True, how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # LEVEL 2: Shrink player-opening scores toward opening-specific means\n",
    "        print(\n",
    "            f\"   Formula: adjusted_score = (num_games √ó player_score + {k_player} √ó opening_mean) / (num_games + {k_player})\"\n",
    "        )\n",
    "        \n",
    "        numerator = (data[\"num_games\"] * data[\"score_original\"]) + (\n",
    "            k_player * data[\"opening_mean\"]\n",
    "        )\n",
    "        denominator = data[\"num_games\"] + k_player\n",
    "        data[\"score\"] = numerator / denominator\n",
    "        \n",
    "        print(f\"   ‚úì Scores adjusted for {len(data):,} entries\")\n",
    "        \n",
    "        # Calculate confidence weights (will be used in loss function later)\n",
    "        data[\"confidence\"] = data[\"num_games\"] / (\n",
    "            data[\"num_games\"] + k_player\n",
    "        )\n",
    "        print(f\"   ‚úì Confidence weights calculated\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Range: [{data['confidence'].min():.4f}, {data['confidence'].max():.4f}]\"\n",
    "        )\n",
    "        \n",
    "        # Statistics on the adjustment\n",
    "        score_diff = data[\"score\"] - data[\"score_original\"]\n",
    "        print(f\"\\n4Ô∏è‚É£  Adjustment statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean adjustment: {score_diff.mean():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Std adjustment: {score_diff.std():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Max adjustment: {score_diff.max():.6f}\")\n",
    "        print(f\"   ‚Ä¢ Min adjustment: {score_diff.min():.6f}\")\n",
    "        \n",
    "        # Show distribution of adjustments\n",
    "        print(f\"\\n   Adjustment by num_games quartiles:\")\n",
    "        quartiles = data[\"num_games\"].quantile([0.25, 0.5, 0.75])\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 25th percentile (n={quartiles[0.25]:.0f} games): avg adjustment = {score_diff[data['num_games'] <= quartiles[0.25]].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 50th percentile (n={quartiles[0.5]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.25]) & (data['num_games'] <= quartiles[0.5])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ 75th percentile (n={quartiles[0.75]:.0f} games): avg adjustment = {score_diff[(data['num_games'] > quartiles[0.5]) & (data['num_games'] <= quartiles[0.75])].mean():.6f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   ‚Ä¢ >75th percentile (n>{quartiles[0.75]:.0f} games): avg adjustment = {score_diff[data['num_games'] > quartiles[0.75]].mean():.6f}\"\n",
    "        )\n",
    "        \n",
    "        # New score distribution after adjustment\n",
    "        print(f\"\\n5Ô∏è‚É£  Adjusted score statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {data['score'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 25th percentile: {data['score'].quantile(0.25):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Median: {data['score'].median():.4f}\")\n",
    "        print(f\"   ‚Ä¢ 75th percentile: {data['score'].quantile(0.75):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {data['score'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {data['score'].mean():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {data['score'].std():.4f}\")\n",
    "        \n",
    "        # Detailed sample showing the effect across different game counts\n",
    "        print(f\"\\n6Ô∏è‚É£  Sample comparisons (showing effect of hierarchical shrinkage):\")\n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Low-game entries (10-20 games) - HIGH shrinkage toward opening mean:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        low_game_sample = data[\n",
    "            (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 10) & (data[\"num_games\"] <= 20)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in low_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   Medium-game entries (50-100 games) - MODERATE shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        med_game_sample = data[\n",
    "            (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "        ].sample(\n",
    "            min(\n",
    "                10,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"num_games\"] >= 50) & (data[\"num_games\"] <= 100)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        for idx, row in med_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   {'='*120}\")\n",
    "        print(f\"   High-game entries (200+ games) - LOW shrinkage:\")\n",
    "        print(f\"   {'='*120}\")\n",
    "        \n",
    "        high_game_sample = data[data[\"num_games\"] >= 200].sample(\n",
    "            min(10, len(data[data[\"num_games\"] >= 200])), random_state=42\n",
    "        )\n",
    "        for idx, row in high_game_sample.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} | Games: {row['num_games']:>3} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí Adjusted: {row['score']:.4f} | \"\n",
    "                f\"Diff: {adjustment:>+.4f} | Confidence: {row['confidence']:.3f}\"\n",
    "            )\n",
    "        \n",
    "        # Show extreme cases - comparing to both opening mean AND global mean\n",
    "        print(f\"\\n7Ô∏è‚É£  Extreme cases (showing why opening-specific shrinkage matters):\")\n",
    "        \n",
    "        # Find entries where opening mean differs significantly from global mean\n",
    "        data[\"opening_deviation_from_global\"] = (\n",
    "            data[\"opening_mean\"] - global_mean_score\n",
    "        ).abs()\n",
    "        \n",
    "        print(f\"\\n   Openings with HIGHEST win rates (strong for White):\")\n",
    "        strong_openings = data.nlargest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in strong_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"(+{deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n   Openings with LOWEST win rates (weak for White):\")\n",
    "        weak_openings = data.nsmallest(5, \"opening_mean\")[\n",
    "            [\"opening_id\", \"opening_mean\", \"eco\"]\n",
    "        ].drop_duplicates(\"opening_id\")\n",
    "        for idx, row in weak_openings.iterrows():\n",
    "            num_entries = len(data[data[\"opening_id\"] == row[\"opening_id\"]])\n",
    "            deviation = row[\"opening_mean\"] - global_mean_score\n",
    "            print(\n",
    "                f\"   Opening {row['opening_id']:>4} ({row['eco']:>3}): mean = {row['opening_mean']:.4f} \"\n",
    "                f\"({deviation:.4f} vs global) | {num_entries} player entries\"\n",
    "            )\n",
    "        \n",
    "        # Show specific examples where hierarchical shrinkage made a difference\n",
    "        print(f\"\\n8Ô∏è‚É£  Examples showing hierarchical shrinkage benefit:\")\n",
    "        \n",
    "        # Find entries with strong openings where player did well\n",
    "        strong_opening_ids = data.nlargest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        strong_examples = data[\n",
    "            (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] > 0.6)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(strong_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] > 0.6)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"\\n   Strong opening + good player performance (shrunk toward HIGH opening mean):\"\n",
    "        )\n",
    "        for idx, row in strong_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would lose {difference:+.4f} of deserved credit)\"\n",
    "            )\n",
    "        \n",
    "        # Find entries with weak openings where player did poorly\n",
    "        weak_opening_ids = data.nsmallest(50, \"opening_mean\")[\"opening_id\"].unique()\n",
    "        weak_examples = data[\n",
    "            (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "            & (data[\"num_games\"] <= 20)\n",
    "            & (data[\"score_original\"] < 0.45)\n",
    "        ].sample(\n",
    "            min(\n",
    "                3,\n",
    "                len(\n",
    "                    data[\n",
    "                        (data[\"opening_id\"].isin(weak_opening_ids))\n",
    "                        & (data[\"num_games\"] <= 20)\n",
    "                        & (data[\"score_original\"] < 0.45)\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Weak opening + poor player performance (shrunk toward LOW opening mean):\")\n",
    "        for idx, row in weak_examples.iterrows():\n",
    "            adjustment = row[\"score\"] - row[\"score_original\"]\n",
    "            global_shrink_would_be = (\n",
    "                (row[\"num_games\"] * row[\"score_original\"]) + (k_player * global_mean_score)\n",
    "            ) / (row[\"num_games\"] + k_player)\n",
    "            difference = row[\"score\"] - global_shrink_would_be\n",
    "            print(\n",
    "                f\"   Player {row['player_id']:>5} | Opening {row['opening_id']:>4} ({row['eco']:>3}) | Games: {row['num_games']:>2} | \"\n",
    "                f\"Opening mean: {row['opening_mean']:.4f} | Original: {row['score_original']:.4f} ‚Üí {row['score']:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"      If we'd shrunk to global mean: {global_shrink_would_be:.4f} (would unfairly boost by {-difference:+.4f})\"\n",
    "            )\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        data = data.drop(\n",
    "            columns=[\"score_original\", \"opening_mean\", \"opening_deviation_from_global\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ HIERARCHICAL BAYESIAN ADJUSTMENT COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nFinal data shape: {data.shape}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        print(f\"\\nNew columns added:\")\n",
    "        print(f\"   ‚Ä¢ 'confidence': weight for loss function (range [0,1])\")\n",
    "        print(f\"   ‚Ä¢ 'score': adjusted using hierarchical Bayesian shrinkage\")\n",
    "        print(f\"\\nKey improvement over simple shrinkage:\")\n",
    "        print(f\"   ‚Ä¢ Player scores now shrink toward OPENING-SPECIFIC means, not global mean\")\n",
    "        print(f\"   ‚Ä¢ Preserves opening difficulty differences\")\n",
    "        print(f\"   ‚Ä¢ More accurate for both strong and weak openings\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    K_PLAYER = 50  # Shrinkage constant for player-opening scores\n",
    "    \n",
    "    clean_data = apply_hierarchical_bayesian_shrinkage(clean_data, k_player=K_PLAYER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2C: PLAYER RATING STATISTICS\n",
      "============================================================\n",
      "   ‚úì Retrieved ratings for 48,468 players\n",
      "   ‚úì All players have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 48,468\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1765.38\n",
      "   ‚Ä¢ Median: 1762\n",
      "   ‚Ä¢ Std Dev: 249.11\n",
      "\n",
      "5Ô∏è‚É£  Ratings Percentiles\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1359    ‚ñà‚ñà‚ñà\n",
      "      10%          1435    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1494    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1541    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1585    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1623    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1660    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1697    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1729    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1762    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1798    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1829    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1864    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1901    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1936    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1980    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2027    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2089    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2184    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400       3,583      7.39%      ‚ñà‚ñà\n",
      "   1400-1600       9,392     19.38%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800      13,785     28.44%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000      12,894     26.60%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       6,609     13.64%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400       1,860      3.84%      ‚ñà\n",
      "   2400-2600         322      0.66%      \n",
      "   2600-3000          23      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 351\n",
      "   ‚Ä¢ 10th-90th percentile range: 654\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1435):\n",
      "      Player 30609: hoto18 - Rating: 1435\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1585):\n",
      "      Player 12651: MM2085 - Rating: 1585\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1762):\n",
      "      Player 19883: Svetlana-55 - Rating: 1762\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1936):\n",
      "      Player 34182: machacan - Rating: 1936\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2089):\n",
      "      Player 5038: Danni_Martin - Rating: 2089\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "   ‚Ä¢ Total players: 48,468\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1765 ¬± 249\n",
      "   ‚Ä¢ Median: 1762\n",
      "   ‚úì Retrieved ratings for 48,468 players\n",
      "   ‚úì All players have ratings\n",
      "\n",
      "3Ô∏è‚É£  Basic rating statistics:\n",
      "   ‚Ä¢ Count: 48,468\n",
      "   ‚Ä¢ Missing: 0\n",
      "   ‚Ä¢ Min: 1200\n",
      "   ‚Ä¢ Max: 2823\n",
      "   ‚Ä¢ Mean: 1765.38\n",
      "   ‚Ä¢ Median: 1762\n",
      "   ‚Ä¢ Std Dev: 249.11\n",
      "\n",
      "5Ô∏è‚É£  Ratings Percentiles\n",
      "\n",
      "   Percentile   Rating     Visual\n",
      "   ------------ ---------- ----------------------------------------\n",
      "       0%          1200    \n",
      "       5%          1359    ‚ñà‚ñà‚ñà\n",
      "      10%          1435    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      15%          1494    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      20%          1541    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      25%          1585    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      30%          1623    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      35%          1660    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      40%          1697    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      45%          1729    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      50%          1762    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      55%          1798    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      60%          1829    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      65%          1864    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      70%          1901    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      75%          1936    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      80%          1980    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      85%          2027    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      90%          2089    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "      95%          2184    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "     100%          2823    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "6Ô∏è‚É£  Rating distribution by range:\n",
      "\n",
      "   Range           Count      Percentage   Visual\n",
      "   --------------- ---------- ------------ ----------------------------------------\n",
      "      0-1000           0      0.00%      \n",
      "   1000-1200           0      0.00%      \n",
      "   1200-1400       3,583      7.39%      ‚ñà‚ñà\n",
      "   1400-1600       9,392     19.38%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1600-1800      13,785     28.44%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1800-2000      12,894     26.60%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2000-2200       6,609     13.64%      ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2200-2400       1,860      3.84%      ‚ñà\n",
      "   2400-2600         322      0.66%      \n",
      "   2600-3000          23      0.05%      \n",
      "\n",
      "7Ô∏è‚É£  Spread statistics:\n",
      "   ‚Ä¢ Range: 1623\n",
      "   ‚Ä¢ Interquartile Range (IQR): 351\n",
      "   ‚Ä¢ 10th-90th percentile range: 654\n",
      "\n",
      "9Ô∏è‚É£  Sample players at different rating levels:\n",
      "\n",
      "   ~10th percentile (rating ‚âà 1435):\n",
      "      Player 30609: hoto18 - Rating: 1435\n",
      "\n",
      "   ~25th percentile (rating ‚âà 1585):\n",
      "      Player 12651: MM2085 - Rating: 1585\n",
      "\n",
      "   ~50th percentile (rating ‚âà 1762):\n",
      "      Player 19883: Svetlana-55 - Rating: 1762\n",
      "\n",
      "   ~75th percentile (rating ‚âà 1936):\n",
      "      Player 34182: machacan - Rating: 1936\n",
      "\n",
      "   ~90th percentile (rating ‚âà 2089):\n",
      "      Player 5038: Danni_Martin - Rating: 2089\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING STATISTICS COMPLETE\n",
      "   ‚Ä¢ Total players: 48,468\n",
      "   ‚Ä¢ Rating range: [1200, 2823]\n",
      "   ‚Ä¢ Mean ¬± std: 1765 ¬± 249\n",
      "   ‚Ä¢ Median: 1762\n"
     ]
    }
   ],
   "source": [
    "# 2c. Gather player rating statistics\n",
    "\n",
    "print(\"STEP 2C: PLAYER RATING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Connect to database and extract player ratings\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "\n",
    "try:    \n",
    "    unique_player_ids = clean_data['player_id'].unique()\n",
    "    player_ids_str = ','.join(map(str, unique_player_ids))\n",
    "    \n",
    "    # Query to get player ratings\n",
    "    rating_query = f\"\"\"\n",
    "        SELECT \n",
    "            id as player_id,\n",
    "            name,\n",
    "            title,\n",
    "            rating\n",
    "        FROM player\n",
    "        WHERE id IN ({player_ids_str})\n",
    "    \"\"\"\n",
    "    \n",
    "    player_ratings = pd.DataFrame(con.execute(rating_query).df())\n",
    "    print(f\"   ‚úì Retrieved ratings for {len(player_ratings):,} players\")\n",
    "    \n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Check for missing ratings\n",
    "missing_ratings = player_ratings['rating'].isna().sum()\n",
    "if missing_ratings > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: {missing_ratings:,} players have missing ratings\")\n",
    "else:\n",
    "    print(f\"   ‚úì All players have ratings\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Basic rating statistics:\")\n",
    "print(f\"   ‚Ä¢ Count: {player_ratings['rating'].notna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Missing: {player_ratings['rating'].isna().sum():,}\")\n",
    "print(f\"   ‚Ä¢ Min: {player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Max: {player_ratings['rating'].max():.0f}\")\n",
    "print(f\"   ‚Ä¢ Mean: {player_ratings['rating'].mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")\n",
    "print(f\"   ‚Ä¢ Std Dev: {player_ratings['rating'].std():.2f}\")\n",
    "\n",
    "\n",
    "print(f\"\\n5Ô∏è‚É£  Ratings Percentiles\")\n",
    "percentiles = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,\n",
    "               0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00]\n",
    "\n",
    "print(f\"\\n   {'Percentile':<12} {'Rating':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*12} {'-'*10} {'-'*40}\")\n",
    "\n",
    "for p in percentiles:\n",
    "    rating_value = player_ratings['rating'].quantile(p)\n",
    "    # Create a simple bar visualization\n",
    "    bar_length = int((rating_value - player_ratings['rating'].min()) / \n",
    "                     (player_ratings['rating'].max() - player_ratings['rating'].min()) * 40)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {p*100:>5.0f}%       {rating_value:>7.0f}    {bar}\")\n",
    "\n",
    "# Rating ranges and counts\n",
    "print(f\"\\n6Ô∏è‚É£  Rating distribution by range:\")\n",
    "rating_ranges = [\n",
    "    (0, 1000), (1000, 1200), (1200, 1400), (1400, 1600), \n",
    "    (1600, 1800), (1800, 2000), (2000, 2200), (2200, 2400), \n",
    "    (2400, 2600), (2600, 3000)\n",
    "]\n",
    "\n",
    "print(f\"\\n   {'Range':<15} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*15} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "\n",
    "for low, high in rating_ranges:\n",
    "    count = len(player_ratings[(player_ratings['rating'] >= low) & (player_ratings['rating'] < high)])\n",
    "    pct = 100 * count / len(player_ratings)\n",
    "    bar_length = int(pct * 0.4)  # Scale for visualization\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {low:>4}-{high:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Interquartile range\n",
    "iqr = player_ratings['rating'].quantile(0.75) - player_ratings['rating'].quantile(0.25)\n",
    "print(f\"\\n7Ô∏è‚É£  Spread statistics:\")\n",
    "print(f\"   ‚Ä¢ Range: {player_ratings['rating'].max() - player_ratings['rating'].min():.0f}\")\n",
    "print(f\"   ‚Ä¢ Interquartile Range (IQR): {iqr:.0f}\")\n",
    "print(f\"   ‚Ä¢ 10th-90th percentile range: {player_ratings['rating'].quantile(0.90) - player_ratings['rating'].quantile(0.10):.0f}\")\n",
    "\n",
    "# Sample of players at different rating levels\n",
    "print(f\"\\n9Ô∏è‚É£  Sample players at different rating levels:\")\n",
    "sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "for p in sample_percentiles:\n",
    "    rating_threshold = player_ratings['rating'].quantile(p)\n",
    "    # Get a player near this rating\n",
    "    sample_player = player_ratings.iloc[(player_ratings['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "    print(f\"\\n   ~{p*100:.0f}th percentile (rating ‚âà {rating_threshold:.0f}):\")\n",
    "    for idx, row in sample_player.iterrows():\n",
    "        # print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f} {f'({row['title']})' if pd.notna(row['title']) else ''}\")\n",
    "        title_str = f\" ({row['title']})\" if pd.notna(row['title']) else \"\"\n",
    "        print(f\"      Player {row['player_id']}: {row['name']} - Rating: {row['rating']:.0f}{title_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ RATING STATISTICS COMPLETE\")\n",
    "print(f\"   ‚Ä¢ Total players: {len(player_ratings):,}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings['rating'].min():.0f}, {player_ratings['rating'].max():.0f}]\")\n",
    "print(f\"   ‚Ä¢ Mean ¬± std: {player_ratings['rating'].mean():.0f} ¬± {player_ratings['rating'].std():.0f}\")\n",
    "print(f\"   ‚Ä¢ Median: {player_ratings['rating'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ab6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Normalization parameters (calculated from 48,468 players):\n",
      "   ‚Ä¢ Mean: 1765.38\n",
      "   ‚Ä¢ Std Dev: 249.11\n",
      "\n",
      "2Ô∏è‚É£  Normalized rating statistics:\n",
      "   ‚Ä¢ Min: -2.2696\n",
      "   ‚Ä¢ Max: 4.2457\n",
      "   ‚Ä¢ Mean: -0.000000 (should be ~0)\n",
      "   ‚Ä¢ Std: 1.000000 (should be ~1)\n",
      "   ‚Ä¢ Range: [-2.27, 4.25]\n",
      "\n",
      "3Ô∏è‚É£  Sample normalized ratings across skill levels:\n",
      "   ~10th percentile: Player 29697 | Rating: 1435 ‚Üí Z-score: -1.326\n",
      "   ~25th percentile: Player 12249 | Rating: 1585 ‚Üí Z-score: -0.724\n",
      "   ~50th percentile: Player 19269 | Rating: 1762 ‚Üí Z-score: -0.014\n",
      "   ~75th percentile: Player 33154 | Rating: 1936 ‚Üí Z-score:  0.685\n",
      "   ~90th percentile: Player 4869 | Rating: 2089 ‚Üí Z-score:  1.299\n",
      "\n",
      "4Ô∏è‚É£  Interpretation guide:\n",
      "   ‚Ä¢ rating_z ‚âà -2.3: 1200 player (minimum)\n",
      "   ‚Ä¢ rating_z ‚âà -0.7: 1585 player (25th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà  0.0: 1765 player (mean)\n",
      "   ‚Ä¢ rating_z ‚âà 0.7: 1936 player (75th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà 4.2: 2823 player (maximum)\n",
      "\n",
      "5Ô∏è‚É£  Player side information table structure:\n",
      "   ‚Ä¢ Shape: (48468, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'rating', 'rating_z']\n",
      "\n",
      "6Ô∏è‚É£  Sample entries from side information table:\n",
      "   Player 14786 | Rating: 1811 ‚Üí Z-score:  0.183\n",
      "   Player 19132 | Rating: 1992 ‚Üí Z-score:  0.910\n",
      "   Player 45755 | Rating: 1814 ‚Üí Z-score:  0.195\n",
      "   Player  5245 | Rating: 1730 ‚Üí Z-score: -0.142\n",
      "   Player 32893 | Rating: 1635 ‚Üí Z-score: -0.523\n",
      "   Player 13298 | Rating: 1933 ‚Üí Z-score:  0.673\n",
      "   Player 18568 | Rating: 2401 ‚Üí Z-score:  2.552\n",
      "   Player 10809 | Rating: 2203 ‚Üí Z-score:  1.757\n",
      "   Player 48616 | Rating: 1561 ‚Üí Z-score: -0.820\n",
      "   Player 40115 | Rating: 2017 ‚Üí Z-score:  1.010\n",
      "   ‚úì All 48,468 players in clean_data have side information\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING NORMALIZATION COMPLETE\n",
      "\n",
      "Created: player_side_info\n",
      "   ‚Ä¢ Shape: (48468, 1)\n",
      "   ‚Ä¢ Index: player_id\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ clean_data: 2,897,700 rows (player-opening interactions)\n",
      "   ‚Ä¢ player_side_info: 48,468 rows (one per player)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\n",
      "   RATING_MEAN = 1765.38\n",
      "   RATING_STD = 249.11\n",
      "\n",
      "   You'll need them to normalize ratings for new users at inference time.\n",
      "   ~75th percentile: Player 33154 | Rating: 1936 ‚Üí Z-score:  0.685\n",
      "   ~90th percentile: Player 4869 | Rating: 2089 ‚Üí Z-score:  1.299\n",
      "\n",
      "4Ô∏è‚É£  Interpretation guide:\n",
      "   ‚Ä¢ rating_z ‚âà -2.3: 1200 player (minimum)\n",
      "   ‚Ä¢ rating_z ‚âà -0.7: 1585 player (25th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà  0.0: 1765 player (mean)\n",
      "   ‚Ä¢ rating_z ‚âà 0.7: 1936 player (75th percentile)\n",
      "   ‚Ä¢ rating_z ‚âà 4.2: 2823 player (maximum)\n",
      "\n",
      "5Ô∏è‚É£  Player side information table structure:\n",
      "   ‚Ä¢ Shape: (48468, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'rating', 'rating_z']\n",
      "\n",
      "6Ô∏è‚É£  Sample entries from side information table:\n",
      "   Player 14786 | Rating: 1811 ‚Üí Z-score:  0.183\n",
      "   Player 19132 | Rating: 1992 ‚Üí Z-score:  0.910\n",
      "   Player 45755 | Rating: 1814 ‚Üí Z-score:  0.195\n",
      "   Player  5245 | Rating: 1730 ‚Üí Z-score: -0.142\n",
      "   Player 32893 | Rating: 1635 ‚Üí Z-score: -0.523\n",
      "   Player 13298 | Rating: 1933 ‚Üí Z-score:  0.673\n",
      "   Player 18568 | Rating: 2401 ‚Üí Z-score:  2.552\n",
      "   Player 10809 | Rating: 2203 ‚Üí Z-score:  1.757\n",
      "   Player 48616 | Rating: 1561 ‚Üí Z-score: -0.820\n",
      "   Player 40115 | Rating: 2017 ‚Üí Z-score:  1.010\n",
      "   ‚úì All 48,468 players in clean_data have side information\n",
      "\n",
      "============================================================\n",
      "‚úÖ RATING NORMALIZATION COMPLETE\n",
      "\n",
      "Created: player_side_info\n",
      "   ‚Ä¢ Shape: (48468, 1)\n",
      "   ‚Ä¢ Index: player_id\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ clean_data: 2,897,700 rows (player-opening interactions)\n",
      "   ‚Ä¢ player_side_info: 48,468 rows (one per player)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\n",
      "   RATING_MEAN = 1765.38\n",
      "   RATING_STD = 249.11\n",
      "\n",
      "   You'll need them to normalize ratings for new users at inference time.\n"
     ]
    }
   ],
   "source": [
    "# 2d. Normalize player ratings using z-score normalization (for use as side information in MF model)\n",
    "\n",
    "# Check if we've already normalized ratings\n",
    "if 'player_side_info' in globals() and 'rating_z' in player_side_info.columns:\n",
    "    print(\"   SKIPPING STEP 2D: RATING NORMALIZATION\")\n",
    "    print(\"Ratings have already been normalized\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing normalized rating statistics:\")\n",
    "    print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "    print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "\n",
    "else:\n",
    "    def normalize_player_ratings(player_ratings_df):\n",
    "        \"\"\"\n",
    "        Apply z-score normalization to player ratings for use as side information.\n",
    "        \n",
    "        This creates a SEPARATE table of player-level features, NOT merged into clean_data.\n",
    "        Rating is side information - it describes the player, not the player-opening interaction.\n",
    "        \n",
    "        During training, the model will LOOK UP each player's rating_z from this table.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ratings_df : pd.DataFrame\n",
    "            Player ratings with columns: player_id, name, title, rating\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (player_side_info DataFrame, RATING_MEAN, RATING_STD)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 2D: NORMALIZE PLAYER RATINGS (SIDE INFORMATION)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calculate normalization parameters\n",
    "        RATING_MEAN = player_ratings_df['rating'].mean()\n",
    "        RATING_STD = player_ratings_df['rating'].std()\n",
    "        \n",
    "        print(f\"\\n1Ô∏è‚É£  Normalization parameters (calculated from {len(player_ratings_df):,} players):\")\n",
    "        print(f\"   ‚Ä¢ Mean: {RATING_MEAN:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Std Dev: {RATING_STD:.2f}\")\n",
    "        \n",
    "        # Create side information table - only keep player_id and rating for now\n",
    "        player_side_info = player_ratings_df[['player_id', 'rating']].copy()\n",
    "        player_side_info['rating_z'] = (player_side_info['rating'] - RATING_MEAN) / RATING_STD\n",
    "        \n",
    "        print(f\"\\n2Ô∏è‚É£  Normalized rating statistics:\")\n",
    "        print(f\"   ‚Ä¢ Min: {player_side_info['rating_z'].min():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {player_side_info['rating_z'].max():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean: {player_side_info['rating_z'].mean():.6f} (should be ~0)\")\n",
    "        print(f\"   ‚Ä¢ Std: {player_side_info['rating_z'].std():.6f} (should be ~1)\")\n",
    "        print(f\"   ‚Ä¢ Range: [{player_side_info['rating_z'].min():.2f}, {player_side_info['rating_z'].max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\n3Ô∏è‚É£  Sample normalized ratings across skill levels:\")\n",
    "        sample_percentiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "        for p in sample_percentiles:\n",
    "            rating_threshold = player_side_info['rating'].quantile(p)\n",
    "            sample_player = player_side_info.iloc[(player_side_info['rating'] - rating_threshold).abs().argsort()[:1]]\n",
    "            for idx, row in sample_player.iterrows():\n",
    "                print(f\"   ~{p*100:.0f}th percentile: Player {idx} | \"\n",
    "                      f\"Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        print(f\"\\n4Ô∏è‚É£  Interpretation guide:\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(1200 - RATING_MEAN)/RATING_STD:.1f}: 1200 player (minimum)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.25) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.25):.0f} player (25th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà  0.0: {RATING_MEAN:.0f} player (mean)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].quantile(0.75) - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].quantile(0.75):.0f} player (75th percentile)\")\n",
    "        print(f\"   ‚Ä¢ rating_z ‚âà {(player_side_info['rating'].max() - RATING_MEAN)/RATING_STD:.1f}: {player_side_info['rating'].max():.0f} player (maximum)\")\n",
    "        \n",
    "        print(f\"\\n5Ô∏è‚É£  Player side information table structure:\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        # Set player_id as index for fast lookups\n",
    "        player_side_info = player_side_info.set_index('player_id')\n",
    "        \n",
    "        print(f\"\\n6Ô∏è‚É£  Sample entries from side information table:\")\n",
    "        sample_data = player_side_info.sample(min(10, len(player_side_info)), random_state=42)\n",
    "        for idx, row in sample_data.iterrows():\n",
    "            print(f\"   Player {idx:>5} | Rating: {row['rating']:>4.0f} ‚Üí Z-score: {row['rating_z']:>6.3f}\")\n",
    "        \n",
    "        # Drop rating column - we only need rating_z for the model\n",
    "        player_side_info = player_side_info.drop(columns=['rating'])\n",
    "        \n",
    "        # This is important - make sure every player in clean_data has a rating\n",
    "        missing_players = set(clean_data['player_id'].unique()) - set(player_side_info.index)\n",
    "        if len(missing_players) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in clean_data are missing from side_info!\")\n",
    "            print(f\"   Missing player IDs: {sorted(list(missing_players))[:10]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚úì All {len(player_side_info):,} players in clean_data have side information\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ RATING NORMALIZATION COMPLETE\")\n",
    "        print(f\"\\nCreated: player_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Index: player_id\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ clean_data: {clean_data.shape[0]:,} rows (player-opening interactions)\")\n",
    "        print(f\"   ‚Ä¢ player_side_info: {len(player_side_info):,} rows (one per player)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these parameters for inference!\")\n",
    "        print(f\"   RATING_MEAN = {RATING_MEAN:.2f}\")\n",
    "        print(f\"   RATING_STD = {RATING_STD:.2f}\")\n",
    "        print(f\"\\n   You'll need them to normalize ratings for new users at inference time.\")\n",
    "        \n",
    "        return player_side_info, RATING_MEAN, RATING_STD\n",
    "    \n",
    "    player_side_info, RATING_MEAN, RATING_STD = normalize_player_ratings(player_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14eba9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         player_id  opening_id  num_games     score  eco  confidence\n",
      "1289916      21872        1373         39  0.529081  C00    0.438202\n",
      "2264209      38265         744         11  0.502277  B00    0.180328\n",
      "971825       16343        1622         15  0.543998  C26    0.230769\n",
      "275669        4643         976         17  0.509269  B15    0.253731\n",
      "1942571      32800        2451         11  0.519303  D01    0.180328\n",
      "1667728      28160        3409         41  0.465559  D02    0.450549\n",
      "2422402      40971        3235         15  0.503054  D00    0.230769\n",
      "826132       13855         838         86  0.470452  B06    0.632353\n",
      "1859762      31404        2003        133  0.530914  C45    0.726776\n",
      "2324807      39295        3481         49  0.477825  C42    0.494949\n",
      "26620          479        1582        262  0.549469  C24    0.839744\n",
      "1647138      27836        1373         22  0.473448  C00    0.305556\n",
      "654266       10956        1167         39  0.469703  B40    0.438202\n",
      "2443045      41332        2620         12  0.521842  D31    0.193548\n",
      "1042647      17551        1204         46  0.541083  B48    0.479167\n",
      "1526364      25810         751         44  0.503532  B01    0.468085\n",
      "1221622      20683        1555         16  0.569565  C23    0.242424\n",
      "2851512      48827        1138         40  0.536269  B32    0.444444\n",
      "564698        9477        2213         18  0.492040  C61    0.264706\n",
      "1515080      25625         276         17  0.530150  A13    0.253731\n"
     ]
    }
   ],
   "source": [
    "print(clean_data.sample(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac119ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           rating_z\n",
      "player_id          \n",
      "29605     -1.904317\n",
      "48600     -0.198210\n",
      "3434      -0.174124\n",
      "22900      0.512333\n",
      "42831      0.580577\n",
      "3957      -1.237932\n",
      "208        1.006101\n",
      "4312       1.391480\n",
      "29316      1.479796\n",
      "32091      0.227313\n"
     ]
    }
   ],
   "source": [
    "print(player_side_info.sample(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30280db",
   "metadata": {},
   "source": [
    "## Step 3: Train/Test/Val splits\n",
    "\n",
    "Here, I split my data and drop columns that are no longer needed. We're very close to being able to train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf3849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: TRAIN/VALIDATION/TEST SPLIT\n",
      "\n",
      "1Ô∏è‚É£  Preparing data for split...\n",
      "   ‚Ä¢ Features (X): (2897700, 4)\n",
      "   ‚Ä¢ Target (y): (2897700,)\n",
      "   ‚Ä¢ Feature columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ Train: 2,173,275 samples (75.0%)\n",
      "   ‚Ä¢ Val: 434,655 samples (15.0%)\n",
      "   ‚Ä¢ Test: 289,770 samples (10.0%)\n",
      "   ‚Ä¢ Total samples: 2,897,700 (should equal 2,897,700)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 48,414 unique players\n",
      "   ‚Ä¢ Val: 47,504 unique players\n",
      "   ‚Ä¢ Test: 46,484 unique players\n",
      "   ‚Ä¢ Total unique: 48,468 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,687 unique openings\n",
      "   ‚Ä¢ Val: 2,448 unique openings\n",
      "   ‚Ä¢ Test: 2,374 unique openings\n",
      "   ‚Ä¢ Total unique: 2,717 openings\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 36 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 18 (0.7%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 24 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 14 (0.6%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "   ‚Ä¢ Train: 2,173,275 samples (75.0%)\n",
      "   ‚Ä¢ Val: 434,655 samples (15.0%)\n",
      "   ‚Ä¢ Test: 289,770 samples (10.0%)\n",
      "   ‚Ä¢ Total samples: 2,897,700 (should equal 2,897,700)\n",
      "   ‚Ä¢ Train %: 75.00% (target: 75%)\n",
      "   ‚Ä¢ Val %: 15.00% (target: 15%)\n",
      "   ‚Ä¢ Test %: 10.00% (target: 10%)\n",
      "\n",
      "   Players:\n",
      "   ‚Ä¢ Train: 48,414 unique players\n",
      "   ‚Ä¢ Val: 47,504 unique players\n",
      "   ‚Ä¢ Test: 46,484 unique players\n",
      "   ‚Ä¢ Total unique: 48,468 players\n",
      "\n",
      "   Openings:\n",
      "   ‚Ä¢ Train: 2,687 unique openings\n",
      "   ‚Ä¢ Val: 2,448 unique openings\n",
      "   ‚Ä¢ Test: 2,374 unique openings\n",
      "   ‚Ä¢ Total unique: 2,717 openings\n",
      "\n",
      "   Validation set:\n",
      "   ‚Ä¢ Players not in train: 36 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 18 (0.7%)\n",
      "\n",
      "   Test set:\n",
      "   ‚Ä¢ Players not in train: 24 (0.1%)\n",
      "   ‚Ä¢ Openings not in train: 14 (0.6%)\n",
      "\n",
      "7Ô∏è‚É£  Score distribution across splits:\n",
      "\n",
      "   Train y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1138\n",
      "   ‚Ä¢ Max: 0.8250\n",
      "\n",
      "   Test y:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1521\n",
      "   ‚Ä¢ Max: 0.7841\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train confidence:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation confidence:\n",
      "   ‚Ä¢ Mean: 0.4150\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test confidence:\n",
      "   ‚Ä¢ Mean: 0.4145\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 2,173,275 samples (75%)\n",
      "   ‚Ä¢ Validation data: 434,655 samples (15%)\n",
      "   ‚Ä¢ Test data: 289,770 samples (10%)\n",
      "   ‚Ä¢ Player side info: 48,468 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n",
      "\n",
      "   Train y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0412\n",
      "   ‚Ä¢ Min: 0.1004\n",
      "   ‚Ä¢ Max: 1.0000\n",
      "\n",
      "   Validation y:\n",
      "   ‚Ä¢ Mean: 0.5120\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1138\n",
      "   ‚Ä¢ Max: 0.8250\n",
      "\n",
      "   Test y:\n",
      "   ‚Ä¢ Mean: 0.5119\n",
      "   ‚Ä¢ Std: 0.0411\n",
      "   ‚Ä¢ Min: 0.1521\n",
      "   ‚Ä¢ Max: 0.7841\n",
      "\n",
      "8Ô∏è‚É£  Confidence distribution across splits:\n",
      "\n",
      "   Train confidence:\n",
      "   ‚Ä¢ Mean: 0.4147\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Validation confidence:\n",
      "   ‚Ä¢ Mean: 0.4150\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "   Test confidence:\n",
      "   ‚Ä¢ Mean: 0.4145\n",
      "   ‚Ä¢ Median: 0.3506\n",
      "\n",
      "============================================================\n",
      "‚úÖ DATA SPLIT COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Training data: 2,173,275 samples (75%)\n",
      "   ‚Ä¢ Validation data: 434,655 samples (15%)\n",
      "   ‚Ä¢ Test data: 289,770 samples (10%)\n",
      "   ‚Ä¢ Player side info: 48,468 players\n",
      "   ‚Ä¢ Side info columns: ['rating_z']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train/Validation/Test Split (75/15/10)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 3: TRAIN/VALIDATION/TEST SPLIT\")\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Preparing data for split...\")\n",
    "\n",
    "# We don't need num_games for modeling because we have the `confidence` based on num_games\n",
    "# Keep: player_id, opening_id, score, eco, confidence\n",
    "X = clean_data[[\"player_id\", \"opening_id\", \"eco\", \"confidence\"]].copy()\n",
    "y = clean_data[\"score\"].copy()\n",
    "\n",
    "print(f\"   ‚Ä¢ Features (X): {X.shape}\")\n",
    "print(f\"   ‚Ä¢ Target (y): {y.shape}\")\n",
    "print(f\"   ‚Ä¢ Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Clean up player_side_info - only keep rating_z\n",
    "# May regret this if we add more side info later and forget we took this step\n",
    "# God help me that would be a nasty bug\n",
    "player_side_info_clean = player_side_info[[\"rating_z\"]].copy()\n",
    "\n",
    "#  Use index-based splitting to avoid DataFrame copies\n",
    "idx = np.arange(len(X))\n",
    "\n",
    "# First split: separate out test set (10%)\n",
    "idx_temp, idx_test = train_test_split(idx, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: split remaining into train (75%) and val (15%)\n",
    "# 15% of original = 15/90 ‚âà 0.1667 of temp\n",
    "idx_train, idx_val = train_test_split(idx_temp, test_size=15/90, random_state=42, shuffle=True)\n",
    "\n",
    "# Create splits using iloc (view, not copy)\n",
    "X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
    "X_test, y_test = X.iloc[idx_test], y.iloc[idx_test]\n",
    "\n",
    "print(f\"   ‚Ä¢ Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Val: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"   ‚Ä¢ Total samples: {total:,} (should equal {len(X):,})\")\n",
    "print(f\"   ‚Ä¢ Train %: {len(X_train)/total*100:.2f}% (target: 75%)\")\n",
    "print(f\"   ‚Ä¢ Val %: {len(X_val)/total*100:.2f}% (target: 15%)\")\n",
    "print(f\"   ‚Ä¢ Test %: {len(X_test)/total*100:.2f}% (target: 10%)\")\n",
    "\n",
    "# Pre-compute unique arrays once\n",
    "players_train = X_train[\"player_id\"].unique()\n",
    "players_val = X_val[\"player_id\"].unique()\n",
    "players_test = X_test[\"player_id\"].unique()\n",
    "\n",
    "openings_train = X_train[\"opening_id\"].unique()\n",
    "openings_val = X_val[\"opening_id\"].unique()\n",
    "openings_test = X_test[\"opening_id\"].unique()\n",
    "\n",
    "print(f\"\\n   Players:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(players_train):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Val: {len(players_val):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Test: {len(players_test):,} unique players\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['player_id'].nunique():,} players\")\n",
    "\n",
    "print(f\"\\n   Openings:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(openings_train):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Val: {len(openings_val):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Test: {len(openings_test):,} unique openings\")\n",
    "print(f\"   ‚Ä¢ Total unique: {X['opening_id'].nunique():,} openings\")\n",
    "\n",
    "# Use NumPy setdiff1d for cold-start analysis (C-speed)\n",
    "\n",
    "val_cold_players = np.setdiff1d(players_val, players_train, assume_unique=True)\n",
    "val_cold_openings = np.setdiff1d(openings_val, openings_train, assume_unique=True)\n",
    "\n",
    "test_cold_players = np.setdiff1d(players_test, players_train, assume_unique=True)\n",
    "test_cold_openings = np.setdiff1d(openings_test, openings_train, assume_unique=True)\n",
    "\n",
    "print(f\"\\n   Validation set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(val_cold_players):,} ({len(val_cold_players)/len(players_val)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(val_cold_openings):,} ({len(val_cold_openings)/len(openings_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Test set:\")\n",
    "print(f\"   ‚Ä¢ Players not in train: {len(test_cold_players):,} ({len(test_cold_players)/len(players_test)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Openings not in train: {len(test_cold_openings):,} ({len(test_cold_openings)/len(openings_test)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n7Ô∏è‚É£  Score distribution across splits:\")\n",
    "\n",
    "y_train_stats = y_train.describe()\n",
    "y_val_stats = y_val.describe()\n",
    "y_test_stats = y_test.describe()\n",
    "\n",
    "print(f\"\\n   Train y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_train_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_train_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_train_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_val_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_val_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_val_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test y:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_test_stats['std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_test_stats['min']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_test_stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\n8Ô∏è‚É£  Confidence distribution across splits:\")\n",
    "\n",
    "conf_train_stats = X_train['confidence'].describe()\n",
    "conf_val_stats = X_val['confidence'].describe()\n",
    "conf_test_stats = X_test['confidence'].describe()\n",
    "\n",
    "print(f\"\\n   Train confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_train_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_train_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Validation confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_val_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_val_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Test confidence:\")\n",
    "print(f\"   ‚Ä¢ Mean: {conf_test_stats['mean']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Median: {conf_test_stats['50%']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DATA SPLIT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   ‚Ä¢ Training data: {len(X_train):,} samples (75%)\")\n",
    "print(f\"   ‚Ä¢ Validation data: {len(X_val):,} samples (15%)\")\n",
    "print(f\"   ‚Ä¢ Test data: {len(X_test):,} samples (10%)\")\n",
    "print(f\"   ‚Ä¢ Player side info: {len(player_side_info_clean):,} players\")\n",
    "print(f\"   ‚Ä¢ Side info columns: {list(player_side_info_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781a382",
   "metadata": {},
   "source": [
    "## Step 3b: Remap Player and Opening IDs to Sequential Integers\n",
    "\n",
    "**Why remap IDs?**\n",
    "- Database IDs may have gaps (e.g., [1, 5, 10, 15, ...]) from deleted entries\n",
    "- Embedding layers need 0-based contiguous indices for efficiency\n",
    "- Remapping saves memory (no unused embedding slots)\n",
    "\n",
    "**Process:**\n",
    "1. Check if IDs are already sequential (0 or 1-based with no gaps)\n",
    "2. If not, create mappings: old_id ‚Üí new_sequential_id\n",
    "3. Remap all DataFrames and side info tables\n",
    "4. Verify mappings with spot checks\n",
    "\n",
    "This ensures embeddings use minimal memory and indices align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81b4c409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\n",
      "\n",
      "1Ô∏è‚É£  Processing player IDs...\n",
      "Checking player IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique players: 48468\n",
      "   ‚Ä¢ ID range: [1, 50000]\n",
      "   ‚ö†Ô∏è  player IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1532\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      player_id 1 ‚Üí 0\n",
      "      player_id 2 ‚Üí 1\n",
      "      player_id 3 ‚Üí 2\n",
      "      player_id 4 ‚Üí 3\n",
      "      player_id 5 ‚Üí 4\n",
      "      player_id 49996 ‚Üí 48463\n",
      "      player_id 49997 ‚Üí 48464\n",
      "      player_id 49998 ‚Üí 48465\n",
      "      player_id 49999 ‚Üí 48466\n",
      "      player_id 50000 ‚Üí 48467\n",
      "\n",
      "   Remapping 5 DataFrames...\n",
      "Checking player IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique players: 48468\n",
      "   ‚Ä¢ ID range: [1, 50000]\n",
      "   ‚ö†Ô∏è  player IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 1532\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      player_id 1 ‚Üí 0\n",
      "      player_id 2 ‚Üí 1\n",
      "      player_id 3 ‚Üí 2\n",
      "      player_id 4 ‚Üí 3\n",
      "      player_id 5 ‚Üí 4\n",
      "      player_id 49996 ‚Üí 48463\n",
      "      player_id 49997 ‚Üí 48464\n",
      "      player_id 49998 ‚Üí 48465\n",
      "      player_id 49999 ‚Üí 48466\n",
      "      player_id 50000 ‚Üí 48467\n",
      "\n",
      "   Remapping 5 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 1/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 2/5\n",
      "   ‚úì Remapped DataFrame 3/5\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2717\n",
      "   ‚Ä¢ ID range: [2, 3589]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 871\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 3 ‚Üí 1\n",
      "      opening_id 4 ‚Üí 2\n",
      "      opening_id 5 ‚Üí 3\n",
      "      opening_id 6 ‚Üí 4\n",
      "      opening_id 3571 ‚Üí 2712\n",
      "      opening_id 3572 ‚Üí 2713\n",
      "      opening_id 3575 ‚Üí 2714\n",
      "      opening_id 3584 ‚Üí 2715\n",
      "      opening_id 3589 ‚Üí 2716\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 4/5\n",
      "   ‚úì Remapped DataFrame 5/5\n",
      "\n",
      "   ‚úÖ Player ID remapping complete!\n",
      "\n",
      "2Ô∏è‚É£  Processing opening IDs...\n",
      "Checking opening IDs...\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique openings: 2717\n",
      "   ‚Ä¢ ID range: [2, 3589]\n",
      "   ‚ö†Ô∏è  opening IDs have gaps - will remap to 0-based sequential\n",
      "   ‚Ä¢ Number of gaps: 871\n",
      "\n",
      "   Creating mapping...\n",
      "   ‚Ä¢ Example mappings:\n",
      "      opening_id 2 ‚Üí 0\n",
      "      opening_id 3 ‚Üí 1\n",
      "      opening_id 4 ‚Üí 2\n",
      "      opening_id 5 ‚Üí 3\n",
      "      opening_id 6 ‚Üí 4\n",
      "      opening_id 3571 ‚Üí 2712\n",
      "      opening_id 3572 ‚Üí 2713\n",
      "      opening_id 3575 ‚Üí 2714\n",
      "      opening_id 3584 ‚Üí 2715\n",
      "      opening_id 3589 ‚Üí 2716\n",
      "\n",
      "   Remapping 4 DataFrames...\n",
      "   ‚úì Remapped DataFrame 1/4\n",
      "   ‚úì Remapped DataFrame 2/4\n",
      "   ‚úì Remapped DataFrame 3/4\n",
      "   ‚úì Remapped DataFrame 4/4\n",
      "\n",
      "   ‚úÖ Opening ID remapping complete!\n",
      "\n",
      "3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\n",
      "   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\n",
      "\n",
      "   Sampling 10 entries from X_train (before remapping was applied)...\n",
      "   ‚Ä¢ Sample indices: [0, 241474, 482948, 724422, 965896, 1207370, 1448844, 1690318, 1931792, 2173274]\n",
      "\n",
      "   Verification checks:\n",
      "   #    Row Idx    New Player   New Opening  Confidence   Status         \n",
      "   ---- ---------- ------------ ------------ ------------ ---------------\n",
      "   1    0          11013        1819         0.7487       ‚úì PASS         \n",
      "   2    241474     42002        398          0.6212       ‚úì PASS         \n",
      "   3    482948     6213         1408         0.4792       ‚úì PASS         \n",
      "   4    724422     22140        1633         0.4186       ‚úì PASS         \n",
      "   5    965896     13458        1484         0.2424       ‚úì PASS         \n",
      "   6    1207370    43373        1648         0.7727       ‚úì PASS         \n",
      "   ‚úì Remapped DataFrame 1/4\n",
      "   ‚úì Remapped DataFrame 2/4\n",
      "   ‚úì Remapped DataFrame 3/4\n",
      "   ‚úì Remapped DataFrame 4/4\n",
      "\n",
      "   ‚úÖ Opening ID remapping complete!\n",
      "\n",
      "3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\n",
      "   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\n",
      "\n",
      "   Sampling 10 entries from X_train (before remapping was applied)...\n",
      "   ‚Ä¢ Sample indices: [0, 241474, 482948, 724422, 965896, 1207370, 1448844, 1690318, 1931792, 2173274]\n",
      "\n",
      "   Verification checks:\n",
      "   #    Row Idx    New Player   New Opening  Confidence   Status         \n",
      "   ---- ---------- ------------ ------------ ------------ ---------------\n",
      "   1    0          11013        1819         0.7487       ‚úì PASS         \n",
      "   2    241474     42002        398          0.6212       ‚úì PASS         \n",
      "   3    482948     6213         1408         0.4792       ‚úì PASS         \n",
      "   4    724422     22140        1633         0.4186       ‚úì PASS         \n",
      "   5    965896     13458        1484         0.2424       ‚úì PASS         \n",
      "   6    1207370    43373        1648         0.7727       ‚úì PASS         \n",
      "   7    1448844    16156        2128         0.4318       ‚úì PASS         \n",
      "   8    1690318    24881        2204         0.3421       ‚úì PASS         \n",
      "   9    1931792    24633        921          0.5652       ‚úì PASS         \n",
      "   10   2173274    20850        1514         0.6815       ‚úì PASS         \n",
      "\n",
      "   Reverse mapping verification (sample of 3 entries):\n",
      "   #    New‚ÜíOld Player                 New‚ÜíOld Opening               \n",
      "   ---- ------------------------------ ------------------------------\n",
      "   1    11013 ‚Üí 11372                  1819 ‚Üí 2397                   \n",
      "   6    43373 ‚Üí 44729                  1648 ‚Üí 2148                   \n",
      "   10   20850 ‚Üí 21518                  1514 ‚Üí 1958                   \n",
      "\n",
      "   ‚úÖ All spot checks passed! ID mappings are correct.\n",
      "\n",
      "4Ô∏è‚É£  Summary:\n",
      "\n",
      "   Player IDs:\n",
      "   ‚Ä¢ Original range: [1, 50000]\n",
      "   ‚Ä¢ New range: [0, 48467]\n",
      "   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\n",
      "\n",
      "   Opening IDs:\n",
      "   ‚Ä¢ Original range: [2, 3589]\n",
      "   ‚Ä¢ New range: [0, 2716]\n",
      "   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\n",
      "\n",
      "   Updated DataFrames:\n",
      "   ‚Ä¢ X_train: (2173275, 4)\n",
      "   ‚Ä¢ X_val: (434655, 4)\n",
      "   ‚Ä¢ X_test: (289770, 4)\n",
      "   ‚Ä¢ clean_data: (2897700, 6)\n",
      "   ‚Ä¢ player_side_info: (48468, 1)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ID REMAPPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Important:\n",
      "   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\n",
      "   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\n",
      "   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\n",
      "   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\n",
      "   7    1448844    16156        2128         0.4318       ‚úì PASS         \n",
      "   8    1690318    24881        2204         0.3421       ‚úì PASS         \n",
      "   9    1931792    24633        921          0.5652       ‚úì PASS         \n",
      "   10   2173274    20850        1514         0.6815       ‚úì PASS         \n",
      "\n",
      "   Reverse mapping verification (sample of 3 entries):\n",
      "   #    New‚ÜíOld Player                 New‚ÜíOld Opening               \n",
      "   ---- ------------------------------ ------------------------------\n",
      "   1    11013 ‚Üí 11372                  1819 ‚Üí 2397                   \n",
      "   6    43373 ‚Üí 44729                  1648 ‚Üí 2148                   \n",
      "   10   20850 ‚Üí 21518                  1514 ‚Üí 1958                   \n",
      "\n",
      "   ‚úÖ All spot checks passed! ID mappings are correct.\n",
      "\n",
      "4Ô∏è‚É£  Summary:\n",
      "\n",
      "   Player IDs:\n",
      "   ‚Ä¢ Original range: [1, 50000]\n",
      "   ‚Ä¢ New range: [0, 48467]\n",
      "   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\n",
      "\n",
      "   Opening IDs:\n",
      "   ‚Ä¢ Original range: [2, 3589]\n",
      "   ‚Ä¢ New range: [0, 2716]\n",
      "   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\n",
      "\n",
      "   Updated DataFrames:\n",
      "   ‚Ä¢ X_train: (2173275, 4)\n",
      "   ‚Ä¢ X_val: (434655, 4)\n",
      "   ‚Ä¢ X_test: (289770, 4)\n",
      "   ‚Ä¢ clean_data: (2897700, 6)\n",
      "   ‚Ä¢ player_side_info: (48468, 1)\n",
      "\n",
      "============================================================\n",
      "‚úÖ ID REMAPPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "üí° Important:\n",
      "   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\n",
      "   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\n",
      "   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\n",
      "   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\n"
     ]
    }
   ],
   "source": [
    "# Step 3b: Remap player and opening IDs to 0-based sequential integers\n",
    "\n",
    "print(\"STEP 3B: REMAP IDs TO SEQUENTIAL INTEGERS\")\n",
    "\n",
    "def check_and_remap_ids(df_list, id_column, entity_name):\n",
    "    \"\"\"\n",
    "    Check if IDs are sequential starting from 0, and remap if not.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of DataFrames\n",
    "        List of DataFrames containing the ID column to check/remap\n",
    "    id_column : str\n",
    "        Name of the ID column ('player_id' or 'opening_id')\n",
    "    entity_name : str\n",
    "        Name for logging ('player' or 'opening')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (df_list with remapped IDs, id_to_idx mapping dict, needs_remapping bool)\n",
    "    \"\"\"\n",
    "    # Get all unique IDs across all dataframes\n",
    "    all_ids = pd.concat([df[id_column] for df in df_list]).unique()\n",
    "    all_ids_sorted = sorted(all_ids)\n",
    "    \n",
    "    print(f\"Checking {entity_name} IDs...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   ‚Ä¢ Total unique {entity_name}s: {len(all_ids_sorted)}\")\n",
    "    print(f\"   ‚Ä¢ ID range: [{all_ids_sorted[0]}, {all_ids_sorted[-1]}]\")\n",
    "    \n",
    "    # Check if IDs are already 0-based sequential (0, 1, 2, ...)\n",
    "    expected_sequential = list(range(len(all_ids_sorted)))\n",
    "    is_sequential = (all_ids_sorted == expected_sequential)\n",
    "    \n",
    "    if is_sequential:\n",
    "        print(f\"   ‚úì {entity_name} IDs are already 0-based sequential - no remapping needed!\")\n",
    "        return df_list, None, False\n",
    "    \n",
    "    # Check if IDs are 1-based sequential (1, 2, 3, ...)\n",
    "    expected_sequential_1based = list(range(1, len(all_ids_sorted) + 1))\n",
    "    is_sequential_1based = (all_ids_sorted == expected_sequential_1based)\n",
    "    \n",
    "    if is_sequential_1based:\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs are 1-based sequential - will remap to 0-based\")\n",
    "    else:\n",
    "        # Calculate gaps\n",
    "        num_gaps = (all_ids_sorted[-1] - all_ids_sorted[0] + 1) - len(all_ids_sorted)\n",
    "        print(f\"   ‚ö†Ô∏è  {entity_name} IDs have gaps - will remap to 0-based sequential\")\n",
    "        print(f\"   ‚Ä¢ Number of gaps: {num_gaps}\")\n",
    "    \n",
    "    # Create mapping: old_id -> new_idx (0-based)\n",
    "    id_to_idx = {old_id: new_idx for new_idx, old_id in enumerate(all_ids_sorted)}\n",
    "    idx_to_id = {new_idx: old_id for old_id, new_idx in id_to_idx.items()}\n",
    "    \n",
    "    print(f\"\\n   Creating mapping...\")\n",
    "    print(f\"   ‚Ä¢ Example mappings:\")\n",
    "    sample_ids = all_ids_sorted[:5] + all_ids_sorted[-5:]\n",
    "    for old_id in sample_ids[:10]:  # Show first 5 and last 5\n",
    "        print(f\"      {entity_name}_id {old_id} ‚Üí {id_to_idx[old_id]}\")\n",
    "    \n",
    "    # Remap all DataFrames\n",
    "    print(f\"\\n   Remapping {len(df_list)} DataFrames...\")\n",
    "    remapped_dfs = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df_copy = df.copy()\n",
    "        df_copy[id_column] = df_copy[id_column].map(id_to_idx)\n",
    "        remapped_dfs.append(df_copy)\n",
    "        print(f\"   ‚úì Remapped DataFrame {i+1}/{len(df_list)}\")\n",
    "    \n",
    "    return remapped_dfs, (id_to_idx, idx_to_id), True\n",
    "\n",
    "# 1. Remap player IDs\n",
    "print(f\"\\n1Ô∏è‚É£  Processing player IDs...\")\n",
    "player_dfs = [X_train, X_val, X_test, clean_data, player_side_info.reset_index()]\n",
    "remapped_player_dfs, player_mappings, player_remapped = check_and_remap_ids(\n",
    "    player_dfs, 'player_id', 'player'\n",
    ")\n",
    "\n",
    "if player_remapped:\n",
    "    X_train, X_val, X_test, clean_data, player_side_info_remapped = remapped_player_dfs\n",
    "    player_id_to_idx, player_idx_to_id = player_mappings\n",
    "    player_side_info = player_side_info_remapped.set_index('player_id')\n",
    "    print(f\"\\n   ‚úÖ Player ID remapping complete!\")\n",
    "else:\n",
    "    player_id_to_idx, player_idx_to_id = None, None\n",
    "\n",
    "# 2. Remap opening IDs\n",
    "print(f\"\\n2Ô∏è‚É£  Processing opening IDs...\")\n",
    "opening_dfs = [X_train, X_val, X_test, clean_data]\n",
    "remapped_opening_dfs, opening_mappings, opening_remapped = check_and_remap_ids(\n",
    "    opening_dfs, 'opening_id', 'opening'\n",
    ")\n",
    "\n",
    "if opening_remapped:\n",
    "    X_train, X_val, X_test, clean_data = remapped_opening_dfs\n",
    "    opening_id_to_idx, opening_idx_to_id = opening_mappings\n",
    "    print(f\"\\n   ‚úÖ Opening ID remapping complete!\")\n",
    "else:\n",
    "    opening_id_to_idx, opening_idx_to_id = None, None\n",
    "\n",
    "# 3. Spot checks to verify mappings\n",
    "print(f\"\\n3Ô∏è‚É£  Running spot checks to verify ID remapping correctness...\")\n",
    "print(f\"   Strategy: Sample entries BEFORE remapping, verify mappings AFTER\")\n",
    "\n",
    "# Sample 10 entries: first, last, and 8 in between\n",
    "print(f\"\\n   Sampling 10 entries from X_train (before remapping was applied)...\")\n",
    "total_rows = len(X_train)\n",
    "# Get indices: first, last, and 8 evenly spaced in between\n",
    "sample_indices = [0]  # First row\n",
    "step = (total_rows - 1) // 9  # Divide remaining rows into 9 parts\n",
    "sample_indices.extend([min(i * step, total_rows - 1) for i in range(1, 9)])\n",
    "sample_indices.append(total_rows - 1)  # Last row\n",
    "\n",
    "print(f\"   ‚Ä¢ Sample indices: {sample_indices}\")\n",
    "\n",
    "# Store samples with their NEW (remapped) IDs\n",
    "samples = []\n",
    "for idx in sample_indices:\n",
    "    row = X_train.iloc[idx]\n",
    "    samples.append({\n",
    "        'index': idx,\n",
    "        'new_player_id': row['player_id'],\n",
    "        'new_opening_id': row['opening_id'],\n",
    "        'confidence': row['confidence']\n",
    "    })\n",
    "\n",
    "print(f\"\\n   Verification checks:\")\n",
    "print(f\"   {'#':<4} {'Row Idx':<10} {'New Player':<12} {'New Opening':<12} {'Confidence':<12} {'Status':<15}\")\n",
    "print(f\"   {'-'*4} {'-'*10} {'-'*12} {'-'*12} {'-'*12} {'-'*15}\")\n",
    "\n",
    "all_checks_passed = True\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    new_player_id = sample['new_player_id']\n",
    "    new_opening_id = sample['new_opening_id']\n",
    "    confidence = sample['confidence']\n",
    "    idx = sample['index']\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: New player ID is valid (0-based sequential)\n",
    "    if player_remapped:\n",
    "        player_valid = 0 <= new_player_id < len(player_id_to_idx)\n",
    "        checks.append((\"player_id\", player_valid))\n",
    "    else:\n",
    "        checks.append((\"player_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 2: New opening ID is valid (0-based sequential)\n",
    "    if opening_remapped:\n",
    "        opening_valid = 0 <= new_opening_id < len(opening_id_to_idx)\n",
    "        checks.append((\"opening_id\", opening_valid))\n",
    "    else:\n",
    "        checks.append((\"opening_id\", True))  # No remapping needed means original was valid\n",
    "    \n",
    "    # Check 3: Player exists in player_side_info\n",
    "    player_exists = new_player_id in player_side_info.index\n",
    "    checks.append((\"in_side_info\", player_exists))\n",
    "    \n",
    "    # Check 4: Opening exists in clean_data\n",
    "    opening_exists = new_opening_id in clean_data['opening_id'].values\n",
    "    checks.append((\"in_clean\", opening_exists))\n",
    "    \n",
    "    # All checks must pass\n",
    "    all_pass = all(check[1] for check in checks)\n",
    "    status = \"‚úì PASS\" if all_pass else f\"‚úó FAIL ({','.join([c[0] for c in checks if not c[1]])})\"\n",
    "    \n",
    "    if not all_pass:\n",
    "        all_checks_passed = False\n",
    "    \n",
    "    print(f\"   {i:<4} {idx:<10} {new_player_id:<12} {new_opening_id:<12} {confidence:<12.4f} {status:<15}\")\n",
    "\n",
    "# Additional verification: check if we can reverse map to original IDs\n",
    "if player_remapped or opening_remapped:\n",
    "    print(f\"\\n   Reverse mapping verification (sample of 3 entries):\")\n",
    "    print(f\"   {'#':<4} {'New‚ÜíOld Player':<30} {'New‚ÜíOld Opening':<30}\")\n",
    "    print(f\"   {'-'*4} {'-'*30} {'-'*30}\")\n",
    "    \n",
    "    for i in [0, len(samples)//2, len(samples)-1]:  # First, middle, last\n",
    "        sample = samples[i]\n",
    "        \n",
    "        # Reverse map player\n",
    "        if player_remapped:\n",
    "            old_player = player_idx_to_id.get(sample['new_player_id'], 'NOT_FOUND')\n",
    "            player_str = f\"{sample['new_player_id']} ‚Üí {old_player}\"\n",
    "        else:\n",
    "            player_str = f\"{sample['new_player_id']} (unchanged)\"\n",
    "        \n",
    "        # Reverse map opening\n",
    "        if opening_remapped:\n",
    "            old_opening = opening_idx_to_id.get(sample['new_opening_id'], 'NOT_FOUND')\n",
    "            opening_str = f\"{sample['new_opening_id']} ‚Üí {old_opening}\"\n",
    "        else:\n",
    "            opening_str = f\"{sample['new_opening_id']} (unchanged)\"\n",
    "        \n",
    "        print(f\"   {i+1:<4} {player_str:<30} {opening_str:<30}\")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(f\"\\n   ‚úÖ All spot checks passed! ID mappings are correct.\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Some spot checks failed - investigate immediately!\")\n",
    "    raise ValueError(\"ID remapping verification failed!\")\n",
    "\n",
    "# 4. Summary\n",
    "print(f\"\\n4Ô∏è‚É£  Summary:\")\n",
    "print(f\"\\n   Player IDs:\")\n",
    "if player_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{player_idx_to_id[0]}, {player_idx_to_id[len(player_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(player_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: player_id_to_idx, player_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "    \n",
    "print(f\"\\n   Opening IDs:\")\n",
    "if opening_remapped:\n",
    "    print(f\"   ‚Ä¢ Original range: [{opening_idx_to_id[0]}, {opening_idx_to_id[len(opening_idx_to_id)-1]}]\")\n",
    "    print(f\"   ‚Ä¢ New range: [0, {len(opening_idx_to_id)-1}]\")\n",
    "    print(f\"   ‚Ä¢ Mapping saved as: opening_id_to_idx, opening_idx_to_id\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ No remapping needed - IDs already sequential\")\n",
    "\n",
    "print(f\"\\n   Updated DataFrames:\")\n",
    "print(f\"   ‚Ä¢ X_train: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ X_val: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ X_test: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ clean_data: {clean_data.shape}\")\n",
    "print(f\"   ‚Ä¢ player_side_info: {player_side_info.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ID REMAPPING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Important:\")\n",
    "print(f\"   ‚Ä¢ All player_id and opening_id values are now 0-based sequential\")\n",
    "print(f\"   ‚Ä¢ Use these for embedding layers: nn.Embedding(num_players, dim)\")\n",
    "print(f\"   ‚Ä¢ Save mappings for inference (to convert new user/opening IDs)\")\n",
    "print(f\"   ‚Ä¢ player_side_info index is now 0-based sequential player IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a580c0",
   "metadata": {},
   "source": [
    "## 4. Enumerate Categorical Variables\n",
    "\n",
    "I believe the only variable we need to enumerate here is `eco`. That's the broad categorization of a specific opening.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- One ECO code will have many openings\n",
    "- They're sorted by letter, then further by number. For instance, C21 and C44 are in the `C` family.\n",
    "- Maybe we make this side information?\n",
    "\n",
    "First, let's get some data on ECO codes to help us better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9504a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: ECO CODE STATISTICS\n",
      "============================================================\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,897,700\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,897,700\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "   ‚Ä¢ Missing ECO values: 0\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 6326.9\n",
      "   ‚Ä¢ Median entries per ECO: 823.5\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 201371\n",
      "   ‚Ä¢ Std: 18387.8\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "2Ô∏è‚É£  Distribution of entries per ECO code:\n",
      "   ‚Ä¢ Mean entries per ECO: 6326.9\n",
      "   ‚Ä¢ Median entries per ECO: 823.5\n",
      "   ‚Ä¢ Min entries: 1\n",
      "   ‚Ä¢ Max entries: 201371\n",
      "   ‚Ä¢ Std: 18387.8\n",
      "\n",
      "3Ô∏è‚É£  ECO families (by first letter):\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        510,237     17.61%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        1,048,720     36.19%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        986,684     34.05%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D        279,053      9.63%      ‚ñà‚ñà‚ñà\n",
      "   E         73,006      2.52%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Family   Count      Percentage   Visual\n",
      "   -------- ---------- ------------ ----------------------------------------\n",
      "   A        510,237     17.61%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   B        1,048,720     36.19%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   C        986,684     34.05%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   D        279,053      9.63%      ‚ñà‚ñà‚ñà\n",
      "   E         73,006      2.52%      ‚ñà\n",
      "\n",
      "4Ô∏è‚É£  Top 20 most common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00    201,371      6.95%      ‚ñà‚ñà\n",
      "   2      B01    197,544      6.82%      ‚ñà‚ñà\n",
      "   3      A40    125,603      4.33%      ‚ñà\n",
      "   4      C50    101,215      3.49%      ‚ñà\n",
      "   5      C00     87,206      3.01%      \n",
      "   6      C40     81,489      2.81%      \n",
      "   7      B06     73,344      2.53%      \n",
      "   8      C42     65,714      2.27%      \n",
      "   9      C44     60,576      2.09%      \n",
      "   10     B10     56,889      1.96%      \n",
      "   11     C41     55,409      1.91%      \n",
      "   12     B40     52,418      1.81%      \n",
      "   13     B12     48,762      1.68%      \n",
      "   14     A00     48,401      1.67%      \n",
      "   15     C02     47,926      1.65%      \n",
      "   16     D00     41,171      1.42%      \n",
      "   17     B21     37,751      1.30%      \n",
      "   18     A43     35,778      1.23%      \n",
      "   19     B32     33,901      1.17%      \n",
      "   20     A04     32,975      1.14%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      A97          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2      E78          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3      D98          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4      D68          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   5      E79          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   6      E25          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   7      A72          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   8      A63          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   9      D29          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   10     C76          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   11     D49          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   12     C94          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   13     E55          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   14     D84          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   15     A99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   16     B69          3    ‚ñà‚ñà‚ñà\n",
      "   17     E96          2    ‚ñà‚ñà\n",
      "   18     A93          2    ‚ñà‚ñà\n",
      "   19     E36          1    ‚ñà\n",
      "   20     A98          1    ‚ñà\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Percentage   Visual\n",
      "   ------ ------ ---------- ------------ ------------------------------\n",
      "   1      B00    201,371      6.95%      ‚ñà‚ñà\n",
      "   2      B01    197,544      6.82%      ‚ñà‚ñà\n",
      "   3      A40    125,603      4.33%      ‚ñà\n",
      "   4      C50    101,215      3.49%      ‚ñà\n",
      "   5      C00     87,206      3.01%      \n",
      "   6      C40     81,489      2.81%      \n",
      "   7      B06     73,344      2.53%      \n",
      "   8      C42     65,714      2.27%      \n",
      "   9      C44     60,576      2.09%      \n",
      "   10     B10     56,889      1.96%      \n",
      "   11     C41     55,409      1.91%      \n",
      "   12     B40     52,418      1.81%      \n",
      "   13     B12     48,762      1.68%      \n",
      "   14     A00     48,401      1.67%      \n",
      "   15     C02     47,926      1.65%      \n",
      "   16     D00     41,171      1.42%      \n",
      "   17     B21     37,751      1.30%      \n",
      "   18     A43     35,778      1.23%      \n",
      "   19     B32     33,901      1.17%      \n",
      "   20     A04     32,975      1.14%      \n",
      "\n",
      "5Ô∏è‚É£  Bottom 20 least common ECO codes:\n",
      "\n",
      "   Rank   ECO    Count      Visual\n",
      "   ------ ------ ---------- ------------------------------\n",
      "   1      A97          9    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2      E78          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3      D98          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4      D68          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   5      E79          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   6      E25          8    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   7      A72          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   8      A63          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   9      D29          7    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   10     C76          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   11     D49          6    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   12     C94          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   13     E55          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   14     D84          5    ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   15     A99          4    ‚ñà‚ñà‚ñà‚ñà\n",
      "   16     B69          3    ‚ñà‚ñà‚ñà\n",
      "   17     E96          2    ‚ñà‚ñà\n",
      "   18     A93          2    ‚ñà‚ñà\n",
      "   19     E36          1    ‚ñà\n",
      "   20     A98          1    ‚ñà\n",
      "\n",
      "7Ô∏è‚É£  Sample of ECO codes:\n",
      "   A07, A20, A71, B13, B22, B53, C03, C43, C48, C66, C84, C87, D16, D33, E10, E30, E40, E71, E78, E96\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   A07, A20, A71, B13, B22, B53, C03, C43, C48, C66, C84, C87, D16, D33, E10, E30, E40, E71, E78, E96\n",
      "\n",
      "8Ô∏è‚É£  ECO distribution across splits:\n",
      "\n",
      "   Train split:\n",
      "   ‚Ä¢ Unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,173,275\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 448\n",
      "   ‚Ä¢ Total entries: 434,655\n",
      "   ‚Ä¢ Unique ECO codes: 458\n",
      "   ‚Ä¢ Total entries: 2,173,275\n",
      "\n",
      "   Validation split:\n",
      "   ‚Ä¢ Unique ECO codes: 448\n",
      "   ‚Ä¢ Total entries: 434,655\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 443\n",
      "   ‚Ä¢ Total entries: 289,770\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "   Test split:\n",
      "   ‚Ä¢ Unique ECO codes: 443\n",
      "   ‚Ä¢ Total entries: 289,770\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "   ‚Ä¢ ECO codes not in train: 0\n",
      "\n",
      "9Ô∏è‚É£  Average score by ECO code:\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C94    0.6200             5\n",
      "   B77    0.5946           555\n",
      "   E96    0.5860             2\n",
      "   B71    0.5852           230\n",
      "   B85    0.5844            16\n",
      "   D42    0.5818            22\n",
      "   D57    0.5816            23\n",
      "   D49    0.5764             6\n",
      "   E99    0.5708            25\n",
      "   C87    0.5702           182\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C74    0.4671            23\n",
      "   B58    0.4669            10\n",
      "   A58    0.4620           960\n",
      "   C76    0.4616             6\n",
      "   C38    0.4582         1,653\n",
      "   C99    0.4345            14\n",
      "   B69    0.4244             3\n",
      "   E55    0.3855             5\n",
      "   E36    0.3500             1\n",
      "   A98    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   Top 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C94    0.6200             5\n",
      "   B77    0.5946           555\n",
      "   E96    0.5860             2\n",
      "   B71    0.5852           230\n",
      "   B85    0.5844            16\n",
      "   D42    0.5818            22\n",
      "   D57    0.5816            23\n",
      "   D49    0.5764             6\n",
      "   E99    0.5708            25\n",
      "   C87    0.5702           182\n",
      "\n",
      "   Bottom 10 ECO codes by average score:\n",
      "\n",
      "   ECO    Avg Score    Count     \n",
      "   ------ ------------ ----------\n",
      "   C74    0.4671            23\n",
      "   B58    0.4669            10\n",
      "   A58    0.4620           960\n",
      "   C76    0.4616             6\n",
      "   C38    0.4582         1,653\n",
      "   C99    0.4345            14\n",
      "   B69    0.4244             3\n",
      "   E55    0.3855             5\n",
      "   E36    0.3500             1\n",
      "   A98    0.2500             1\n",
      "\n",
      "üîü  ECO codes with highest score variance (may indicate difficulty):\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   A93    0.0069       0.0829             2\n",
      "   C57    0.0068       0.0823        11,861\n",
      "   E45    0.0068       0.0823            17\n",
      "   A97    0.0061       0.0784             9\n",
      "   C37    0.0044       0.0663         5,926\n",
      "   C51    0.0038       0.0619         3,959\n",
      "   C39    0.0037       0.0612         1,444\n",
      "   A61    0.0036       0.0603           144\n",
      "   C31    0.0036       0.0598        10,939\n",
      "   C56    0.0036       0.0597         9,548\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Most common family: B (1,048,720 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (201,371 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n",
      "\n",
      "   ECO    Variance     Std Dev      Count     \n",
      "   ------ ------------ ------------ ----------\n",
      "   A93    0.0069       0.0829             2\n",
      "   C57    0.0068       0.0823        11,861\n",
      "   E45    0.0068       0.0823            17\n",
      "   A97    0.0061       0.0784             9\n",
      "   C37    0.0044       0.0663         5,926\n",
      "   C51    0.0038       0.0619         3,959\n",
      "   C39    0.0037       0.0612         1,444\n",
      "   A61    0.0036       0.0603           144\n",
      "   C31    0.0036       0.0598        10,939\n",
      "   C56    0.0036       0.0597         9,548\n",
      "\n",
      "============================================================\n",
      "‚úÖ ECO CODE STATISTICS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Key takeaways:\n",
      "   ‚Ä¢ Total unique ECO codes: 458\n",
      "   ‚Ä¢ Most common family: B (1,048,720 entries)\n",
      "   ‚Ä¢ Most common ECO: B00 (201,371 entries)\n",
      "   ‚Ä¢ ECO codes appear in all splits (good for training)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: ECO Code Statistics (no mutations, just exploration)\n",
    "\n",
    "print(\"STEP 4: ECO CODE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(clean_data):,}\")\n",
    "print(f\"   ‚Ä¢ Missing ECO values: {clean_data['eco'].isna().sum()}\")\n",
    "\n",
    "# ECO value counts\n",
    "eco_counts = clean_data['eco'].value_counts().sort_index()\n",
    "print(f\"\\n2Ô∏è‚É£  Distribution of entries per ECO code:\")\n",
    "print(f\"   ‚Ä¢ Mean entries per ECO: {eco_counts.mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Median entries per ECO: {eco_counts.median():.1f}\")\n",
    "print(f\"   ‚Ä¢ Min entries: {eco_counts.min()}\")\n",
    "print(f\"   ‚Ä¢ Max entries: {eco_counts.max()}\")\n",
    "print(f\"   ‚Ä¢ Std: {eco_counts.std():.1f}\")\n",
    "\n",
    "# ECO by first letter (family)\n",
    "print(f\"\\n3Ô∏è‚É£  ECO families (by first letter):\")\n",
    "eco_families = clean_data['eco'].str[0].value_counts().sort_index()\n",
    "print(f\"\\n   {'Family':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "for family, count in eco_families.items():\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.4)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {family:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Top 20 most common ECO codes\n",
    "print(f\"\\n4Ô∏è‚É£  Top 20 most common ECO codes:\")\n",
    "top_eco = clean_data['eco'].value_counts().head(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*12} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(top_eco.items(), 1):\n",
    "    pct = 100 * count / len(clean_data)\n",
    "    bar_length = int(pct * 0.3)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "\n",
    "# Bottom 20 least common ECO codes\n",
    "print(f\"\\n5Ô∏è‚É£  Bottom 20 least common ECO codes:\")\n",
    "bottom_eco = clean_data['eco'].value_counts().tail(20)\n",
    "print(f\"\\n   {'Rank':<6} {'ECO':<6} {'Count':<10} {'Visual'}\")\n",
    "print(f\"   {'-'*6} {'-'*6} {'-'*10} {'-'*30}\")\n",
    "for i, (eco, count) in enumerate(bottom_eco.items(), 1):\n",
    "    bar_length = min(count, 30)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f\"   {i:<6} {eco:<6} {count:>7,}    {bar}\")\n",
    "\n",
    "# Check for any unusual ECO codes\n",
    "print(f\"\\n7Ô∏è‚É£  Sample of ECO codes:\")\n",
    "sample_eco = clean_data['eco'].drop_duplicates().sample(min(20, clean_data['eco'].nunique()), random_state=42).sort_values()\n",
    "print(f\"   {', '.join(sample_eco.values)}\")\n",
    "\n",
    "# ECO statistics by split\n",
    "print(f\"\\n8Ô∏è‚É£  ECO distribution across splits:\")\n",
    "print(f\"\\n   Train split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_train['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_train):,}\")\n",
    "\n",
    "print(f\"\\n   Validation split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_val['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_val):,}\")\n",
    "val_new_eco = set(X_val['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(val_new_eco)}\")\n",
    "\n",
    "print(f\"\\n   Test split:\")\n",
    "print(f\"   ‚Ä¢ Unique ECO codes: {X_test['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Total entries: {len(X_test):,}\")\n",
    "test_new_eco = set(X_test['eco'].unique()) - set(X_train['eco'].unique())\n",
    "print(f\"   ‚Ä¢ ECO codes not in train: {len(test_new_eco)}\")\n",
    "\n",
    "# Average score by ECO code (top 10 and bottom 10)\n",
    "print(f\"\\n9Ô∏è‚É£  Average score by ECO code:\")\n",
    "eco_scores = clean_data.groupby('eco')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\n   Top 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.head(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(f\"\\n   Bottom 10 ECO codes by average score:\")\n",
    "print(f\"\\n   {'ECO':<6} {'Avg Score':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_scores.tail(10).iterrows():\n",
    "    print(f\"   {eco:<6} {row['mean']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "# ECO codes with high variance in scores\n",
    "print(f\"\\nüîü  ECO codes with highest score variance (may indicate difficulty):\")\n",
    "eco_variance = clean_data.groupby('eco')['score'].agg(['var', 'std', 'count']).sort_values('var', ascending=False).head(10)\n",
    "print(f\"\\n   {'ECO':<6} {'Variance':<12} {'Std Dev':<12} {'Count':<10}\")\n",
    "print(f\"   {'-'*6} {'-'*12} {'-'*12} {'-'*10}\")\n",
    "for eco, row in eco_variance.iterrows():\n",
    "    print(f\"   {eco:<6} {row['var']:<12.4f} {row['std']:<12.4f} {int(row['count']):>7,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ECO CODE STATISTICS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Key takeaways:\")\n",
    "print(f\"   ‚Ä¢ Total unique ECO codes: {clean_data['eco'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Most common family: {eco_families.idxmax()} ({eco_families.max():,} entries)\")\n",
    "print(f\"   ‚Ä¢ Most common ECO: {top_eco.index[0]} ({top_eco.iloc[0]:,} entries)\")\n",
    "print(f\"   ‚Ä¢ ECO codes appear in all splits (good for training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbad18",
   "metadata": {},
   "source": [
    "## 4b. Create ECO Side Information\n",
    "\n",
    "**Why ECO is Side Information:**\n",
    "- ECO codes describe **opening characteristics**, not individual player-opening interactions\n",
    "- Similar to how player ratings describe players, ECO describes openings\n",
    "- Each opening has ONE ECO code (not per player-opening pair)\n",
    "\n",
    "**Implementation Strategy:**\n",
    "- Split ECO codes into two categorical features:\n",
    "  - `eco_letter`: A, B, C, D, or E ‚Üí encoded as integers 0-4\n",
    "  - `eco_number`: The numeric part (e.g., \"21\" from \"C21\") ‚Üí encoded as sequential integers\n",
    "- Store in a separate `opening_side_info` lookup table (indexed by opening_id)\n",
    "- Remove `eco` from train/test/val DataFrames (it's not interaction data)\n",
    "- During training, model will lookup opening_id ‚Üí (eco_letter, eco_number)\n",
    "\n",
    "**Why Split ECO into Letter and Number:**\n",
    "- ECO families (A-E) represent fundamentally different opening types:\n",
    "  - **A**: Flank openings (English, R√©ti, Bird's, etc.)\n",
    "  - **B**: Semi-Open games (Sicilian, French, Caro-Kann, etc.)\n",
    "  - **C**: Open games (King's pawn openings, Spanish, Italian, etc.)\n",
    "  - **D**: Closed games (Queen's Gambit variations)\n",
    "  - **E**: Indian defenses (King's Indian, Nimzo-Indian, etc.)\n",
    "- Numbers within each family represent variations (C20-C29, C30-C39, etc.)\n",
    "- Model can learn separate embeddings for family vs variation\n",
    "\n",
    "**Categorical Encoding:**\n",
    "- Both features will be treated as categorical (not ordinal)\n",
    "- Higher numbers don't mean \"better\" openings\n",
    "- Model will learn embedding vectors for each category\n",
    "- This allows the model to capture non-linear relationships between ECO codes and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64124475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4B: CREATE ECO SIDE INFORMATION\n",
      "============================================================\n",
      "   ‚úì Extracted 2,717 unique openings\n",
      "   ‚úì Extracted 2,717 unique openings\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['B' 'C' 'A' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (578 openings)\n",
      "      'B' ‚Üí 1 (590 openings)\n",
      "      'C' ‚Üí 2 (894 openings)\n",
      "      'D' ‚Üí 3 (419 openings)\n",
      "      'E' ‚Üí 4 (236 openings)\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 290 openings\n",
      "      '40' (‚Üí 40):  98 openings\n",
      "      '20' (‚Üí 20):  70 openings\n",
      "      '01' (‚Üí  1):  70 openings\n",
      "      '42' (‚Üí 42):  68 openings\n",
      "      '02' (‚Üí  2):  67 openings\n",
      "      '45' (‚Üí 45):  64 openings\n",
      "      '10' (‚Üí 10):  63 openings\n",
      "      '21' (‚Üí 21):  61 openings\n",
      "      '15' (‚Üí 15):  58 openings\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2717, 2)\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ X_train before: (2173275, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (434655, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (289770, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   2651         4               62              E62               \n",
      "   583          1               1               B01               \n",
      "   2285         4               47              E47               \n",
      "   1776         2               80              C80               \n",
      "   199          0               12              A12               \n",
      "   461          0               69              A69               \n",
      "   184          0               10              A10               \n",
      "   255          0               23              A23               \n",
      "   2686         0               0               A00               \n",
      "   2443         3               0               D00               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            578     21.27%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            590     21.72%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            894     32.90%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            419     15.42%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            236      8.69%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2717, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 2,173,275 rows, 3 features\n",
      "   ‚Ä¢ X_val: 434,655 rows, 3 features\n",
      "   ‚Ä¢ X_test: 289,770 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,717 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n",
      "   ‚úì Extracted letter and number components\n",
      "   ‚Ä¢ Unique letters: ['B' 'C' 'A' 'D' 'E']\n",
      "   ‚Ä¢ Unique numbers: 100\n",
      "   ‚úì Letter encoding created:\n",
      "      'A' ‚Üí 0 (578 openings)\n",
      "      'B' ‚Üí 1 (590 openings)\n",
      "      'C' ‚Üí 2 (894 openings)\n",
      "      'D' ‚Üí 3 (419 openings)\n",
      "      'E' ‚Üí 4 (236 openings)\n",
      "   ‚úì Number encoding created:\n",
      "      100 unique numbers mapped to [0, 99]\n",
      "      Range: '00' ‚Üí 0, ..., '99' ‚Üí 99\n",
      "\n",
      "   Distribution of ECO numbers (top 10):\n",
      "      '00' (‚Üí  0): 290 openings\n",
      "      '40' (‚Üí 40):  98 openings\n",
      "      '20' (‚Üí 20):  70 openings\n",
      "      '01' (‚Üí  1):  70 openings\n",
      "      '42' (‚Üí 42):  68 openings\n",
      "      '02' (‚Üí  2):  67 openings\n",
      "      '45' (‚Üí 45):  64 openings\n",
      "      '10' (‚Üí 10):  63 openings\n",
      "      '21' (‚Üí 21):  61 openings\n",
      "      '15' (‚Üí 15):  58 openings\n",
      "   ‚úì Created opening_side_info\n",
      "      ‚Ä¢ Shape: (2717, 2)\n",
      "      ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ X_train before: (2173275, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_val before: (434655, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "   ‚Ä¢ X_test before: (289770, 4), columns: ['player_id', 'opening_id', 'eco', 'confidence']\n",
      "\n",
      "8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\n",
      "\n",
      "   Opening ID   Letter (Cat)    Number (Cat)    Reconstructed ECO \n",
      "   ------------ --------------- --------------- ------------------\n",
      "   2651         4               62              E62               \n",
      "   583          1               1               B01               \n",
      "   2285         4               47              E47               \n",
      "   1776         2               80              C80               \n",
      "   199          0               12              A12               \n",
      "   461          0               69              A69               \n",
      "   184          0               10              A10               \n",
      "   255          0               23              A23               \n",
      "   2686         0               0               A00               \n",
      "   2443         3               0               D00               \n",
      "\n",
      "9Ô∏è‚É£  ECO family distribution in opening_side_info:\n",
      "\n",
      "   Encoded  Letter   Count      Percentage   Visual\n",
      "   -------- -------- ---------- ------------ ----------------------------------------\n",
      "   0        A            578     21.27%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   1        B            590     21.72%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   2        C            894     32.90%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   3        D            419     15.42%      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   4        E            236      8.69%      ‚ñà‚ñà‚ñà\n",
      "\n",
      "Created: opening_side_info\n",
      "   ‚Ä¢ Shape: (2717, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "üìä Data structure summary:\n",
      "   ‚Ä¢ X_train: 2,173,275 rows, 3 features\n",
      "   ‚Ä¢ X_val: 434,655 rows, 3 features\n",
      "   ‚Ä¢ X_test: 289,770 rows, 3 features\n",
      "   ‚Ä¢ opening_side_info: 2,717 openings (one per opening)\n",
      "   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\n",
      "   ‚Ä¢ eco_letter_to_int: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
      "   ‚Ä¢ eco_number_to_int: (dict with 100 entries)\n",
      "\n",
      "   You'll need them to encode ECO codes for new openings at inference time.\n",
      "\n",
      "‚úì Created reverse mappings for ECO decoding:\n",
      "   ‚Ä¢ eco_int_to_letter: 5 entries\n",
      "   ‚Ä¢ eco_int_to_number: 100 entries\n"
     ]
    }
   ],
   "source": [
    "# 4b. Create ECO side information and remove ECO from train/test/val DataFrames\n",
    "\n",
    "# Check if ECO processing has already been done\n",
    "if 'eco' not in X_train.columns and 'opening_side_info' in globals():\n",
    "    print(\"‚è≠Ô∏è  SKIPPING STEP 4B: ECO SIDE INFORMATION CREATION\")\n",
    "    print(\"\\n‚úì ECO column already removed from train/test/val data\")\n",
    "    print(f\"\\nOpening side info shape: {opening_side_info.shape}\")\n",
    "    print(f\"Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "    print(\"opening side info:\", opening_side_info.head().to_string())\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä Existing ECO encoding statistics:\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_letter values: {opening_side_info['eco_letter_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Unique eco_number values: {opening_side_info['eco_number_cat'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ eco_letter range: [{opening_side_info['eco_letter_cat'].min()}, {opening_side_info['eco_letter_cat'].max()}]\")\n",
    "    print(f\"   ‚Ä¢ eco_number range: [{opening_side_info['eco_number_cat'].min()}, {opening_side_info['eco_number_cat'].max()}]\")\n",
    "    \n",
    "    print(f\"\\nüìã Sample of existing ECO encoding:\")\n",
    "    sample_data = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "    print(sample_data.to_string())\n",
    "else:\n",
    "    def create_eco_side_information(clean_data_df, X_train_df, X_val_df, X_test_df):\n",
    "        \"\"\"\n",
    "        Create ECO side information table and remove ECO from train/test/val DataFrames.\n",
    "        \n",
    "        ECO codes are opening-level features, not player-opening interaction features.\n",
    "        We split each ECO code (e.g., \"C21\") into:\n",
    "        - eco_letter: The letter part (A, B, C, D, or E)\n",
    "        - eco_number: The numeric part (e.g., 21)\n",
    "        \n",
    "        Both are encoded as sequential integers for use as categorical features in embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        clean_data_df : pd.DataFrame\n",
    "            Full cleaned data with opening_id and eco columns\n",
    "        X_train_df, X_val_df, X_test_df : pd.DataFrame\n",
    "            Train/val/test feature DataFrames (will be modified to remove 'eco')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 4B: CREATE ECO SIDE INFORMATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Extract unique opening ‚Üí ECO mappings\n",
    "        opening_eco_map = clean_data_df[['opening_id', 'eco']].drop_duplicates().set_index('opening_id')\n",
    "        print(f\"   ‚úì Extracted {len(opening_eco_map):,} unique openings\")\n",
    "        \n",
    "        # Verify one-to-one mapping\n",
    "        eco_per_opening = clean_data_df.groupby('opening_id')['eco'].nunique()\n",
    "        if (eco_per_opening > 1).any():\n",
    "            problematic = eco_per_opening[eco_per_opening > 1]\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(problematic)} openings have multiple ECO codes!\")\n",
    "            print(f\"   Problematic opening IDs: {problematic.index.tolist()[:10]}...\")\n",
    "        \n",
    "        # Split ECO into letter and number components\n",
    "        opening_eco_map['eco_letter_str'] = opening_eco_map['eco'].str[0]  # First character (A-E)\n",
    "        opening_eco_map['eco_number_str'] = opening_eco_map['eco'].str[1:]  # Remaining characters (numeric)\n",
    "        \n",
    "        print(f\"   ‚úì Extracted letter and number components\")\n",
    "        print(f\"   ‚Ä¢ Unique letters: {opening_eco_map['eco_letter_str'].unique()}\")\n",
    "        print(f\"   ‚Ä¢ Unique numbers: {opening_eco_map['eco_number_str'].nunique()}\")\n",
    "        \n",
    "        # Create encoding mappings for eco_letter (A-E ‚Üí 0-4)\n",
    "        unique_letters = sorted(opening_eco_map['eco_letter_str'].unique())\n",
    "        eco_letter_to_int = {letter: idx for idx, letter in enumerate(unique_letters)}\n",
    "        eco_int_to_letter = {idx: letter for letter, idx in eco_letter_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_letter_cat'] = opening_eco_map['eco_letter_str'].map(eco_letter_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Letter encoding created:\")\n",
    "        for letter, idx in sorted(eco_letter_to_int.items()):\n",
    "            count = (opening_eco_map['eco_letter_str'] == letter).sum()\n",
    "            print(f\"      '{letter}' ‚Üí {idx} ({count:,} openings)\")\n",
    "        \n",
    "        # Create encoding mappings for eco_number (00-99 ‚Üí sequential integers)\n",
    "        unique_numbers = sorted(opening_eco_map['eco_number_str'].unique())\n",
    "        eco_number_to_int = {num: idx for idx, num in enumerate(unique_numbers)}\n",
    "        eco_int_to_number = {idx: num for num, idx in eco_number_to_int.items()}\n",
    "        \n",
    "        opening_eco_map['eco_number_cat'] = opening_eco_map['eco_number_str'].map(eco_number_to_int)\n",
    "        \n",
    "        print(f\"   ‚úì Number encoding created:\")\n",
    "        print(f\"      {len(unique_numbers)} unique numbers mapped to [0, {len(unique_numbers)-1}]\")\n",
    "        print(f\"      Range: '{unique_numbers[0]}' ‚Üí 0, ..., '{unique_numbers[-1]}' ‚Üí {len(unique_numbers)-1}\")\n",
    "        \n",
    "        # Show distribution of numbers\n",
    "        print(f\"\\n   Distribution of ECO numbers (top 10):\")\n",
    "        number_counts = opening_eco_map['eco_number_str'].value_counts().head(10)\n",
    "        for num, count in number_counts.items():\n",
    "            encoded = eco_number_to_int[num]\n",
    "            print(f\"      '{num}' (‚Üí {encoded:>2}): {count:>3} openings\")\n",
    "        \n",
    "        # Create final opening_side_info table\n",
    "        # Only keep the encoded categorical columns and rename them for clarity\n",
    "        opening_side_info = opening_eco_map[['eco_letter_cat', 'eco_number_cat']].copy()\n",
    "\n",
    "        print(f\"   ‚úì Created opening_side_info\")\n",
    "        print(f\"      ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"      ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "\n",
    "        # Verify all openings in train/val/test have ECO info\n",
    "        all_openings = set(X_train_df['opening_id'].unique()) | \\\n",
    "                       set(X_val_df['opening_id'].unique()) | \\\n",
    "                       set(X_test_df['opening_id'].unique())\n",
    "        \n",
    "        missing_openings = all_openings - set(opening_side_info.index)\n",
    "        if len(missing_openings) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits are missing ECO info!\")\n",
    "            print(f\"   Missing opening IDs: {sorted(list(missing_openings))[:10]}...\")\n",
    "        \n",
    "        # Remove ECO from train/val/test DataFrames\n",
    "        print(f\"   ‚Ä¢ X_train before: {X_train_df.shape}, columns: {list(X_train_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_val before: {X_val_df.shape}, columns: {list(X_val_df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ X_test before: {X_test_df.shape}, columns: {list(X_test_df.columns)}\")\n",
    "        \n",
    "        X_train_clean = X_train_df.drop(columns=['eco'])\n",
    "        X_val_clean = X_val_df.drop(columns=['eco'])\n",
    "        X_test_clean = X_test_df.drop(columns=['eco'])\n",
    "        \n",
    "        # Sample data showing the transformation\n",
    "        print(f\"\\n8Ô∏è‚É£  Sample of ECO encoding (10 random openings):\")\n",
    "        sample_openings = opening_side_info.sample(min(10, len(opening_side_info)), random_state=42)\n",
    "        \n",
    "        # For display purposes, reconstruct original values from the categorical encodings\n",
    "        print(f\"\\n   {'Opening ID':<12} {'Letter (Cat)':<15} {'Number (Cat)':<15} {'Reconstructed ECO':<18}\")\n",
    "        print(f\"   {'-'*12} {'-'*15} {'-'*15} {'-'*18}\")\n",
    "        for idx, row in sample_openings.iterrows():\n",
    "            letter_str = eco_int_to_letter[row['eco_letter_cat']]\n",
    "            number_str = eco_int_to_number[row['eco_number_cat']]\n",
    "            reconstructed = f\"{letter_str}{number_str}\"\n",
    "            print(f\"   {idx:<12} {row['eco_letter_cat']:<15} {row['eco_number_cat']:<15} {reconstructed:<18}\")\n",
    "        \n",
    "        # Show ECO family distribution\n",
    "        print(f\"\\n9Ô∏è‚É£  ECO family distribution in opening_side_info:\")\n",
    "        letter_dist = opening_side_info['eco_letter_cat'].value_counts().sort_index()\n",
    "        print(f\"\\n   {'Encoded':<8} {'Letter':<8} {'Count':<10} {'Percentage':<12} {'Visual'}\")\n",
    "        print(f\"   {'-'*8} {'-'*8} {'-'*10} {'-'*12} {'-'*40}\")\n",
    "        for encoded_val, count in letter_dist.items():\n",
    "            letter = eco_int_to_letter[encoded_val]\n",
    "            pct = 100 * count / len(opening_side_info)\n",
    "            bar_length = int(pct * 0.4)\n",
    "            bar = '‚ñà' * bar_length\n",
    "            print(f\"   {encoded_val:<8} {letter:<8} {count:>7,}    {pct:>6.2f}%      {bar}\")\n",
    "        \n",
    "        print(f\"\\nCreated: opening_side_info\")\n",
    "        print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Data structure summary:\")\n",
    "        print(f\"   ‚Ä¢ X_train: {X_train_clean.shape[0]:,} rows, {X_train_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_val: {X_val_clean.shape[0]:,} rows, {X_val_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ X_test: {X_test_clean.shape[0]:,} rows, {X_test_clean.shape[1]} features\")\n",
    "        print(f\"   ‚Ä¢ opening_side_info: {len(opening_side_info):,} openings (one per opening)\")\n",
    "        print(f\"   ‚Ä¢ ECO storage: ONE entry per opening (not duplicated per interaction)\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  CRITICAL: Save these mappings for inference!\")\n",
    "        print(f\"   ‚Ä¢ eco_letter_to_int: {eco_letter_to_int}\")\n",
    "        print(f\"   ‚Ä¢ eco_number_to_int: (dict with {len(eco_number_to_int)} entries)\")\n",
    "        print(f\"\\n   You'll need them to encode ECO codes for new openings at inference time.\")\n",
    "\n",
    "        return opening_side_info, eco_letter_to_int, eco_number_to_int, X_train_clean, X_val_clean, X_test_clean\n",
    "    \n",
    "    # Call the function\n",
    "    opening_side_info, eco_letter_map, eco_number_map, X_train, X_val, X_test = create_eco_side_information(\n",
    "        clean_data, X_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Create reverse mappings for decoding (needed for verification and debugging)\n",
    "    eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "    eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "    print(f\"\\n‚úì Created reverse mappings for ECO decoding:\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_letter: {len(eco_int_to_letter)} entries\")\n",
    "    print(f\"   ‚Ä¢ eco_int_to_number: {len(eco_int_to_number)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4b4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICATION: FINAL DATA STRUCTURE\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\n",
      "\n",
      "   X_train:\n",
      "   ‚Ä¢ Shape: (2173275, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "   ‚Ä¢ Sample:\n",
      "         player_id  opening_id  confidence\n",
      "679380       11013        1819    0.748744\n",
      "1544705      25322         398    0.612403\n",
      "2323873      38086        2430    0.305556\n",
      "\n",
      "   X_val:\n",
      "   ‚Ä¢ Shape: (434655, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "   X_test:\n",
      "   ‚Ä¢ Shape: (289770, 3)\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'confidence']\n",
      "\n",
      "2Ô∏è‚É£  Side Information Tables:\n",
      "\n",
      "   player_side_info (indexed by player_id):\n",
      "   ‚Ä¢ Shape: (48468, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Sample:\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.608678\n",
      "1          1.058287\n",
      "2          0.560506\n",
      "\n",
      "   opening_side_info (indexed by opening_id):\n",
      "   ‚Ä¢ Shape: (2717, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Sample:\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "531                      1               0\n",
      "565                      1               0\n",
      "572                      1               0\n",
      "\n",
      "3Ô∏è‚É£  Encoding Mappings (for inference):\n",
      "\n",
      "   eco_letter_map:\n",
      "      'A' ‚Üí 0\n",
      "      'B' ‚Üí 1\n",
      "      'C' ‚Üí 2\n",
      "      'D' ‚Üí 3\n",
      "      'E' ‚Üí 4\n",
      "\n",
      "   eco_number_map (first 10):\n",
      "      '00' ‚Üí 0\n",
      "      '01' ‚Üí 1\n",
      "      '02' ‚Üí 2\n",
      "      '03' ‚Üí 3\n",
      "      '04' ‚Üí 4\n",
      "      '05' ‚Üí 5\n",
      "      '06' ‚Üí 6\n",
      "      '07' ‚Üí 7\n",
      "      '08' ‚Üí 8\n",
      "      '09' ‚Üí 9\n",
      "      ... (100 total)\n",
      "\n",
      "4Ô∏è‚É£  Example: Lookup flow for a random train sample:\n",
      "\n",
      "   Sample interaction:\n",
      "   ‚Ä¢ player_id: 18357.0\n",
      "   ‚Ä¢ opening_id: 653.0\n",
      "   ‚Ä¢ confidence: 0.5283\n",
      "\n",
      "   Player side info lookup:\n",
      "   ‚Ä¢ rating_z: -0.3307\n",
      "\n",
      "   Opening side info lookup:\n",
      "   ‚Ä¢ eco_letter_cat: 1 (letter: B)\n",
      "   ‚Ä¢ eco_number_cat: 6 (number: 06)\n",
      "\n",
      "============================================================\n",
      "‚úÖ VERIFICATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Ready for PyTorch tensor conversion:\n",
      "   ‚Ä¢ Features: player_id, opening_id, confidence\n",
      "   ‚Ä¢ Target: score (in y_train, y_val, y_test)\n",
      "   ‚Ä¢ Player side info: rating_z\n",
      "   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\n",
      "\n",
      "   All ECO and rating data is now properly separated as side information!\n",
      "\n",
      "   Side info tables are clean - only contain necessary model inputs!\n",
      "\n",
      "   Sample interaction:\n",
      "   ‚Ä¢ player_id: 18357.0\n",
      "   ‚Ä¢ opening_id: 653.0\n",
      "   ‚Ä¢ confidence: 0.5283\n",
      "\n",
      "   Player side info lookup:\n",
      "   ‚Ä¢ rating_z: -0.3307\n",
      "\n",
      "   Opening side info lookup:\n",
      "   ‚Ä¢ eco_letter_cat: 1 (letter: B)\n",
      "   ‚Ä¢ eco_number_cat: 6 (number: 06)\n",
      "\n",
      "============================================================\n",
      "‚úÖ VERIFICATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üì¶ Ready for PyTorch tensor conversion:\n",
      "   ‚Ä¢ Features: player_id, opening_id, confidence\n",
      "   ‚Ä¢ Target: score (in y_train, y_val, y_test)\n",
      "   ‚Ä¢ Player side info: rating_z\n",
      "   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\n",
      "\n",
      "   All ECO and rating data is now properly separated as side information!\n",
      "\n",
      "   Side info tables are clean - only contain necessary model inputs!\n"
     ]
    }
   ],
   "source": [
    "# Verify final data structure after ECO processing\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION: FINAL DATA STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Train/Val/Test DataFrames (ECO removed):\")\n",
    "print(f\"\\n   X_train:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_train.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_train.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(X_train.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   X_val:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_val.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_val.columns)}\")\n",
    "\n",
    "print(f\"\\n   X_test:\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_test.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(X_test.columns)}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Side Information Tables:\")\n",
    "\n",
    "print(f\"\\n   player_side_info (indexed by player_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(player_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n   opening_side_info (indexed by opening_id):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Sample:\")\n",
    "print(opening_side_info.head(3).to_string())\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Encoding Mappings (for inference):\")\n",
    "print(f\"\\n   eco_letter_map:\")\n",
    "for k, v in sorted(eco_letter_map.items()):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "\n",
    "print(f\"\\n   eco_number_map (first 10):\")\n",
    "for i, (k, v) in enumerate(sorted(eco_number_map.items())[:10]):\n",
    "    print(f\"      '{k}' ‚Üí {v}\")\n",
    "print(f\"      ... ({len(eco_number_map)} total)\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£  Example: Lookup flow for a random train sample:\")\n",
    "sample = X_train.sample(1, random_state=42).iloc[0]\n",
    "player_id = sample['player_id']\n",
    "opening_id = sample['opening_id']\n",
    "\n",
    "print(f\"\\n   Sample interaction:\")\n",
    "print(f\"   ‚Ä¢ player_id: {player_id}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {opening_id}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Player side info lookup:\")\n",
    "player_info = player_side_info.loc[player_id]\n",
    "print(f\"   ‚Ä¢ rating_z: {player_info['rating_z']:.4f}\")\n",
    "\n",
    "print(f\"\\n   Opening side info lookup:\")\n",
    "opening_info = opening_side_info.loc[opening_id]\n",
    "print(f\"   ‚Ä¢ eco_letter_cat: {opening_info['eco_letter_cat']} (letter: {eco_int_to_letter[opening_info['eco_letter_cat']]})\")\n",
    "print(f\"   ‚Ä¢ eco_number_cat: {opening_info['eco_number_cat']} (number: {eco_int_to_number[opening_info['eco_number_cat']]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüì¶ Ready for PyTorch tensor conversion:\")\n",
    "print(f\"   ‚Ä¢ Features: player_id, opening_id, confidence\")\n",
    "print(f\"   ‚Ä¢ Target: score (in y_train, y_val, y_test)\")\n",
    "print(f\"   ‚Ä¢ Player side info: rating_z\")\n",
    "print(f\"   ‚Ä¢ Opening side info: eco_letter_cat, eco_number_cat\")\n",
    "print(f\"\\n   All ECO and rating data is now properly separated as side information!\")\n",
    "print(f\"\\n   Side info tables are clean - only contain necessary model inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43752aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "============================================================\n",
      "STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\n",
      "============================================================\n",
      "============================================================\n",
      "STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\n",
      "============================================================\n",
      "Final DataFrame for correlation created.\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'score', 'confidence', 'rating_z', 'eco_letter_cat', 'eco_number_cat']\n",
      "Final DataFrame for correlation created.\n",
      "   ‚Ä¢ Columns: ['player_id', 'opening_id', 'score', 'confidence', 'rating_z', 'eco_letter_cat', 'eco_number_cat']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRELATIONS WITH TARGET VARIABLE (score)\n",
      "================================================================================\n",
      "eco_letter_cat    0.116399\n",
      "confidence        0.097993\n",
      "eco_number_cat    0.055865\n",
      "rating_z          0.042897\n"
     ]
    }
   ],
   "source": [
    "# Step 4c: Correlation Analysis of Processed Data\n",
    "\n",
    "%pip install seaborn --quiet\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4C: CORRELATION ANALYSIS OF PROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Start with the main interaction data (already has remapped IDs, adjusted score, confidence)\n",
    "corr_df = clean_data[[\"player_id\", \"opening_id\", \"score\", \"confidence\"]].copy()\n",
    "\n",
    "# Merge player side information (rating_z)\n",
    "# player_side_info is indexed by the remapped player_id\n",
    "corr_df = corr_df.merge(\n",
    "    player_side_info[[\"rating_z\"]], left_on=\"player_id\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# Merge opening side information (eco categories)\n",
    "# opening_side_info is indexed by the remapped opening_id\n",
    "corr_df = corr_df.merge(\n",
    "    opening_side_info[[\"eco_letter_cat\", \"eco_number_cat\"]],\n",
    "    left_on=\"opening_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(f\"Final DataFrame for correlation created.\")\n",
    "print(f\"   ‚Ä¢ Columns: {corr_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "correlation_matrix = corr_df.corr().drop(columns=['player_id', 'opening_id']).drop(index=['player_id', 'opening_id'])\n",
    "\n",
    "\n",
    "# 3. Visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Model Features\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 4. Analyze correlations with the target variable 'score'\n",
    "print(\"CORRELATIONS WITH TARGET VARIABLE (score)\")\n",
    "print(\"=\" * 80)\n",
    "score_correlations = (\n",
    "    correlation_matrix[\"score\"].drop(\"score\").sort_values(ascending=False)\n",
    ")\n",
    "print(score_correlations.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831d7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "017515d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\n",
      "====================================================================================================\n",
      "   ‚Ä¢ Converting 88 NEW opening IDs to OLD database IDs for query\n",
      "   ‚Ä¢ Example: NEW ID 653 ‚Üí OLD ID 838\n",
      "#    Player   Opening   ECO (DB)   Reconstructed Match  Opening Name                                      \n",
      "====================================================================================================\n",
      "1    18357    653       B06        B06           ‚úì      Modern Defense                                    \n",
      "2    45188    668       B06        B06           ‚úì      Modern Defense: Two Knights Variation             \n",
      "3    32831    653       B06        B06           ‚úì      Modern Defense                                    \n",
      "4    19492    1819      D00        D00           ‚úì      Blackmar-Diemer Gambit                            \n",
      "5    14696    849       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Dragon         \n",
      "6    24876    1346      C34        C34           ‚úì      King's Gambit Accepted: Fischer Defense           \n",
      "7    6247     729       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Botvinn...  \n",
      "8    37318    1367      C37        C37           ‚úì      King's Gambit Accepted: Muzio Gambit, Wild Mu...  \n",
      "9    30476    999       B75        B75           ‚úì      Sicilian Defense: Dragon Variation, Yugoslav ...  \n",
      "10   24386    578       B00        B00           ‚úì      Ware Defense                                      \n",
      "11   14320    849       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Dragon         \n",
      "12   37172    1484      C42        C42           ‚úì      Russian Game: Three Knights Game                  \n",
      "13   30332    1514      C44        C44           ‚úì      Scotch Game                                       \n",
      "14   10852    531       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "15   24176    2452      B01        B01           ‚úì      Scandinavian Defense: Valencian Variation         \n",
      "16   21242    1107      C07        C07           ‚úì      French Defense: Tarrasch Variation, Open Syst...  \n",
      "17   30324    1217      C22        C22           ‚úì      Center Game: Normal Variation                     \n",
      "18   44575    2451      D00        D00           ‚úì      Queen's Pawn Game: Accelerated London System      \n",
      "19   1855     413       A53        A53           ‚úì      Old Indian Defense: Czech Variation               \n",
      "20   2923     2351      E76        E76           ‚úì      King's Indian Defense: Four Pawns Attack          \n",
      "21   26910    1075      C00        C00           ‚úì      Rat Defense: Small Center Defense                 \n",
      "22   43852    1487      C43        C43           ‚úì      Russian Game: Modern Attack                       \n",
      "23   35287    1287      C30        C30           ‚úì      King's Gambit Declined: Keene Defense             \n",
      "24   9426     756       B14        B14           ‚úì      Caro-Kann Defense: Panov Attack, Main Line        \n",
      "25   9628     1266      C27        C27           ‚úì      Vienna Game: Frankenstein-Dracula Variation       \n",
      "26   10758    1295      C30        C30           ‚úì      King's Gambit Declined: Queen's Knight Defense    \n",
      "27   21121    1429      C41        C41           ‚úì      Philidor Defense                                  \n",
      "28   34398    1520      C44        C44           ‚úì      Scotch Game: G√∂ring Gambit, Double Pawn Sacri...  \n",
      "29   31677    2462      A40        A40           ‚úì      Queen's Pawn Game: Modern Defense                 \n",
      "30   33329    2714      C11        C11           ‚úì      French Defense: Classical Variation, Shirov-A...  \n",
      "31   12141    233       A16        A16           ‚úì      English Opening: Anglo-Indian Defense, Queen'...  \n",
      "32   19504    2462      A40        A40           ‚úì      Queen's Pawn Game: Modern Defense                 \n",
      "33   21581    597       B01        B01           ‚úì      Scandinavian Defense: Marshall Variation          \n",
      "34   17790    2431      B07        B07           ‚úì      King's Pawn Game: Mar√≥czy Defense                 \n",
      "35   41614    594       B01        B01           ‚úì      Scandinavian Defense: Main Line                   \n",
      "36   12918    525       B00        B00           ‚úì      Goldsmith Defense                                 \n",
      "37   24121    774       B18        B18           ‚úì      Caro-Kann Defense: Classical Variation            \n",
      "38   3186     338       A43        A43           ‚úì      Benoni Defense: Benoni-Indian Defense             \n",
      "39   36331    2613      B15        B15           ‚úì      Caro-Kann Defense: Alien Gambit                   \n",
      "40   32073    1052      C00        C00           ‚úì      French Defense: Chigorin Variation                \n",
      "41   24690    1058      C00        C00           ‚úì      French Defense: Knight Variation                  \n",
      "42   20085    72        A00        A00           ‚úì      Valencia Opening                                  \n",
      "43   8218     1279      C29        C29           ‚úì      Vienna Game: Vienna Gambit, Paulsen Attack        \n",
      "44   29220    1562      C47        C47           ‚úì      Four Knights Game                                 \n",
      "45   34933    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "46   33805    1282      C30        C30           ‚úì      King's Gambit                                     \n",
      "47   40587    1567      C47        C47           ‚úì      Four Knights Game: Scotch Variation               \n",
      "48   18161    1484      C42        C42           ‚úì      Russian Game: Three Knights Game                  \n",
      "49   40123    1072      C00        C00           ‚úì      French Defense: Two Knights Variation             \n",
      "50   40141    653       B06        B06           ‚úì      Modern Defense                                    \n",
      "51   31561    1849      D00        D00           ‚úì      Queen's Pawn Game: Chigorin Variation, Anti-V...  \n",
      "52   8072     1680      C60        C60           ‚úì      Ruy Lopez: Cozio Defense                          \n",
      "53   26702    598       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "54   31445    2456      C56        C56           ‚úì      Italian Game: Two Knights Defense, Open Varia...  \n",
      "55   3243     840       B23        B23           ‚úì      Sicilian Defense: Grand Prix Attack               \n",
      "56   48370    598       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "57   22560    2452      B01        B01           ‚úì      Scandinavian Defense: Valencian Variation         \n",
      "58   20609    693       B08        B08           ‚úì      Pirc Defense: Classical Variation                 \n",
      "59   39288    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "60   27800    711       B10        B10           ‚úì      Caro-Kann Defense: Breyer Variation               \n",
      "61   10655    1561      C46        C46           ‚úì      Three Knights Opening: Winawer Defense            \n",
      "62   1133     575       B00        B00           ‚úì      St. George Defense                                \n",
      "63   12679    941       B43        B43           ‚úì      Sicilian Defense: Kan Variation, Wing Attack      \n",
      "64   39471    1100      C05        C05           ‚úì      French Defense: Tarrasch Variation, Closed Va...  \n",
      "65   3784     783       B20        B20           ‚úì      Sicilian Defense: Bowdler Attack                  \n",
      "66   36213    1557      C46        C46           ‚úì      Three Knights Opening                             \n",
      "67   32988    1588      C50        C50           ‚úì      Giuoco Piano                                      \n",
      "68   31239    1931      D15        D15           ‚úì      Slav Defense: Alekhine Variation                  \n",
      "69   7920     1523      C44        C44           ‚úì      Scotch Game: Lolli Variation                      \n",
      "70   5453     1345      C34        C34           ‚úì      King's Gambit Accepted: Bonsch-Osmolovsky Var...  \n",
      "71   14818    1526      C44        C44           ‚úì      Scotch Game: Scotch Gambit                        \n",
      "72   6123     1532      C44        C44           ‚úì      Scotch Game: Scotch Gambit, London Defense        \n",
      "73   29126    727       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation              \n",
      "74   38385    2643      A40        A40           ‚úì      Englund Gambit: Soller Gambit                     \n",
      "75   4524     783       B20        B20           ‚úì      Sicilian Defense: Bowdler Attack                  \n",
      "76   41180    836       B23        B23           ‚úì      Sicilian Defense: Closed                          \n",
      "77   47148    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "78   7246     1079      C02        C02           ‚úì      French Defense: Advance Variation                 \n",
      "79   7997     302       A40        A40           ‚úì      Englund Gambit Complex: Hartlaub-Charlick Gambit  \n",
      "80   21844    531       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "81   13471    627       B02        B02           ‚úì      Alekhine Defense: Two Pawn Attack                 \n",
      "82   41493    295       A40        A40           ‚úì      English Defense                                   \n",
      "83   32083    1790      C88        C88           ‚úì      Ruy Lopez: Closed                                 \n",
      "84   9301     1025      B90        B90           ‚úì      Sicilian Defense: Najdorf Variation               \n",
      "85   44698    894       B32        B32           ‚úì      Sicilian Defense: Kalashnikov Variation           \n",
      "86   28935    571       B00        B00           ‚úì      Rat Defense: Antal Defense                        \n",
      "87   21140    854       B27        B27           ‚úì      Sicilian Defense: Katalimov Variation             \n",
      "88   15829    503       A85        A85           ‚úì      Dutch Defense: Queen's Knight Variation           \n",
      "89   37463    898       B32        B32           ‚úì      Sicilian Defense: Open                            \n",
      "90   40711    150       A04        A04           ‚úì      Zukertort Opening: Vos Gambit                     \n",
      "91   20654    522       B00        B00           ‚úì      Carr Defense                                      \n",
      "92   11134    569       B00        B00           ‚úì      Pirc Defense                                      \n",
      "93   4773     2424      A36        A36           ‚úì      English Opening: Symmetrical Variation, Two K...  \n",
      "94   11699    333       A42        A42           ‚úì      Modern Defense: Kotov Variation                   \n",
      "95   8577     667       B06        B06           ‚úì      Modern Defense: Three Pawns Attack                \n",
      "96   43386    1595      C50        C50           ‚úì      Italian Game: Giuoco Pianissimo, Italian Four...  \n",
      "97   7011     1660      C57        C57           ‚úì      Italian Game: Two Knights Defense, Ponziani-S...  \n",
      "98   20376    124       A03        A03           ‚úì      Bird Opening: Dutch Variation                     \n",
      "99   6603     1282      C30        C30           ‚úì      King's Gambit                                     \n",
      "100  5032     298       A40        A40           ‚úì      Englund Gambit Complex Declined                   \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Verification Results:\n",
      "   ‚Ä¢ Total samples: 100\n",
      "   ‚Ä¢ Matches: 100/100 (100.0%)\n",
      "   ‚Ä¢ Mismatches: 0\n",
      "\n",
      " All ECO codes reconstructed correctly\n",
      "   ‚Ä¢ ID remapping preserved all ECO code mappings\n",
      "   ‚Ä¢ Converting 88 NEW opening IDs to OLD database IDs for query\n",
      "   ‚Ä¢ Example: NEW ID 653 ‚Üí OLD ID 838\n",
      "#    Player   Opening   ECO (DB)   Reconstructed Match  Opening Name                                      \n",
      "====================================================================================================\n",
      "1    18357    653       B06        B06           ‚úì      Modern Defense                                    \n",
      "2    45188    668       B06        B06           ‚úì      Modern Defense: Two Knights Variation             \n",
      "3    32831    653       B06        B06           ‚úì      Modern Defense                                    \n",
      "4    19492    1819      D00        D00           ‚úì      Blackmar-Diemer Gambit                            \n",
      "5    14696    849       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Dragon         \n",
      "6    24876    1346      C34        C34           ‚úì      King's Gambit Accepted: Fischer Defense           \n",
      "7    6247     729       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation, Botvinn...  \n",
      "8    37318    1367      C37        C37           ‚úì      King's Gambit Accepted: Muzio Gambit, Wild Mu...  \n",
      "9    30476    999       B75        B75           ‚úì      Sicilian Defense: Dragon Variation, Yugoslav ...  \n",
      "10   24386    578       B00        B00           ‚úì      Ware Defense                                      \n",
      "11   14320    849       B27        B27           ‚úì      Sicilian Defense: Hyperaccelerated Dragon         \n",
      "12   37172    1484      C42        C42           ‚úì      Russian Game: Three Knights Game                  \n",
      "13   30332    1514      C44        C44           ‚úì      Scotch Game                                       \n",
      "14   10852    531       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "15   24176    2452      B01        B01           ‚úì      Scandinavian Defense: Valencian Variation         \n",
      "16   21242    1107      C07        C07           ‚úì      French Defense: Tarrasch Variation, Open Syst...  \n",
      "17   30324    1217      C22        C22           ‚úì      Center Game: Normal Variation                     \n",
      "18   44575    2451      D00        D00           ‚úì      Queen's Pawn Game: Accelerated London System      \n",
      "19   1855     413       A53        A53           ‚úì      Old Indian Defense: Czech Variation               \n",
      "20   2923     2351      E76        E76           ‚úì      King's Indian Defense: Four Pawns Attack          \n",
      "21   26910    1075      C00        C00           ‚úì      Rat Defense: Small Center Defense                 \n",
      "22   43852    1487      C43        C43           ‚úì      Russian Game: Modern Attack                       \n",
      "23   35287    1287      C30        C30           ‚úì      King's Gambit Declined: Keene Defense             \n",
      "24   9426     756       B14        B14           ‚úì      Caro-Kann Defense: Panov Attack, Main Line        \n",
      "25   9628     1266      C27        C27           ‚úì      Vienna Game: Frankenstein-Dracula Variation       \n",
      "26   10758    1295      C30        C30           ‚úì      King's Gambit Declined: Queen's Knight Defense    \n",
      "27   21121    1429      C41        C41           ‚úì      Philidor Defense                                  \n",
      "28   34398    1520      C44        C44           ‚úì      Scotch Game: G√∂ring Gambit, Double Pawn Sacri...  \n",
      "29   31677    2462      A40        A40           ‚úì      Queen's Pawn Game: Modern Defense                 \n",
      "30   33329    2714      C11        C11           ‚úì      French Defense: Classical Variation, Shirov-A...  \n",
      "31   12141    233       A16        A16           ‚úì      English Opening: Anglo-Indian Defense, Queen'...  \n",
      "32   19504    2462      A40        A40           ‚úì      Queen's Pawn Game: Modern Defense                 \n",
      "33   21581    597       B01        B01           ‚úì      Scandinavian Defense: Marshall Variation          \n",
      "34   17790    2431      B07        B07           ‚úì      King's Pawn Game: Mar√≥czy Defense                 \n",
      "35   41614    594       B01        B01           ‚úì      Scandinavian Defense: Main Line                   \n",
      "36   12918    525       B00        B00           ‚úì      Goldsmith Defense                                 \n",
      "37   24121    774       B18        B18           ‚úì      Caro-Kann Defense: Classical Variation            \n",
      "38   3186     338       A43        A43           ‚úì      Benoni Defense: Benoni-Indian Defense             \n",
      "39   36331    2613      B15        B15           ‚úì      Caro-Kann Defense: Alien Gambit                   \n",
      "40   32073    1052      C00        C00           ‚úì      French Defense: Chigorin Variation                \n",
      "41   24690    1058      C00        C00           ‚úì      French Defense: Knight Variation                  \n",
      "42   20085    72        A00        A00           ‚úì      Valencia Opening                                  \n",
      "43   8218     1279      C29        C29           ‚úì      Vienna Game: Vienna Gambit, Paulsen Attack        \n",
      "44   29220    1562      C47        C47           ‚úì      Four Knights Game                                 \n",
      "45   34933    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "46   33805    1282      C30        C30           ‚úì      King's Gambit                                     \n",
      "47   40587    1567      C47        C47           ‚úì      Four Knights Game: Scotch Variation               \n",
      "48   18161    1484      C42        C42           ‚úì      Russian Game: Three Knights Game                  \n",
      "49   40123    1072      C00        C00           ‚úì      French Defense: Two Knights Variation             \n",
      "50   40141    653       B06        B06           ‚úì      Modern Defense                                    \n",
      "51   31561    1849      D00        D00           ‚úì      Queen's Pawn Game: Chigorin Variation, Anti-V...  \n",
      "52   8072     1680      C60        C60           ‚úì      Ruy Lopez: Cozio Defense                          \n",
      "53   26702    598       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "54   31445    2456      C56        C56           ‚úì      Italian Game: Two Knights Defense, Open Varia...  \n",
      "55   3243     840       B23        B23           ‚úì      Sicilian Defense: Grand Prix Attack               \n",
      "56   48370    598       B01        B01           ‚úì      Scandinavian Defense: Mieses-Kotroc Variation     \n",
      "57   22560    2452      B01        B01           ‚úì      Scandinavian Defense: Valencian Variation         \n",
      "58   20609    693       B08        B08           ‚úì      Pirc Defense: Classical Variation                 \n",
      "59   39288    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "60   27800    711       B10        B10           ‚úì      Caro-Kann Defense: Breyer Variation               \n",
      "61   10655    1561      C46        C46           ‚úì      Three Knights Opening: Winawer Defense            \n",
      "62   1133     575       B00        B00           ‚úì      St. George Defense                                \n",
      "63   12679    941       B43        B43           ‚úì      Sicilian Defense: Kan Variation, Wing Attack      \n",
      "64   39471    1100      C05        C05           ‚úì      French Defense: Tarrasch Variation, Closed Va...  \n",
      "65   3784     783       B20        B20           ‚úì      Sicilian Defense: Bowdler Attack                  \n",
      "66   36213    1557      C46        C46           ‚úì      Three Knights Opening                             \n",
      "67   32988    1588      C50        C50           ‚úì      Giuoco Piano                                      \n",
      "68   31239    1931      D15        D15           ‚úì      Slav Defense: Alekhine Variation                  \n",
      "69   7920     1523      C44        C44           ‚úì      Scotch Game: Lolli Variation                      \n",
      "70   5453     1345      C34        C34           ‚úì      King's Gambit Accepted: Bonsch-Osmolovsky Var...  \n",
      "71   14818    1526      C44        C44           ‚úì      Scotch Game: Scotch Gambit                        \n",
      "72   6123     1532      C44        C44           ‚úì      Scotch Game: Scotch Gambit, London Defense        \n",
      "73   29126    727       B12        B12           ‚úì      Caro-Kann Defense: Advance Variation              \n",
      "74   38385    2643      A40        A40           ‚úì      Englund Gambit: Soller Gambit                     \n",
      "75   4524     783       B20        B20           ‚úì      Sicilian Defense: Bowdler Attack                  \n",
      "76   41180    836       B23        B23           ‚úì      Sicilian Defense: Closed                          \n",
      "77   47148    706       B10        B10           ‚úì      Caro-Kann Defense                                 \n",
      "78   7246     1079      C02        C02           ‚úì      French Defense: Advance Variation                 \n",
      "79   7997     302       A40        A40           ‚úì      Englund Gambit Complex: Hartlaub-Charlick Gambit  \n",
      "80   21844    531       B00        B00           ‚úì      Nimzowitsch Defense                               \n",
      "81   13471    627       B02        B02           ‚úì      Alekhine Defense: Two Pawn Attack                 \n",
      "82   41493    295       A40        A40           ‚úì      English Defense                                   \n",
      "83   32083    1790      C88        C88           ‚úì      Ruy Lopez: Closed                                 \n",
      "84   9301     1025      B90        B90           ‚úì      Sicilian Defense: Najdorf Variation               \n",
      "85   44698    894       B32        B32           ‚úì      Sicilian Defense: Kalashnikov Variation           \n",
      "86   28935    571       B00        B00           ‚úì      Rat Defense: Antal Defense                        \n",
      "87   21140    854       B27        B27           ‚úì      Sicilian Defense: Katalimov Variation             \n",
      "88   15829    503       A85        A85           ‚úì      Dutch Defense: Queen's Knight Variation           \n",
      "89   37463    898       B32        B32           ‚úì      Sicilian Defense: Open                            \n",
      "90   40711    150       A04        A04           ‚úì      Zukertort Opening: Vos Gambit                     \n",
      "91   20654    522       B00        B00           ‚úì      Carr Defense                                      \n",
      "92   11134    569       B00        B00           ‚úì      Pirc Defense                                      \n",
      "93   4773     2424      A36        A36           ‚úì      English Opening: Symmetrical Variation, Two K...  \n",
      "94   11699    333       A42        A42           ‚úì      Modern Defense: Kotov Variation                   \n",
      "95   8577     667       B06        B06           ‚úì      Modern Defense: Three Pawns Attack                \n",
      "96   43386    1595      C50        C50           ‚úì      Italian Game: Giuoco Pianissimo, Italian Four...  \n",
      "97   7011     1660      C57        C57           ‚úì      Italian Game: Two Knights Defense, Ponziani-S...  \n",
      "98   20376    124       A03        A03           ‚úì      Bird Opening: Dutch Variation                     \n",
      "99   6603     1282      C30        C30           ‚úì      King's Gambit                                     \n",
      "100  5032     298       A40        A40           ‚úì      Englund Gambit Complex Declined                   \n",
      "====================================================================================================\n",
      "\n",
      "‚úÖ Verification Results:\n",
      "   ‚Ä¢ Total samples: 100\n",
      "   ‚Ä¢ Matches: 100/100 (100.0%)\n",
      "   ‚Ä¢ Mismatches: 0\n",
      "\n",
      " All ECO codes reconstructed correctly\n",
      "   ‚Ä¢ ID remapping preserved all ECO code mappings\n"
     ]
    }
   ],
   "source": [
    "# Verification: Sample player-opening pairs with reconstructed ECO codes and opening names\n",
    "# Doing this to make sure that our ECO encoding/decoding is correct\n",
    "\n",
    "print(\"VERIFICATION: ECO RECONSTRUCTION AND OPENING NAMES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Sample random player-opening pairs from training data\n",
    "sample_size = 100\n",
    "sample_data = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "\n",
    "# Get unique opening IDs from sample (these are NEW remapped IDs)\n",
    "new_opening_ids = sample_data['opening_id'].unique()\n",
    "\n",
    "# Convert NEW opening IDs back to OLD database IDs for query\n",
    "if opening_remapped:\n",
    "    old_opening_ids = [opening_idx_to_id[int(new_id)] for new_id in new_opening_ids]\n",
    "    print(f\"   ‚Ä¢ Converting {len(new_opening_ids)} NEW opening IDs to OLD database IDs for query\")\n",
    "    print(f\"   ‚Ä¢ Example: NEW ID {new_opening_ids[0]} ‚Üí OLD ID {old_opening_ids[0]}\")\n",
    "else:\n",
    "    old_opening_ids = new_opening_ids\n",
    "    print(f\"   ‚Ä¢ No remapping was done - using opening IDs directly\")\n",
    "\n",
    "opening_ids_str = ','.join(map(str, old_opening_ids))\n",
    "\n",
    "# Query database for opening names using OLD IDs\n",
    "con = get_db_connection(str(DB_PATH))\n",
    "try:\n",
    "    opening_query = f\"\"\"\n",
    "        SELECT id, name, eco\n",
    "        FROM opening\n",
    "        WHERE id IN ({opening_ids_str})\n",
    "    \"\"\"\n",
    "    opening_names = pd.DataFrame(con.execute(opening_query).df()).set_index('id')\n",
    "finally:\n",
    "    con.close()\n",
    "\n",
    "# Create reverse mappings for ECO decoding\n",
    "eco_int_to_letter = {v: k for k, v in eco_letter_map.items()}\n",
    "eco_int_to_number = {v: k for k, v in eco_number_map.items()}\n",
    "\n",
    "# Build verification table\n",
    "print(f\"{'#':<4} {'Player':<8} {'Opening':<9} {'ECO (DB)':<10} {'Reconstructed':<13} {'Match':<6} {'Opening Name':<50}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "matches = 0\n",
    "for i, (idx, row) in enumerate(sample_data.iterrows(), 1):\n",
    "    player_id = int(row['player_id'])\n",
    "    new_opening_id = int(row['opening_id'])\n",
    "    \n",
    "    # Convert NEW opening ID to OLD database ID for lookup\n",
    "    if opening_remapped:\n",
    "        old_opening_id = opening_idx_to_id[new_opening_id]\n",
    "    else:\n",
    "        old_opening_id = new_opening_id\n",
    "    \n",
    "    # Lookup opening side info using NEW ID\n",
    "    opening_info = opening_side_info.loc[new_opening_id]\n",
    "    \n",
    "    # Reconstruct ECO from encoded categorical values\n",
    "    eco_letter_encoded = opening_info['eco_letter_cat']\n",
    "    eco_number_encoded = opening_info['eco_number_cat']\n",
    "    \n",
    "    eco_letter_decoded = eco_int_to_letter[eco_letter_encoded]\n",
    "    eco_number_decoded = eco_int_to_number[eco_number_encoded]\n",
    "    \n",
    "    reconstructed_eco = f\"{eco_letter_decoded}{eco_number_decoded}\"\n",
    "    \n",
    "    # Get original ECO from database using OLD ID\n",
    "    db_eco = opening_names.loc[old_opening_id, 'eco']\n",
    "    opening_name = opening_names.loc[old_opening_id, 'name']\n",
    "    \n",
    "    # Check if they match\n",
    "    match = \"‚úì\" if reconstructed_eco == db_eco else \"‚úó\"\n",
    "    if reconstructed_eco == db_eco:\n",
    "        matches += 1\n",
    "    \n",
    "    # Truncate opening name if too long\n",
    "    if len(opening_name) > 48:\n",
    "        opening_name = opening_name[:45] + \"...\"\n",
    "    \n",
    "    # Display using NEW opening ID (what's in the data now)\n",
    "    print(f\"{i:<4} {player_id:<8} {new_opening_id:<9} {db_eco:<10} {reconstructed_eco:<13} {match:<6} {opening_name:<50}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n‚úÖ Verification Results:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(sample_data)}\")\n",
    "print(f\"   ‚Ä¢ Matches: {matches}/{len(sample_data)} ({100*matches/len(sample_data):.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Mismatches: {len(sample_data) - matches}\")\n",
    "\n",
    "if matches == len(sample_data):\n",
    "    print(f\"\\n All ECO codes reconstructed correctly\")\n",
    "    print(f\"   ‚Ä¢ ID remapping preserved all ECO code mappings\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Some ECO codes did not match. Investigate mismatches above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf4878",
   "metadata": {},
   "source": [
    "## 5. Data Verification and Examination\n",
    "We're almost there. Let's examine our data structures to check for any obvious flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f76afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train \n",
      "          player_id  opening_id  confidence\n",
      "679380       11013        1819    0.748744\n",
      "1544705      25322         398    0.612403\n",
      "2323873      38086        2430    0.305556\n",
      "1941596      31801         598    0.825784\n",
      "1403739      23037           6    0.180328\n",
      "============================================================\n",
      "X_val \n",
      "          player_id  opening_id  confidence\n",
      "444255        7206         297    0.557522\n",
      "675589       10950        1703    0.230769\n",
      "2397491      39307        1919    0.253731\n",
      "80045         1330        2671    0.596774\n",
      "1714910      28088        1709    0.576271\n",
      "============================================================\n",
      "X_test \n",
      "          player_id  opening_id  confidence\n",
      "1400658      22991        1079    0.719101\n",
      "2321194      38038        1671    0.645390\n",
      "2710107      44570        1263    0.367089\n",
      "1204342      19746         166    0.425287\n",
      "419673        6824        1236    0.812734\n",
      "============================================================\n",
      "y_train \n",
      " 679380     0.528668\n",
      "1544705    0.543867\n",
      "2323873    0.458324\n",
      "1941596    0.513303\n",
      "1403739    0.480933\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_val \n",
      " 444255     0.516579\n",
      "675589     0.494777\n",
      "2397491    0.492091\n",
      "80045      0.463084\n",
      "1714910    0.575341\n",
      "Name: score, dtype: float64\n",
      "============================================================\n",
      "y_test \n",
      " 1400658    0.459333\n",
      "2321194    0.530896\n",
      "2710107    0.548859\n",
      "1204342    0.465350\n",
      "419673     0.479154\n",
      "Name: score, dtype: float64\n",
      "player_side_info \n",
      "            rating_z\n",
      "player_id          \n",
      "0          0.608678\n",
      "1          1.058287\n",
      "2          0.560506\n",
      "3          0.616707\n",
      "4         -2.169266\n",
      "============================================================\n",
      "opening_side_info \n",
      "             eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "531                      1               0\n",
      "565                      1               0\n",
      "572                      1               0\n",
      "579                      1               1\n",
      "589                      1               1\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train \\n\", X_train.head())\n",
    "print(\"=\"*60)\n",
    "print(\"X_val \\n\", X_val.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"X_test \\n\", X_test.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_train \\n\", y_train.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"y_val \\n\", y_val.head())\n",
    "print(\"=\" * 60) \n",
    "print(\"y_test \\n\", y_test.head())\n",
    "\n",
    "# Now side information\n",
    "print(\"player_side_info \\n\", player_side_info.head())\n",
    "print(\"=\" * 60)\n",
    "print(\"opening_side_info \\n\", opening_side_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52936cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANED SIDE INFORMATION TABLES\n",
      "============================================================\n",
      "\n",
      "üìä player_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (48468, 1)\n",
      "   ‚Ä¢ Columns: ['rating_z']\n",
      "   ‚Ä¢ Index: player_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "           rating_z\n",
      "player_id          \n",
      "0          0.608678\n",
      "1          1.058287\n",
      "2          0.560506\n",
      "3          0.616707\n",
      "4         -2.169266\n",
      "\n",
      "üìä opening_side_info (cleaned):\n",
      "   ‚Ä¢ Shape: (2717, 2)\n",
      "   ‚Ä¢ Columns: ['eco_letter_cat', 'eco_number_cat']\n",
      "   ‚Ä¢ Index: opening_id\n",
      "\n",
      "   Sample (5 rows):\n",
      "            eco_letter_cat  eco_number_cat\n",
      "opening_id                                \n",
      "531                      1               0\n",
      "565                      1               0\n",
      "572                      1               0\n",
      "579                      1               1\n",
      "589                      1               1\n"
     ]
    }
   ],
   "source": [
    "# Final verification: Display cleaned side info tables\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANED SIDE INFORMATION TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä player_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {player_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(player_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: player_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(player_side_info.head().to_string())\n",
    "\n",
    "print(f\"\\nüìä opening_side_info (cleaned):\")\n",
    "print(f\"   ‚Ä¢ Shape: {opening_side_info.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(opening_side_info.columns)}\")\n",
    "print(f\"   ‚Ä¢ Index: opening_id\")\n",
    "print(f\"\\n   Sample (5 rows):\")\n",
    "print(opening_side_info.head().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19883cba",
   "metadata": {},
   "source": [
    "## Step 5: Convert to PyTorch Tensors\n",
    "\n",
    "**What are tensors?**\n",
    "Tensors are PyTorch's version of arrays - just multi-dimensional data structures optimized for deep learning. Think of them like fancy NumPy arrays that can run on GPUs.\n",
    "\n",
    "**What we need to convert:**\n",
    "\n",
    "1. **Main features** (X_train, X_val, X_test):\n",
    "   - `player_id` ‚Üí long tensor (integer IDs)\n",
    "   - `opening_id` ‚Üí long tensor (integer IDs)\n",
    "   - `confidence` ‚Üí float tensor (weights for loss function)\n",
    "\n",
    "2. **Targets** (y_train, y_val, y_test):\n",
    "   - `score` ‚Üí float tensor (what we're predicting)\n",
    "\n",
    "3. **Player side info** (player_side_info):\n",
    "   - `rating_z` ‚Üí float tensor (normalized ratings)\n",
    "   - Indexed by player_id for fast lookup\n",
    "\n",
    "4. **Opening side info** (opening_side_info):\n",
    "   - `eco_letter_cat` ‚Üí long tensor (categorical)\n",
    "   - `eco_number_cat` ‚Üí long tensor (categorical)\n",
    "   - Indexed by opening_id for fast lookup\n",
    "\n",
    "**Why these data types?**\n",
    "- `long` (int64): For IDs and categorical features that will be embedded\n",
    "- `float` (float32): For continuous values like scores, confidence, and normalized ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "350be514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4c/gt84bby56mqdnzwsg72zvd680000gn/T/ipykernel_2065/1794647647.py\", line 6, in <module>\n",
      "    import torch\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# It says subprocess is unused but complains at me if I don't import it\n",
    "# Something to do with incompatible versions of blah blah blah\n",
    "# Don't really care as long as it works\n",
    "import subprocess\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda01b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: CONVERT TO PYTORCH TENSORS\n",
      "============================================================\n",
      "   ‚Ä¢ CUDA available: False\n",
      "   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\n",
      "\n",
      "0Ô∏è‚É£  Index alignment sanity checks...\n",
      "   ‚úì player_side_info index is sorted\n",
      "   ‚Ä¢ Index range: [0, 48467]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\n",
      "   ‚Ä¢ Index range: [0, 2716]\n",
      "   ‚Ä¢ Index dtype: int64\n",
      "\n",
      "   Checking if indices are contiguous 0-based...\n",
      "   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\n",
      "   ‚Ä¢ Training tensors:\n",
      "   ‚Ä¢ player_ids_train: torch.Size([2173275]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_train: torch.Size([2173275]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_train: torch.Size([2173275]), dtype=torch.float32\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([434655]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([434655]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([434655]), dtype=torch.float32\n",
      "   ‚Ä¢ Training tensors:\n",
      "   ‚Ä¢ player_ids_train: torch.Size([2173275]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_train: torch.Size([2173275]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_train: torch.Size([2173275]), dtype=torch.float32\n",
      "\n",
      "   Validation tensors:\n",
      "   ‚Ä¢ player_ids_val: torch.Size([434655]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_val: torch.Size([434655]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_val: torch.Size([434655]), dtype=torch.float32\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([289770]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([289770]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([289770]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   ‚Ä¢ scores_train: torch.Size([2173275]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([434655]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([289770]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1004, 1.0000]\n",
      "   ‚Ä¢ Val: [0.1138, 0.8250]\n",
      "   ‚Ä¢ Test: [0.1521, 0.7841]\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([48468]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([48468]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.2696, 4.2457]\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2717]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2717]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2717]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "   ‚Ä¢ Train: 2,173,275 samples\n",
      "   ‚Ä¢ Val: 434,655 samples\n",
      "   ‚Ä¢ Test: 289,770 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 48,468\n",
      "   ‚Ä¢ Openings: 2,717\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 48468 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2717 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 66.55 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n",
      "\n",
      "   Test tensors:\n",
      "   ‚Ä¢ player_ids_test: torch.Size([289770]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_test: torch.Size([289770]), dtype=torch.int64\n",
      "   ‚Ä¢ confidence_test: torch.Size([289770]), dtype=torch.float32\n",
      "\n",
      "2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\n",
      "   ‚Ä¢ scores_train: torch.Size([2173275]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_val: torch.Size([434655]), dtype=torch.float32\n",
      "   ‚Ä¢ scores_test: torch.Size([289770]), dtype=torch.float32\n",
      "\n",
      "   Score ranges (sanity check):\n",
      "   ‚Ä¢ Train: [0.1004, 1.0000]\n",
      "   ‚Ä¢ Val: [0.1138, 0.8250]\n",
      "   ‚Ä¢ Test: [0.1521, 0.7841]\n",
      "   ‚Ä¢ player_ratings_tensor: torch.Size([48468]), dtype=torch.float32\n",
      "   ‚Ä¢ player_ids_in_side_info: torch.Size([48468]), dtype=torch.int64\n",
      "   ‚Ä¢ Rating range: [-2.2696, 4.2457]\n",
      "\n",
      "4Ô∏è‚É£  Converting opening side information...\n",
      "   ‚Ä¢ opening_eco_letter_tensor: torch.Size([2717]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_eco_number_tensor: torch.Size([2717]), dtype=torch.int64\n",
      "   ‚Ä¢ opening_ids_in_side_info: torch.Size([2717]), dtype=torch.int64\n",
      "   ‚Ä¢ ECO letter range: [0, 4]\n",
      "   ‚Ä¢ ECO number range: [0, 99]\n",
      "\n",
      "5Ô∏è‚É£  Summary statistics:\n",
      "   ‚Ä¢ Train: 2,173,275 samples\n",
      "   ‚Ä¢ Val: 434,655 samples\n",
      "   ‚Ä¢ Test: 289,770 samples\n",
      "\n",
      "   Unique entities:\n",
      "   ‚Ä¢ Players: 48,468\n",
      "   ‚Ä¢ Openings: 2,717\n",
      "\n",
      "   Vocabulary sizes (for embedding layers):\n",
      "   ‚Ä¢ num_players: 48468 (max player_id + 1)\n",
      "   ‚Ä¢ num_openings: 2717 (max opening_id + 1)\n",
      "   ‚Ä¢ num_eco_letters: 5 (max eco_letter_cat + 1)\n",
      "   ‚Ä¢ num_eco_numbers: 100 (max eco_number_cat + 1)\n",
      "\n",
      "6Ô∏è‚É£  Approximate memory usage:\n",
      "   ‚Ä¢ Total tensor memory: 66.55 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ TENSOR CONVERSION COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Convert all data to PyTorch tensors\n",
    "\n",
    "print(\"STEP 5: CONVERT TO PYTORCH TENSORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   ‚Ä¢ Default dtype: float32 for continuous, int64 for IDs/categorical\")\n",
    "\n",
    "# 0. Index alignment sanity checks\n",
    "print(f\"\\n0Ô∏è‚É£  Index alignment sanity checks...\")\n",
    "\n",
    "# Check player_side_info is properly indexed\n",
    "player_ids_sorted = sorted(player_side_info.index.values)\n",
    "if player_ids_sorted != list(player_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  player_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì player_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{player_side_info.index.min()}, {player_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {player_side_info.index.dtype}\")\n",
    "\n",
    "# Check opening_side_info is properly indexed\n",
    "opening_ids_sorted = sorted(opening_side_info.index.values)\n",
    "if opening_ids_sorted != list(opening_side_info.index.values):\n",
    "    print(f\"   ‚ö†Ô∏è  opening_side_info index is not sorted - this is OK, we'll use index values directly\")\n",
    "else:\n",
    "    print(f\"   ‚úì opening_side_info index is sorted\")\n",
    "print(f\"   ‚Ä¢ Index range: [{opening_side_info.index.min()}, {opening_side_info.index.max()}]\")\n",
    "print(f\"   ‚Ä¢ Index dtype: {opening_side_info.index.dtype}\")\n",
    "\n",
    "# CRITICAL: Check if indices are contiguous 0-based\n",
    "# If opening_side_info.index = [0, 1, 2, ..., N-1], then we can use opening_id as direct array index\n",
    "# If not (e.g., [10, 15, 23, ...]), we'll need a mapping dictionary\n",
    "print(f\"\\n   Checking if indices are contiguous 0-based...\")\n",
    "player_ids_are_contiguous = (player_side_info.index == range(len(player_side_info))).all()\n",
    "opening_ids_are_contiguous = (opening_side_info.index == range(len(opening_side_info))).all()\n",
    "\n",
    "\n",
    "if not player_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Player IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "if not opening_ids_are_contiguous:\n",
    "    print(f\"   ‚ÑπÔ∏è  Opening IDs are NOT 0-based contiguous - will need mapping for embedding lookup\")\n",
    "\n",
    "# 1. Convert main features (train/val/test)\n",
    "\n",
    "# Train set\n",
    "player_ids_train = torch.tensor(X_train['player_id'].values, dtype=torch.long)\n",
    "opening_ids_train = torch.tensor(X_train['opening_id'].values, dtype=torch.long)\n",
    "confidence_train = torch.tensor(X_train['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ Training tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_train: {player_ids_train.shape}, dtype={player_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_train: {opening_ids_train.shape}, dtype={opening_ids_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_train: {confidence_train.shape}, dtype={confidence_train.dtype}\")\n",
    "\n",
    "# Validation set\n",
    "player_ids_val = torch.tensor(X_val['player_id'].values, dtype=torch.long)\n",
    "opening_ids_val = torch.tensor(X_val['opening_id'].values, dtype=torch.long)\n",
    "confidence_val = torch.tensor(X_val['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Validation tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_val: {player_ids_val.shape}, dtype={player_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_val: {opening_ids_val.shape}, dtype={opening_ids_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_val: {confidence_val.shape}, dtype={confidence_val.dtype}\")\n",
    "\n",
    "# Test set\n",
    "player_ids_test = torch.tensor(X_test['player_id'].values, dtype=torch.long)\n",
    "opening_ids_test = torch.tensor(X_test['opening_id'].values, dtype=torch.long)\n",
    "confidence_test = torch.tensor(X_test['confidence'].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Test tensors:\")\n",
    "print(f\"   ‚Ä¢ player_ids_test: {player_ids_test.shape}, dtype={player_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_test: {opening_ids_test.shape}, dtype={opening_ids_test.dtype}\")\n",
    "print(f\"   ‚Ä¢ confidence_test: {confidence_test.shape}, dtype={confidence_test.dtype}\")\n",
    "\n",
    "# 2. Convert targets (scores)\n",
    "print(f\"\\n2Ô∏è‚É£  Converting target scores (y_train, y_val, y_test)...\")\n",
    "\n",
    "scores_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "scores_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "scores_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "print(f\"   ‚Ä¢ scores_train: {scores_train.shape}, dtype={scores_train.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_val: {scores_val.shape}, dtype={scores_val.dtype}\")\n",
    "print(f\"   ‚Ä¢ scores_test: {scores_test.shape}, dtype={scores_test.dtype}\")\n",
    "\n",
    "print(f\"\\n   Score ranges (sanity check):\")\n",
    "print(f\"   ‚Ä¢ Train: [{scores_train.min():.4f}, {scores_train.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Val: [{scores_val.min():.4f}, {scores_val.max():.4f}]\")\n",
    "print(f\"   ‚Ä¢ Test: [{scores_test.min():.4f}, {scores_test.max():.4f}]\")\n",
    "\n",
    "# 3. Convert player side information\n",
    "\n",
    "# Create tensor of all player ratings (indexed by player_id)\n",
    "# Since player_side_info is indexed by player_id, we need to ensure coverage\n",
    "player_ratings_tensor = torch.tensor(player_side_info['rating_z'].values, dtype=torch.float32)\n",
    "player_ids_in_side_info = torch.tensor(player_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ player_ratings_tensor: {player_ratings_tensor.shape}, dtype={player_ratings_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ player_ids_in_side_info: {player_ids_in_side_info.shape}, dtype={player_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ Rating range: [{player_ratings_tensor.min():.4f}, {player_ratings_tensor.max():.4f}]\")\n",
    "\n",
    "# Verify all player IDs in train/val/test are covered\n",
    "all_player_ids = torch.cat([player_ids_train, player_ids_val, player_ids_test]).unique()\n",
    "missing_players = set(all_player_ids.tolist()) - set(player_ids_in_side_info.tolist())\n",
    "if len(missing_players) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_players)} players in splits missing from side_info!\")\n",
    "\n",
    "# 4. Convert opening side information\n",
    "print(f\"\\n4Ô∏è‚É£  Converting opening side information...\")\n",
    "\n",
    "# Verify column names exist (using eco_letter_cat and eco_number_cat consistently)\n",
    "if 'eco_letter_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_letter_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "if 'eco_number_cat' not in opening_side_info.columns:\n",
    "    raise ValueError(f\"Column 'eco_number_cat' not found. Available: {opening_side_info.columns.tolist()}\")\n",
    "\n",
    "# Create tensors for opening ECO features (indexed by opening_id)\n",
    "opening_eco_letter_tensor = torch.tensor(opening_side_info['eco_letter_cat'].values, dtype=torch.long)\n",
    "opening_eco_number_tensor = torch.tensor(opening_side_info['eco_number_cat'].values, dtype=torch.long)\n",
    "opening_ids_in_side_info = torch.tensor(opening_side_info.index.values, dtype=torch.long)\n",
    "\n",
    "print(f\"   ‚Ä¢ opening_eco_letter_tensor: {opening_eco_letter_tensor.shape}, dtype={opening_eco_letter_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_eco_number_tensor: {opening_eco_number_tensor.shape}, dtype={opening_eco_number_tensor.dtype}\")\n",
    "print(f\"   ‚Ä¢ opening_ids_in_side_info: {opening_ids_in_side_info.shape}, dtype={opening_ids_in_side_info.dtype}\")\n",
    "print(f\"   ‚Ä¢ ECO letter range: [{opening_eco_letter_tensor.min()}, {opening_eco_letter_tensor.max()}]\")\n",
    "print(f\"   ‚Ä¢ ECO number range: [{opening_eco_number_tensor.min()}, {opening_eco_number_tensor.max()}]\")\n",
    "\n",
    "# Verify all opening IDs in train/val/test are covered\n",
    "all_opening_ids = torch.cat([opening_ids_train, opening_ids_val, opening_ids_test]).unique()\n",
    "missing_openings = set(all_opening_ids.tolist()) - set(opening_ids_in_side_info.tolist())\n",
    "if len(missing_openings) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: {len(missing_openings)} openings in splits missing from side_info!\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(f\"\\n5Ô∏è‚É£  Summary statistics:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(scores_train):,} samples\")\n",
    "print(f\"   ‚Ä¢ Val: {len(scores_val):,} samples\")\n",
    "print(f\"   ‚Ä¢ Test: {len(scores_test):,} samples\")\n",
    "\n",
    "print(f\"\\n   Unique entities:\")\n",
    "print(f\"   ‚Ä¢ Players: {len(player_ids_in_side_info):,}\")\n",
    "print(f\"   ‚Ä¢ Openings: {len(opening_ids_in_side_info):,}\")\n",
    "\n",
    "print(f\"\\n   Vocabulary sizes (for embedding layers):\")\n",
    "print(f\"   ‚Ä¢ num_players: {player_ids_in_side_info.max() + 1} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_openings: {opening_ids_in_side_info.max() + 1} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_letters: {opening_eco_letter_tensor.max() + 1} (max eco_letter_cat + 1)\")\n",
    "print(f\"   ‚Ä¢ num_eco_numbers: {opening_eco_number_tensor.max() + 1} (max eco_number_cat + 1)\")\n",
    "\n",
    "# 6. Memory usage (approximate - using simple calculation)\n",
    "print(f\"\\n6Ô∏è‚É£  Approximate memory usage:\")\n",
    "def tensor_memory_mb(t):\n",
    "    \"\"\"Calculate tensor memory in MB\"\"\"\n",
    "    return (t.element_size() * t.nelement()) / (1024 * 1024)\n",
    "\n",
    "tensors = [\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train,\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val,\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test,\n",
    "    player_ratings_tensor, opening_eco_letter_tensor, opening_eco_number_tensor\n",
    "]\n",
    "total_memory_mb = sum(tensor_memory_mb(t) for t in tensors)\n",
    "print(f\"   ‚Ä¢ Total tensor memory: {total_memory_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TENSOR CONVERSION COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fc308",
   "metadata": {},
   "source": [
    "## Step 6: Training Setup\n",
    "\n",
    "Define all constants, loss functions, optimizer, and helper functions for training.\n",
    "\n",
    "**What we need to set up:**\n",
    "\n",
    "1. **Constants and Hyperparameters:**\n",
    "   - `NUM_FACTORS`: Dimensionality of latent factor embeddings (e.g., 50)\n",
    "   - `LEARNING_RATE`: Step size for SGD optimizer (e.g., 0.01)\n",
    "   - `BATCH_SIZE`: Number of samples per training batch (e.g., 1024)\n",
    "   - `N_EPOCHS`: Number of full passes through training data (e.g., 10)\n",
    "   - Random seeds for reproducibility\n",
    "\n",
    "2. **Loss Functions:**\n",
    "   - MSE (Mean Squared Error): Main training loss\n",
    "   - RMSE (Root Mean Squared Error): Evaluation metric\n",
    "   - Confidence-weighted loss: Down-weight uncertain predictions\n",
    "\n",
    "3. **Optimizer:**\n",
    "   - SGD (Stochastic Gradient Descent) with momentum\n",
    "\n",
    "4. **Helper Functions:**\n",
    "   - `train_one_epoch()`: Train for one epoch\n",
    "   - `evaluate_model()`: Evaluate on validation/test set\n",
    "   - `calculate_rmse()`: Compute RMSE metric\n",
    "   - `save_checkpoint()`: Save model state\n",
    "\n",
    "5. **Logging:**\n",
    "   - Progress bars with ETA\n",
    "   - Loss tracking per epoch\n",
    "   - Model checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54d13c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\n",
      "============================================================\n",
      "\n",
      "üìã Hyperparameters:\n",
      "   ‚Ä¢ NUM_FACTORS: 50\n",
      "   ‚Ä¢ LEARNING_RATE: 0.01\n",
      "   ‚Ä¢ MOMENTUM: 0.9\n",
      "   ‚Ä¢ BATCH_SIZE: 1024\n",
      "   ‚Ä¢ N_EPOCHS: 20\n",
      "   ‚Ä¢ WEIGHT_DECAY: 0.0\n",
      "\n",
      "üé≤ Random Seed:\n",
      "   ‚Ä¢ RANDOM_SEED: 42\n",
      "\n",
      "üíª Device Configuration:\n",
      "   ‚Ä¢ DEVICE: cpu\n",
      "\n",
      "üíæ Model Saving:\n",
      "   ‚Ä¢ Save directory: /Users/a/Documents/personalprojects/chess-opening-recommender/data/models\n",
      "   ‚úì Python random seed set to 42\n",
      "   ‚úì NumPy random seed set to 42\n",
      "   ‚úì PyTorch CPU random seed set to 42\n",
      "\n",
      "üìä Vocabulary Sizes (for embedding layers):\n",
      "   ‚Ä¢ NUM_PLAYERS: 48,468 (max player_id + 1)\n",
      "   ‚Ä¢ NUM_OPENINGS: 2,717 (max opening_id + 1)\n",
      "   ‚Ä¢ NUM_ECO_LETTERS: 5 (ECO letter categories)\n",
      "   ‚Ä¢ NUM_ECO_NUMBERS: 100 (ECO number categories)\n",
      "\n",
      "üìà Dataset Statistics:\n",
      "   ‚Ä¢ Train samples: 2,173,275\n",
      "   ‚Ä¢ Validation samples: 434,655\n",
      "   ‚Ä¢ Test samples: 289,770\n",
      "   ‚Ä¢ Total samples: 2,897,700\n",
      "\n",
      "   ‚Ä¢ Batches per epoch: 2,122\n",
      "   ‚Ä¢ Training iterations (total): 42,440\n",
      "\n",
      "============================================================\n",
      "‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\n"
     ]
    }
   ],
   "source": [
    "# Step 6a: Define Hyperparameters and Constants\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 6A: DEFINE HYPERPARAMETERS AND CONSTANTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HYPERPARAMETERS AND CONSTANTS CONFIG\n",
    "# ========================================\n",
    "\n",
    "NUM_FACTORS = 50  # Dimensionality of latent factor embeddings\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9  # SGD momentum\n",
    "BATCH_SIZE = 1024 \n",
    "N_EPOCHS = 20\n",
    "\n",
    "# Regularization (if needed)\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_SAVE_DIR = Path.cwd().parent / \"data\" / \"models\"  # Saves to projectroot/data/models\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"\\nüìã Hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ NUM_FACTORS: {NUM_FACTORS}\")\n",
    "print(f\"   ‚Ä¢ LEARNING_RATE: {LEARNING_RATE}\")\n",
    "print(f\"   ‚Ä¢ MOMENTUM: {MOMENTUM}\")\n",
    "print(f\"   ‚Ä¢ BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ N_EPOCHS: {N_EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ WEIGHT_DECAY: {WEIGHT_DECAY}\")\n",
    "\n",
    "print(f\"\\nüé≤ Random Seed:\")\n",
    "print(f\"   ‚Ä¢ RANDOM_SEED: {RANDOM_SEED}\")\n",
    "\n",
    "print(f\"\\nüíª Device Configuration:\")\n",
    "print(f\"   ‚Ä¢ DEVICE: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ‚Ä¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüíæ Model Saving:\")\n",
    "print(f\"   ‚Ä¢ Save directory: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# ========================================\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# ========================================\n",
    "\n",
    "# Python random\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì Python random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì NumPy random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "# PyTorch random\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "print(f\"   ‚úì PyTorch CPU random seed set to {RANDOM_SEED}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU\n",
    "    print(f\"   ‚úì PyTorch GPU random seed set to {RANDOM_SEED}\")\n",
    "    \n",
    "    # Additional CUDA reproducibility settings\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"   ‚úì CUDA deterministic mode enabled\")\n",
    "\n",
    "# ========================================\n",
    "# CALCULATE VOCABULARY SIZES\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìä Vocabulary Sizes (for embedding layers):\")\n",
    "\n",
    "# Calculate from our tensors\n",
    "NUM_PLAYERS = int(player_ids_in_side_info.max()) + 1\n",
    "NUM_OPENINGS = int(opening_ids_in_side_info.max()) + 1\n",
    "NUM_ECO_LETTERS = int(opening_eco_letter_tensor.max()) + 1\n",
    "NUM_ECO_NUMBERS = int(opening_eco_number_tensor.max()) + 1\n",
    "\n",
    "print(f\"   ‚Ä¢ NUM_PLAYERS: {NUM_PLAYERS:,} (max player_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_OPENINGS: {NUM_OPENINGS:,} (max opening_id + 1)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_LETTERS: {NUM_ECO_LETTERS} (ECO letter categories)\")\n",
    "print(f\"   ‚Ä¢ NUM_ECO_NUMBERS: {NUM_ECO_NUMBERS} (ECO number categories)\")\n",
    "\n",
    "# ========================================\n",
    "# DATASET STATISTICS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Train samples: {len(scores_train):,}\")\n",
    "print(f\"   ‚Ä¢ Validation samples: {len(scores_val):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(scores_test):,}\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(scores_train) + len(scores_val) + len(scores_test):,}\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Batches per epoch: {len(scores_train) // BATCH_SIZE:,}\")\n",
    "print(f\"   ‚Ä¢ Training iterations (total): {(len(scores_train) // BATCH_SIZE) * N_EPOCHS:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HYPERPARAMETERS AND CONSTANTS DEFINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a93e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6B: DEFINE LOSS FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "üß™ Testing loss functions with dummy data...\n",
      "\n",
      "   Dummy data:\n",
      "   ‚Ä¢ Predictions: [0.5, 0.6000000238418579, 0.699999988079071, 0.800000011920929]\n",
      "   ‚Ä¢ Targets: [0.550000011920929, 0.6200000047683716, 0.6800000071525574, 0.75]\n",
      "   ‚Ä¢ Confidence: [0.800000011920929, 0.8999999761581421, 0.699999988079071, 1.0]\n",
      "\n",
      "   MSE (unweighted): 0.001450\n",
      "   MSE (weighted): 0.001512\n",
      "\n",
      "   RMSE (unweighted): 0.038079\n",
      "   RMSE (weighted): 0.038881\n",
      "\n",
      "   calculate_rmse() result: 0.038881\n",
      "   ‚úì Returns Python float (not tensor): <class 'float'>\n",
      "\n",
      "============================================================\n",
      "‚úÖ LOSS FUNCTIONS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6b: Define Loss Functions\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "print(\"STEP 6B: DEFINE LOSS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "def mse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss with optional confidence weighting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        Higher confidence = larger loss contribution\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Compute squared error\n",
    "    squared_error = (predictions - targets) ** 2\n",
    "    \n",
    "    # Apply confidence weighting if provided\n",
    "    if confidence_weights is not None:\n",
    "        weighted_squared_error = squared_error * confidence_weights\n",
    "        # Average over all samples (sum of weighted errors / sum of weights)\n",
    "        loss = weighted_squared_error.sum() / confidence_weights.sum()\n",
    "    else:\n",
    "        # Standard MSE (average over all samples)\n",
    "        loss = squared_error.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def rmse_loss(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error loss.\n",
    "    \n",
    "    This is the square root of MSE and is our primary evaluation metric.\n",
    "    RMSE is in the same units as the target (score), making it interpretable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor\n",
    "        Scalar RMSE value\n",
    "    \"\"\"\n",
    "    mse = mse_loss(predictions, targets, confidence_weights)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def calculate_rmse(predictions, targets, confidence_weights=None):\n",
    "    \"\"\"\n",
    "    Calculate RMSE metric (convenience function for evaluation).\n",
    "    \n",
    "    This is the same as rmse_loss but with clearer naming for evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : torch.Tensor\n",
    "        Model predictions (shape: [batch_size])\n",
    "    targets : torch.Tensor\n",
    "        Ground truth scores (shape: [batch_size])\n",
    "    confidence_weights : torch.Tensor, optional\n",
    "        Confidence weights for each sample (shape: [batch_size])\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        RMSE value (Python float, not tensor)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Don't compute gradients for evaluation\n",
    "        rmse = rmse_loss(predictions, targets, confidence_weights)\n",
    "    return rmse.item()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\nüß™ Testing loss functions with dummy data...\")\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_predictions = torch.tensor([0.5, 0.6, 0.7, 0.8], dtype=torch.float32)\n",
    "dummy_targets = torch.tensor([0.55, 0.62, 0.68, 0.75], dtype=torch.float32)\n",
    "dummy_confidence = torch.tensor([0.8, 0.9, 0.7, 1.0], dtype=torch.float32)\n",
    "\n",
    "print(f\"\\n   Dummy data:\")\n",
    "print(f\"   ‚Ä¢ Predictions: {dummy_predictions.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Targets: {dummy_targets.tolist()}\")\n",
    "print(f\"   ‚Ä¢ Confidence: {dummy_confidence.tolist()}\")\n",
    "\n",
    "# Test MSE without confidence weighting\n",
    "mse_unweighted = mse_loss(dummy_predictions, dummy_targets)\n",
    "print(f\"\\n   MSE (unweighted): {mse_unweighted.item():.6f}\")\n",
    "\n",
    "# Test MSE with confidence weighting\n",
    "mse_weighted = mse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"   MSE (weighted): {mse_weighted.item():.6f}\")\n",
    "\n",
    "# Test RMSE\n",
    "rmse_unweighted = rmse_loss(dummy_predictions, dummy_targets)\n",
    "rmse_weighted = rmse_loss(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   RMSE (unweighted): {rmse_unweighted.item():.6f}\")\n",
    "print(f\"   RMSE (weighted): {rmse_weighted.item():.6f}\")\n",
    "\n",
    "# Test calculate_rmse\n",
    "rmse_eval = calculate_rmse(dummy_predictions, dummy_targets, dummy_confidence)\n",
    "print(f\"\\n   calculate_rmse() result: {rmse_eval:.6f}\")\n",
    "print(f\"   ‚úì Returns Python float (not tensor): {type(rmse_eval)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LOSS FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35e00b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£  Creating PyTorch Datasets...\n",
      "   ‚úì Train dataset: 2,173,275 samples\n",
      "   ‚úì Validation dataset: 434,655 samples\n",
      "   ‚úì Test dataset: 289,770 samples\n",
      "\n",
      "2Ô∏è‚É£  Testing dataset access...\n",
      "   Sample from train_dataset[0]:\n",
      "   ‚Ä¢ player_id: 11013\n",
      "   ‚Ä¢ opening_id: 1819\n",
      "   ‚Ä¢ confidence: 0.7487\n",
      "   ‚Ä¢ score: 0.5287\n",
      "\n",
      "3Ô∏è‚É£  Creating PyTorch DataLoaders...\n"
     ]
    }
   ],
   "source": [
    "# Step 6c: Create PyTorch Dataset and DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"STEP 6C: CREATE PYTORCH DATASET AND DATALOADER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# CUSTOM DATASET CLASS\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for chess player-opening interactions.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - player_id: Integer ID for embedding lookup\n",
    "    - opening_id: Integer ID for embedding lookup\n",
    "    - confidence: Confidence weight for loss function\n",
    "    - score: Target value (win rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, player_ids, opening_ids, confidence, scores):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (long tensor)\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (long tensor)\n",
    "        confidence : torch.Tensor\n",
    "            Confidence weights (float tensor)\n",
    "        scores : torch.Tensor\n",
    "            Target scores (float tensor)\n",
    "        \"\"\"\n",
    "        self.player_ids = player_ids\n",
    "        self.opening_ids = opening_ids\n",
    "        self.confidence = confidence\n",
    "        self.scores = scores\n",
    "        \n",
    "        # Validate shapes\n",
    "        assert len(player_ids) == len(opening_ids) == len(confidence) == len(scores), \\\n",
    "            \"All tensors must have the same length\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.player_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with keys: 'player_id', 'opening_id', 'confidence', 'score'\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'player_id': self.player_ids[idx],\n",
    "            'opening_id': self.opening_ids[idx],\n",
    "            'confidence': self.confidence[idx],\n",
    "            'score': self.scores[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATASETS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Creating PyTorch Datasets...\")\n",
    "\n",
    "train_dataset = ChessOpeningDataset(\n",
    "    player_ids_train, opening_ids_train, confidence_train, scores_train\n",
    ")\n",
    "val_dataset = ChessOpeningDataset(\n",
    "    player_ids_val, opening_ids_val, confidence_val, scores_val\n",
    ")\n",
    "test_dataset = ChessOpeningDataset(\n",
    "    player_ids_test, opening_ids_test, confidence_test, scores_test\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"   ‚úì Validation dataset: {len(val_dataset):,} samples\")\n",
    "print(f\"   ‚úì Test dataset: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Test dataset access\n",
    "print(f\"\\n2Ô∏è‚É£  Testing dataset access...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"   Sample from train_dataset[0]:\")\n",
    "print(f\"   ‚Ä¢ player_id: {sample['player_id']}\")\n",
    "print(f\"   ‚Ä¢ opening_id: {sample['opening_id']}\")\n",
    "print(f\"   ‚Ä¢ confidence: {sample['confidence']:.4f}\")\n",
    "print(f\"   ‚Ä¢ score: {sample['score']:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# CREATE DATALOADERS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£  Creating PyTorch DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle training data each epoch\n",
    "    num_workers=0,  # Number of subprocesses for data loading (0 = main process)\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # Don't shuffle test data\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e5fbb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6D: DEFINE HELPER FUNCTIONS\n",
      "============================================================\n",
      "\n",
      "   format_time() tests:\n",
      "   ‚Ä¢ 30 seconds ‚Üí 30.0s\n",
      "   ‚Ä¢ 90 seconds ‚Üí 1m 30s\n",
      "   ‚Ä¢ 300 seconds ‚Üí 5m 0s\n",
      "   ‚Ä¢ 3661 seconds ‚Üí 1h 1m 1s\n",
      "   ‚Ä¢ 7384 seconds ‚Üí 2h 3m 4s\n",
      "\n",
      "   print_progress() test (simulating batch progress):\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "   Epoch 1 [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 101.00% | Loss: 0.020000 | ETA: -0.5s\n",
      "\n",
      "============================================================\n",
      "‚úÖ HELPER FUNCTIONS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6d: Define Helper Functions for Training\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"STEP 6D: DEFINE HELPER FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Save Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to save\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer state to save\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    train_loss : float\n",
    "        Training loss for this epoch\n",
    "    val_loss : float\n",
    "        Validation loss for this epoch\n",
    "    filepath : Path or str\n",
    "        Where to save the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        \n",
    "        # Save hyperparameters for reproducibility\n",
    "        'hyperparameters': {\n",
    "            'num_factors': NUM_FACTORS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'momentum': MOMENTUM,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'num_players': NUM_PLAYERS,\n",
    "            'num_openings': NUM_OPENINGS,\n",
    "            'num_eco_letters': NUM_ECO_LETTERS,\n",
    "            'num_eco_numbers': NUM_ECO_NUMBERS,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"   Checkpoint saved to: {filepath}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Load Checkpoint\n",
    "# ========================================\n",
    "\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to load weights into\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to load state into\n",
    "    filepath : Path or str\n",
    "        Where to load the checkpoint from\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        The epoch number from the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"    Checkpoint loaded from: {filepath}\")\n",
    "    print(f\"   ‚Ä¢ Epoch: {epoch}\")\n",
    "    print(f\"   ‚Ä¢ Train loss: {checkpoint['train_loss']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Val loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    return epoch\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Format Time\n",
    "# ========================================\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"\n",
    "    Format seconds as human-readable time string.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seconds : float\n",
    "        Number of seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted time string (e.g., \"1h 23m 45s\")\n",
    "    \"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# HELPER FUNCTION: Print Progress\n",
    "# ========================================\n",
    "\n",
    "def print_progress(epoch, batch_idx, total_batches, loss, elapsed_time):\n",
    "    \"\"\"\n",
    "    Print training progress with ETA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epoch : int\n",
    "        Current epoch number\n",
    "    batch_idx : int\n",
    "        Current batch index (0-based)\n",
    "    total_batches : int\n",
    "        Total number of batches per epoch\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    elapsed_time : float\n",
    "        Elapsed time in seconds since epoch start\n",
    "    \"\"\"\n",
    "    # Calculate progress\n",
    "    progress_pct = 100.0 * (batch_idx + 1) / total_batches\n",
    "    \n",
    "    # Estimate time remaining\n",
    "    time_per_batch = elapsed_time / (batch_idx + 1)\n",
    "    remaining_batches = total_batches - (batch_idx + 1)\n",
    "    eta_seconds = time_per_batch * remaining_batches\n",
    "    \n",
    "    # Print progress bar\n",
    "    bar_length = 40\n",
    "    filled_length = int(bar_length * (batch_idx + 1) / total_batches)\n",
    "    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)\n",
    "    \n",
    "    print(f\"   Epoch {epoch} [{bar}] {progress_pct:>6.2f}% | \"\n",
    "          f\"Loss: {loss:.6f} | \"\n",
    "          f\"ETA: {format_time(eta_seconds)}\", end='\\r')\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST HELPER FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "# Test format_time\n",
    "print(f\"\\n   format_time() tests:\")\n",
    "test_times = [30, 90, 300, 3661, 7384]\n",
    "for t in test_times:\n",
    "    print(f\"   ‚Ä¢ {t} seconds ‚Üí {format_time(t)}\")\n",
    "\n",
    "# Test print_progress (will overwrite line)\n",
    "print(f\"\\n   print_progress() test (simulating batch progress):\")\n",
    "for i in range(0, 101, 20):\n",
    "    total = 100\n",
    "    loss = 0.05 - i * 0.0003\n",
    "    elapsed = i * 0.5\n",
    "    print_progress(epoch=1, batch_idx=i, total_batches=total, loss=loss, elapsed_time=elapsed)\n",
    "    time.sleep(0.1)  # Brief pause to show animation\n",
    "print()  # New line after progress bar\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HELPER FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cab0cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\n",
      "============================================================\n",
      " TRAINING AND EVALUATION FUNCTIONS COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìã Training workflow:\n",
      "   1. Initialize model and optimizer\n",
      "   2. For each epoch:\n",
      "      a. Call train_one_epoch()\n",
      "      b. Call evaluate_model() on validation set\n",
      "      c. Save checkpoint with save_checkpoint()\n",
      "      d. Log metrics\n",
      "   3. After training, evaluate on test set\n"
     ]
    }
   ],
   "source": [
    "# Step 6e: Define Training and Evaluation Functions\n",
    "\n",
    "print(\"STEP 6E: DEFINE TRAINING AND EVALUATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# TRAINING FUNCTION: Train One Epoch\n",
    "# ========================================\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, device, epoch_num):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to train\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for updating model parameters\n",
    "    device : torch.device\n",
    "        Device to train on (CPU or CUDA)\n",
    "    epoch_num : int\n",
    "        Current epoch number (for logging)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (average_loss, elapsed_time)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Move batch to device\n",
    "        player_ids = batch['player_id'].to(device)\n",
    "        opening_ids = batch['opening_id'].to(device)\n",
    "        confidence = batch['confidence'].to(device)\n",
    "        targets = batch['score'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(player_ids, opening_ids)\n",
    "        \n",
    "        # Compute loss with confidence weighting\n",
    "        loss = mse_loss(predictions, targets, confidence)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 10 batches or on last batch\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print_progress(epoch_num, batch_idx, num_batches, loss.item(), elapsed)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    elapsed_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Clear progress line and print summary\n",
    "    print()  # New line after progress bar\n",
    "    \n",
    "    return avg_loss, elapsed_time\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# EVALUATION FUNCTION: Evaluate Model\n",
    "# ========================================\n",
    "\n",
    "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model to evaluate\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for evaluation data\n",
    "    device : torch.device\n",
    "        Device to evaluate on (CPU or CUDA)\n",
    "    dataset_name : str\n",
    "        Name of dataset for logging (e.g., \"Validation\", \"Test\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (mse, rmse)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_mse = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in data_loader:\n",
    "            # Move batch to device\n",
    "            player_ids = batch['player_id'].to(device)\n",
    "            opening_ids = batch['opening_id'].to(device)\n",
    "            confidence = batch['confidence'].to(device)\n",
    "            targets = batch['score'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(player_ids, opening_ids)\n",
    "            \n",
    "            # Compute MSE with confidence weighting\n",
    "            batch_mse = mse_loss(predictions, targets, confidence)\n",
    "            \n",
    "            # Track weighted sum of MSE\n",
    "            total_mse += batch_mse.item() * len(targets)\n",
    "            total_samples += len(targets)\n",
    "    \n",
    "    # Calculate average MSE and RMSE\n",
    "    avg_mse = total_mse / total_samples\n",
    "    avg_rmse = avg_mse ** 0.5\n",
    "    \n",
    "    return avg_mse, avg_rmse\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TEST FUNCTIONS (with dummy model)\n",
    "# ========================================\n",
    "\n",
    "print(\" TRAINING AND EVALUATION FUNCTIONS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã Training workflow:\")\n",
    "print(f\"   1. Initialize model and optimizer\")\n",
    "print(f\"   2. For each epoch:\")\n",
    "print(f\"      a. Call train_one_epoch()\")\n",
    "print(f\"      b. Call evaluate_model() on validation set\")\n",
    "print(f\"      c. Save checkpoint with save_checkpoint()\")\n",
    "print(f\"      d. Log metrics\")\n",
    "print(f\"   3. After training, evaluate on test set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50c5eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6F: DEFINE MODEL ARCHITECTURE\n",
      "============================================================\n",
      "\n",
      "‚úÖ ChessOpeningRecommender model class defined\n",
      "\n",
      "üìä Model Architecture:\n",
      "   Player Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(48468, 50)\n",
      "      ‚Ä¢ Biases: Embedding(48468, 1)\n",
      "      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\n",
      "      ‚Ä¢ Combiner: Linear(51, 50)\n",
      "\n",
      "   Opening Components:\n",
      "      ‚Ä¢ Latent factors: Embedding(2717, 50)\n",
      "      ‚Ä¢ Biases: Embedding(2717, 1)\n",
      "      ‚Ä¢ ECO letter embedding: Embedding(5, 4)\n",
      "      ‚Ä¢ ECO number embedding: Embedding(100, 4)\n",
      "      ‚Ä¢ Combiner: Linear(58, 50)\n",
      "\n",
      "   Prediction:\n",
      "      ‚Ä¢ Dot product of player and opening representations\n",
      "      ‚Ä¢ + player bias + opening bias + global bias\n",
      "      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\n",
      "\n",
      " Model Statistics:\n",
      "   ‚Ä¢ Total trainable parameters: 2,616,406\n",
      "   ‚Ä¢ Player parameters: 2,474,468\n",
      "   ‚Ä¢ Opening parameters: 141,937\n",
      "   ‚Ä¢ Global parameters: 1\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6f: Define Model Architecture\n",
    "\n",
    "print(\"STEP 6F: DEFINE MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Chess Opening Recommender Model\n",
    "# ========================================\n",
    "\n",
    "class ChessOpeningRecommender(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for chess opening recommendations.\n",
    "    \n",
    "    The model learns latent factors for players and openings, incorporating\n",
    "    side information:\n",
    "    - Player ratings (normalized)\n",
    "    - Opening ECO codes (letter and number as categorical features)\n",
    "    \n",
    "    Architecture:\n",
    "    - Player embedding: learnable latent factors\n",
    "    - Opening embedding: learnable latent factors\n",
    "    - Player rating: fixed side information (no embedding)\n",
    "    - ECO letter/number: categorical embeddings\n",
    "    \n",
    "    Prediction: dot product of player and opening representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_players, num_openings, num_factors,\n",
    "                 player_ratings, opening_eco_letters, opening_eco_numbers,\n",
    "                 num_eco_letters, num_eco_numbers, eco_embed_dim=4):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_players : int\n",
    "            Total number of unique players\n",
    "        num_openings : int\n",
    "            Total number of unique openings\n",
    "        num_factors : int\n",
    "            Dimensionality of latent factor embeddings\n",
    "        player_ratings : torch.Tensor\n",
    "            Z-score normalized ratings for all players (shape: [num_players])\n",
    "        opening_eco_letters : torch.Tensor\n",
    "            ECO letter categories for all openings (shape: [num_openings])\n",
    "        opening_eco_numbers : torch.Tensor\n",
    "            ECO number categories for all openings (shape: [num_openings])\n",
    "        num_eco_letters : int\n",
    "            Number of unique ECO letter categories\n",
    "        num_eco_numbers : int\n",
    "            Number of unique ECO number categories\n",
    "        eco_embed_dim : int\n",
    "            Dimensionality of ECO categorical embeddings (default: 4)\n",
    "        \"\"\"\n",
    "        super(ChessOpeningRecommender, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_players = num_players\n",
    "        self.num_openings = num_openings\n",
    "        self.num_factors = num_factors\n",
    "        self.eco_embed_dim = eco_embed_dim\n",
    "        \n",
    "        # ========================================\n",
    "        # Player Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.player_factors = torch.nn.Embedding(num_players, num_factors)\n",
    "        \n",
    "        self.player_biases = torch.nn.Embedding(num_players, 1)\n",
    "        \n",
    "        # Player ratings (fixed side information - registered as buffer, not parameter)\n",
    "        self.register_buffer('player_ratings', player_ratings)\n",
    "        \n",
    "        # ========================================\n",
    "        # Opening Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.opening_factors = torch.nn.Embedding(num_openings, num_factors)\n",
    "        \n",
    "        self.opening_biases = torch.nn.Embedding(num_openings, 1)\n",
    "        \n",
    "        # ECO letter and number (fixed side information - registered as buffers)\n",
    "        self.register_buffer('opening_eco_letters', opening_eco_letters)\n",
    "        self.register_buffer('opening_eco_numbers', opening_eco_numbers)\n",
    "        \n",
    "        # ECO embeddings (learnable)\n",
    "        self.eco_letter_embedding = torch.nn.Embedding(num_eco_letters, eco_embed_dim)\n",
    "        self.eco_number_embedding = torch.nn.Embedding(num_eco_numbers, eco_embed_dim)\n",
    "        \n",
    "        # ========================================\n",
    "        # Combination Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Combine player latent factors with rating\n",
    "        # Input: [num_factors + 1] ‚Üí Output: [num_factors]\n",
    "        self.player_combiner = torch.nn.Linear(num_factors + 1, num_factors)\n",
    "        \n",
    "        # Combine opening latent factors with ECO embeddings\n",
    "        # Input: [num_factors + 2*eco_embed_dim] ‚Üí Output: [num_factors]\n",
    "        self.opening_combiner = torch.nn.Linear(num_factors + 2 * eco_embed_dim, num_factors)\n",
    "        \n",
    "        # ========================================\n",
    "        # Global Bias\n",
    "        # ========================================\n",
    "        \n",
    "        # Global bias term (learnable scalar)\n",
    "        self.global_bias = torch.nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # ========================================\n",
    "        # Initialize Weights\n",
    "        # ========================================\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        torch.nn.init.normal_(self.player_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.opening_factors.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_letter_embedding.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(self.eco_number_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        torch.nn.init.zeros_(self.player_biases.weight)\n",
    "        torch.nn.init.zeros_(self.opening_biases.weight)\n",
    "        \n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        torch.nn.init.xavier_uniform_(self.player_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.player_combiner.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.opening_combiner.weight)\n",
    "        torch.nn.init.zeros_(self.opening_combiner.bias)\n",
    "    \n",
    "    def forward(self, player_ids, opening_ids):\n",
    "        \"\"\"\n",
    "        Forward pass: predict player-opening scores.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_ids : torch.Tensor\n",
    "            Player IDs (shape: [batch_size])\n",
    "        opening_ids : torch.Tensor\n",
    "            Opening IDs (shape: [batch_size])\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Predicted scores (shape: [batch_size])\n",
    "        \"\"\"\n",
    "        # ========================================\n",
    "        # Get Player Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get player latent factors [batch_size, num_factors]\n",
    "        player_embed = self.player_factors(player_ids)\n",
    "        \n",
    "        # Get player ratings [batch_size, 1]\n",
    "        player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "        \n",
    "        # Concatenate player factors with rating [batch_size, num_factors + 1]\n",
    "        player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "        \n",
    "        # Combine into final player representation [batch_size, num_factors]\n",
    "        player_repr = self.player_combiner(player_concat)\n",
    "        \n",
    "        # Get player bias [batch_size]\n",
    "        player_bias = self.player_biases(player_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Get Opening Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get opening latent factors [batch_size, num_factors]\n",
    "        opening_embed = self.opening_factors(opening_ids)\n",
    "        \n",
    "        # Get ECO embeddings\n",
    "        eco_letters = self.opening_eco_letters[opening_ids]  # [batch_size]\n",
    "        eco_numbers = self.opening_eco_numbers[opening_ids]  # [batch_size]\n",
    "        \n",
    "        eco_letter_embed = self.eco_letter_embedding(eco_letters)  # [batch_size, eco_embed_dim]\n",
    "        eco_number_embed = self.eco_number_embedding(eco_numbers)  # [batch_size, eco_embed_dim]\n",
    "        \n",
    "        # Concatenate opening factors with ECO embeddings\n",
    "        # [batch_size, num_factors + 2*eco_embed_dim]\n",
    "        opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "        \n",
    "        # Combine into final opening representation [batch_size, num_factors]\n",
    "        opening_repr = self.opening_combiner(opening_concat)\n",
    "        \n",
    "        # Get opening bias [batch_size]\n",
    "        opening_bias = self.opening_biases(opening_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Compute Prediction\n",
    "        # ========================================\n",
    "        \n",
    "        # Dot product of player and opening representations [batch_size]\n",
    "        interaction = (player_repr * opening_repr).sum(dim=1)\n",
    "        \n",
    "        # Add biases and global bias\n",
    "        prediction = interaction + player_bias + opening_bias + self.global_bias\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] range (since scores are win rates)\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def get_player_embedding(self, player_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific player.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        player_id : int\n",
    "            Player ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Player representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            player_ids = torch.tensor([player_id], device=self.player_factors.weight.device)\n",
    "            player_embed = self.player_factors(player_ids)\n",
    "            player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "            player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "            player_repr = self.player_combiner(player_concat)\n",
    "            return player_repr.squeeze()\n",
    "    \n",
    "    def get_opening_embedding(self, opening_id):\n",
    "        \"\"\"\n",
    "        Get the full embedding for a specific opening.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        opening_id : int\n",
    "            Opening ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Opening representation (shape: [num_factors])\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            opening_ids = torch.tensor([opening_id], device=self.opening_factors.weight.device)\n",
    "            opening_embed = self.opening_factors(opening_ids)\n",
    "            \n",
    "            eco_letters = self.opening_eco_letters[opening_ids]\n",
    "            eco_numbers = self.opening_eco_numbers[opening_ids]\n",
    "            \n",
    "            eco_letter_embed = self.eco_letter_embedding(eco_letters)\n",
    "            eco_number_embed = self.eco_number_embedding(eco_numbers)\n",
    "            \n",
    "            opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "            opening_repr = self.opening_combiner(opening_concat)\n",
    "            return opening_repr.squeeze()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model Summary\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n‚úÖ ChessOpeningRecommender model class defined\")\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Player Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_PLAYERS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_PLAYERS}, 1)\")\n",
    "print(f\"      ‚Ä¢ Ratings: Fixed side info (z-score normalized)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 1}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Opening Components:\")\n",
    "print(f\"      ‚Ä¢ Latent factors: Embedding({NUM_OPENINGS}, {NUM_FACTORS})\")\n",
    "print(f\"      ‚Ä¢ Biases: Embedding({NUM_OPENINGS}, 1)\")\n",
    "print(f\"      ‚Ä¢ ECO letter embedding: Embedding({NUM_ECO_LETTERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ ECO number embedding: Embedding({NUM_ECO_NUMBERS}, 4)\")\n",
    "print(f\"      ‚Ä¢ Combiner: Linear({NUM_FACTORS + 8}, {NUM_FACTORS})\")\n",
    "\n",
    "print(f\"\\n   Prediction:\")\n",
    "print(f\"      ‚Ä¢ Dot product of player and opening representations\")\n",
    "print(f\"      ‚Ä¢ + player bias + opening bias + global bias\")\n",
    "print(f\"      ‚Ä¢ ‚Üí Sigmoid activation (output ‚àà [0, 1])\")\n",
    "\n",
    "# Calculate total parameters\n",
    "player_factor_params = NUM_PLAYERS * NUM_FACTORS\n",
    "player_bias_params = NUM_PLAYERS\n",
    "player_combiner_params = (NUM_FACTORS + 1) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "opening_factor_params = NUM_OPENINGS * NUM_FACTORS\n",
    "opening_bias_params = NUM_OPENINGS\n",
    "eco_letter_params = NUM_ECO_LETTERS * 4\n",
    "eco_number_params = NUM_ECO_NUMBERS * 4\n",
    "opening_combiner_params = (NUM_FACTORS + 8) * NUM_FACTORS + NUM_FACTORS\n",
    "\n",
    "global_bias_params = 1\n",
    "\n",
    "total_params = (player_factor_params + player_bias_params + player_combiner_params +\n",
    "                opening_factor_params + opening_bias_params + \n",
    "                eco_letter_params + eco_number_params + opening_combiner_params +\n",
    "                global_bias_params)\n",
    "\n",
    "print(f\"\\n Model Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total trainable parameters: {total_params:,}\")\n",
    "print(f\"   ‚Ä¢ Player parameters: {player_factor_params + player_bias_params + player_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Opening parameters: {opening_factor_params + opening_bias_params + eco_letter_params + eco_number_params + opening_combiner_params:,}\")\n",
    "print(f\"   ‚Ä¢ Global parameters: {global_bias_params}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e08708d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6.1: CV CONFIGURATION & DATA SUBSETTING\n",
      "============================================================\n",
      "Total players: 48,468\n",
      "Unique strata: 16\n",
      "\n",
      "Strata distribution:\n",
      "strata\n",
      "high_few         2778\n",
      "high_lots        3480\n",
      "high_many        3007\n",
      "high_some        2840\n",
      "low_few          3548\n",
      "low_lots         2497\n",
      "low_many         2939\n",
      "low_some         3180\n",
      "med_high_few     2767\n",
      "med_high_lots    3221\n",
      "med_high_many    3083\n",
      "med_high_some    3050\n",
      "med_low_few      3027\n",
      "med_low_lots     2918\n",
      "med_low_many     3081\n",
      "med_low_some     3052\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampled 7,493 players for CV (target was 7,500)\n",
      "Total players: 48,468\n",
      "Unique strata: 16\n",
      "\n",
      "Strata distribution:\n",
      "strata\n",
      "high_few         2778\n",
      "high_lots        3480\n",
      "high_many        3007\n",
      "high_some        2840\n",
      "low_few          3548\n",
      "low_lots         2497\n",
      "low_many         2939\n",
      "low_some         3180\n",
      "med_high_few     2767\n",
      "med_high_lots    3221\n",
      "med_high_many    3083\n",
      "med_high_some    3050\n",
      "med_low_few      3027\n",
      "med_low_lots     2918\n",
      "med_low_many     3081\n",
      "med_low_some     3052\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampled 7,493 players for CV (target was 7,500)\n",
      "\n",
      "CV subset player statistics:\n",
      "  Players: 7,493\n",
      "  Interactions: 448,025\n",
      "  Rating range: -2 - 4\n",
      "  Rating mean: -0.0 (full data: -0.0)\n",
      "  Games range: 10 - 24726\n",
      "  Games mean: 4261.2 (full data: 4256.8)\n",
      "\n",
      "============================================================\n",
      "‚úÖ BALANCED SAMPLING COMPLETE\n",
      "============================================================\n",
      "\n",
      "CV subset player statistics:\n",
      "  Players: 7,493\n",
      "  Interactions: 448,025\n",
      "  Rating range: -2 - 4\n",
      "  Rating mean: -0.0 (full data: -0.0)\n",
      "  Games range: 10 - 24726\n",
      "  Games mean: 4261.2 (full data: 4256.8)\n",
      "\n",
      "============================================================\n",
      "‚úÖ BALANCED SAMPLING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6.1 CROSS VALIDATION CONFIGURATION & DATA SUBSETTING\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"STEP 6.1: CV CONFIGURATION & DATA SUBSETTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# 1. Configuration\n",
    "# ========================================\n",
    "NUM_CV_PLAYERS = 7_500\n",
    "K_FOLDS = 3\n",
    "CV_EPOCHS = 25 # Number of epochs to train for each fold/config\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"num_factors\": [5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2],\n",
    "    \"batch_size\": [512, 1024, 2048],\n",
    "}\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 2. Balanced Player Sampling\n",
    "# ========================================\n",
    "# Sample players in a balanced way by rating and number of games\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get player-level statistics\n",
    "player_stats = (\n",
    "    clean_data.groupby(\"player_id\")\n",
    "    .agg({\"num_games\": \"sum\"})  # Sum all games across all openings\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge with player_side_info to get rating_z\n",
    "player_stats = player_stats.merge(\n",
    "    player_side_info.reset_index()[[\"player_id\", \"rating_z\"]],\n",
    "    on=\"player_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Create stratification bins\n",
    "# Bin by rating (quartiles)\n",
    "player_stats['rating_bin'] = pd.qcut(player_stats['rating_z'], q=4, labels=['low', 'med_low', 'med_high', 'high'], duplicates='drop')\n",
    "\n",
    "# Bin by number of games (quartiles)\n",
    "player_stats['games_bin'] = pd.qcut(player_stats['num_games'], q=4, labels=['few', 'some', 'many', 'lots'], duplicates='drop')\n",
    "\n",
    "# Create combined stratification key\n",
    "player_stats['strata'] = player_stats['rating_bin'].astype(str) + '_' + player_stats['games_bin'].astype(str)\n",
    "\n",
    "print(f\"Total players: {len(player_stats):,}\")\n",
    "print(f\"Unique strata: {player_stats['strata'].nunique()}\")\n",
    "print(\"\\nStrata distribution:\")\n",
    "print(player_stats['strata'].value_counts().sort_index())\n",
    "\n",
    "# Sample proportionally from each stratum\n",
    "cv_player_ids = []\n",
    "for stratum in player_stats['strata'].unique():\n",
    "    stratum_players = player_stats[player_stats['strata'] == stratum]['player_id'].values\n",
    "    n_sample = max(1, int(len(stratum_players) * NUM_CV_PLAYERS / len(player_stats)))\n",
    "    n_sample = min(n_sample, len(stratum_players))  # Don't sample more than available\n",
    "    sampled = np.random.choice(stratum_players, size=n_sample, replace=False)\n",
    "    cv_player_ids.extend(sampled)\n",
    "\n",
    "cv_player_ids = np.array(cv_player_ids)\n",
    "print(f\"\\nSampled {len(cv_player_ids):,} players for CV (target was {NUM_CV_PLAYERS:,})\")\n",
    "\n",
    "# Create a dataframe containing only the interactions from the selected players\n",
    "cv_data = clean_data[clean_data[\"player_id\"].isin(cv_player_ids)].copy()\n",
    "\n",
    "# Verify balanced distribution\n",
    "cv_player_stats = player_stats[player_stats['player_id'].isin(cv_player_ids)]\n",
    "print(f\"\\nCV subset player statistics:\")\n",
    "print(f\"  Players: {len(cv_player_ids):,}\")\n",
    "print(f\"  Interactions: {len(cv_data):,}\")\n",
    "print(f\"  Rating range: {cv_player_stats['rating_z'].min():.0f} - {cv_player_stats['rating_z'].max():.0f}\")\n",
    "print(f\"  Rating mean: {cv_player_stats['rating_z'].mean():.1f} (full data: {player_stats['rating_z'].mean():.1f})\")\n",
    "print(f\"  Games range: {cv_player_stats['num_games'].min()} - {cv_player_stats['num_games'].max()}\")\n",
    "print(f\"  Games mean: {cv_player_stats['num_games'].mean():.1f} (full data: {player_stats['num_games'].mean():.1f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ BALANCED SAMPLING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6.2: K-FOLD CROSS-VALIDATION LOOP\n",
      "============================================================\n",
      "Using Random Search with 25 hyperparameter combinations\n",
      "Total CV runs: 75\n",
      "\n",
      "\n",
      "============================================================\n",
      "CONFIG 1/25: factors=5, lr=0.00100, batch=1024\n",
      "============================================================\n",
      "  Fold 1/3... RMSE: 0.0414, Spearman: 0.107, Time: 105.7s\n",
      "  Fold 2/3... RMSE: 0.0414, Spearman: 0.107, Time: 105.7s\n",
      "  Fold 2/3... "
     ]
    }
   ],
   "source": [
    "# 6.2 K-FOLD CROSS-VALIDATION LOOP WITH RANDOM SEARCH\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import numpy as np\n",
    "\n",
    "print(\"STEP 6.2: K-FOLD CROSS-VALIDATION LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ========================================\n",
    "# Random Search Configuration\n",
    "# ========================================\n",
    "N_RANDOM_SAMPLES = 25  # Number of random hyperparameter combinations to try\n",
    "random_state = 42\n",
    "\n",
    "# Use sklearn's ParameterSampler for random search\n",
    "param_sampler = ParameterSampler(\n",
    "    param_grid, n_iter=N_RANDOM_SAMPLES, random_state=random_state\n",
    ")\n",
    "\n",
    "print(f\"Using Random Search with {N_RANDOM_SAMPLES} hyperparameter combinations\")\n",
    "print(f\"Total CV runs: {N_RANDOM_SAMPLES * K_FOLDS}\")\n",
    "print()\n",
    "\n",
    "# ========================================\n",
    "# K-Fold Cross-Validation Loop\n",
    "# ========================================\n",
    "\n",
    "# Store results\n",
    "cv_results = []\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Loop through random hyperparameter combinations\n",
    "for config_idx, params in enumerate(param_sampler):\n",
    "    num_factors = params['num_factors']\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    config_str = f\"factors={num_factors}, lr={learning_rate:.5f}, batch={batch_size}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CONFIG {config_idx+1}/{N_RANDOM_SAMPLES}: {config_str}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    fold_metrics = {'rmse': [], 'mae': [], 'huber': [], 'weighted_rmse': [], 'spearman': [], 'baseline_delta': [], 'time': []}\n",
    "    \n",
    "    # K-Fold CV for this configuration\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(cv_data)):\n",
    "        fold_start_time = time.time()\n",
    "        print(f\"  Fold {fold_idx+1}/{K_FOLDS}...\", end=\" \", flush=True)\n",
    "\n",
    "        # --- Data preparation for this fold ---\n",
    "        train_fold_df = cv_data.iloc[train_idx]\n",
    "        val_fold_df = cv_data.iloc[val_idx]\n",
    "\n",
    "        # --- Convert to Tensors for this fold ---\n",
    "        train_dataset_f = ChessOpeningDataset(\n",
    "            torch.tensor(train_fold_df['player_id'].values, dtype=torch.long),\n",
    "            torch.tensor(train_fold_df['opening_id'].values, dtype=torch.long),\n",
    "            torch.tensor(train_fold_df['confidence'].values, dtype=torch.float32),\n",
    "            torch.tensor(train_fold_df['score'].values, dtype=torch.float32)\n",
    "        )\n",
    "        val_dataset_f = ChessOpeningDataset(\n",
    "            torch.tensor(val_fold_df['player_id'].values, dtype=torch.long),\n",
    "            torch.tensor(val_fold_df['opening_id'].values, dtype=torch.long),\n",
    "            torch.tensor(val_fold_df['confidence'].values, dtype=torch.float32),\n",
    "            torch.tensor(val_fold_df['score'].values, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        train_loader_f = DataLoader(train_dataset_f, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_f = DataLoader(val_dataset_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # --- Model and Optimizer for this fold ---\n",
    "        model_f = ChessOpeningRecommender(\n",
    "            num_players=NUM_PLAYERS, \n",
    "            num_openings=NUM_OPENINGS, \n",
    "            num_factors=num_factors,\n",
    "            player_ratings=player_ratings_tensor, \n",
    "            opening_eco_letters=opening_eco_letter_tensor,\n",
    "            opening_eco_numbers=opening_eco_number_tensor, \n",
    "            num_eco_letters=NUM_ECO_LETTERS,\n",
    "            num_eco_numbers=NUM_ECO_NUMBERS\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        optimizer_f = torch.optim.SGD(model_f.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "        # --- Training loop for this fold (silent mode) ---\n",
    "        model_f.train()\n",
    "        for epoch in range(1, CV_EPOCHS + 1):\n",
    "            for batch in train_loader_f:\n",
    "                optimizer_f.zero_grad()\n",
    "                predictions = model_f(\n",
    "                    batch['player_id'].to(DEVICE), \n",
    "                    batch['opening_id'].to(DEVICE)\n",
    "                )\n",
    "                loss = mse_loss(predictions, batch['score'].to(DEVICE), batch['confidence'].to(DEVICE))\n",
    "                loss.backward()\n",
    "                optimizer_f.step()\n",
    "        \n",
    "        # --- Evaluation for this fold ---\n",
    "        val_mse, val_rmse = evaluate_model(model_f, val_loader_f, DEVICE)\n",
    "        \n",
    "        # Calculate MAE and collect predictions/targets/confidence\n",
    "        model_f.eval()\n",
    "        all_preds, all_targets, all_conf = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_f:\n",
    "                preds = model_f(batch['player_id'].to(DEVICE), batch['opening_id'].to(DEVICE))\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_targets.append(batch['score'].cpu())\n",
    "                all_conf.append(batch['confidence'].cpu())\n",
    "        \n",
    "        # Concatenate all batches - keep as PyTorch tensors\n",
    "        preds_all = torch.cat(all_preds)\n",
    "        targets_all = torch.cat(all_targets)\n",
    "        conf_all = torch.cat(all_conf)\n",
    "        \n",
    "        # MAE\n",
    "        val_mae = torch.nn.functional.l1_loss(preds_all, targets_all).item()\n",
    "        \n",
    "        # ============================================================\n",
    "        # ADDITIONAL METRICS\n",
    "        # ============================================================\n",
    "        \n",
    "        # 1. Huber Loss (robust to outliers)\n",
    "        huber_fn = torch.nn.HuberLoss(delta=1.0)\n",
    "        val_huber = huber_fn(preds_all, targets_all).item()\n",
    "        \n",
    "        # 2. Weighted RMSE (using confidence as weights)\n",
    "        weighted_mse = torch.sum(conf_all * (preds_all - targets_all)**2) / torch.sum(conf_all)\n",
    "        val_weighted_rmse = torch.sqrt(weighted_mse).item()\n",
    "        \n",
    "        # 3. Spearman correlation (rank correlation between predictions and true scores)\n",
    "        # PyTorch implementation of Spearman correlation\n",
    "        def spearman_torch(x, y):\n",
    "            \"\"\"Calculate Spearman correlation using PyTorch\"\"\"\n",
    "            # Convert to ranks\n",
    "            x_rank = torch.argsort(torch.argsort(x)).float()\n",
    "            y_rank = torch.argsort(torch.argsort(y)).float()\n",
    "            # Calculate Pearson correlation on ranks\n",
    "            x_rank_centered = x_rank - x_rank.mean()\n",
    "            y_rank_centered = y_rank - y_rank.mean()\n",
    "            numerator = (x_rank_centered * y_rank_centered).sum()\n",
    "            denominator = torch.sqrt((x_rank_centered**2).sum() * (y_rank_centered**2).sum())\n",
    "            return (numerator / denominator).item()\n",
    "        \n",
    "        val_spearman = spearman_torch(preds_all, targets_all)\n",
    "        \n",
    "        # 4. Baseline comparison: RMSE if we predicted the mean target\n",
    "        mean_target = targets_all.mean()\n",
    "        baseline_mse = torch.mean((targets_all - mean_target)**2)\n",
    "        baseline_rmse = torch.sqrt(baseline_mse).item()\n",
    "        val_baseline_delta = baseline_rmse - val_rmse  # positive means we beat the baseline\n",
    "\n",
    "        fold_time = time.time() - fold_start_time\n",
    "        fold_metrics['rmse'].append(val_rmse)\n",
    "        fold_metrics['mae'].append(val_mae)\n",
    "        fold_metrics['huber'].append(val_huber)\n",
    "        fold_metrics['weighted_rmse'].append(val_weighted_rmse)\n",
    "        fold_metrics['spearman'].append(val_spearman)\n",
    "        fold_metrics['baseline_delta'].append(val_baseline_delta)\n",
    "        fold_metrics['time'].append(fold_time)\n",
    "        \n",
    "        print(f\"RMSE: {val_rmse:.4f}, Spearman: {val_spearman:.3f}, Time: {fold_time:.1f}s\")\n",
    "\n",
    "    # --- Aggregate metrics for this hyperparameter config ---\n",
    "    avg_rmse = np.mean(fold_metrics['rmse'])\n",
    "    std_rmse = np.std(fold_metrics['rmse'])\n",
    "    avg_mae = np.mean(fold_metrics['mae'])\n",
    "    std_mae = np.std(fold_metrics['mae'])\n",
    "    avg_huber = np.mean(fold_metrics['huber'])\n",
    "    avg_weighted_rmse = np.mean(fold_metrics['weighted_rmse'])\n",
    "    avg_spearman = np.mean(fold_metrics['spearman'])\n",
    "    avg_baseline_delta = np.mean(fold_metrics['baseline_delta'])\n",
    "    avg_time = np.mean(fold_metrics['time'])\n",
    "    \n",
    "    cv_results.append({\n",
    "        'num_factors': num_factors,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'mean_rmse': avg_rmse,\n",
    "        'std_rmse': std_rmse,\n",
    "        'mean_mae': avg_mae,\n",
    "        'std_mae': std_mae,\n",
    "        'mean_huber': avg_huber,\n",
    "        'mean_weighted_rmse': avg_weighted_rmse,\n",
    "        'mean_spearman': avg_spearman,\n",
    "        'mean_baseline_delta': avg_baseline_delta,\n",
    "        'mean_time_per_fold': avg_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úì Config {config_idx+1} complete: Avg RMSE = {avg_rmse:.4f} ¬± {std_rmse:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ CROSS-VALIDATION LOOP COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b79ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6.3 Analyze and Visualize Results\n",
    "# ========================================\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "results_df = results_df.sort_values(\"mean_rmse\", ascending=True)\n",
    "\n",
    "# Set up colorblind-friendly styling (grayscale + patterns)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"gray\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-VALIDATION RESULTS (Top 10)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display top 10 configs with all metrics\n",
    "display_cols = ['num_factors', 'learning_rate', 'batch_size', \n",
    "                'mean_rmse', 'mean_mae', 'mean_huber', \n",
    "                'mean_weighted_rmse', 'mean_spearman', 'mean_baseline_delta']\n",
    "print(results_df[display_cols].head(10).to_string(index=False))\n",
    "\n",
    "best_params = results_df.iloc[0]\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Num Factors:      {best_params['num_factors']}\")\n",
    "print(f\"  Learning Rate:    {best_params['learning_rate']:.6f}\")\n",
    "print(f\"  Batch Size:       {best_params['batch_size']}\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  RMSE:             {best_params['mean_rmse']:.4f} ¬± {best_params['std_rmse']:.4f}\")\n",
    "print(f\"  MAE:              {best_params['mean_mae']:.4f}\")\n",
    "print(f\"  Huber Loss:       {best_params['mean_huber']:.4f}\")\n",
    "print(f\"  Weighted RMSE:    {best_params['mean_weighted_rmse']:.4f}\")\n",
    "print(f\"  Spearman œÅ:       {best_params['mean_spearman']:.3f}\")\n",
    "print(f\"  Baseline Œî:       {best_params['mean_baseline_delta']:.4f}\")\n",
    "print(f\"  Time/Fold:        {best_params['mean_time_per_fold']:.1f}s\")\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION 1: Top 10 Configs - All Metrics Comparison\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZATION 1: Top 10 Configurations - Metric Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "top_10 = results_df.head(10).copy()\n",
    "top_10['config_label'] = top_10.apply(\n",
    "    lambda x: f\"F{x['num_factors']}_LR{x['learning_rate']:.0e}_B{x['batch_size']}\", axis=1\n",
    ")\n",
    "\n",
    "# Normalize metrics to 0-1 scale for comparison\n",
    "metrics_to_plot = ['mean_rmse', 'mean_mae', 'mean_huber', 'mean_weighted_rmse']\n",
    "top_10_normalized = top_10.copy()\n",
    "for metric in metrics_to_plot:\n",
    "    min_val = top_10[metric].min()\n",
    "    max_val = top_10[metric].max()\n",
    "    if max_val > min_val:\n",
    "        top_10_normalized[metric + '_norm'] = (top_10[metric] - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        top_10_normalized[metric + '_norm'] = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = np.arange(len(top_10))\n",
    "width = 0.2\n",
    "\n",
    "# Use different grayscale shades and hatches for colorblind accessibility\n",
    "colors = ['#000000', '#404040', '#808080', '#BFBFBF']\n",
    "hatches = ['///', '\\\\\\\\\\\\', '|||', '---']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    offset = (i - 1.5) * width\n",
    "    bars = ax.bar(x + offset, top_10_normalized[metric + '_norm'], width,\n",
    "                   label=metric.replace('mean_', '').replace('_', ' ').upper(),\n",
    "                   color=colors[i], edgecolor='black', linewidth=1.5, hatch=hatches[i])\n",
    "\n",
    "ax.set_xlabel('Configuration', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('Normalized Score (0 = Best, 1 = Worst)', fontweight='bold', fontsize=11)\n",
    "ax.set_title('Top 10 Configurations: Normalized Error Metrics Comparison\\n(Lower bars = Better performance)', \n",
    "             fontweight='bold', fontsize=13, pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(range(1, 11), fontsize=10)\n",
    "ax.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION 2: Spearman Correlation - Higher is Better\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZATION 2: Spearman Correlation (Prediction Quality)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars = ax.bar(range(1, 11), top_10['mean_spearman'], \n",
    "               color='#404040', edgecolor='black', linewidth=2, hatch='xxx')\n",
    "\n",
    "# Highlight the best config\n",
    "bars[0].set_color('#000000')\n",
    "bars[0].set_hatch('///')\n",
    "\n",
    "ax.set_xlabel('Configuration Rank', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('Spearman Correlation (œÅ)', fontweight='bold', fontsize=11)\n",
    "ax.set_title('Top 10 Configurations: Spearman Rank Correlation\\n(Higher = Better rank-order agreement)', \n",
    "             fontweight='bold', fontsize=13, pad=20)\n",
    "ax.set_xticks(range(1, 11))\n",
    "ax.axhline(y=top_10['mean_spearman'].mean(), color='red', linestyle='--', \n",
    "           linewidth=2, label=f\"Mean: {top_10['mean_spearman'].mean():.3f}\")\n",
    "ax.legend(frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION 3: Baseline Improvement\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZATION 3: Improvement Over Baseline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars = ax.bar(range(1, 11), top_10['mean_baseline_delta'], \n",
    "               color='#606060', edgecolor='black', linewidth=2, hatch='||')\n",
    "\n",
    "# Highlight the best config\n",
    "bars[0].set_color('#000000')\n",
    "bars[0].set_hatch('\\\\\\\\\\\\')\n",
    "\n",
    "ax.set_xlabel('Configuration Rank', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('RMSE Improvement Over Baseline', fontweight='bold', fontsize=11)\n",
    "ax.set_title('Top 10 Configurations: Improvement Over Mean-Prediction Baseline\\n(Higher = More improvement)', \n",
    "             fontweight='bold', fontsize=13, pad=20)\n",
    "ax.set_xticks(range(1, 11))\n",
    "ax.axhline(y=0, color='red', linestyle='-', linewidth=1, alpha=0.5)\n",
    "ax.axhline(y=top_10['mean_baseline_delta'].mean(), color='red', linestyle='--', \n",
    "           linewidth=2, label=f\"Mean: {top_10['mean_baseline_delta'].mean():.3f}\")\n",
    "ax.legend(frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION 4: Metric Correlation Heatmap\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZATION 4: Metric Correlation Matrix\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metric_cols = ['mean_rmse', 'mean_mae', 'mean_huber', 'mean_weighted_rmse', \n",
    "               'mean_spearman', 'mean_baseline_delta']\n",
    "correlation_matrix = results_df[metric_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# Use Greys colormap for true black/white/gray gradient\n",
    "im = ax.imshow(correlation_matrix, cmap='Greys', aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "# Add gridlines\n",
    "ax.set_xticks(np.arange(len(metric_cols)))\n",
    "ax.set_yticks(np.arange(len(metric_cols)))\n",
    "labels = [col.replace('mean_', '').replace('_', ' ').title() for col in metric_cols]\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_yticklabels(labels, fontsize=10)\n",
    "\n",
    "# Add correlation values as text\n",
    "for i in range(len(metric_cols)):\n",
    "    for j in range(len(metric_cols)):\n",
    "        text_color = 'white' if correlation_matrix.iloc[i, j] < -0.3 or correlation_matrix.iloc[i, j] > 0.7 else 'black'\n",
    "        text = ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                      ha=\"center\", va=\"center\", color=text_color, fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_title('Metric Correlation Matrix\\n(Darker = Stronger correlation)', \n",
    "             fontweight='bold', fontsize=13, pad=20)\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Correlation Coefficient', fontweight='bold', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION 5: Hyperparameter Influence on RMSE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZATION 5: Hyperparameter Effects on RMSE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Number of Factors\n",
    "factors_grouped = results_df.groupby('num_factors')['mean_rmse'].agg(['mean', 'std', 'count'])\n",
    "axes[0].errorbar(factors_grouped.index, factors_grouped['mean'], \n",
    "                yerr=factors_grouped['std'], fmt='o-', color='black', \n",
    "                linewidth=2, markersize=8, capsize=5, capthick=2)\n",
    "axes[0].set_xlabel('Number of Factors', fontweight='bold', fontsize=11)\n",
    "axes[0].set_ylabel('Mean RMSE', fontweight='bold', fontsize=11)\n",
    "axes[0].set_title('Effect of Embedding Size', fontweight='bold', fontsize=12)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# Learning Rate\n",
    "lr_grouped = results_df.groupby('learning_rate')['mean_rmse'].agg(['mean', 'std', 'count'])\n",
    "axes[1].errorbar(range(len(lr_grouped)), lr_grouped['mean'], \n",
    "                yerr=lr_grouped['std'], fmt='s-', color='black', \n",
    "                linewidth=2, markersize=8, capsize=5, capthick=2)\n",
    "axes[1].set_xlabel('Learning Rate', fontweight='bold', fontsize=11)\n",
    "axes[1].set_ylabel('Mean RMSE', fontweight='bold', fontsize=11)\n",
    "axes[1].set_title('Effect of Learning Rate', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xticks(range(len(lr_grouped)))\n",
    "axes[1].set_xticklabels([f'{lr:.0e}' for lr in lr_grouped.index], rotation=45, ha='right')\n",
    "axes[1].grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# Batch Size\n",
    "batch_grouped = results_df.groupby('batch_size')['mean_rmse'].agg(['mean', 'std', 'count'])\n",
    "axes[2].errorbar(batch_grouped.index, batch_grouped['mean'], \n",
    "                yerr=batch_grouped['std'], fmt='^-', color='black', \n",
    "                linewidth=2, markersize=8, capsize=5, capthick=2)\n",
    "axes[2].set_xlabel('Batch Size', fontweight='bold', fontsize=11)\n",
    "axes[2].set_ylabel('Mean RMSE', fontweight='bold', fontsize=11)\n",
    "axes[2].set_title('Effect of Batch Size', fontweight='bold', fontsize=12)\n",
    "axes[2].grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.suptitle('Hyperparameter Influence Analysis\\n(Error bars show ¬±1 std dev)', \n",
    "             fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION 6: RMSE vs Spearman Scatter\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZATION 6: RMSE vs Spearman Correlation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Different markers for different batch sizes\n",
    "batch_sizes = results_df['batch_size'].unique()\n",
    "markers = ['o', 's', '^', 'D', 'v', 'p', '*']\n",
    "colors_gray = ['#000000', '#404040', '#808080', '#A0A0A0', '#C0C0C0']\n",
    "\n",
    "for i, bs in enumerate(sorted(batch_sizes)):\n",
    "    subset = results_df[results_df['batch_size'] == bs]\n",
    "    ax.scatter(subset['mean_rmse'], subset['mean_spearman'], \n",
    "              s=100, marker=markers[i % len(markers)], \n",
    "              color=colors_gray[i % len(colors_gray)], \n",
    "              edgecolor='black', linewidth=1.5,\n",
    "              label=f'Batch Size {bs}', alpha=0.8)\n",
    "\n",
    "# Highlight best config\n",
    "ax.scatter(best_params['mean_rmse'], best_params['mean_spearman'], \n",
    "          s=400, marker='*', color='red', edgecolor='black', \n",
    "          linewidth=2, label='Best Config', zorder=10)\n",
    "\n",
    "ax.set_xlabel('Mean RMSE (Lower = Better)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Mean Spearman œÅ (Higher = Better)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Trade-off Between RMSE and Rank Correlation\\n(Ideal: Bottom-right corner)', \n",
    "             fontweight='bold', fontsize=13, pad=20)\n",
    "ax.legend(loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "ax.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION 7: Performance Summary Dashboard\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZATION 7: Performance Summary Dashboard\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Top 5 configs RMSE comparison\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "top_5 = results_df.head(5)\n",
    "bars = ax1.barh(range(5), top_5['mean_rmse'], color=['#000000', '#303030', '#606060', '#909090', '#C0C0C0'],\n",
    "                edgecolor='black', linewidth=1.5)\n",
    "ax1.set_yticks(range(5))\n",
    "ax1.set_yticklabels([f\"Config {i+1}\" for i in range(5)], fontsize=10)\n",
    "ax1.set_xlabel('Mean RMSE', fontweight='bold')\n",
    "ax1.set_title('Top 5 Configurations by RMSE', fontweight='bold', fontsize=11)\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', linestyle='--', alpha=0.4)\n",
    "\n",
    "# 2. Best config details\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "ax2.axis('off')\n",
    "details_text = f\"\"\"\n",
    "BEST CONFIGURATION\n",
    "\n",
    "Hyperparameters:\n",
    "  Factors: {best_params['num_factors']}\n",
    "  LR: {best_params['learning_rate']:.0e}\n",
    "  Batch: {best_params['batch_size']}\n",
    "\n",
    "Metrics:\n",
    "  RMSE: {best_params['mean_rmse']:.4f}\n",
    "  Spearman: {best_params['mean_spearman']:.3f}\n",
    "  Œî Baseline: {best_params['mean_baseline_delta']:.4f}\n",
    "\"\"\"\n",
    "ax2.text(0.1, 0.5, details_text, fontsize=10, verticalalignment='center',\n",
    "         family='monospace', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "# 3. RMSE distribution\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.hist(results_df['mean_rmse'], bins=15, color='#606060', \n",
    "         edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax3.axvline(best_params['mean_rmse'], color='red', linestyle='--', \n",
    "            linewidth=2, label='Best')\n",
    "ax3.set_xlabel('Mean RMSE', fontweight='bold', fontsize=9)\n",
    "ax3.set_ylabel('Frequency', fontweight='bold', fontsize=9)\n",
    "ax3.set_title('RMSE Distribution', fontweight='bold', fontsize=10)\n",
    "ax3.legend(fontsize=8)\n",
    "ax3.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "\n",
    "# 4. Spearman distribution\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax4.hist(results_df['mean_spearman'], bins=15, color='#808080', \n",
    "         edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax4.axvline(best_params['mean_spearman'], color='red', linestyle='--', \n",
    "            linewidth=2, label='Best')\n",
    "ax4.set_xlabel('Mean Spearman œÅ', fontweight='bold', fontsize=9)\n",
    "ax4.set_ylabel('Frequency', fontweight='bold', fontsize=9)\n",
    "ax4.set_title('Spearman Distribution', fontweight='bold', fontsize=10)\n",
    "ax4.legend(fontsize=8)\n",
    "ax4.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "\n",
    "# 5. Baseline delta distribution\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "ax5.hist(results_df['mean_baseline_delta'], bins=15, color='#A0A0A0', \n",
    "         edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax5.axvline(best_params['mean_baseline_delta'], color='red', linestyle='--', \n",
    "            linewidth=2, label='Best')\n",
    "ax5.set_xlabel('Baseline Improvement', fontweight='bold', fontsize=9)\n",
    "ax5.set_ylabel('Frequency', fontweight='bold', fontsize=9)\n",
    "ax5.set_title('Baseline Œî Distribution', fontweight='bold', fontsize=10)\n",
    "ax5.legend(fontsize=8)\n",
    "ax5.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "\n",
    "# 6. Num factors vs RMSE\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "for bs in sorted(results_df['batch_size'].unique()):\n",
    "    subset = results_df[results_df['batch_size'] == bs]\n",
    "    ax6.scatter(subset['num_factors'], subset['mean_rmse'], \n",
    "               label=f'BS={bs}', s=50, edgecolor='black', linewidth=1, alpha=0.7)\n",
    "ax6.set_xlabel('Num Factors', fontweight='bold', fontsize=9)\n",
    "ax6.set_ylabel('Mean RMSE', fontweight='bold', fontsize=9)\n",
    "ax6.set_title('Factors vs RMSE', fontweight='bold', fontsize=10)\n",
    "ax6.legend(fontsize=7, ncol=2)\n",
    "ax6.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# 7. Learning rate vs RMSE\n",
    "ax7 = fig.add_subplot(gs[2, 1])\n",
    "lr_values = sorted(results_df['learning_rate'].unique())\n",
    "for i, lr in enumerate(lr_values):\n",
    "    subset = results_df[results_df['learning_rate'] == lr]\n",
    "    ax7.scatter(subset['batch_size'], subset['mean_rmse'], \n",
    "               label=f'{lr:.0e}', s=50, edgecolor='black', linewidth=1, alpha=0.7)\n",
    "ax7.set_xlabel('Batch Size', fontweight='bold', fontsize=9)\n",
    "ax7.set_ylabel('Mean RMSE', fontweight='bold', fontsize=9)\n",
    "ax7.set_title('Batch Size vs RMSE', fontweight='bold', fontsize=10)\n",
    "ax7.legend(title='LR', fontsize=7, ncol=2)\n",
    "ax7.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# 8. Time vs RMSE\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "scatter = ax8.scatter(results_df['mean_time_per_fold'], results_df['mean_rmse'], \n",
    "                     c=results_df['num_factors'], cmap='Greys', \n",
    "                     s=100, edgecolor='black', linewidth=1, alpha=0.8)\n",
    "ax8.scatter(best_params['mean_time_per_fold'], best_params['mean_rmse'], \n",
    "           s=300, marker='*', color='red', edgecolor='black', linewidth=2, zorder=10)\n",
    "ax8.set_xlabel('Time per Fold (s)', fontweight='bold', fontsize=9)\n",
    "ax8.set_ylabel('Mean RMSE', fontweight='bold', fontsize=9)\n",
    "ax8.set_title('Training Time vs RMSE', fontweight='bold', fontsize=10)\n",
    "cbar = plt.colorbar(scatter, ax=ax8)\n",
    "cbar.set_label('Factors', fontsize=8)\n",
    "ax8.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.suptitle('Cross-Validation Performance Summary Dashboard', \n",
    "             fontweight='bold', fontsize=16, y=0.995)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ HYPERPARAMETER TUNING & VISUALIZATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03690a7e",
   "metadata": {},
   "source": [
    "## Step 6: Hyperparameter Tuning with K-Fold Cross-Validation\n",
    "\n",
    "Before training the final model, we will perform k-fold cross-validation to find the optimal hyperparameters. This provides a more robust evaluation than a single validation set.\n",
    "\n",
    "**Process:**\n",
    "1.  **Subset Data**: We'll use a smaller, representative sample of players to make the tuning process faster.\n",
    "2.  **K-Fold Split**: The player data will be split into `k` folds. For each fold, we train on `k-1` folds and validate on the remaining one.\n",
    "3.  **Hyperparameter Search**: We will iterate through different combinations of `NUM_FACTORS`, `LEARNING_RATE`, and `BATCH_SIZE`.\n",
    "4.  **Aggregate & Evaluate**: We'll average the performance metrics (like RMSE) across all folds for each hyperparameter combination to determine the best set.\n",
    "5.  **Visualize**: Results will be plotted to visualize the impact of each hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc2c6d",
   "metadata": {},
   "source": [
    "## Step 7: Final Model Training Setup\n",
    "\n",
    "Now we'll set up the training components for the **final model**: the model instance, optimizer, and learning rate scheduler.\n",
    "\n",
    "We'll use the best hyperparameters found during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 7: FINAL MODEL TRAINING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Hyperparameters for Final Training\n",
    "# ==================================\n",
    "# These would be set by the results from the CV step.\n",
    "# For now, we'll use the initial defaults.\n",
    "NUM_FACTORS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# ==================================\n",
    "# Model Instantiation\n",
    "# ==================================\n",
    "final_model = ChessOpeningRecommender(\n",
    "    num_players=NUM_PLAYERS,\n",
    "    num_openings=NUM_OPENINGS,\n",
    "    num_factors=NUM_FACTORS,\n",
    "    player_ratings=player_ratings_tensor,\n",
    "    opening_eco_letters=opening_eco_letter_tensor,\n",
    "    opening_eco_numbers=opening_eco_number_tensor,\n",
    "    num_eco_letters=NUM_ECO_LETTERS,\n",
    "    num_eco_numbers=NUM_ECO_NUMBERS\n",
    ").to(DEVICE)\n",
    "\n",
    "# ==================================\n",
    "# Optimizer\n",
    "# ==================================\n",
    "# We'll use SGD with momentum, a classic choice for matrix factorization.\n",
    "optimizer = torch.optim.SGD(final_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# ==================================\n",
    "# Learning Rate Scheduler\n",
    "# ==================================\n",
    "# Reduces the learning rate when a metric has stopped improving.\n",
    "# This helps to fine-tune the model in the later stages of training.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',      # The scheduler will step when the quantity monitored has stopped decreasing\n",
    "    factor=0.1,      # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "    patience=2,      # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=True     # If True, prints a message to stdout for each update\n",
    ")\n",
    "\n",
    "# ==================================\n",
    "# Checkpoint and Model Save Paths\n",
    "# ==================================\n",
    "CHECKPOINT_DIR = \"data/models\"\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"best_model.pt\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Final model hyperparameters:\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Num Factors: {NUM_FACTORS}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Num Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"\\nModel, optimizer, and scheduler initialized.\")\n",
    "print(f\"Checkpoints will be saved in: '{CHECKPOINT_DIR}'\")\n",
    "print(f\"Best model will be saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ SETUP COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becba99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0dc479f",
   "metadata": {},
   "source": [
    "## Step 8: Final Model Training Loop\n",
    "\n",
    "Here's the main training loop for the final model. We'll iterate for a specified number of epochs, training the model and evaluating its performance on the validation set periodically. We'll also save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"STEP 8: TRAINING THE FINAL MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==================================\n",
    "# Training State Tracking\n",
    "# ==================================\n",
    "history = defaultdict(list)\n",
    "best_val_rmse = float('inf')\n",
    "epochs_since_improvement = 0\n",
    "\n",
    "# ==================================\n",
    "# Main Training Loop\n",
    "# ==================================\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    train_loss, train_rmse = train_one_epoch(final_model, train_loader, optimizer, DEVICE, epoch)\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    val_loss, val_rmse = evaluate_model(final_model, val_loader, DEVICE)\n",
    "    \n",
    "    # --- Learning Rate Scheduler Step ---\n",
    "    scheduler.step(val_rmse)\n",
    "    \n",
    "    # --- Logging ---\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_rmse'].append(train_rmse)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    \n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f} | \"\n",
    "          f\"Duration: {epoch_duration:.2f}s\")\n",
    "\n",
    "    # --- Checkpoint Saving ---\n",
    "    # Save a checkpoint every epoch\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch:03d}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_rmse': val_rmse,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Save the best model based on validation RMSE\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        epochs_since_improvement = 0\n",
    "        torch.save(final_model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  -> New best model saved with Val RMSE: {val_rmse:.4f}\")\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "\n",
    "# ==================================\n",
    "# Post-Training Summary\n",
    "# ==================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ FINAL TRAINING COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best Validation RMSE: {best_val_rmse:.4f}\")\n",
    "print(f\"Best model saved to: '{BEST_MODEL_PATH}'\")\n",
    "print(f\"Last checkpoint saved to: '{checkpoint_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68202800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
