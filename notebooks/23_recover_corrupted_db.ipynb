{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88016ed",
   "metadata": {},
   "source": [
    "# Database Recovery Notebook\n",
    "\n",
    "This notebook recovers a corrupted database (`chess_games_corrupted_copy.db`) that has been converted to CSV-like format.\n",
    "\n",
    "## Recovery Plan:\n",
    "1. **Examine** the corrupted database to confirm its structure\n",
    "2. **Create** a new recovery database with proper schema\n",
    "3. **Restore** player_opening_stats data from corrupted file\n",
    "4. **Copy** player and opening tables from backup database\n",
    "5. **Verify** the recovered database\n",
    "\n",
    "## Files:\n",
    "- **Corrupted DB**: `chess_games_corrupted_copy.db` (CSV-formatted player_opening_stats)\n",
    "- **Backup DB**: `chess_games_backup_before_optimization.db` (has player & opening tables)\n",
    "- **Recovery DB**: `recovery_chess_games.db` (NEW - will not modify existing DBs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc18dc3",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "Import necessary modules for database operations and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96d25504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.database.db_utils import get_db_connection, setup_database\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459050b",
   "metadata": {},
   "source": [
    "## Step 2: Define File Paths\n",
    "Set up paths for the corrupted database, backup database, and new recovery database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27ad0a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted DB exists: True\n",
      "Backup DB exists: True\n",
      "Recovery DB exists: True (should be False)\n",
      "\n",
      "Paths:\n",
      "  Corrupted: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games_corrupted_copy.db\n",
      "  Backup: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games_backup_before_optimization.db\n",
      "  Recovery: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/recovery_chess_games.db\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_dir = Path.cwd().parent / \"data\" / \"processed\"\n",
    "corrupted_db_path = data_dir / \"chess_games_corrupted_copy.db\"\n",
    "backup_db_path = data_dir / \"chess_games_backup_before_optimization.db\"\n",
    "recovery_db_path = data_dir / \"recovery_chess_games.db\"\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Corrupted DB exists: {corrupted_db_path.exists()}\")\n",
    "print(f\"Backup DB exists: {backup_db_path.exists()}\")\n",
    "print(f\"Recovery DB exists: {recovery_db_path.exists()} (should be False)\")\n",
    "\n",
    "print(f\"\\nPaths:\")\n",
    "print(f\"  Corrupted: {corrupted_db_path}\")\n",
    "print(f\"  Backup: {backup_db_path}\")\n",
    "print(f\"  Recovery: {recovery_db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfc93d",
   "metadata": {},
   "source": [
    "## Step 3: Examine the Corrupted Database\n",
    "Load the corrupted database to understand its structure.\n",
    "\n",
    "### Expected Structure:\n",
    "Based on the provided sample, we expect:\n",
    "- **Format**: CSV-like with comma separators\n",
    "- **Columns**: `player_id`, `opening_id`, `color`, `num_wins`, `num_draws`, `num_losses`\n",
    "- **Data Types**: integers for IDs and stats, single character ('w' or 'b') for color\n",
    "- **Content**: Only player_opening_stats data (no player or opening tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db0bd958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corrupted database as CSV...\n",
      "✓ Loaded 26,843,374 rows\n",
      "\n",
      "Column names:\n",
      "['player_id', 'opening_id', 'color', 'num_wins', 'num_draws', 'num_losses']\n",
      "\n",
      "Data types:\n",
      "player_id      int64\n",
      "opening_id     int64\n",
      "color         object\n",
      "num_wins       int64\n",
      "num_draws      int64\n",
      "num_losses     int64\n",
      "dtype: object\n",
      "\n",
      "First 10 rows:\n",
      "   player_id  opening_id color  num_wins  num_draws  num_losses\n",
      "0       5770         457     w        54          7          48\n",
      "1      16740         178     w         5          0           3\n",
      "2      27190         382     w        23          4          36\n",
      "3      15977         427     w         3          0           5\n",
      "4      43758         207     w         2          0           4\n",
      "5      38292          71     w         1          1           2\n",
      "6       5415         417     w         2          0           3\n",
      "7      10179         445     w         9          1           5\n",
      "8      11333         376     w        49         10          70\n",
      "9         61         252     w         8          1           7\n",
      "✓ Loaded 26,843,374 rows\n",
      "\n",
      "Column names:\n",
      "['player_id', 'opening_id', 'color', 'num_wins', 'num_draws', 'num_losses']\n",
      "\n",
      "Data types:\n",
      "player_id      int64\n",
      "opening_id     int64\n",
      "color         object\n",
      "num_wins       int64\n",
      "num_draws      int64\n",
      "num_losses     int64\n",
      "dtype: object\n",
      "\n",
      "First 10 rows:\n",
      "   player_id  opening_id color  num_wins  num_draws  num_losses\n",
      "0       5770         457     w        54          7          48\n",
      "1      16740         178     w         5          0           3\n",
      "2      27190         382     w        23          4          36\n",
      "3      15977         427     w         3          0           5\n",
      "4      43758         207     w         2          0           4\n",
      "5      38292          71     w         1          1           2\n",
      "6       5415         417     w         2          0           3\n",
      "7      10179         445     w         9          1           5\n",
      "8      11333         376     w        49         10          70\n",
      "9         61         252     w         8          1           7\n"
     ]
    }
   ],
   "source": [
    "# Load corrupted data as CSV\n",
    "print(\"Loading corrupted database as CSV...\")\n",
    "try:\n",
    "    corrupted_data = pd.read_csv(corrupted_db_path)\n",
    "    print(f\"✓ Loaded {len(corrupted_data):,} rows\\n\")\n",
    "    \n",
    "    print(\"Column names:\")\n",
    "    print(corrupted_data.columns.tolist())\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    print(corrupted_data.dtypes)\n",
    "    \n",
    "    print(\"\\nFirst 10 rows:\")\n",
    "    print(corrupted_data.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading corrupted data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78606214",
   "metadata": {},
   "source": [
    "## Step 4: Validate Data Structure\n",
    "Confirm the data matches our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ed35329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Validation:\n",
      "✓ All expected columns present and in correct order\n",
      "\n",
      "Value Ranges:\n",
      "  player_id: 1 to 50000\n",
      "  opening_id: 1 to 3593\n",
      "  Unique players: 50,000\n",
      "  Unique players: 50,000\n",
      "  Unique openings: 3,361\n",
      "  Colors: ['w' 'b']\n",
      "\n",
      "Null Values:\n",
      "player_id     0\n",
      "opening_id    0\n",
      "color         0\n",
      "num_wins      0\n",
      "num_draws     0\n",
      "num_losses    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for expected columns\n",
    "expected_columns = ['player_id', 'opening_id', 'color', 'num_wins', 'num_draws', 'num_losses']\n",
    "actual_columns = corrupted_data.columns.tolist()\n",
    "\n",
    "print(\"Column Validation:\")\n",
    "if actual_columns == expected_columns:\n",
    "    print(\"✓ All expected columns present and in correct order\")\n",
    "else:\n",
    "    print(\"✗ Column mismatch!\")\n",
    "    print(f\"  Expected: {expected_columns}\")\n",
    "    print(f\"  Actual: {actual_columns}\")\n",
    "\n",
    "# Check value ranges\n",
    "print(\"\\nValue Ranges:\")\n",
    "print(f\"  player_id: {corrupted_data['player_id'].min()} to {corrupted_data['player_id'].max()}\")\n",
    "print(f\"  opening_id: {corrupted_data['opening_id'].min()} to {corrupted_data['opening_id'].max()}\")\n",
    "print(f\"  Unique players: {corrupted_data['player_id'].nunique():,}\")\n",
    "print(f\"  Unique openings: {corrupted_data['opening_id'].nunique():,}\")\n",
    "print(f\"  Colors: {corrupted_data['color'].unique()}\")\n",
    "\n",
    "# Check for nulls\n",
    "print(\"\\nNull Values:\")\n",
    "print(corrupted_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3ff92",
   "metadata": {},
   "source": [
    "## Step 5: Create New Recovery Database\n",
    "Initialize a new database with the proper schema using `setup_database()` from db_utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badd2dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new recovery database...\n",
      "Removed existing recovery database\n",
      "Initializing database schema...\n",
      "Database tables and partitioned stats tables are ready.\n",
      "✓ Recovery database created at: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/recovery_chess_games.db\n"
     ]
    }
   ],
   "source": [
    "# Create new recovery database with proper schema\n",
    "print(\"Creating new recovery database...\")\n",
    "\n",
    "# Delete if exists (fresh start)\n",
    "if recovery_db_path.exists():\n",
    "    recovery_db_path.unlink()\n",
    "    print(\"Removed existing recovery database\")\n",
    "\n",
    "# Create and setup schema\n",
    "with get_db_connection(recovery_db_path) as con:\n",
    "    setup_database(con)\n",
    "\n",
    "print(f\"✓ Recovery database created at: {recovery_db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfdb7a3",
   "metadata": {},
   "source": [
    "## Step 6: Copy Player and Opening Tables from Backup\n",
    "Before inserting player_opening_stats, we need the player and opening reference tables.\n",
    "Copy these from the backup database first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3084ebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying player table from backup...\n",
      "  Loaded 50,000 players from backup\n",
      "✓ Inserted 50,000 players into recovery database\n",
      "  Verified: 50,000 players in recovery database\n"
     ]
    }
   ],
   "source": [
    "# Copy player table from backup\n",
    "print(\"Copying player table from backup...\")\n",
    "with get_db_connection(backup_db_path) as backup_con:\n",
    "    players_df = backup_con.execute(\"SELECT * FROM player\").fetchdf()\n",
    "    print(f\"  Loaded {len(players_df):,} players from backup\")\n",
    "\n",
    "with get_db_connection(recovery_db_path) as recovery_con:\n",
    "    # Insert using DuckDB's register + INSERT SELECT pattern for better performance\n",
    "    recovery_con.register('players_temp', players_df)\n",
    "    recovery_con.execute(\"INSERT INTO player SELECT * FROM players_temp\")\n",
    "    print(f\"✓ Inserted {len(players_df):,} players into recovery database\")\n",
    "\n",
    "# Verify\n",
    "with get_db_connection(recovery_db_path) as recovery_con:\n",
    "    count = recovery_con.execute(\"SELECT COUNT(*) FROM player\").fetchone()[0]\n",
    "    print(f\"  Verified: {count:,} players in recovery database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "179308f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying opening table from backup...\n",
      "  Loaded 3,593 openings from backup\n",
      "✓ Inserted 3,593 openings into recovery database\n",
      "  Verified: 3,593 openings in recovery database\n"
     ]
    }
   ],
   "source": [
    "# Copy opening table from backup\n",
    "print(\"Copying opening table from backup...\")\n",
    "with get_db_connection(backup_db_path) as backup_con:\n",
    "    openings_df = backup_con.execute(\"SELECT * FROM opening\").fetchdf()\n",
    "    print(f\"  Loaded {len(openings_df):,} openings from backup\")\n",
    "\n",
    "with get_db_connection(recovery_db_path) as recovery_con:\n",
    "    # Insert using DuckDB's register + INSERT SELECT pattern\n",
    "    recovery_con.register('openings_temp', openings_df)\n",
    "    recovery_con.execute(\"INSERT INTO opening SELECT * FROM openings_temp\")\n",
    "    print(f\"✓ Inserted {len(openings_df):,} openings into recovery database\")\n",
    "\n",
    "# Verify\n",
    "with get_db_connection(recovery_db_path) as recovery_con:\n",
    "    count = recovery_con.execute(\"SELECT COUNT(*) FROM opening\").fetchone()[0]\n",
    "    print(f\"  Verified: {count:,} openings in recovery database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23deba",
   "metadata": {},
   "source": [
    "## Step 6.5: Get Opening ECO Mapping\n",
    "We need to map opening_ids to their ECO codes to determine which partition each row belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b19405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get opening ECO codes to determine partitioning\n",
    "print(\"Loading opening ECO codes for partitioning...\")\n",
    "with get_db_connection(recovery_db_path) as con:\n",
    "    openings_eco_df = con.execute(\"SELECT id, eco FROM opening\").fetchdf()\n",
    "    \n",
    "# Create a mapping from opening_id to partition letter\n",
    "def get_partition_letter(eco):\n",
    "    \"\"\"Determine which partition based on ECO code first letter.\"\"\"\n",
    "    if pd.isna(eco) or len(eco) == 0:\n",
    "        return 'other'\n",
    "    first_letter = eco[0].upper()\n",
    "    if first_letter in 'ABCDE':\n",
    "        return first_letter\n",
    "    return 'other'\n",
    "\n",
    "openings_eco_df['partition'] = openings_eco_df['eco'].apply(get_partition_letter)\n",
    "opening_to_partition = dict(zip(openings_eco_df['id'], openings_eco_df['partition']))\n",
    "\n",
    "print(f\"✓ Loaded {len(opening_to_partition):,} opening-to-partition mappings\")\n",
    "print(f\"\\\\nPartition distribution:\")\n",
    "for letter in list(\"ABCDE\") + [\"other\"]:\n",
    "    count = sum(1 for p in opening_to_partition.values() if p == letter)\n",
    "    print(f\"  {letter}: {count:,} openings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f814849",
   "metadata": {},
   "source": [
    "## Step 7: Test Insert - 1% of Data\n",
    "Before inserting all data, test with 1% to verify the process works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385f970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 268,433 rows (1% of 26,843,374 total)...\n"
     ]
    },
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: player_opening_stats is not an table",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCatalogException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTesting with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows (1% of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_rows\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m total)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Insert test data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_db_connection(recovery_db_path) \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Use DuckDB's register + INSERT SELECT pattern\u001b[39;00m\n\u001b[32m     11\u001b[39m     con.register(\u001b[33m'\u001b[39m\u001b[33mtest_temp\u001b[39m\u001b[33m'\u001b[39m, test_data)\n\u001b[32m     12\u001b[39m     con.execute(\u001b[33m\"\u001b[39m\u001b[33mINSERT INTO player_opening_stats SELECT * FROM test_temp\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_db_connection(recovery_db_path) \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Use DuckDB's register + INSERT SELECT pattern\u001b[39;00m\n\u001b[32m     11\u001b[39m     con.register(\u001b[33m'\u001b[39m\u001b[33mtest_temp\u001b[39m\u001b[33m'\u001b[39m, test_data)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mINSERT INTO player_opening_stats SELECT * FROM test_temp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Verify insertion\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_db_connection(recovery_db_path) \u001b[38;5;28;01mas\u001b[39;00m con:\n",
      "\u001b[31mCatalogException\u001b[39m: Catalog Error: player_opening_stats is not an table"
     ]
    }
   ],
   "source": [
    "# Calculate 1% subset\n",
    "total_rows = len(corrupted_data)\n",
    "test_size = int(total_rows * 0.01)\n",
    "test_data = corrupted_data.head(test_size).copy()\n",
    "\n",
    "print(f\"Testing with {test_size:,} rows (1% of {total_rows:,} total)...\")\n",
    "\n",
    "# Add partition column to test data\n",
    "test_data['partition'] = test_data['opening_id'].map(opening_to_partition)\n",
    "\n",
    "# Insert test data into appropriate partitioned tables\n",
    "with get_db_connection(recovery_db_path) as con:\n",
    "    for letter in list(\"ABCDE\") + [\"other\"]:\n",
    "        partition_data = test_data[test_data['partition'] == letter].drop('partition', axis=1)\n",
    "        if len(partition_data) > 0:\n",
    "            con.register('test_temp', partition_data)\n",
    "            con.execute(f\"INSERT INTO player_opening_stats_{letter} SELECT * FROM test_temp\")\n",
    "            print(f\"  Inserted {len(partition_data):,} rows into player_opening_stats_{letter}\")\n",
    "\n",
    "# Verify insertion\n",
    "with get_db_connection(recovery_db_path) as con:\n",
    "    count = con.execute(\"SELECT COUNT(*) FROM player_opening_stats\").fetchone()[0]\n",
    "    print(f\"\\\\n✓ Test insert successful: {count:,} rows in player_opening_stats\")\n",
    "    \n",
    "    # Show sample of inserted data\n",
    "    sample = con.execute(\"SELECT * FROM player_opening_stats LIMIT 5\").fetchdf()\n",
    "    print(\"\\\\nSample of inserted data:\")\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79ad3b",
   "metadata": {},
   "source": [
    "## Step 8: Insert Full player_opening_stats Data\n",
    "If the test was successful, proceed to insert all remaining data with progress tracking.\n",
    "\n",
    "**Note**: This may take several minutes depending on data size. Progress bar will show status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e817dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear test data first\n",
    "with get_db_connection(recovery_db_path) as con:\n",
    "    con.execute(\"DELETE FROM player_opening_stats_A\")\n",
    "    con.execute(\"DELETE FROM player_opening_stats_B\")\n",
    "    con.execute(\"DELETE FROM player_opening_stats_C\")\n",
    "    con.execute(\"DELETE FROM player_opening_stats_D\")\n",
    "    con.execute(\"DELETE FROM player_opening_stats_E\")\n",
    "    con.execute(\"DELETE FROM player_opening_stats_other\")\n",
    "    print(\"Cleared test data\")\n",
    "\n",
    "# Add partition column to full dataset\n",
    "print(\"\\\\nMapping rows to partitions...\")\n",
    "corrupted_data['partition'] = corrupted_data['opening_id'].map(opening_to_partition)\n",
    "print(\"✓ Partition mapping complete\")\n",
    "\n",
    "# Insert full dataset in chunks for better performance and progress tracking\n",
    "chunk_size = 100000  # Larger chunks for better performance\n",
    "total_rows = len(corrupted_data)\n",
    "num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "print(f\"\\\\nInserting {total_rows:,} rows in {num_chunks:,} chunks of {chunk_size:,}...\")\n",
    "\n",
    "with get_db_connection(recovery_db_path) as con:\n",
    "    for i in tqdm(range(0, total_rows, chunk_size), desc=\"Inserting data\"):\n",
    "        chunk = corrupted_data.iloc[i:i+chunk_size]\n",
    "        \n",
    "        # Insert each partition separately\n",
    "        for letter in list(\"ABCDE\") + [\"other\"]:\n",
    "            partition_chunk = chunk[chunk['partition'] == letter].drop('partition', axis=1)\n",
    "            if len(partition_chunk) > 0:\n",
    "                con.register('chunk_temp', partition_chunk)\n",
    "                con.execute(f\"INSERT INTO player_opening_stats_{letter} SELECT * FROM chunk_temp\")\n",
    "\n",
    "print(\"\\\\n✓ All data inserted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d3809",
   "metadata": {},
   "source": [
    "## Step 9: Verify Recovery\n",
    "Perform comprehensive verification of the recovered database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATABASE RECOVERY VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with get_db_connection(recovery_db_path) as con:\n",
    "    # Table counts\n",
    "    player_count = con.execute(\"SELECT COUNT(*) FROM player\").fetchone()[0]\n",
    "    opening_count = con.execute(\"SELECT COUNT(*) FROM opening\").fetchone()[0]\n",
    "    stats_count = con.execute(\"SELECT COUNT(*) FROM player_opening_stats\").fetchone()[0]\n",
    "    \n",
    "    print(f\"\\nTable Row Counts:\")\n",
    "    print(f\"  player: {player_count:,}\")\n",
    "    print(f\"  opening: {opening_count:,}\")\n",
    "    print(f\"  player_opening_stats: {stats_count:,}\")\n",
    "    \n",
    "    # Verify against original\n",
    "    print(f\"\\nData Integrity:\")\n",
    "    if stats_count == len(corrupted_data):\n",
    "        print(f\"  ✓ Row count matches corrupted data ({len(corrupted_data):,})\")\n",
    "    else:\n",
    "        print(f\"  ✗ Row count mismatch! Expected {len(corrupted_data):,}, got {stats_count:,}\")\n",
    "    \n",
    "    # Check partitioned tables\n",
    "    print(f\"\\nPartitioned Table Breakdown:\")\n",
    "    for letter in list(\"ABCDE\") + [\"other\"]:\n",
    "        count = con.execute(f\"SELECT COUNT(*) FROM player_opening_stats_{letter}\").fetchone()[0]\n",
    "        print(f\"  player_opening_stats_{letter}: {count:,}\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(f\"\\nSample Data from Recovery Database:\")\n",
    "    sample = con.execute(\"\"\"\n",
    "        SELECT * FROM player_opening_stats \n",
    "        ORDER BY RANDOM() \n",
    "        LIMIT 5\n",
    "    \"\"\").fetchdf()\n",
    "    print(sample)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOVERY COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nRecovered database saved to: {recovery_db_path}\")\n",
    "print(\"\\nNote: Some openings in the backup may be unused (we deleted some).\")\n",
    "print(\"This will be cleaned up in a future notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
