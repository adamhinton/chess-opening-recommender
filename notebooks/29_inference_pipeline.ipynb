{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d163cc",
   "metadata": {},
   "source": [
    "# Notebook 29 â€” Inference Pipeline: Predicting Opening Performance for New Players\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook demonstrates the complete inference pipeline for making opening recommendations to players **not** in our training set.\n",
    "We use the 1,000 holdout players (reserved in notebook 28) to validate our approach before deploying to production.\n",
    "\n",
    "**Production Use**: This pipeline will be used on the website to:\n",
    "- Fetch a player's game history from the Lichess API\n",
    "    - Note that for the testing in this notebook, we're just getting an untouched holdout player from our local DB.\n",
    "- Transform their opening statistics into model-ready features\n",
    "- Generate personalized opening recommendations\n",
    "- Display predictions with confidence scores\n",
    "\n",
    "**Development Focus**: We're building **granular, reusable functions** that can be easily adapted from local DB testing to production API integration.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Steps\n",
    "\n",
    "### 1. **Load Holdout Player Data**\n",
    "- Select a holdout player from the database (never seen during training)\n",
    "- Extract their complete opening statistics (wins, draws, losses per opening)\n",
    "- Retrieve player metadata (rating, name, etc.)\n",
    "- Verify data quality and completeness\n",
    "- Will use the same player every time this is run, for testing\n",
    "\n",
    "### 2. **Transform Data for Model Input**\n",
    "- Calculate raw performance scores: `(wins + 0.5 Ã— draws) / total_games`\n",
    "- Completely ignore openings not in training set\n",
    "    - We got rid of those for good reasons, not helpful/unrepresentative/too generic\n",
    "- Apply hierarchical Bayesian shrinkage toward opening-specific means\n",
    "- Normalize player rating using training set parameters (z-score)\n",
    "- Remap database IDs to training IDs (sequential 0-based indices)\n",
    "- Encode ECO codes into categorical features (letter and number)\n",
    "- Convert to PyTorch tensors\n",
    "    - player id (not sure this is needed), opening ids, eco letters and numbers, rating_z\n",
    "\n",
    "### 3. **Generate Predictions**\n",
    "- Load trained model and all required artifacts (mappings, normalization params, etc.)\n",
    "- Feed transformed data through the model\n",
    "- Generate predicted performance scores for all valid openings\n",
    "- Separate predictions into: openings player has played vs. new recommendations\n",
    "\n",
    "### 4. **Analyze and Display Results**\n",
    "- Compare predictions to actual performance (for openings player has played)\n",
    "- Rank and display top opening recommendations\n",
    "- Visualize prediction quality and confidence\n",
    "- Save predictions for later analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This notebook only performs inferenceâ€”the model weights remain fixed. We are validating the deployment pipeline, not retraining.\n",
    "\n",
    "**âœ“ Resolved**: Opening statistics files (`opening_stats_white.json`, `opening_stats_black.json`) have been created via notebook 30 to support hierarchical Bayesian shrinkage. These files contain opening-specific means calculated from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23f689",
   "metadata": {},
   "source": [
    "## Required Model Artifacts\n",
    "\n",
    "In this notebook (and in production), we will need to load various model artifact files to help in data processing.\n",
    "\n",
    "I currently believe that we only need very small files (< 1MB total), which is very doable on the client or server.\n",
    "\n",
    "From the trained model's artifact directory, we'll need:\n",
    "\n",
    "### **Core Model & Mappings**\n",
    "- **`best_model.pt`** (8.3 MB) - Trained PyTorch model weights for inference\n",
    "  - This will live on the HuggingFace space where we host our model, so we don't need to slow down production by loading it in.\n",
    "- **Note**: Need to make sure we're using a valid player id, if any. It might have to be the next sequential player id that the model's dataset would expect? Not sure, need to investigate.\n",
    "- **`opening_id_mappings.json`** (96 KB) - Maps database opening IDs â†’ training IDs (0-based sequential)\n",
    "  - *Why*: Same as above - embeddings require contiguous indices\n",
    "\n",
    "### **Normalization & Encoding**\n",
    "- **`rating_normalization.json`** (4 KB) - Contains `rating_mean` and `rating_std` from training\n",
    "  - *Why*: Must normalize new player ratings with exact same parameters: `z = (rating - mean) / std`\n",
    "- **`eco_encodings.json`** (4 KB) - Maps ECO codes to integers (letter: A-E â†’ 0-4, numbers â†’ sequential ints)\n",
    "  - *Why*: Model expects categorical integers, not raw ECO strings like \"B02\"\n",
    "\n",
    "### **Model Configuration**\n",
    "- **`hyperparameters.json`** (4 KB) - NUM_FACTORS (40), NUM_PLAYERS, NUM_OPENINGS, embedding dimensions\n",
    "  - *Why*: Need exact architecture specs to instantiate the model correctly\n",
    "- **`model_architecture.json`** (4 KB) - Complete model structure details\n",
    "  - *Why*: Documents the model class and layer configurations\n",
    "\n",
    "### **Reference Data**\n",
    "- **`opening_mappings.csv`** (168 KB) - Full opening metadata (db_id, training_id, eco, name)\n",
    "  - *Why*: Look up opening details when displaying recommendations\n",
    "- **`holdout_players.csv`** (24 KB) - List of 1,000 holdout player IDs for testing\n",
    "  - *Why*: Select test players that were never seen during training\n",
    "\n",
    "### **âœ“ Opening Statistics for Bayesian Shrinkage**\n",
    "- **`opening_stats_white.json`** (~50-100 KB) - White opening statistics (compact format)\n",
    "- **`opening_stats_black.json`** (~50-100 KB) - Black opening statistics (compact format)\n",
    "- **`opening_stats_white.csv`** (~200-300 KB) - Human-readable CSV with full columns\n",
    "- **`opening_stats_black.csv`** (~200-300 KB) - Human-readable CSV with full columns\n",
    "  - *Why*: Required for hierarchical Bayesian shrinkage during inference\n",
    "  - *Status*: **âœ“ Created via notebook 30** (`30_create_opening_stats.ipynb`)\n",
    "  - *Structure*: JSON uses compact array format `{training_id: [opening_mean, total_games, db_id]}`\n",
    "    - Index 0: `opening_mean` - Mean score for Bayesian shrinkage (float)\n",
    "    - Index 1: `total_games` - Total games across all players (int)\n",
    "    - Index 2: `db_id` - Database opening ID for reference (int)\n",
    "  - *Usage*: Load at startup, use `opening_stats[training_id][0]` for shrinkage target\n",
    "  - *CSV*: For manual inspection, contains full columns (training_id, db_id, eco, name, opening_mean, opening_total_games, opening_num_players)\n",
    "\n",
    "---\n",
    "\n",
    "**Total artifact size**: ~1MB for all files combined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc2a09",
   "metadata": {},
   "source": [
    "## Step 1: Load Holdout Player Data\n",
    "\n",
    "We'll:\n",
    "1. Load the holdout players list from model artifacts\n",
    "2. Select a deterministic player (using fixed index from specs)\n",
    "3. Verify they're NOT in the training set\n",
    "4. Extract their complete opening statistics from database (for specified color)\n",
    "5. Retrieve their player metadata\n",
    "6. Display everything for verification\n",
    "\n",
    "**Deterministic Selection**: Using `HOLDOUT_PLAYER_INDEX` from specs for consistency across runs.\n",
    "\n",
    "**Color**: Extracting data for the color specified in pipeline specs (White='w' or Black='b')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d296e",
   "metadata": {},
   "source": [
    "## Pipeline Specifications\n",
    "\n",
    "Adjustable parameters for the inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172cf5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pipeline Specifications:\n",
      "   Color: Black ('b')\n",
      "   Holdout player index: 0\n",
      "   Model directory: 20251212_152017_black\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE SPECIFICATIONS - Adjust these parameters as needed\n",
    "# ============================================================================\n",
    "\n",
    "# Color to analyze ('w' for White, 'b' for Black)\n",
    "COLOR = 'b'\n",
    "\n",
    "# Deterministic player selection (which holdout player to use)\n",
    "HOLDOUT_PLAYER_INDEX = 0\n",
    "\n",
    "# Model artifacts directory name\n",
    "MODEL_DIR_NAME = \"20251212_152017_black\"\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Derived display values\n",
    "COLOR_NAME = \"White\" if COLOR == 'w' else \"Black\"\n",
    "\n",
    "print(\" Pipeline Specifications:\")\n",
    "print(f\"   Color: {COLOR_NAME} ('{COLOR}')\")\n",
    "print(f\"   Holdout player index: {HOLDOUT_PLAYER_INDEX}\")\n",
    "print(f\"   Model directory: {MODEL_DIR_NAME}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ab21d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path for module imports\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from utils.database.db_utils import get_db_connection\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b278824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Verifying paths...\n",
      "   Database: True - /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "   Model artifacts: True - /Users/a/Documents/personalprojects/chess-opening-recommender/data/models/20251212_152017_black\n",
      "âœ“ All paths verified\n"
     ]
    }
   ],
   "source": [
    "# Configuration - paths constructed from specifications\n",
    "DB_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "MODEL_ARTIFACTS_DIR = PROJECT_ROOT / \"data\" / \"models\" / MODEL_DIR_NAME\n",
    "\n",
    "# Verify paths exist\n",
    "print(\" Verifying paths...\")\n",
    "print(f\"   Database: {DB_PATH.exists()} - {DB_PATH}\")\n",
    "print(f\"   Model artifacts: {MODEL_ARTIFACTS_DIR.exists()} - {MODEL_ARTIFACTS_DIR}\")\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")\n",
    "if not MODEL_ARTIFACTS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Model artifacts not found at {MODEL_ARTIFACTS_DIR}\")\n",
    "\n",
    "print(\"âœ“ All paths verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b111a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading holdout players...\n",
      "   Total holdout players: 1,000\n",
      "   Columns: ['db_id', 'name', 'title', 'rating']\n",
      "\n",
      "   First 5 holdout players:\n",
      "   db_id        name  title  rating\n",
      "0     53    AAashraf    NaN    1723\n",
      "1    129       AK988    NaN    1489\n",
      "2    187  ALexxxey89    NaN    1635\n",
      "3    425   Abgrenzer    NaN    1787\n",
      "4    498      Ac1000    NaN    2541\n",
      "\n",
      "âœ“ Selected holdout player #0:\n",
      "   Database ID: 53\n",
      "   Name: AAashraf\n",
      "   Rating: 1723\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1: Load holdout players list\n",
    "print(\"Loading holdout players...\")\n",
    "\n",
    "holdout_players_path = MODEL_ARTIFACTS_DIR / \"holdout_players.csv\"\n",
    "holdout_players_df = pd.read_csv(holdout_players_path)\n",
    "\n",
    "print(f\"   Total holdout players: {len(holdout_players_df):,}\")\n",
    "print(f\"   Columns: {list(holdout_players_df.columns)}\")\n",
    "print(f\"\\n   First 5 holdout players:\")\n",
    "print(holdout_players_df.head())\n",
    "\n",
    "# Select our test player deterministically (always use the first player)\n",
    "test_player_db_id = int(holdout_players_df.iloc[HOLDOUT_PLAYER_INDEX]['db_id'])\n",
    "test_player_name = holdout_players_df.iloc[HOLDOUT_PLAYER_INDEX]['name']\n",
    "test_player_rating = int(holdout_players_df.iloc[HOLDOUT_PLAYER_INDEX]['rating'])\n",
    "\n",
    "print(f\"\\nâœ“ Selected holdout player #{HOLDOUT_PLAYER_INDEX}:\")\n",
    "print(f\"   Database ID: {test_player_db_id}\")\n",
    "print(f\"   Name: {test_player_name}\")\n",
    "print(f\"   Rating: {test_player_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda4ead1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying player is not in training set...\n",
      "   Total training players: 48,551\n",
      "   Columns: ['db_id', 'name', 'title', 'rating', 'training_id']\n"
     ]
    }
   ],
   "source": [
    "# Step 1.2: Verify player is NOT in training set\n",
    "print(\"Verifying player is not in training set...\")\n",
    "\n",
    "player_mappings_path = MODEL_ARTIFACTS_DIR / \"player_mappings.csv\"\n",
    "player_mappings_df = pd.read_csv(player_mappings_path)\n",
    "\n",
    "print(f\"   Total training players: {len(player_mappings_df):,}\")\n",
    "print(f\"   Columns: {list(player_mappings_df.columns)}\")\n",
    "\n",
    "# Check if our test player is in the training set\n",
    "is_in_training = test_player_db_id in player_mappings_df['db_id'].values\n",
    "\n",
    "if is_in_training:\n",
    "    raise ValueError(\n",
    "        f\"âŒ ERROR: Player {test_player_name} (ID: {test_player_db_id}) \"\n",
    "        f\"is in the training set! They should be a holdout player.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd9ea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extracting Black opening statistics for AAashraf...\n",
      "âœ“ Database query complete\n",
      "   Total openings played: 194\n",
      "   Total games: 3,502\n",
      "âœ“ Database query complete\n",
      "   Total openings played: 194\n",
      "   Total games: 3,502\n"
     ]
    }
   ],
   "source": [
    "# Step 1.3: Extract player's opening statistics and create PlayerData object\n",
    "print(f\" Extracting {COLOR_NAME} opening statistics for {test_player_name}...\")\n",
    "\n",
    "from utils.foldin_data_processing.types import PlayerData\n",
    "\n",
    "conn = get_db_connection(DB_PATH)\n",
    "\n",
    "# Query returns data matching OpeningStatsRow schema\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    pos.player_id,\n",
    "    pos.opening_id,\n",
    "    o.eco,\n",
    "    o.name as opening_name,\n",
    "    pos.num_wins + pos.num_draws + pos.num_losses as num_games,\n",
    "    pos.num_wins,\n",
    "    pos.num_draws,\n",
    "    pos.num_losses\n",
    "FROM player_opening_stats pos\n",
    "JOIN opening o ON pos.opening_id = o.id\n",
    "WHERE pos.player_id = ?\n",
    "  AND pos.color = ?\n",
    "  AND (pos.num_wins + pos.num_draws + pos.num_losses) > 0\n",
    "ORDER BY (pos.num_wins + pos.num_draws + pos.num_losses) DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and get DataFrame conforming to OpeningStatsRow schema\n",
    "opening_stats_df = conn.execute(query, [int(test_player_db_id), COLOR]).df()\n",
    "conn.close()\n",
    "\n",
    "# Immediately wrap in PlayerData object\n",
    "player_data = PlayerData(\n",
    "    player_id=test_player_db_id,\n",
    "    name=test_player_name,\n",
    "    rating=test_player_rating,\n",
    "    color=COLOR,\n",
    "    opening_stats_df=opening_stats_df\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Database query complete\")\n",
    "print(f\"   Total openings played: {len(player_data.opening_stats_df)}\")\n",
    "print(f\"   Total games: {player_data.total_games():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90502146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA FOR TEST PLAYER: AAashraf (Black)\n",
      "================================================================================\n",
      "\n",
      " PLAYER METADATA:\n",
      "   Database ID: 53\n",
      "   Name: AAashraf\n",
      "   Rating: 1723\n",
      "   Color: Black\n",
      "   Status: HOLDOUT PLAYER (not in training set)\n",
      "\n",
      " BLACK OPENING STATISTICS SUMMARY:\n",
      "   Total openings played: 194\n",
      "   Total games: 3,502\n",
      "   Total wins: 1531\n",
      "   Total draws: 292\n",
      "   Total losses: 1679\n",
      "   Overall win rate: 43.7%\n",
      "\n",
      " TOP 5 MOST-PLAYED BLACK OPENINGS:\n",
      "                                opening_name eco  num_games  num_wins  num_draws  num_losses\n",
      "                            Philidor Defense C41       1450       642        119         689\n",
      "Queen's Pawn Game: Accelerated London System D00        206        99         20          87\n",
      "             Bishop's Opening: Boi Variation C23        136        64         11          61\n",
      "                                 Vienna Game C25        127        61         13          53\n",
      "               Queen's Pawn Game: Anti-Torre D02         98        46          7          45\n",
      "\n",
      " Black opening data extracted successfully for AAashraf\n",
      "   Ready to proceed to data transformation (Step 2)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1.4: Display complete player data for verification\n",
    "print(\"=\"*80)\n",
    "print(f\"DATA FOR TEST PLAYER: {test_player_name} ({COLOR_NAME})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n PLAYER METADATA:\")\n",
    "print(f\"   Database ID: {test_player_db_id}\")\n",
    "print(f\"   Name: {test_player_name}\")\n",
    "print(f\"   Rating: {test_player_rating}\")\n",
    "print(f\"   Color: {COLOR_NAME}\")\n",
    "print(f\"   Status: HOLDOUT PLAYER (not in training set)\")\n",
    "\n",
    "print(f\"\\n {COLOR_NAME.upper()} OPENING STATISTICS SUMMARY:\")\n",
    "print(f\"   Total openings played: {len(player_data.opening_stats_df)}\")\n",
    "print(f\"   Total games: {player_data.total_games():,}\")\n",
    "print(f\"   Total wins: {player_data.total_wins()}\")\n",
    "print(f\"   Total draws: {player_data.opening_stats_df['num_draws'].sum()}\")\n",
    "print(f\"   Total losses: {player_data.opening_stats_df['num_losses'].sum()}\")\n",
    "print(f\"   Overall win rate: {player_data.total_wins() / player_data.total_games():.1%}\")\n",
    "\n",
    "print(f\"\\n TOP 5 MOST-PLAYED {COLOR_NAME.upper()} OPENINGS:\")\n",
    "print(player_data.opening_stats_df[['opening_name', 'eco', 'num_games', 'num_wins', 'num_draws', 'num_losses']].head(5).to_string(index=False))\n",
    "\n",
    "print(f\"\\n {COLOR_NAME} opening data extracted successfully for {test_player_name}\")\n",
    "print(f\"   Ready to proceed to data transformation (Step 2)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe248fa8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# STEP 2: Transform Player Data for Model Inference\n",
    "\n",
    "This is the HEART of the production system. Every time a player visits the website, I'll:\n",
    "1. Download their games from Lichess API\n",
    "2. Process them through the pipeline below\n",
    "3. Send the result to HuggingFace for fold-in inference\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Transform raw player opening statistics into model-ready tensors that can be sent directly to HuggingFace.\n",
    "\n",
    "**Key Principle**: This transformation must be IDENTICAL whether data comes from:\n",
    "- Local DB (for notebook testing)\n",
    "- Lichess API (for production)\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "PlayerData (from Step 1)\n",
    "    |\n",
    "    v\n",
    "[2.1] Define Type System\n",
    "    |\n",
    "    v\n",
    "[2.2] Load Model Artifacts (once at startup)\n",
    "    |\n",
    "    v\n",
    "[2.3] Filter & Validate Openings\n",
    "    |\n",
    "    v\n",
    "[2.4] Calculate Raw Scores\n",
    "    |\n",
    "    v\n",
    "[2.5] Remap Database IDs -> Training IDs\n",
    "    |\n",
    "    v\n",
    "[2.6] Normalize Rating\n",
    "    |\n",
    "    v\n",
    "[2.7] Apply Bayesian Shrinkage\n",
    "    |\n",
    "    v\n",
    "[2.8] Encode ECO Features\n",
    "    |\n",
    "    v\n",
    "[2.9] Convert to NumPy Arrays\n",
    "    |\n",
    "    v\n",
    "ModelInput -> HuggingFace API\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "### 2.1: Define Type System\n",
    "\n",
    "#### ACCOMPLISHED: Defined these types and stored them in notebooks/utils/foldin_data_processing/types.py\n",
    "\n",
    "**Goal**: Create strongly-typed dataclasses for all data structures\n",
    "\n",
    "**What I'll build**:\n",
    "- `RawOpeningStats`: Input from Step 1 (database/API)\n",
    "- `PlayerData`: Complete player profile\n",
    "- `ProcessedOpening`: After transformations\n",
    "- `ModelInput`: Final structure for HuggingFace\n",
    "\n",
    "**Why this matters**:\n",
    "- Catch type errors at development time\n",
    "- Self-documenting code\n",
    "- IDE autocomplete and type checking\n",
    "- Makes data flow crystal clear\n",
    "\n",
    "**Outputs**: Python dataclasses with type hints and docstrings\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2: Load Model Artifacts\n",
    "\n",
    "**Goal**: Load all training artifacts into memory for fast lookup\n",
    "\n",
    "**Note that I will revisit this once I'm more sure what I need to load and when**\n",
    "\n",
    "**Required artifacts** (from training notebook 28):\n",
    "1. `opening_mappings.csv` - Database IDs to Training IDs\n",
    "2. `rating_normalization.json` - Rating mean/std\n",
    "3. `eco_encodings.json` - ECO letter/number mappings\n",
    "4. `opening_stats.json` - NEED TO CREATE THIS\n",
    "\n",
    "**Data structures to build**:\n",
    "- `valid_training_ids: Set[int]` - O(1) lookup for filtering\n",
    "- `db_to_training_id: Dict[int, int]` - O(1) ID remapping\n",
    "- `opening_means: Dict[int, float]` - Bayesian shrinkage targets\n",
    "- `eco_letter_map: Dict[str, int]` - ECO letter encoding\n",
    "- `eco_number_map: Dict[str, int]` - ECO number encoding\n",
    "\n",
    "**Critical**: Load ONCE at startup, cache in production\n",
    "\n",
    "**Outputs**: Dictionary of all artifacts plus fast lookup structures\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3: Filter & Validate Openings\n",
    "\n",
    "**Goal**: Remove openings not in training set or below threshold\n",
    "\n",
    "**NOTE**: This is originally being written as a proof-of-concept, tested against a holdout player who was part of our training db (but the model wasn't trained on their games). So, in development, this should filter out few or no openings. But in dev, we'll be getting players directly from lichess so they will have openings filtered out.\n",
    "\n",
    "**Filtering criteria**:\n",
    "1. Opening must be in training set (check against `valid_training_ids`)\n",
    "2. Player must have played >= `MIN_GAMES_THRESHOLD` with this opening (default: 10)\n",
    "\n",
    "**Edge case handling**:\n",
    "- What if player has 0 valid openings after filtering? -> Raise `ValueError`\n",
    "- What if opening ID not in mapping? -> Skip silently (expected)\n",
    "\n",
    "**Efficiency**: Use Set for O(1) membership testing\n",
    "\n",
    "**Outputs**: Filtered list of `RawOpeningStats`\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4: Calculate Raw Scores\n",
    "\n",
    "**Goal**: Convert W/D/L to performance scores\n",
    "\n",
    "**Formula**: \n",
    "```\n",
    "raw_score = (wins + 0.5 * draws) / total_games\n",
    "```\n",
    "\n",
    "**Validation**:\n",
    "- Score must be in range [0.0, 1.0]\n",
    "- Handle edge case where `num_games = 0` (shouldn't happen after filtering)\n",
    "\n",
    "**Outputs**: Add `raw_score` property to each opening\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5: Remap Opening IDs\n",
    "\n",
    "**Goal**: Convert database IDs to training IDs (0-based sequential)\n",
    "\n",
    "**Why this is necessary**:\n",
    "- Database IDs are arbitrary (10, 15, 23, ...)\n",
    "- Model embedding layers expect 0-based sequential IDs (0, 1, 2, ...)\n",
    "- Training created this mapping during preprocessing\n",
    "\n",
    "**Lookup**: Use `db_to_training_id` dictionary (O(1))\n",
    "\n",
    "**Validation**: Ensure all remapped IDs are in valid range [0, num_openings)\n",
    "\n",
    "**Outputs**: Add `training_opening_id` column to DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6: Normalize Rating\n",
    "\n",
    "**Goal**: Z-score normalize player rating\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "rating_z = (rating - RATING_MEAN) / RATING_STD\n",
    "```\n",
    "\n",
    "**Constants from artifacts**:\n",
    "- `RATING_MEAN` and `RATING_STD` from `rating_normalization.json`\n",
    "\n",
    "**Why this matters**:\n",
    "- Model was trained on normalized ratings\n",
    "- Keeps all features on similar scales\n",
    "\n",
    "**Outputs**: Single float `rating_z`\n",
    "\n",
    "---\n",
    "\n",
    "### 2.7: Apply Bayesian Shrinkage\n",
    "\n",
    "**Goal**: Adjust scores toward opening-specific means (hierarchical Bayesian)\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "adjusted_score = (num_games * raw_score + k * opening_mean) / (num_games + k)\n",
    "confidence = num_games / (num_games + k)\n",
    "```\n",
    "\n",
    "**Constants** (must match training):\n",
    "- `k = 50` (shrinkage strength)\n",
    "- `opening_mean` from `opening_stats.json` (keyed by training_id)\n",
    "\n",
    "**Why this matters**:\n",
    "- Prevents overfitting to small sample sizes\n",
    "- Makes inference consistent with training\n",
    "- Different openings have different baseline win rates\n",
    "- Confidence weights tell the model how reliable each score is\n",
    "\n",
    "**Critical**: Use opening-specific means, NOT global mean. Must use training_id (not db_id) for lookup.\n",
    "\n",
    "**Outputs**: Add `adjusted_score` and `confidence` columns to DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“Š Opening Stats Files Documentation\n",
    "\n",
    "The opening statistics files were created in **notebook 30** (`30_create_opening_stats.ipynb`) to support hierarchical Bayesian shrinkage during inference.\n",
    "\n",
    "**Files created for each color**:\n",
    "- `opening_stats_{color}.json` - Compact format for production (~50-100 KB per color)\n",
    "- `opening_stats_{color}.csv` - Human-readable format for inspection (~200-300 KB per color)\n",
    "\n",
    "**JSON Structure (Compact Array Format)**:\n",
    "```json\n",
    "{\n",
    "  \"training_id\": [opening_mean, total_games, db_id],\n",
    "  \"0\": [0.5234, 15847, 123],\n",
    "  \"1\": [0.4891, 8932, 456],\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Array indices**:\n",
    "- `[0]`: `opening_mean` (float) - Mean score for this opening across all training players. Use this for Bayesian shrinkage.\n",
    "- `[1]`: `total_games` (int) - Total games played with this opening across all players.\n",
    "- `[2]`: `db_id` (int) - Original database opening ID (for reference/debugging).\n",
    "\n",
    "**Usage in Step 2.5**:\n",
    "```python\n",
    "# Load at startup (once)\n",
    "with open(f'opening_stats_{color}.json', 'r') as f:\n",
    "    opening_stats = json.load(f)\n",
    "\n",
    "# During shrinkage (per opening)\n",
    "training_id = db_to_training_id[opening_db_id]\n",
    "opening_mean = opening_stats[str(training_id)][0]  # JSON keys are strings\n",
    "adjusted_score = (num_games * raw_score + k * opening_mean) / (num_games + k)\n",
    "```\n",
    "\n",
    "**CSV Structure** (for manual inspection):\n",
    "- Columns: `training_id`, `db_id`, `opening_id`, `eco`, `name`, `opening_mean`, `opening_total_games`, `opening_num_players`\n",
    "- Sorted by training_id for easy lookup\n",
    "- Contains full opening names and ECO codes for human readability\n",
    "\n",
    "**Data Source**:\n",
    "- Aggregated from `player_opening_stats` table in training database\n",
    "- Filtered to openings with â‰¥10 games per player (same threshold as training)\n",
    "- Calculated as: `mean((wins + 0.5 * draws) / total_games)` across all players\n",
    "\n",
    "**Why compact format?**:\n",
    "- Original verbose dict format was ~500 KB (too large for production)\n",
    "- Compact array format reduces size to ~50-100 KB per color\n",
    "- Faster to load and parse in production environment\n",
    "- Keys are training IDs (not db IDs) for O(1) lookup during inference\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6: Normalize Rating\n",
    "\n",
    "**Goal**: Z-score normalize player rating\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "rating_z = (rating - RATING_MEAN) / RATING_STD\n",
    "```\n",
    "\n",
    "**Constants from artifacts**:\n",
    "- `RATING_MEAN` and `RATING_STD` from `rating_normalization.json`\n",
    "\n",
    "**Why this matters**:\n",
    "- Model was trained on normalized ratings\n",
    "- Keeps all features on similar scales\n",
    "\n",
    "**Outputs**: Single float `rating_z`\n",
    "\n",
    "---\n",
    "\n",
    "### 2.7: Apply Bayesian Shrinkage\n",
    "\n",
    "**Goal**: Adjust scores toward opening-specific means (hierarchical Bayesian)\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "adjusted_score = (num_games * raw_score + k * opening_mean) / (num_games + k)\n",
    "confidence = num_games / (num_games + k)\n",
    "```\n",
    "\n",
    "**Constants** (must match training):\n",
    "- `k = 50` (shrinkage strength)\n",
    "- `opening_mean` from `opening_stats.json` (keyed by training_id)\n",
    "\n",
    "**Why this matters**:\n",
    "- Prevents overfitting to small sample sizes\n",
    "- Makes inference consistent with training\n",
    "- Different openings have different baseline win rates\n",
    "- Confidence weights tell the model how reliable each score is\n",
    "\n",
    "**Critical**: Use opening-specific means, NOT global mean. Must use training_id (not db_id) for lookup.\n",
    "\n",
    "**Outputs**: Add `adjusted_score` and `confidence` columns to DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“Š Opening Stats Files Documentation\n",
    "\n",
    "The opening statistics files were created in **notebook 30** (`30_create_opening_stats.ipynb`) to support hierarchical Bayesian shrinkage during inference.\n",
    "\n",
    "**Files created for each color**:\n",
    "- `opening_stats_{color}.json` - Compact format for production (~50-100 KB per color)\n",
    "- `opening_stats_{color}.csv` - Human-readable format for inspection (~200-300 KB per color)\n",
    "\n",
    "**JSON Structure (Compact Array Format)**:\n",
    "```json\n",
    "{\n",
    "  \"training_id\": [opening_mean, total_games, db_id],\n",
    "  \"0\": [0.5234, 15847, 123],\n",
    "  \"1\": [0.4891, 8932, 456],\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Array indices**:\n",
    "- `[0]`: `opening_mean` (float) - Mean score for this opening across all training players. Use this for Bayesian shrinkage.\n",
    "- `[1]`: `total_games` (int) - Total games played with this opening across all players.\n",
    "- `[2]`: `db_id` (int) - Original database opening ID (for reference/debugging).\n",
    "\n",
    "**Usage in Step 2.7**:\n",
    "```python\n",
    "# Load at startup (once)\n",
    "with open(f'opening_stats_{color}.json', 'r') as f:\n",
    "    opening_stats = json.load(f)\n",
    "\n",
    "# During shrinkage (per opening)\n",
    "training_id = row['training_opening_id']  # Already remapped in Step 2.5\n",
    "opening_mean = opening_stats[str(training_id)][0]  # JSON keys are strings\n",
    "adjusted_score = (num_games * raw_score + k * opening_mean) / (num_games + k)\n",
    "```\n",
    "\n",
    "**CSV Structure** (for manual inspection):\n",
    "- Columns: `training_id`, `db_id`, `opening_id`, `eco`, `name`, `opening_mean`, `opening_total_games`, `opening_num_players`\n",
    "- Sorted by training_id for easy lookup\n",
    "- Contains full opening names and ECO codes for human readability\n",
    "\n",
    "**Data Source**:\n",
    "- Aggregated from `player_opening_stats` table in training database\n",
    "- Filtered to openings with â‰¥10 games per player (same threshold as training)\n",
    "- Calculated as: `mean((wins + 0.5 * draws) / total_games)` across all players\n",
    "\n",
    "**Why compact format?**:\n",
    "- Original verbose dict format was ~500 KB (too large for production)\n",
    "- Compact array format reduces size to ~50-100 KB per color\n",
    "- Faster to load and parse in production environment\n",
    "- Keys are training IDs (not db IDs) for O(1) lookup during inference\n",
    "\n",
    "---\n",
    "\n",
    "### 2.8: Encode ECO Features\n",
    "\n",
    "**Goal**: Convert ECO strings to categorical integers\n",
    "\n",
    "**ECO structure**: \"C21\" -> letter (\"C\") + number (\"21\")\n",
    "\n",
    "**Encoding**:\n",
    "- `eco_letter_cat`: \"A\"->0, \"B\"->1, \"C\"->2, \"D\"->3, \"E\"->4\n",
    "- `eco_number_cat`: \"00\"->0, \"01\"->1, ..., \"99\"->99 (sequential)\n",
    "\n",
    "**Edge case**: What if ECO not in mapping? -> Log warning, use default value\n",
    "\n",
    "**Why this matters**:\n",
    "- Model learns patterns by ECO category (e.g., all \"C\" openings share features)\n",
    "- Categorical encoding is more efficient than one-hot\n",
    "\n",
    "**Outputs**: Add `eco_letter_cat` and `eco_number_cat` to each `ProcessedOpening`\n",
    "\n",
    "---\n",
    "\n",
    "### 2.9: Build ModelInput Structure\n",
    "\n",
    "**Goal**: Convert processed openings to NumPy arrays for HuggingFace\n",
    "\n",
    "**Output structure**:\n",
    "```python\n",
    "ModelInput(\n",
    "    player_name: str,\n",
    "    rating_z: float,\n",
    "    color: str,\n",
    "    opening_ids: np.ndarray,      # shape (N,), dtype int64\n",
    "    eco_letter_cats: np.ndarray,  # shape (N,), dtype int64\n",
    "    eco_number_cats: np.ndarray,  # shape (N,), dtype int64\n",
    "    scores: np.ndarray,           # shape (N,), dtype float32\n",
    "    confidence: np.ndarray,       # shape (N,), dtype float32\n",
    "    opening_names: List[str],     # for display\n",
    "    num_openings_filtered: int,   # metadata\n",
    "    total_games: int              # metadata\n",
    ")\n",
    "```\n",
    "\n",
    "**Methods**:\n",
    "- `to_json_dict()` -> JSON for HuggingFace API\n",
    "- `to_tensors()` -> PyTorch tensors for local testing\n",
    "\n",
    "**Why NumPy not PyTorch**: \n",
    "- NumPy serializes to JSON easily\n",
    "- HuggingFace endpoint will convert to tensors on its side\n",
    "\n",
    "**Outputs**: Ready-to-send `ModelInput` object\n",
    "\n",
    "---\n",
    "\n",
    "### 2.10: Main Transformation Function\n",
    "\n",
    "**Goal**: Orchestrate all chunks into one reusable function\n",
    "\n",
    "**Signature**:\n",
    "```python\n",
    "def transform_player_for_inference(\n",
    "    player_data: PlayerData,\n",
    "    model_artifacts: Dict,\n",
    "    min_games_threshold: int = 10,\n",
    "    k_shrinkage: int = 50\n",
    ") -> ModelInput:\n",
    "    \"\"\"Production-ready transformation function.\"\"\"\n",
    "```\n",
    "\n",
    "**This function is the PUBLIC API** - it's what will be called in production\n",
    "\n",
    "**Error handling**:\n",
    "- Raise `ValueError` if no valid openings\n",
    "- Raise `FileNotFoundError` if artifacts missing\n",
    "- Log warnings for edge cases\n",
    "\n",
    "**Outputs**: Single `ModelInput` object ready for inference\n",
    "\n",
    "---\n",
    "\n",
    "### 2.11: Test & Validate\n",
    "\n",
    "**Goal**: Verify transformation on holdout player\n",
    "\n",
    "**Tests**:\n",
    "1. Run transformation on test player from Step 1\n",
    "2. Check ID ranges (0 <= opening_id < num_openings)\n",
    "3. Verify scores in [0, 1]\n",
    "4. Confirm ECO encodings are valid\n",
    "5. Validate rating normalization\n",
    "6. Check JSON serialization works\n",
    "7. Compare with training data format\n",
    "\n",
    "**Edge cases to test**:\n",
    "- Player with many openings\n",
    "- Player with few openings\n",
    "- Player near MIN_GAMES_THRESHOLD\n",
    "- High/low rated players\n",
    "\n",
    "**Outputs**: Validated `ModelInput` plus test results\n",
    "\n",
    "---\n",
    "\n",
    "## Critical Notes\n",
    "\n",
    "### ID Systems - PAY ATTENTION\n",
    "\n",
    "- **Database IDs**: What's in the DB and Lichess API (arbitrary integers)\n",
    "- **Training IDs**: 0-based sequential IDs used by model (0, 1, 2, ...)\n",
    "- **Note:** When I originally trained the model, I tried to normalize these two, mutating the DB ids to match the new training IDs. But, it turned out that was really hard so I stopped trying.\n",
    "- **Transformation happens in 2.7**\n",
    "\n",
    "### Bayesian Shrinkage - MUST MATCH TRAINING\n",
    "\n",
    "- Training used opening-specific means (hierarchical)\n",
    "- Inference MUST use the same approach\n",
    "- Requires `opening_stats.json` (keyed by training_id)\n",
    "\n",
    "### Production vs Testing\n",
    "\n",
    "- Testing: Data from local DB (Step 1)\n",
    "- Production: Data from Lichess API (future code)\n",
    "- Transformation is IDENTICAL - that's the whole point\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "- Load artifacts ONCE at startup\n",
    "- Use Sets for O(1) membership testing\n",
    "- Use Dicts for O(1) ID remapping\n",
    "- Vectorize NumPy operations (no loops)\n",
    "\n",
    "---\n",
    "\n",
    "## Before Starting Coding Checklist\n",
    "\n",
    "- [ ] Do I have all 4 artifact files?\n",
    "  - [ ] `opening_mappings.csv`\n",
    "  - [ ] `rating_normalization.json`\n",
    "  - [ ] `eco_encodings.json`\n",
    "  - [ ] `opening_stats.json` (NEED TO CREATE)\n",
    "\n",
    "- [ ] Have I reviewed the training preprocessing to match exactly?\n",
    "- [ ] Do I understand the ID remapping workflow?\n",
    "- [ ] Am I clear on hierarchical vs global Bayesian shrinkage?\n",
    "\n",
    "---\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "Step 2 is complete when:\n",
    "1. All type definitions are complete and well-documented\n",
    "2. All artifacts load successfully\n",
    "3. Transformation function runs without errors\n",
    "4. Output structure matches HuggingFace API expectations\n",
    "5. All validation checks pass\n",
    "6. Edge cases are handled gracefully\n",
    "7. Code is production-ready and reusable\n",
    "\n",
    "---\n",
    "\n",
    "Ready to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9cd5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Define Tpes, Classes and Data Structures\n",
    "\n",
    "# We've done that in notebooks/utils/foldin_data_processing/types.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34e7258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Load model artifacts\n",
    "# Load artifacts here, not in individual cells below\n",
    "\n",
    "# Load opening mappings\n",
    "opening_mappings_df = pd.read_csv(MODEL_ARTIFACTS_DIR / \"opening_mappings.csv\")\n",
    "valid_training_opening_ids = set(opening_mappings_df[\"db_id\"].values)\n",
    "\n",
    "# For step 2.6 rating normalization:\n",
    "# Load rating normalization parameters\n",
    "rating_norm_path = MODEL_ARTIFACTS_DIR / \"rating_normalization.json\"\n",
    "\n",
    "if not rating_norm_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Rating normalization file not found: {rating_norm_path}\\n\"\n",
    "        f\"This file should have been created during training (notebook 28).\"\n",
    "    )\n",
    "\n",
    "with open(rating_norm_path, \"r\") as f:\n",
    "    rating_norm_params = json.load(f)\n",
    "\n",
    "RATING_MEAN = rating_norm_params[\"rating_mean\"]\n",
    "RATING_STD = rating_norm_params[\"rating_std\"]\n",
    "\n",
    "# Done with loading artifacts for step 2.6: Normalize player rating\n",
    "\n",
    "# Load opening stats for step 2.7 Bayesian shrinkage\n",
    "opening_stats_path = MODEL_ARTIFACTS_DIR / f\"opening_stats_{'white' if COLOR == 'w' else 'black'}.json\"\n",
    "\n",
    "if not opening_stats_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Opening stats file not found: {opening_stats_path}\\n\"\n",
    "        f\"You need to run notebook 30 (create_opening_stats.ipynb) first to create this file.\"\n",
    "    )\n",
    "\n",
    "with open(opening_stats_path, 'r') as f:\n",
    "    opening_stats = json.load(f)\n",
    "\n",
    "# Done with loading artifacts for step 2.7: Apply Bayesian shrinkage\n",
    "\n",
    "# Load ECO encodings for step 2.8: Encode ECO features\n",
    "eco_encodings_path = MODEL_ARTIFACTS_DIR / \"eco_encodings.json\"\n",
    "\n",
    "if not eco_encodings_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"ECO encodings file not found: {eco_encodings_path}\\n\"\n",
    "        f\"This file should have been created during training (notebook 28).\"\n",
    "    )\n",
    "\n",
    "with open(eco_encodings_path, 'r') as f:\n",
    "    eco_encodings = json.load(f)\n",
    "\n",
    "eco_letter_map = eco_encodings['eco_letter_to_int']\n",
    "eco_number_map = eco_encodings['eco_number_to_int']\n",
    "\n",
    "# Done with loading artifacts for step 2.8: Encode ECO features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cc245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757a233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Filter and Validate Openings\n",
    "\n",
    "# I axed certain openings from the training data because they were unrepresentative, had weird scores, low sample size, etc\n",
    "\n",
    "# Resultingly, this pipeline will completely ignore those openings when making recommendations. This cell filters them out.\n",
    "\n",
    "# NOTE: This function is originally being tested as a proof-of-concept against a holdout player who is extracted from a local db where the openings in question have already been filtered out.\n",
    "    # So you won't see any openings dropped here when you run it against that holdout player.\n",
    "# However, in prod, when we run against real users whose data is not pre-filtered, we should see openings dropped here.\n",
    "\n",
    "\n",
    "def filter_valid_openings(\n",
    "    player_data: PlayerData,\n",
    "    valid_opening_local_db_ids: set[int],\n",
    "    min_num_games_threshold: int = 10,\n",
    ") -> PlayerData:\n",
    "    \"\"\"\n",
    "    Filter player's opening statistics to only include openings\n",
    "    that exist in the training set and meet minimum game threshold.\n",
    "\n",
    "    Args:\n",
    "        player_data: Complete player profile from Step 1\n",
    "        valid_opening_local_db_ids: Set of database opening IDs from training\n",
    "        min_num_games_threshold: Minimum games required per opening (default: 10)\n",
    "\n",
    "    Returns:\n",
    "        New PlayerData object with filtered opening_stats_df\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no valid openings remain after filtering\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_data = filter_valid_openings(player_data, valid_ids, min_num_games_threshold=10)\n",
    "        >>> print(f\"Kept {len(filtered_data.opening_stats_df)} / {len(player_data.opening_stats_df)} openings\")\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 2.3: Filtering Valid Openings\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    original_df = player_data.opening_stats_df\n",
    "    print(f\"\\nOriginal data:\")\n",
    "    print(f\"   Total openings: {len(original_df)}\")\n",
    "    print(f\"   Total games: {original_df['num_games'].sum():,}\")\n",
    "    print(f\"   Training set size: {len(valid_opening_local_db_ids):,} openings\")\n",
    "\n",
    "    # Filter 1: Opening must be in training set (O(1) lookup with set)\n",
    "    print(f\"\\nFilter 1: Must be in training set...\")\n",
    "    in_training_mask = original_df[\"opening_id\"].isin(valid_opening_local_db_ids)\n",
    "    after_training_filter = original_df[in_training_mask].copy()\n",
    "\n",
    "    dropped_not_in_training = len(original_df) - len(after_training_filter)\n",
    "    print(f\"   Kept: {len(after_training_filter)} openings\")\n",
    "    print(f\"   Dropped: {dropped_not_in_training} openings (not in training set)\")\n",
    "\n",
    "    if dropped_not_in_training > 0:\n",
    "        dropped_games = original_df[~in_training_mask][\"num_games\"].sum()\n",
    "        print(\n",
    "            f\"   Games in dropped openings: {dropped_games:,} ({dropped_games / original_df['num_games'].sum():.1%} of total)\"\n",
    "        )\n",
    "\n",
    "    # Filter 2: Must have minimum games\n",
    "    print(f\"\\nFilter 2: Must have >= {min_num_games_threshold} games...\")\n",
    "    games_mask = after_training_filter[\"num_games\"] >= min_num_games_threshold\n",
    "    filtered_df = after_training_filter[games_mask].copy()\n",
    "\n",
    "    dropped_low_games = len(after_training_filter) - len(filtered_df)\n",
    "    print(f\"   Kept: {len(filtered_df)} openings\")\n",
    "    print(f\"   Dropped: {dropped_low_games} openings (< {min_num_games_threshold} games)\")\n",
    "\n",
    "    if dropped_low_games > 0:\n",
    "        dropped_games = after_training_filter[~games_mask][\"num_games\"].sum()\n",
    "        print(f\"   Games in dropped openings: {dropped_games:,}\")\n",
    "\n",
    "    # Validation: Must have at least one opening remaining\n",
    "    if len(filtered_df) == 0:\n",
    "        total_dropped = dropped_not_in_training + dropped_low_games\n",
    "        raise ValueError(\n",
    "            f\"No valid openings remaining for player {player_data.name}!\\n\"\n",
    "            f\"   Original openings: {len(original_df)}\\n\"\n",
    "            f\"   Dropped (not in training): {dropped_not_in_training}\\n\"\n",
    "            f\"   Dropped (< {min_num_games_threshold} games): {dropped_low_games}\\n\"\n",
    "            f\"   Remaining: 0\\n\"\n",
    "            f\"Cannot make recommendations for this player.\"\n",
    "        )\n",
    "\n",
    "    # Create new PlayerData with filtered DataFrame\n",
    "    filtered_player_data = PlayerData(\n",
    "        player_id=player_data.player_id,\n",
    "        name=player_data.name,\n",
    "        rating=player_data.rating,\n",
    "        color=player_data.color,\n",
    "        opening_stats_df=filtered_df,\n",
    "    )\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\nâœ“ Filtering complete:\")\n",
    "    print(\n",
    "        f\"   Final openings: {len(filtered_df)} ({len(filtered_df) / len(original_df):.1%} of original)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   Final games: {filtered_df['num_games'].sum():,} ({filtered_df['num_games'].sum() / original_df['num_games'].sum():.1%} of original)\"\n",
    "    )\n",
    "    print(f\"   Total dropped: {dropped_not_in_training + dropped_low_games} openings\")\n",
    "\n",
    "    # Show sample of kept openings\n",
    "    print(f\"\\n   Top 5 kept openings (by games):\")\n",
    "    top_5 = filtered_df.nlargest(5, \"num_games\")[[\"opening_name\", \"eco\", \"num_games\"]]\n",
    "    for idx, row in top_5.iterrows():\n",
    "        print(\n",
    "            f\"      {row['eco']:>3} | {row['opening_name']:<50} | {row['num_games']:>4} games\"\n",
    "        )\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return filtered_player_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5642c29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2.3: Filtering Valid Openings\n",
      "================================================================================\n",
      "\n",
      "Original data:\n",
      "   Total openings: 194\n",
      "   Total games: 3,502\n",
      "   Training set size: 2,728 openings\n",
      "\n",
      "Filter 1: Must be in training set...\n",
      "   Kept: 192 openings\n",
      "   Dropped: 2 openings (not in training set)\n",
      "   Games in dropped openings: 2 (0.1% of total)\n",
      "\n",
      "Filter 2: Must have >= 3 games...\n",
      "   Kept: 94 openings\n",
      "   Dropped: 98 openings (< 3 games)\n",
      "   Games in dropped openings: 138\n",
      "\n",
      "âœ“ Filtering complete:\n",
      "   Final openings: 94 (48.5% of original)\n",
      "   Final games: 3,362 (96.0% of original)\n",
      "   Total dropped: 100 openings\n",
      "\n",
      "   Top 5 kept openings (by games):\n",
      "      C41 | Philidor Defense                                   | 1450 games\n",
      "      D00 | Queen's Pawn Game: Accelerated London System       |  206 games\n",
      "      C23 | Bishop's Opening: Boi Variation                    |  136 games\n",
      "      C25 | Vienna Game                                        |  127 games\n",
      "      D02 | Queen's Pawn Game: Anti-Torre                      |   98 games\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "player_data = filter_valid_openings(\n",
    "    player_data,\n",
    "    valid_opening_local_db_ids=valid_training_opening_ids,\n",
    "    min_num_games_threshold=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a557483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2.4: Calculating Raw Scores\n",
      "================================================================================\n",
      "\n",
      "âœ“ Raw scores computed\n",
      "   Openings: 94\n",
      "   Total games: 3,362\n",
      "   raw_score range: [0.0000, 1.0000]\n",
      "   raw_score mean:  0.5035\n",
      "\n",
      "Top 5 most-played openings with raw_score:\n",
      "                                opening_name eco  num_games  num_wins  num_draws  num_losses  raw_score\n",
      "                            Philidor Defense C41       1450       642        119         689   0.483793\n",
      "Queen's Pawn Game: Accelerated London System D00        206        99         20          87   0.529126\n",
      "             Bishop's Opening: Boi Variation C23        136        64         11          61   0.511029\n",
      "                                 Vienna Game C25        127        61         13          53   0.531496\n",
      "               Queen's Pawn Game: Anti-Torre D02         98        46          7          45   0.505102\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>opening_id</th>\n",
       "      <th>eco</th>\n",
       "      <th>opening_name</th>\n",
       "      <th>num_games</th>\n",
       "      <th>num_wins</th>\n",
       "      <th>num_draws</th>\n",
       "      <th>num_losses</th>\n",
       "      <th>raw_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>1854</td>\n",
       "      <td>C41</td>\n",
       "      <td>Philidor Defense</td>\n",
       "      <td>1450</td>\n",
       "      <td>642</td>\n",
       "      <td>119</td>\n",
       "      <td>689</td>\n",
       "      <td>0.483793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>3203</td>\n",
       "      <td>D00</td>\n",
       "      <td>Queen's Pawn Game: Accelerated London System</td>\n",
       "      <td>206</td>\n",
       "      <td>99</td>\n",
       "      <td>20</td>\n",
       "      <td>87</td>\n",
       "      <td>0.529126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>1555</td>\n",
       "      <td>C23</td>\n",
       "      <td>Bishop's Opening: Boi Variation</td>\n",
       "      <td>136</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>0.511029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>1584</td>\n",
       "      <td>C25</td>\n",
       "      <td>Vienna Game</td>\n",
       "      <td>127</td>\n",
       "      <td>61</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>0.531496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>2462</td>\n",
       "      <td>D02</td>\n",
       "      <td>Queen's Pawn Game: Anti-Torre</td>\n",
       "      <td>98</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>0.505102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   player_id  opening_id  eco                                  opening_name  \\\n",
       "0         53        1854  C41                              Philidor Defense   \n",
       "1         53        3203  D00  Queen's Pawn Game: Accelerated London System   \n",
       "2         53        1555  C23               Bishop's Opening: Boi Variation   \n",
       "3         53        1584  C25                                   Vienna Game   \n",
       "4         53        2462  D02                 Queen's Pawn Game: Anti-Torre   \n",
       "\n",
       "   num_games  num_wins  num_draws  num_losses  raw_score  \n",
       "0       1450       642        119         689   0.483793  \n",
       "1        206        99         20          87   0.529126  \n",
       "2        136        64         11          61   0.511029  \n",
       "3        127        61         13          53   0.531496  \n",
       "4         98        46          7          45   0.505102  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2.4: Calculate Raw Scores (vectorized, production-ready)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def add_raw_scores(player_data: PlayerData) -> PlayerData:\n",
    "    \"\"\"\n",
    "    Add raw performance score per opening:\n",
    "        raw_score = (wins + 0.5 * draws) / num_games\n",
    "\n",
    "    Assumes player_data.opening_stats_df contains:\n",
    "      - num_wins, num_draws, num_losses, num_games\n",
    "\n",
    "    Returns a NEW PlayerData with an additional 'raw_score' float column.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 2.4: Calculating Raw Scores\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    df = player_data.opening_stats_df.copy()\n",
    "\n",
    "    required_cols = {\"num_wins\", \"num_draws\", \"num_losses\", \"num_games\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"opening_stats_df missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    # Sanity checks\n",
    "    if (df[\"num_games\"] < 0).any():\n",
    "        raise ValueError(\"Found negative num_games values (invalid data).\")\n",
    "\n",
    "    # Enforce consistency if num_games is present (it is in your query)\n",
    "    computed_games = df[\"num_wins\"] + df[\"num_draws\"] + df[\"num_losses\"]\n",
    "    inconsistent = (computed_games != df[\"num_games\"])\n",
    "    if inconsistent.any():\n",
    "        bad = df.loc[inconsistent, [\"opening_id\", \"num_wins\", \"num_draws\", \"num_losses\", \"num_games\"]].head(10)\n",
    "        raise ValueError(\n",
    "            \"num_games is inconsistent with wins+draws+losses for some rows. \"\n",
    "            \"Sample:\\n\" + bad.to_string(index=False)\n",
    "        )\n",
    "\n",
    "    # Vectorized score calculation; guard against divide-by-zero\n",
    "    denom = df[\"num_games\"].replace({0: np.nan})\n",
    "    df[\"raw_score\"] = (df[\"num_wins\"] + 0.5 * df[\"num_draws\"]) / denom\n",
    "    df[\"raw_score\"] = df[\"raw_score\"].fillna(0.0).astype(float)\n",
    "\n",
    "    # Range validation (allow tiny floating error)\n",
    "    min_score = float(df[\"raw_score\"].min())\n",
    "    max_score = float(df[\"raw_score\"].max())\n",
    "    if min_score < -1e-9 or max_score > 1.0 + 1e-9:\n",
    "        bad = df.loc[(df[\"raw_score\"] < 0) | (df[\"raw_score\"] > 1), [\"opening_id\", \"num_games\", \"num_wins\", \"num_draws\", \"raw_score\"]].head(10)\n",
    "        raise ValueError(\n",
    "            f\"raw_score out of range [0,1]. Observed range [{min_score:.6f}, {max_score:.6f}]. \"\n",
    "            \"Sample offending rows:\\n\" + bad.to_string(index=False)\n",
    "        )\n",
    "\n",
    "    # Logging\n",
    "    print(f\"\\nâœ“ Raw scores computed\")\n",
    "    print(f\"   Openings: {len(df):,}\")\n",
    "    print(f\"   Total games: {int(df['num_games'].sum()):,}\")\n",
    "    print(f\"   raw_score range: [{min_score:.4f}, {max_score:.4f}]\")\n",
    "    print(f\"   raw_score mean:  {float(df['raw_score'].mean()):.4f}\")\n",
    "\n",
    "    # Show the top 5 by volume with their raw scores\n",
    "    preview_cols = [\"opening_name\", \"eco\", \"num_games\", \"num_wins\", \"num_draws\", \"num_losses\", \"raw_score\"]\n",
    "    top5 = df.nlargest(5, \"num_games\")[preview_cols]\n",
    "    print(f\"\\nTop 5 most-played openings with raw_score:\")\n",
    "    print(top5.to_string(index=False))\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return PlayerData(\n",
    "        player_id=player_data.player_id,\n",
    "        name=player_data.name,\n",
    "        rating=player_data.rating,\n",
    "        color=player_data.color,\n",
    "        opening_stats_df=df,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Then add raw scores\n",
    "player_data = add_raw_scores(player_data)\n",
    "\n",
    "# Display the result\n",
    "player_data.opening_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1d5ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.5: Remap Database IDs to Training IDs (vectorized, production-ready)\n",
    "\n",
    "def remap_opening_ids(\n",
    "    player_data: PlayerData, \n",
    "    opening_mappings_df: pd.DataFrame\n",
    ") -> PlayerData:\n",
    "    \"\"\"\n",
    "    Remap database opening IDs to 0-based sequential training IDs.\n",
    "    \n",
    "    The model's embedding layers expect sequential IDs starting at 0. Database IDs\n",
    "    are arbitrary integers that may have gaps (e.g., 10, 15, 23...) because some\n",
    "    openings were filtered out during training preprocessing.\n",
    "    \n",
    "    This function performs O(1) lookup using a dictionary created from the mappings\n",
    "    CSV file generated during training (notebook 28).\n",
    "    \n",
    "    Args:\n",
    "        player_data: Player profile with opening_stats_df containing 'opening_id' (db IDs)\n",
    "        opening_mappings_df: DataFrame with columns ['db_id', 'training_id']\n",
    "    \n",
    "    Returns:\n",
    "        New PlayerData with additional 'training_opening_id' column added to opening_stats_df\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any opening_id cannot be mapped (should never happen after Step 2.3 filtering)\n",
    "    \n",
    "    Example:\n",
    "        >>> # After loading: opening_mappings_df = pd.read_csv(\"opening_mappings.csv\")\n",
    "        >>> player_data = remap_opening_ids(player_data, opening_mappings_df)\n",
    "        >>> print(player_data.opening_stats_df[['opening_id', 'training_opening_id']].head())\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 2.5: Remapping Database IDs to Training IDs\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = player_data.opening_stats_df.copy()\n",
    "    \n",
    "    # Create O(1) lookup dictionary: db_id -> training_id\n",
    "    db_to_training = dict(zip(opening_mappings_df['db_id'], opening_mappings_df['training_id']))\n",
    "    \n",
    "    print(f\"\\nMapping statistics:\")\n",
    "    print(f\"   Total mappings available: {len(db_to_training):,}\")\n",
    "    print(f\"   Training ID range: [0, {max(db_to_training.values())}]\")\n",
    "    print(f\"   Player openings to remap: {len(df)}\")\n",
    "    \n",
    "    # Vectorized mapping\n",
    "    df['training_opening_id'] = df['opening_id'].map(db_to_training)\n",
    "    \n",
    "    # Validation: Check for unmapped IDs (should never happen after Step 2.3)\n",
    "    unmapped = df['training_opening_id'].isna()\n",
    "    if unmapped.any():\n",
    "        unmapped_ids = df.loc[unmapped, 'opening_id'].unique()\n",
    "        raise ValueError(\n",
    "            f\"Found {unmapped.sum()} openings that could not be mapped to training IDs!\\n\"\n",
    "            f\"   Unmapped database IDs: {sorted(unmapped_ids)}\\n\"\n",
    "            f\"This should never happen after Step 2.3 filtering. Check opening_mappings.csv.\"\n",
    "        )\n",
    "    \n",
    "    # Convert to int (was float due to NaN check)\n",
    "    df['training_opening_id'] = df['training_opening_id'].astype(int)\n",
    "    \n",
    "    # Validation: Check ID range\n",
    "    min_training_id = int(df['training_opening_id'].min())\n",
    "    max_training_id = int(df['training_opening_id'].max())\n",
    "    expected_max = len(db_to_training) - 1\n",
    "    \n",
    "    if min_training_id < 0 or max_training_id > expected_max:\n",
    "        raise ValueError(\n",
    "            f\"Training IDs out of expected range [0, {expected_max}]!\\n\"\n",
    "            f\"   Observed range: [{min_training_id}, {max_training_id}]\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nâœ“ ID remapping complete\")\n",
    "    print(f\"   All {len(df)} openings successfully mapped\")\n",
    "    print(f\"   Training ID range: [{min_training_id}, {max_training_id}]\")\n",
    "    \n",
    "    # Show sample mappings\n",
    "    print(f\"\\n   Sample mappings (db_id -> training_id):\")\n",
    "    sample = df.nlargest(5, 'num_games')[['opening_name', 'eco', 'opening_id', 'training_opening_id', 'num_games']]\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"      {row['eco']:>3} | {row['opening_name']:<45} | {row['opening_id']:>4} -> {row['training_opening_id']:>4} | {row['num_games']:>4} games\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return PlayerData(\n",
    "        player_id=player_data.player_id,\n",
    "        name=player_data.name,\n",
    "        rating=player_data.rating,\n",
    "        color=player_data.color,\n",
    "        opening_stats_df=df\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65055f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2.5: Remapping Database IDs to Training IDs\n",
      "================================================================================\n",
      "\n",
      "Mapping statistics:\n",
      "   Total mappings available: 2,728\n",
      "   Training ID range: [0, 2727]\n",
      "   Player openings to remap: 94\n",
      "\n",
      "âœ“ ID remapping complete\n",
      "   All 94 openings successfully mapped\n",
      "   Training ID range: [10, 2627]\n",
      "\n",
      "   Sample mappings (db_id -> training_id):\n",
      "      C41 | Philidor Defense                              | 1854 -> 1441 | 1450 games\n",
      "      D00 | Queen's Pawn Game: Accelerated London System  | 3203 -> 2466 |  206 games\n",
      "      C23 | Bishop's Opening: Boi Variation               | 1555 -> 1222 |  136 games\n",
      "      C25 | Vienna Game                                   | 1584 -> 1239 |  127 games\n",
      "      D02 | Queen's Pawn Game: Anti-Torre                 | 2462 -> 1889 |   98 games\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "player_data = remap_opening_ids(player_data, opening_mappings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e8d6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.6: Normalize Player Rating\n",
    "\n",
    "def normalize_player_rating(\n",
    "    player_data: PlayerData,\n",
    "    rating_mean: float,\n",
    "    rating_std: float\n",
    ") -> PlayerData:\n",
    "    \"\"\"\n",
    "    Apply z-score normalization to player rating.\n",
    "    \n",
    "    This uses the exact same normalization parameters that were calculated\n",
    "    during training from the full training dataset. This ensures that the\n",
    "    model receives rating values in the same scale it was trained on.\n",
    "    \n",
    "    Formula: rating_z = (rating - RATING_MEAN) / RATING_STD\n",
    "    \n",
    "    Args:\n",
    "        player_data: Complete player profile from previous steps\n",
    "        rating_mean: Mean rating from training data\n",
    "        rating_std: Standard deviation of ratings from training data\n",
    "    \n",
    "    Returns:\n",
    "        New PlayerData object with rating_z attribute added\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If rating is missing or invalid\n",
    "    \n",
    "    Example:\n",
    "        >>> normalized_data = normalize_player_rating(player_data, RATING_MEAN, RATING_STD)\n",
    "        >>> print(f\"Rating: {player_data.rating} â†’ Z-score: {normalized_data.rating_z:.4f}\")\n",
    "    \"\"\"\n",
    "    print(\"STEP 2.6: Normalizing Player Rating\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Validation: Check rating exists and is valid\n",
    "    if player_data.rating is None:\n",
    "        raise ValueError(\n",
    "            f\"Player {player_data.name} has no rating! \"\n",
    "            f\"Cannot proceed with inference.\"\n",
    "        )\n",
    "    \n",
    "    if not isinstance(player_data.rating, (int, float)) or player_data.rating <= 0:\n",
    "        raise ValueError(\n",
    "            f\"Invalid rating for player {player_data.name}: {player_data.rating}. \"\n",
    "            f\"Rating must be a positive number.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nPlayer: {player_data.name}\")\n",
    "    print(f\"   â€¢ Raw rating: {player_data.rating}\")\n",
    "    \n",
    "    # Apply z-score normalization\n",
    "    # This is performed the same way as during training\n",
    "    rating_z = (player_data.rating - rating_mean) / rating_std\n",
    "    \n",
    "    print(f\"   â€¢ Result: rating_z = {rating_z:.4f}\")\n",
    "    \n",
    "    # Validation: Check if rating_z is reasonable\n",
    "    if abs(rating_z) > 5:\n",
    "        print(f\"\\n   âš ï¸  WARNING: rating_z = {rating_z:.4f} is unusually extreme!\")\n",
    "        print(f\"   â€¢ This suggests the player's rating is far outside the training distribution\")\n",
    "        print(f\"   â€¢ Model predictions may be less reliable\")\n",
    "    \n",
    "    # Create new PlayerData with rating_z added\n",
    "    normalized_player_data = PlayerData(\n",
    "        player_id=player_data.player_id,\n",
    "        name=player_data.name,\n",
    "        rating=player_data.rating,\n",
    "        color=player_data.color,\n",
    "        opening_stats_df=player_data.opening_stats_df,\n",
    "        rating_z=rating_z\n",
    "    )\n",
    "    \n",
    "    print(f\"   â€¢ rating_z = {rating_z:.4f}\")\n",
    "    \n",
    "    return normalized_player_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "293c1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2.6: Normalizing Player Rating\n",
      "================================================================================\n",
      "\n",
      "Player: AAashraf\n",
      "   â€¢ Raw rating: 1723\n",
      "   â€¢ Result: rating_z = -0.1696\n",
      "   â€¢ rating_z = -0.1696\n",
      "   â€¢ Name: AAashraf\n",
      "   â€¢ Rating (raw): 1723\n",
      "   â€¢ Rating (z-score): -0.1696\n",
      "   â€¢ Color: b\n",
      "   â€¢ Openings: 94\n"
     ]
    }
   ],
   "source": [
    "# Apply step 2.6: rating normalization\n",
    "player_data = normalize_player_rating(\n",
    "    player_data,\n",
    "    rating_mean=RATING_MEAN,\n",
    "    rating_std=RATING_STD\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(f\"   â€¢ Name: {player_data.name}\")\n",
    "print(f\"   â€¢ Rating (raw): {player_data.rating}\")\n",
    "print(f\"   â€¢ Rating (z-score): {player_data.rating_z:.4f}\")\n",
    "print(f\"   â€¢ Color: {player_data.color}\")\n",
    "print(f\"   â€¢ Openings: {len(player_data.opening_stats_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce3e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.7: Apply Bayesian Shrinkage (vectorized, production-ready)\n",
    "\n",
    "def apply_bayesian_shrinkage(\n",
    "    player_data: PlayerData,\n",
    "    opening_stats_dict: dict[str, list[float]],\n",
    "    k_shrinkage: int = 50\n",
    ") -> PlayerData:\n",
    "    \"\"\"\n",
    "    Apply hierarchical Bayesian shrinkage to opening scores.\n",
    "    \n",
    "    This adjusts each player's raw opening score toward that opening's mean performance\n",
    "    across all training players. The adjustment is stronger for openings with fewer games\n",
    "    (less reliable data) and weaker for openings with many games (more reliable data).\n",
    "    \n",
    "    This is the EXACT same transformation applied during training (notebook 28). The formula\n",
    "    shrinks toward opening-specific means, not a global mean, which is critical for accuracy\n",
    "    since different openings have vastly different baseline win rates.\n",
    "    \n",
    "    Formula:\n",
    "        adjusted_score = (num_games * raw_score + k * opening_mean) / (num_games + k)\n",
    "        confidence = num_games / (num_games + k)\n",
    "    \n",
    "    Where:\n",
    "        - raw_score: Player's win rate with this opening (from Step 2.4)\n",
    "        - num_games: Number of games player has with this opening\n",
    "        - opening_mean: Average score across all training players for this opening\n",
    "        - k: Shrinkage strength (higher k = more shrinkage toward mean)\n",
    "        - confidence: Weight for the model's loss function (0=low confidence, 1=high confidence)\n",
    "    \n",
    "    The confidence weight tells the model: \"Trust this score more for high-game-count openings,\n",
    "    less for low-game-count openings.\" This prevents overfitting to lucky streaks.\n",
    "    \n",
    "    Args:\n",
    "        player_data: Player profile with 'raw_score' and 'training_opening_id' columns\n",
    "        opening_stats_dict: Loaded from opening_stats_{color}.json, format:\n",
    "                           {training_id: [opening_mean, total_games, db_id]}\n",
    "        k_shrinkage: Shrinkage constant (default: 50, MUST match training value)\n",
    "    \n",
    "    Returns:\n",
    "        New PlayerData with 'adjusted_score' and 'confidence' columns added\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If training_opening_id is missing or opening_mean lookup fails\n",
    "    \n",
    "    Example:\n",
    "        >>> # Load opening stats once at startup\n",
    "        >>> with open(f'opening_stats_{color}.json', 'r') as f:\n",
    "        >>>     opening_stats = json.load(f)\n",
    "        >>> \n",
    "        >>> # Apply shrinkage (per player)\n",
    "        >>> player_data = apply_bayesian_shrinkage(player_data, opening_stats, k_shrinkage=50)\n",
    "        >>> print(player_data.opening_stats_df[['opening_name', 'num_games', 'raw_score', 'adjusted_score', 'confidence']].head())\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 2.7: Applying Bayesian Shrinkage\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = player_data.opening_stats_df.copy()\n",
    "    \n",
    "    # Validation: Ensure prerequisites are met\n",
    "    required_cols = {'raw_score', 'training_opening_id', 'num_games'}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"opening_stats_df missing required columns: {sorted(missing)}\\n\"\n",
    "            f\"   Ensure Step 2.4 (raw scores) and Step 2.5 (ID remapping) have run.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"   Shrinkage constant (k): {k_shrinkage}\")\n",
    "    print(f\"   Opening stats loaded: {len(opening_stats_dict):,} openings\")\n",
    "    print(f\"   Player openings to process: {len(df)}\")\n",
    "    \n",
    "    # Vectorized lookup of opening means\n",
    "    # JSON keys are strings, so convert training_id to string for lookup\n",
    "    def get_opening_mean(training_id: int) -> float:\n",
    "        \"\"\"Extract opening mean from stats dict, with error handling.\"\"\"\n",
    "        key = str(training_id)\n",
    "        if key not in opening_stats_dict:\n",
    "            raise ValueError(\n",
    "                f\"Training ID {training_id} not found in opening_stats!\\n\"\n",
    "                f\"This should never happen. Check opening_stats_{player_data.color}.json.\"\n",
    "            )\n",
    "        return opening_stats_dict[key][0]  # Index 0 = opening_mean\n",
    "    \n",
    "    df['opening_mean'] = df['training_opening_id'].apply(get_opening_mean)\n",
    "    \n",
    "    # Vectorized Bayesian shrinkage calculation\n",
    "    numerator = (df['num_games'] * df['raw_score']) + (k_shrinkage * df['opening_mean'])\n",
    "    denominator = df['num_games'] + k_shrinkage\n",
    "    \n",
    "    df['adjusted_score'] = numerator / denominator\n",
    "    df['confidence'] = df['num_games'] / denominator\n",
    "    \n",
    "    # Validation: Check output ranges\n",
    "    if not df['adjusted_score'].between(0, 1).all():\n",
    "        bad = df[~df['adjusted_score'].between(0, 1)][['opening_name', 'num_games', 'raw_score', 'opening_mean', 'adjusted_score']].head()\n",
    "        raise ValueError(\n",
    "            f\"adjusted_score out of range [0, 1]!\\n{bad}\"\n",
    "        )\n",
    "    \n",
    "    if not df['confidence'].between(0, 1).all():\n",
    "        bad = df[~df['confidence'].between(0, 1)][['opening_name', 'num_games', 'confidence']].head()\n",
    "        raise ValueError(\n",
    "            f\"confidence out of range [0, 1]!\\n{bad}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nâœ“ Bayesian shrinkage complete\")\n",
    "    print(f\"   Adjusted {len(df)} opening scores\")\n",
    "    \n",
    "    # Show shrinkage statistics\n",
    "    df['adjustment'] = df['adjusted_score'] - df['raw_score']\n",
    "    print(f\"\\n   Score adjustment statistics:\")\n",
    "    print(f\"      Mean adjustment: {df['adjustment'].mean():+.4f}\")\n",
    "    print(f\"      Adjustment range: [{df['adjustment'].min():+.4f}, {df['adjustment'].max():+.4f}]\")\n",
    "    print(f\"      Openings pushed up: {(df['adjustment'] > 0).sum()}\")\n",
    "    print(f\"      Openings pushed down: {(df['adjustment'] < 0).sum()}\")\n",
    "    \n",
    "    print(f\"\\n   Confidence statistics:\")\n",
    "    print(f\"      Mean: {df['confidence'].mean():.4f}\")\n",
    "    print(f\"      Range: [{df['confidence'].min():.4f}, {df['confidence'].max():.4f}]\")\n",
    "    \n",
    "    # Show examples of shrinkage effect\n",
    "    print(f\"\\n   Sample adjustments (sorted by game count):\")\n",
    "    sample = pd.concat([df.nlargest(3, 'num_games'), df.nsmallest(3, 'num_games')])\n",
    "    sample = sample[['opening_name', 'eco', 'num_games', 'raw_score', 'opening_mean', 'adjusted_score', 'adjustment', 'confidence']]\n",
    "    \n",
    "    for idx, row in sample.iterrows():\n",
    "        direction = \"â†‘\" if row['adjustment'] > 0 else \"â†“\" if row['adjustment'] < 0 else \"â†’\"\n",
    "        print(f\"      {row['eco']:>3} | {row['opening_name']:<35} | \"\n",
    "              f\"Games: {row['num_games']:>4} | Raw: {row['raw_score']:.3f} | \"\n",
    "              f\"Mean: {row['opening_mean']:.3f} | {direction} Adj: {row['adjusted_score']:.3f} | \"\n",
    "              f\"Conf: {row['confidence']:.3f}\")\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    df = df.drop(columns=['opening_mean', 'adjustment'])\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return PlayerData(\n",
    "        player_id=player_data.player_id,\n",
    "        name=player_data.name,\n",
    "        rating=player_data.rating,\n",
    "        color=player_data.color,\n",
    "        opening_stats_df=df\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2185bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2.7: Applying Bayesian Shrinkage\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "   Shrinkage constant (k): 50\n",
      "   Opening stats loaded: 2,728 openings\n",
      "   Player openings to process: 94\n",
      "\n",
      "âœ“ Bayesian shrinkage complete\n",
      "   Adjusted 94 opening scores\n",
      "\n",
      "   Score adjustment statistics:\n",
      "      Mean adjustment: -0.0331\n",
      "      Adjustment range: [-0.5596, +0.4993]\n",
      "      Openings pushed up: 41\n",
      "      Openings pushed down: 53\n",
      "\n",
      "   Confidence statistics:\n",
      "      Mean: 0.2216\n",
      "      Range: [0.0566, 0.9667]\n",
      "\n",
      "   Sample adjustments (sorted by game count):\n",
      "      C41 | Philidor Defense                    | Games: 1450 | Raw: 0.484 | Mean: 0.463 | â†“ Adj: 0.483 | Conf: 0.967\n",
      "      D00 | Queen's Pawn Game: Accelerated London System | Games:  206 | Raw: 0.529 | Mean: 0.466 | â†“ Adj: 0.517 | Conf: 0.805\n",
      "      C23 | Bishop's Opening: Boi Variation     | Games:  136 | Raw: 0.511 | Mean: 0.462 | â†“ Adj: 0.498 | Conf: 0.731\n",
      "      A00 | Grob Opening: Grob Gambit           | Games:    3 | Raw: 0.667 | Mean: 0.481 | â†“ Adj: 0.491 | Conf: 0.057\n",
      "      A00 | KÃ¡das Opening                       | Games:    3 | Raw: 0.333 | Mean: 0.503 | â†‘ Adj: 0.494 | Conf: 0.057\n",
      "      A00 | Saragossa Opening                   | Games:    3 | Raw: 1.000 | Mean: 0.486 | â†“ Adj: 0.515 | Conf: 0.057\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 2.7 Apply Bayesian Shrinkage to player's opening scores\n",
    "# opening_stats was already loaded in Step 2.2\n",
    "\n",
    "player_data = apply_bayesian_shrinkage(player_data, opening_stats, k_shrinkage=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24055832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final DataFrame with all transformations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opening_name</th>\n",
       "      <th>eco</th>\n",
       "      <th>num_games</th>\n",
       "      <th>raw_score</th>\n",
       "      <th>adjusted_score</th>\n",
       "      <th>confidence</th>\n",
       "      <th>training_opening_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Philidor Defense</td>\n",
       "      <td>C41</td>\n",
       "      <td>1450</td>\n",
       "      <td>0.483793</td>\n",
       "      <td>0.483096</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Queen's Pawn Game: Accelerated London System</td>\n",
       "      <td>D00</td>\n",
       "      <td>206</td>\n",
       "      <td>0.529126</td>\n",
       "      <td>0.516792</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bishop's Opening: Boi Variation</td>\n",
       "      <td>C23</td>\n",
       "      <td>136</td>\n",
       "      <td>0.511029</td>\n",
       "      <td>0.497970</td>\n",
       "      <td>0.731183</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vienna Game</td>\n",
       "      <td>C25</td>\n",
       "      <td>127</td>\n",
       "      <td>0.531496</td>\n",
       "      <td>0.512767</td>\n",
       "      <td>0.717514</td>\n",
       "      <td>1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queen's Pawn Game: Anti-Torre</td>\n",
       "      <td>D02</td>\n",
       "      <td>98</td>\n",
       "      <td>0.505102</td>\n",
       "      <td>0.493321</td>\n",
       "      <td>0.662162</td>\n",
       "      <td>1889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Queen's Gambit Accepted: Old Variation</td>\n",
       "      <td>D20</td>\n",
       "      <td>96</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.403849</td>\n",
       "      <td>0.657534</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>King's Gambit Accepted: King's Knight's Gambit</td>\n",
       "      <td>C34</td>\n",
       "      <td>91</td>\n",
       "      <td>0.335165</td>\n",
       "      <td>0.372857</td>\n",
       "      <td>0.645390</td>\n",
       "      <td>2502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Queen's Gambit Accepted: Central Variation, Gr...</td>\n",
       "      <td>D20</td>\n",
       "      <td>84</td>\n",
       "      <td>0.505952</td>\n",
       "      <td>0.489995</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Queen's Gambit Accepted</td>\n",
       "      <td>D20</td>\n",
       "      <td>54</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.426004</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English Opening: King's English Variation, Kra...</td>\n",
       "      <td>A21</td>\n",
       "      <td>46</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.510377</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        opening_name  eco  num_games  \\\n",
       "0                                   Philidor Defense  C41       1450   \n",
       "1       Queen's Pawn Game: Accelerated London System  D00        206   \n",
       "2                    Bishop's Opening: Boi Variation  C23        136   \n",
       "3                                        Vienna Game  C25        127   \n",
       "4                      Queen's Pawn Game: Anti-Torre  D02         98   \n",
       "5             Queen's Gambit Accepted: Old Variation  D20         96   \n",
       "6     King's Gambit Accepted: King's Knight's Gambit  C34         91   \n",
       "7  Queen's Gambit Accepted: Central Variation, Gr...  D20         84   \n",
       "8                            Queen's Gambit Accepted  D20         54   \n",
       "9  English Opening: King's English Variation, Kra...  A21         46   \n",
       "\n",
       "   raw_score  adjusted_score  confidence  training_opening_id  \n",
       "0   0.483793        0.483096    0.966667                 1441  \n",
       "1   0.529126        0.516792    0.804688                 2466  \n",
       "2   0.511029        0.497970    0.731183                 1222  \n",
       "3   0.531496        0.512767    0.717514                 1239  \n",
       "4   0.505102        0.493321    0.662162                 1889  \n",
       "5   0.390625        0.403849    0.657534                 1986  \n",
       "6   0.335165        0.372857    0.645390                 2502  \n",
       "7   0.505952        0.489995    0.626866                 1979  \n",
       "8   0.407407        0.426004    0.519231                 1976  \n",
       "9   0.543478        0.510377    0.479167                  221  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"\\nFinal DataFrame with all transformations:\")\n",
    "display_cols = ['opening_name', 'eco', 'num_games', 'raw_score', 'adjusted_score', 'confidence', 'training_opening_id']\n",
    "player_data.opening_stats_df[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e038a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2.8: Encoding ECO Features\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "   ECO letter mappings: 5 categories\n",
      "   ECO number mappings: 100 categories\n",
      "   Openings to encode: 94\n",
      "\n",
      "âœ“ Parsed ECO codes\n",
      "   Sample ECO codes:\n",
      "      'C41' â†’ letter='C', number='41'\n",
      "      'D00' â†’ letter='D', number='00'\n",
      "      'C23' â†’ letter='C', number='23'\n",
      "      'C25' â†’ letter='C', number='25'\n",
      "      'D02' â†’ letter='D', number='02'\n",
      "\n",
      "âœ“ ECO encoding complete\n",
      "   All 94 openings encoded\n",
      "\n",
      "   Encoded value ranges:\n",
      "   â€¢ eco_letter_cat: [0, 3] (expected: [0, 4])\n",
      "   â€¢ eco_number_cat: [0, 50] (expected: [0, 99])\n",
      "\n",
      "   ECO letter distribution:\n",
      "      0 ('A'):   26 openings ( 27.7%)\n",
      "      2 ('C'):   44 openings ( 46.8%)\n",
      "      3 ('D'):   24 openings ( 25.5%)\n",
      "\n",
      "   Sample encodings (verification):\n",
      "\n",
      "   ECO    Letter Cat  Number Cat  Reconstructed  Match  Opening Name                            \n",
      "   ------ ----------- ----------- -------------- ------ ----------------------------------------\n",
      "   C41    2           41          C41            âœ“      Philidor Defense                        \n",
      "   D00    3           0           D00            âœ“      Queen's Pawn Game: Accelerated London   \n",
      "   C23    2           23          C23            âœ“      Bishop's Opening: Boi Variation         \n",
      "   C25    2           25          C25            âœ“      Vienna Game                             \n",
      "   D02    3           2           D02            âœ“      Queen's Pawn Game: Anti-Torre           \n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2.8: Encode ECO Features\n",
    "def encode_eco_features(\n",
    "    player_data: PlayerData,\n",
    "    eco_letter_map: dict[str, int],\n",
    "    eco_number_map: dict[str, int]\n",
    ") -> PlayerData:\n",
    "    \"\"\"\n",
    "    Encode ECO codes into categorical integer features.\n",
    "\n",
    "    This morphs each ECO code to match how the model was trained.\n",
    "\n",
    "    Each ECO code (e.g., \"C21\") is split into:\n",
    "    - Letter component (\"C\") â†’ eco_letter_cat (categorical int)\n",
    "    - Number component (\"21\") â†’ eco_number_cat (categorical int)\n",
    "    \n",
    "    These categorical features will be used as inputs to embedding layers in the model.\n",
    "    \n",
    "    Args:\n",
    "        player_data: Player profile with 'eco' column in opening_stats_df\n",
    "        eco_letter_map: Mapping from ECO letters to integers (e.g., {'A': 0, 'B': 1, ...})\n",
    "        eco_number_map: Mapping from ECO numbers to integers (e.g., {'00': 0, '01': 1, ...})\n",
    "    \n",
    "    Returns:\n",
    "        New PlayerData with 'eco_letter_cat' and 'eco_number_cat' columns added\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If required 'eco' column is missing\n",
    "        Warning: If any ECO codes cannot be encoded (logged but not fatal)\n",
    "    \n",
    "    Example:\n",
    "        >>> # eco_letter_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "        >>> # eco_number_map = {'00': 0, '01': 1, ..., '21': 21, ...}\n",
    "        >>> player_data = encode_eco_features(player_data, eco_letter_map, eco_number_map)\n",
    "        >>> # ECO \"C21\" becomes: eco_letter_cat=2, eco_number_cat=21\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 2.8: Encoding ECO Features\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = player_data.opening_stats_df.copy()\n",
    "    \n",
    "    if 'eco' not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"opening_stats_df missing required 'eco' column!\\n\"\n",
    "            f\"   Available columns: {df.columns.tolist()}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"   ECO letter mappings: {len(eco_letter_map)} categories\")\n",
    "    print(f\"   ECO number mappings: {len(eco_number_map)} categories\")\n",
    "    print(f\"   Openings to encode: {len(df)}\")\n",
    "    \n",
    "    # Parse ECO codes: split into letter and number\n",
    "    df['eco_letter_str'] = df['eco'].str[0]  # First character\n",
    "    df['eco_number_str'] = df['eco'].str[1:]  # Remaining characters\n",
    "    \n",
    "    print(f\"\\nâœ“ Parsed ECO codes\")\n",
    "    print(f\"   Sample ECO codes:\")\n",
    "    sample = df[['eco', 'eco_letter_str', 'eco_number_str']].head(5)\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"      '{row['eco']}' â†’ letter='{row['eco_letter_str']}', number='{row['eco_number_str']}'\")\n",
    "    \n",
    "    # Encode letter component\n",
    "    df['eco_letter_cat'] = df['eco_letter_str'].map(eco_letter_map)\n",
    "    \n",
    "    # Check for unmapped letters\n",
    "    unmapped_letters = df['eco_letter_cat'].isna()\n",
    "    if unmapped_letters.any():\n",
    "        unmapped_vals = df.loc[unmapped_letters, 'eco_letter_str'].unique()\n",
    "        print(f\"\\n   âš ï¸  WARNING: {unmapped_letters.sum()} openings have unmapped ECO letters!\")\n",
    "        print(f\"   Unmapped letters: {unmapped_vals}\")\n",
    "        print(f\"   These openings will use default value (0)\")\n",
    "        df.loc[unmapped_letters, 'eco_letter_cat'] = 0  # Default to 'A'\n",
    "    \n",
    "    df['eco_letter_cat'] = df['eco_letter_cat'].astype(int)\n",
    "    \n",
    "    # Encode number component\n",
    "    df['eco_number_cat'] = df['eco_number_str'].map(eco_number_map)\n",
    "    \n",
    "    # Check for unmapped numbers\n",
    "    unmapped_numbers = df['eco_number_cat'].isna()\n",
    "    if unmapped_numbers.any():\n",
    "        unmapped_vals = df.loc[unmapped_numbers, 'eco_number_str'].unique()\n",
    "        print(f\"\\n   âš ï¸  WARNING: {unmapped_numbers.sum()} openings have unmapped ECO numbers!\")\n",
    "        print(f\"   Unmapped numbers: {unmapped_vals}\")\n",
    "        print(f\"   These openings will use default value (0)\")\n",
    "        df.loc[unmapped_numbers, 'eco_number_cat'] = 0  # Default to '00'\n",
    "    \n",
    "    df['eco_number_cat'] = df['eco_number_cat'].astype(int)\n",
    "    \n",
    "    print(f\"\\nâœ“ ECO encoding complete\")\n",
    "    print(f\"   All {len(df)} openings encoded\")\n",
    "    \n",
    "    # Validation: Check encoded values are in valid ranges\n",
    "    min_letter = int(df['eco_letter_cat'].min())\n",
    "    max_letter = int(df['eco_letter_cat'].max())\n",
    "    min_number = int(df['eco_number_cat'].min())\n",
    "    max_number = int(df['eco_number_cat'].max())\n",
    "    \n",
    "    expected_max_letter = len(eco_letter_map) - 1\n",
    "    expected_max_number = len(eco_number_map) - 1\n",
    "    \n",
    "    print(f\"\\n   Encoded value ranges:\")\n",
    "    print(f\"   â€¢ eco_letter_cat: [{min_letter}, {max_letter}] (expected: [0, {expected_max_letter}])\")\n",
    "    print(f\"   â€¢ eco_number_cat: [{min_number}, {max_number}] (expected: [0, {expected_max_number}])\")\n",
    "    \n",
    "    if max_letter > expected_max_letter or max_number > expected_max_number:\n",
    "        raise ValueError(\n",
    "            f\"Encoded values exceed expected ranges!\\n\"\n",
    "            f\"   This suggests encoding mappings are incomplete.\"\n",
    "        )\n",
    "    \n",
    "    # Show distribution of ECO categories\n",
    "    print(f\"\\n   ECO letter distribution:\")\n",
    "    letter_dist = df['eco_letter_cat'].value_counts().sort_index()\n",
    "    # Create reverse mapping for display\n",
    "    reverse_letter_map = {v: k for k, v in eco_letter_map.items()}\n",
    "    for cat, count in letter_dist.items():\n",
    "        letter = reverse_letter_map.get(cat, '?')\n",
    "        pct = 100 * count / len(df)\n",
    "        print(f\"      {cat} ('{letter}'): {count:>4} openings ({pct:>5.1f}%)\")\n",
    "    \n",
    "    # Show sample encodings with verification\n",
    "    print(f\"\\n   Sample encodings (verification):\")\n",
    "    sample = df.nlargest(5, 'num_games')[['opening_name', 'eco', 'eco_letter_cat', 'eco_number_cat', 'num_games']]\n",
    "    \n",
    "    # Create reverse mappings for verification\n",
    "    reverse_number_map = {v: k for k, v in eco_number_map.items()}\n",
    "    \n",
    "    print(f\"\\n   {'ECO':<6} {'Letter Cat':<11} {'Number Cat':<11} {'Reconstructed':<14} {'Match':<6} {'Opening Name':<40}\")\n",
    "    print(f\"   {'-'*6} {'-'*11} {'-'*11} {'-'*14} {'-'*6} {'-'*40}\")\n",
    "    \n",
    "    for idx, row in sample.iterrows():\n",
    "        # Reconstruct ECO from encoded values\n",
    "        reconstructed_letter = reverse_letter_map.get(row['eco_letter_cat'], '?')\n",
    "        reconstructed_number = reverse_number_map.get(row['eco_number_cat'], '??')\n",
    "        reconstructed_eco = f\"{reconstructed_letter}{reconstructed_number}\"\n",
    "        \n",
    "        # Check if reconstruction matches original\n",
    "        match = \"âœ“\" if reconstructed_eco == row['eco'] else \"âœ—\"\n",
    "        \n",
    "        # Truncate opening name if too long\n",
    "        opening_name = row['opening_name'][:38] if len(row['opening_name']) > 38 else row['opening_name']\n",
    "        \n",
    "        print(f\"   {row['eco']:<6} {row['eco_letter_cat']:<11} {row['eco_number_cat']:<11} \"\n",
    "              f\"{reconstructed_eco:<14} {match:<6} {opening_name:<40}\")\n",
    "    \n",
    "    # Drop temporary string columns (only keep categorical encodings)\n",
    "    df = df.drop(columns=['eco_letter_str', 'eco_number_str'])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    return PlayerData(\n",
    "        player_id=player_data.player_id,\n",
    "        name=player_data.name,\n",
    "        rating=player_data.rating,\n",
    "        color=player_data.color,\n",
    "        opening_stats_df=df,\n",
    "        rating_z=player_data.rating_z\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply Step 2.8: Encode ECO Features\n",
    "player_data = encode_eco_features(player_data, eco_letter_map, eco_number_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d490e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2.8 Verification:\n",
      "Opening stats columns: ['player_id', 'opening_id', 'eco', 'opening_name', 'num_games', 'num_wins', 'num_draws', 'num_losses', 'raw_score', 'training_opening_id', 'adjusted_score', 'confidence', 'eco_letter_cat', 'eco_number_cat']\n",
      "\n",
      "Sample of encoded ECO features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opening_name</th>\n",
       "      <th>eco</th>\n",
       "      <th>eco_letter_cat</th>\n",
       "      <th>eco_number_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Philidor Defense</td>\n",
       "      <td>C41</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Queen's Pawn Game: Accelerated London System</td>\n",
       "      <td>D00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bishop's Opening: Boi Variation</td>\n",
       "      <td>C23</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vienna Game</td>\n",
       "      <td>C25</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queen's Pawn Game: Anti-Torre</td>\n",
       "      <td>D02</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Queen's Gambit Accepted: Old Variation</td>\n",
       "      <td>D20</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>King's Gambit Accepted: King's Knight's Gambit</td>\n",
       "      <td>C34</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Queen's Gambit Accepted: Central Variation, Gr...</td>\n",
       "      <td>D20</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Queen's Gambit Accepted</td>\n",
       "      <td>D20</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English Opening: King's English Variation, Kra...</td>\n",
       "      <td>A21</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        opening_name  eco  eco_letter_cat  \\\n",
       "0                                   Philidor Defense  C41               2   \n",
       "1       Queen's Pawn Game: Accelerated London System  D00               3   \n",
       "2                    Bishop's Opening: Boi Variation  C23               2   \n",
       "3                                        Vienna Game  C25               2   \n",
       "4                      Queen's Pawn Game: Anti-Torre  D02               3   \n",
       "5             Queen's Gambit Accepted: Old Variation  D20               3   \n",
       "6     King's Gambit Accepted: King's Knight's Gambit  C34               2   \n",
       "7  Queen's Gambit Accepted: Central Variation, Gr...  D20               3   \n",
       "8                            Queen's Gambit Accepted  D20               3   \n",
       "9  English Opening: King's English Variation, Kra...  A21               0   \n",
       "\n",
       "   eco_number_cat  \n",
       "0              41  \n",
       "1               0  \n",
       "2              23  \n",
       "3              25  \n",
       "4               2  \n",
       "5              20  \n",
       "6              34  \n",
       "7              20  \n",
       "8              20  \n",
       "9              21  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify Step 2.8 results\n",
    "print(\"Step 2.8 Verification:\")\n",
    "print(f\"Opening stats columns: {list(player_data.opening_stats_df.columns)}\")\n",
    "print(f\"\\nSample of encoded ECO features:\")\n",
    "player_data.opening_stats_df[['opening_name', 'eco', 'eco_letter_cat', 'eco_number_cat']].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
