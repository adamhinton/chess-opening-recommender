{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6321962c",
   "metadata": {},
   "source": [
    "# Auto-Download Parquet Files\n",
    "\n",
    "This notebook demonstrates a proof-of-concept for automatically downloading parquet files from a Hugging Face dataset repository. It includes configuration, helper functions, and a main execution flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbfa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = (\n",
    "    Path(__file__).resolve().parent.parent\n",
    "    if \"__file__\" in globals()\n",
    "    else Path.cwd().parent\n",
    ")\n",
    "output_dir = str(project_root / \"data\" / \"raw\" / \"auto_download_parquets\")\n",
    "\n",
    "# CONFIG\n",
    "# I did my best to put all of the variables that we might need to adjust here.\n",
    "# This is a proof of concept right now; later we may make this some sort of drop down picker for month, year etc\n",
    "config = {\n",
    "    \"repo\": \"Lichess/standard-chess-games\",  # Hugging Face repo id\n",
    "    \"year\": \"2025\",  # 4-digit year (string or int)\n",
    "    \"month\": \"7\",  # numeric month (e.g., \"7\" or \"07\")\n",
    "    \"max_parquets\": 20,  # int or None to download all available\n",
    "    # Download to /data/raw/auto_download_parquets relative to project root\n",
    "    \"output_dir\": output_dir,\n",
    "    \"hf_token\": None,  # set to your HF token string if you need to access gated datasets\n",
    "    \"probe_max_attempts\": 1000,  # for fallback probing\n",
    "    \"probe_patterns\": [  # tried in order if APIs gave no URLs\n",
    "        # Pattern A: common \"train-00000-of-00066.parquet\" style\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}-of-{total:05d}.parquet\",\n",
    "        # Pattern B: some datasets use plain shard names\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}.parquet\",\n",
    "        # Pattern C: fall back to zero-padded 4-digit name\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/000{idx}.parquet\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Extract config variables for use in the rest of the notebook\n",
    "repo = config[\"repo\"]\n",
    "year = str(config[\"year\"])\n",
    "month_raw = str(config[\"month\"])\n",
    "month_padded = month_raw.zfill(2)\n",
    "max_parquets = config[\"max_parquets\"]\n",
    "out_dir = Path(config[\"output_dir\"])\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "hf_headers = {\"Authorization\": f\"Bearer {config['hf_token']}\"} if config.get(\"hf_token\") else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beca570",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "The following cells define helper functions for interacting with the Hugging Face API, filtering URLs, and downloading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99bb4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "from typing import List\n",
    "import requests\n",
    "\n",
    "# Helper functions\n",
    "def flatten_parquet_mapping(mapping: dict) -> List[str]:\n",
    "    \"\"\"Flatten the Hub /api/datasets/.../parquet mapping to a list of URLs.\"\"\"\n",
    "    urls = []\n",
    "    if not isinstance(mapping, dict):\n",
    "        return urls\n",
    "    for subset_val in mapping.values():\n",
    "        if isinstance(subset_val, dict):\n",
    "            for split_val in subset_val.values():\n",
    "                if isinstance(split_val, list):\n",
    "                    urls.extend(split_val)\n",
    "    return urls\n",
    "\n",
    "def get_urls_from_hub_api(repo: str) -> List[str]:\n",
    "    \"\"\"Call https://huggingface.co/api/datasets/{repo}/parquet (Hub API).\"\"\"\n",
    "    try:\n",
    "        api = f\"https://huggingface.co/api/datasets/{repo}/parquet\"\n",
    "        r = requests.get(api, headers=hf_headers, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return []\n",
    "        data = r.json()\n",
    "        urls = flatten_parquet_mapping(data)\n",
    "        return urls\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def get_urls_from_dataset_viewer(repo: str) -> List[str]:\n",
    "    \"\"\"Call dataset-viewer endpoint: https://datasets-server.huggingface.co/parquet?dataset={repo}\"\"\"\n",
    "    try:\n",
    "        api = \"https://datasets-server.huggingface.co/parquet\"\n",
    "        params = {\"dataset\": repo}\n",
    "        r = requests.get(api, headers=hf_headers, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return []\n",
    "        data = r.json()\n",
    "        urls = [entry.get(\"url\") for entry in data.get(\"parquet_files\", []) if entry.get(\"url\")]\n",
    "        return urls\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def filter_urls_for_month(urls: List[str], year: str, month_padded: str) -> List[str]:\n",
    "    \"\"\"Return only URLs that contain the month/year partition (decoded).\"\"\"\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        decoded = urllib.parse.unquote(u)\n",
    "        if f\"year={year}/month={month_padded}\" in decoded or f\"year={year}/month={int(month_padded)}\" in decoded:\n",
    "            out.append(u)\n",
    "    def shard_index(u):\n",
    "        m = re.search(r\"(\\d{1,5})\\.parquet$\", urllib.parse.unquote(u))\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        m2 = re.search(r\"train-(\\d{1,5})-of-(\\d{1,5})\\.parquet\", urllib.parse.unquote(u))\n",
    "        if m2:\n",
    "            return int(m2.group(1))\n",
    "        return 10**9\n",
    "    return sorted(out, key=shard_index)\n",
    "\n",
    "def probe_fallback_urls(repo: str, year: str, month: str, max_attempts: int, patterns: List[str]) -> List[str]:\n",
    "    \"\"\"If APIs fail, try probing plausible URL patterns until 404.\"\"\"\n",
    "    found = []\n",
    "    for pattern in patterns:\n",
    "        if \"{total\" in pattern:\n",
    "            for total_guess in range(1, 201):\n",
    "                consecutive_not_found = 0\n",
    "                for idx in range(max_attempts):\n",
    "                    url = pattern.format(repo=repo, year=year, month=month, idx=idx, total=total_guess)\n",
    "                    if try_head(url):\n",
    "                        found.append(url)\n",
    "                        consecutive_not_found = 0\n",
    "                    else:\n",
    "                        consecutive_not_found += 1\n",
    "                        break\n",
    "                if found:\n",
    "                    return found\n",
    "        else:\n",
    "            for idx in range(max_attempts):\n",
    "                url = pattern.format(repo=repo, year=year, month=month, idx=idx)\n",
    "                if try_head(url):\n",
    "                    found.append(url)\n",
    "                else:\n",
    "                    break\n",
    "            if found:\n",
    "                return found\n",
    "    return found\n",
    "\n",
    "def try_head(url: str, timeout: int = 20) -> bool:\n",
    "    \"\"\"Quick HEAD-ish check (GET with stream and immediate close) to see if URL exists.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=hf_headers, stream=True, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            r.raw.read(1)\n",
    "            r.close()\n",
    "            return True\n",
    "        r.close()\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def download_file(url: str, dest: Path, chunk_size: int = 1024*32) -> bool:\n",
    "    \"\"\"Download url -> dest. Return True on success, False on 404 or error.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=hf_headers, stream=True, timeout=60)\n",
    "        if r.status_code == 404:\n",
    "            return False\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            if r is not None:\n",
    "                r.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"  download error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f808c19",
   "metadata": {},
   "source": [
    "## Main Execution Flow\n",
    "\n",
    "The following cell contains the main logic for querying parquet URLs, filtering them, and downloading the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07368e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Querying Hub API for parquet URLs...\n",
      "  Hub API returned 26010 total parquet URLs (unfiltered).\n",
      "2) Trying dataset-viewer endpoint...\n",
      "  Hub API returned 26010 total parquet URLs (unfiltered).\n",
      "2) Trying dataset-viewer endpoint...\n",
      "  dataset-viewer returned 26010 total parquet entries.\n",
      "3) No parquet URLs found via API; falling back to incremental probing (may be slower).\n",
      "  dataset-viewer returned 26010 total parquet entries.\n",
      "3) No parquet URLs found via API; falling back to incremental probing (may be slower).\n",
      "\n",
      "Will download 2 file(s) into /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/auto_download_parquets\n",
      "\n",
      "[1/2] Downloading: train-00000-of-00066.parquet\n",
      "\n",
      "Will download 2 file(s) into /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/auto_download_parquets\n",
      "\n",
      "[1/2] Downloading: train-00000-of-00066.parquet\n",
      "  Download completed in 22.33 seconds.\n",
      "  Speed: 162.86 GB/hour.\n",
      "  ETA for remaining files: 0.37 minutes.\n",
      "  Download completed in 22.33 seconds.\n",
      "  Speed: 162.86 GB/hour.\n",
      "  ETA for remaining files: 0.37 minutes.\n",
      "[2/2] Downloading: train-00001-of-00066.parquet\n",
      "[2/2] Downloading: train-00001-of-00066.parquet\n",
      "  Download completed in 22.23 seconds.\n",
      "  Speed: 163.53 GB/hour.\n",
      "  ETA for remaining files: 0.00 minutes.\n",
      "  Download completed in 22.23 seconds.\n",
      "  Speed: 163.53 GB/hour.\n",
      "  ETA for remaining files: 0.00 minutes.\n",
      "\n",
      "Done. 2 file(s) downloaded to: /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/auto_download_parquets\n",
      "\n",
      "Done. 2 file(s) downloaded to: /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/auto_download_parquets\n"
     ]
    }
   ],
   "source": [
    "# Main flow\n",
    "import time\n",
    "print(\"1) Querying Hub API for parquet URLs...\")\n",
    "urls = get_urls_from_hub_api(repo)\n",
    "if urls:\n",
    "    print(f\"  Hub API returned {len(urls)} total parquet URLs (unfiltered).\")\n",
    "else:\n",
    "    print(\"  Hub API returned nothing (or failed).\")\n",
    "\n",
    "filtered = filter_urls_for_month(urls, year, month_padded)\n",
    "if filtered:\n",
    "    print(f\"  Found {len(filtered)} parquet URLs for {year}/{month_padded} via Hub API.\")\n",
    "else:\n",
    "    print(\"2) Trying dataset-viewer endpoint...\")\n",
    "    urls2 = get_urls_from_dataset_viewer(repo)\n",
    "    if urls2:\n",
    "        print(f\"  dataset-viewer returned {len(urls2)} total parquet entries.\")\n",
    "        filtered = filter_urls_for_month(urls2, year, month_padded)\n",
    "        if filtered:\n",
    "            print(f\"  Found {len(filtered)} parquet URLs for {year}/{month_padded} via dataset-viewer.\")\n",
    "if not filtered:\n",
    "    print(\"3) No parquet URLs found via API; falling back to incremental probing (may be slower).\")\n",
    "    patterns = config.get(\"probe_patterns\", [])\n",
    "    found = probe_fallback_urls(repo, year, month_padded, config[\"probe_max_attempts\"], patterns)\n",
    "    filtered = found\n",
    "\n",
    "if not filtered:\n",
    "    print(\"ERROR: no parquet URLs discovered for that month/year by API or fallback probing. Aborting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if max_parquets is not None:\n",
    "    filtered = filtered[:int(max_parquets)]\n",
    "\n",
    "print(f\"\\nWill download {len(filtered)} file(s) into {out_dir.resolve()}\\n\")\n",
    "\n",
    "success_count = 0\n",
    "start_time = time.time()\n",
    "for i, url in enumerate(filtered):\n",
    "    decoded = urllib.parse.unquote(url)\n",
    "    filename = Path(decoded).name\n",
    "    dest = out_dir / filename\n",
    "    if dest.exists():\n",
    "        print(f\"[{i+1}/{len(filtered)}] Skipping (already exists): {filename}\")\n",
    "        success_count += 1\n",
    "        continue\n",
    "    print(f\"[{i+1}/{len(filtered)}] Downloading: {filename}\")\n",
    "    file_start_time = time.time()\n",
    "    ok = download_file(url, dest)\n",
    "    if not ok:\n",
    "        print(f\"  Failed to download (skipping): {url}\")\n",
    "        if urls == []:\n",
    "            print(\"  Probe-based download hit missing file — stopping probe downloads.\")\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    # Time tracking stuff\n",
    "    file_end_time = time.time()\n",
    "    elapsed_time = file_end_time - file_start_time\n",
    "    total_elapsed_time = file_end_time - start_time\n",
    "    file_size_gb = 1.01  # All parquet files will be about 1.01 GB, except the last one which may be smaller\n",
    "    download_speed = file_size_gb / (elapsed_time / 3600)  # GB per hour\n",
    "    remaining_files = len(filtered) - (i + 1)\n",
    "    eta = (total_elapsed_time / (i + 1)) * remaining_files\n",
    "    print(f\"  Download completed in {elapsed_time:.2f} seconds.\")\n",
    "    print(f\"  Speed: {download_speed:.2f} GB/hour.\")\n",
    "    print(f\"  ETA for remaining files: {eta / 60:.2f} minutes.\")\n",
    "    success_count += 1\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"\\nDone. {success_count} file(s) downloaded to: {out_dir.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
