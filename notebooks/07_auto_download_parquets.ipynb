{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6321962c",
   "metadata": {},
   "source": [
    "# Auto-Download Parquet Files\n",
    "\n",
    "This notebook is an auto-downloader of raw parquet data.\n",
    "In this case, parquets are giant collections of Lichess games data, about 1GB each. \n",
    "This notebook will download a month's worth of games data at a time, or whatever maximum number of files the user inputs.\n",
    "Right now (9.7.25) it just downloads the files, but soon I will have it process the files and auto-delete them when finished.\n",
    "\n",
    "## Wifi Speed\n",
    "\n",
    "My personal Wifi speed is quite fast (350 mbps); This is a lot of GBs of data, but download speed isn't the bottleneck because my system will take longer to process each file than it will to download the next one. But, if you have slower download speed, you may need to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7cbfa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "project_root = (\n",
    "    Path(__file__).resolve().parent.parent\n",
    "    if \"__file__\" in globals()\n",
    "    else Path.cwd().parent\n",
    ")\n",
    "output_dir = str(project_root / \"data\" / \"raw\" / \"auto_download_parquets\")\n",
    "\n",
    "# CONFIG\n",
    "# I did my best to put all of the variables that we might need to adjust here.\n",
    "# This is a proof of concept right now; later we may make this some sort of drop down picker for month, year etc\n",
    "config = {\n",
    "    \"repo\": \"Lichess/standard-chess-games\",  # Hugging Face repo id\n",
    "    \"year\": \"2025\",  # 4-digit year (string or int)\n",
    "    \"month\": \"7\",  # numeric month (e.g., \"7\" or \"07\")\n",
    "    \"max_parquets\": 30,  # int or None to download all available\n",
    "    # Download to /data/raw/auto_download_parquets relative to project root\n",
    "    \"output_dir\": output_dir,\n",
    "    \"hf_token\": None,  # set to your HF token string if you need to access gated datasets\n",
    "    \"probe_max_attempts\": 1000,  # for fallback probing\n",
    "    \"probe_patterns\": [  # tried in order if APIs gave no URLs\n",
    "        # Pattern A: common \"train-00000-of-00066.parquet\" style\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}-of-{total:05d}.parquet\",\n",
    "        # Pattern B: some datasets use plain shard names\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}.parquet\",\n",
    "        # Pattern C: fall back to zero-padded 4-digit name\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/000{idx}.parquet\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Extract config variables for use in the rest of the notebook\n",
    "repo = config[\"repo\"]\n",
    "year = str(config[\"year\"])\n",
    "month_raw = str(config[\"month\"])\n",
    "month_padded = month_raw.zfill(2)\n",
    "max_parquets = config[\"max_parquets\"]\n",
    "out_dir = Path(config[\"output_dir\"])\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "hf_headers = {\"Authorization\": f\"Bearer {config['hf_token']}\"} if config.get(\"hf_token\") else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beca570",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Getting helper functions that we're defined elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99bb4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.downloading_raw_parquet_data.api_interaction import (\n",
    "    get_urls_from_hub_api,\n",
    "    get_urls_from_dataset_viewer,\n",
    "    filter_urls_for_month\n",
    ")\n",
    "from utils.downloading_raw_parquet_data.file_downloader import (\n",
    "    probe_fallback_urls,\n",
    "    download_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f808c19",
   "metadata": {},
   "source": [
    "## Main Execution Flow\n",
    "\n",
    "The following cell contains the main logic for querying parquet URLs, filtering them, and downloading the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07368e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Querying Hub API for parquet URLs...\n",
      "  Hub API returned 26010 total parquet URLs (unfiltered).\n",
      "2) Trying dataset-viewer endpoint...\n",
      "  dataset-viewer returned 26010 total parquet entries.\n",
      "3) No parquet URLs found via API; falling back to incremental probing (may be slower).\n",
      "\n",
      "Will download 30 file(s) into /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/auto_download_parquets\n",
      "\n",
      "[1/30] Skipping (already exists): train-00000-of-00066.parquet\n",
      "[2/30] Skipping (already exists): train-00001-of-00066.parquet\n",
      "[3/30] Skipping (already exists): train-00002-of-00066.parquet\n",
      "[4/30] Skipping (already exists): train-00003-of-00066.parquet\n",
      "[5/30] Skipping (already exists): train-00004-of-00066.parquet\n",
      "[6/30] Skipping (already exists): train-00005-of-00066.parquet\n",
      "[7/30] Skipping (already exists): train-00006-of-00066.parquet\n",
      "[8/30] Skipping (already exists): train-00007-of-00066.parquet\n",
      "[9/30] Skipping (already exists): train-00008-of-00066.parquet\n",
      "[10/30] Skipping (already exists): train-00009-of-00066.parquet\n",
      "[11/30] Skipping (already exists): train-00010-of-00066.parquet\n",
      "[12/30] Skipping (already exists): train-00011-of-00066.parquet\n",
      "[13/30] Skipping (already exists): train-00012-of-00066.parquet\n",
      "[14/30] Skipping (already exists): train-00013-of-00066.parquet\n",
      "[15/30] Skipping (already exists): train-00014-of-00066.parquet\n",
      "[16/30] Skipping (already exists): train-00015-of-00066.parquet\n",
      "[17/30] Skipping (already exists): train-00016-of-00066.parquet\n",
      "[18/30] Skipping (already exists): train-00017-of-00066.parquet\n",
      "[19/30] Skipping (already exists): train-00018-of-00066.parquet\n",
      "[20/30] Skipping (already exists): train-00019-of-00066.parquet\n",
      "[21/30] Downloading: train-00020-of-00066.parquet\n",
      "  Download completed in 19.86 seconds.\n",
      "  Current Speed: 183.06 GB/hour (416.56 Mbps).\n",
      "  Average Speed: 183.03 GB/hour (416.50 Mbps).\n",
      "  ETA for remaining files: 0.14 minutes.\n",
      "  Total elapsed time: 0.33 minutes.\n",
      "[22/30] Downloading: train-00021-of-00066.parquet\n",
      "  Download completed in 19.47 seconds.\n",
      "  Current Speed: 186.75 GB/hour (424.97 Mbps).\n",
      "  Average Speed: 182.54 GB/hour (415.38 Mbps).\n",
      "  ETA for remaining files: 0.24 minutes.\n",
      "  Total elapsed time: 0.66 minutes.\n",
      "[23/30] Downloading: train-00022-of-00066.parquet\n",
      "  Download completed in 20.56 seconds.\n",
      "  Current Speed: 176.89 GB/hour (402.52 Mbps).\n",
      "  Average Speed: 179.13 GB/hour (407.61 Mbps).\n",
      "  ETA for remaining files: 0.31 minutes.\n",
      "  Total elapsed time: 1.01 minutes.\n",
      "[24/30] Downloading: train-00023-of-00066.parquet\n",
      "  Download completed in 20.34 seconds.\n",
      "  Current Speed: 178.73 GB/hour (406.71 Mbps).\n",
      "  Average Speed: 177.92 GB/hour (404.88 Mbps).\n",
      "  ETA for remaining files: 0.34 minutes.\n",
      "  Total elapsed time: 1.36 minutes.\n",
      "[25/30] Downloading: train-00024-of-00066.parquet\n",
      "  Download completed in 22.54 seconds.\n",
      "  Current Speed: 161.31 GB/hour (367.06 Mbps).\n",
      "  Average Speed: 173.49 GB/hour (394.80 Mbps).\n",
      "  ETA for remaining files: 0.35 minutes.\n",
      "  Total elapsed time: 1.75 minutes.\n",
      "[26/30] Downloading: train-00025-of-00066.parquet\n",
      "  Download completed in 21.47 seconds.\n",
      "  Current Speed: 169.38 GB/hour (385.44 Mbps).\n",
      "  Average Speed: 172.11 GB/hour (391.65 Mbps).\n",
      "  ETA for remaining files: 0.33 minutes.\n",
      "  Total elapsed time: 2.11 minutes.\n",
      "[27/30] Downloading: train-00026-of-00066.parquet\n",
      "  Download completed in 20.13 seconds.\n",
      "  Current Speed: 180.64 GB/hour (411.05 Mbps).\n",
      "  Average Speed: 172.69 GB/hour (392.96 Mbps).\n",
      "  ETA for remaining files: 0.27 minutes.\n",
      "  Total elapsed time: 2.46 minutes.\n",
      "[28/30] Downloading: train-00027-of-00066.parquet\n",
      "  Download completed in 21.53 seconds.\n",
      "  Current Speed: 168.92 GB/hour (384.38 Mbps).\n",
      "  Average Speed: 171.70 GB/hour (390.71 Mbps).\n",
      "  ETA for remaining files: 0.20 minutes.\n",
      "  Total elapsed time: 2.82 minutes.\n",
      "[29/30] Downloading: train-00028-of-00066.parquet\n",
      "  Download completed in 20.60 seconds.\n",
      "  Current Speed: 176.50 GB/hour (401.63 Mbps).\n",
      "  Average Speed: 171.76 GB/hour (390.85 Mbps).\n",
      "  ETA for remaining files: 0.11 minutes.\n",
      "  Total elapsed time: 3.18 minutes.\n",
      "[30/30] Downloading: train-00029-of-00066.parquet\n",
      "  Download completed in 20.20 seconds.\n",
      "  Current Speed: 179.99 GB/hour (409.59 Mbps).\n",
      "  Average Speed: 172.14 GB/hour (391.72 Mbps).\n",
      "  ETA for remaining files: 0.00 minutes.\n",
      "  Total elapsed time: 3.52 minutes.\n",
      "\n",
      "Done. 30 file(s) downloaded to: /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/auto_download_parquets\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "# Main flow\n",
    "import time\n",
    "print(\"1) Querying Hub API for parquet URLs...\")\n",
    "urls = get_urls_from_hub_api(repo, hf_headers)\n",
    "if urls:\n",
    "    print(f\"  Hub API returned {len(urls)} total parquet URLs (unfiltered).\")\n",
    "else:\n",
    "    print(\"  Hub API returned nothing (or failed).\")\n",
    "\n",
    "filtered = filter_urls_for_month(urls, year, month_padded)\n",
    "if filtered:\n",
    "    print(f\"  Found {len(filtered)} parquet URLs for {year}/{month_padded} via Hub API.\")\n",
    "else:\n",
    "    print(\"2) Trying dataset-viewer endpoint...\")\n",
    "    urls2 = get_urls_from_dataset_viewer(repo, hf_headers)\n",
    "    if urls2:\n",
    "        print(f\"  dataset-viewer returned {len(urls2)} total parquet entries.\")\n",
    "        filtered = filter_urls_for_month(urls2, year, month_padded)\n",
    "        if filtered:\n",
    "            print(f\"  Found {len(filtered)} parquet URLs for {year}/{month_padded} via dataset-viewer.\")\n",
    "if not filtered:\n",
    "    print(\"3) No parquet URLs found via API; falling back to incremental probing (may be slower).\")\n",
    "    patterns = config.get(\"probe_patterns\", [])\n",
    "    found = probe_fallback_urls(repo, year, month_padded, config[\"probe_max_attempts\"], patterns, hf_headers)\n",
    "    filtered = found\n",
    "\n",
    "if not filtered:\n",
    "    print(\"ERROR: no parquet URLs discovered for that month/year by API or fallback probing. Aborting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if max_parquets is not None:\n",
    "    filtered = filtered[:int(max_parquets)]\n",
    "\n",
    "print(f\"\\nWill download {len(filtered)} file(s) into {out_dir.resolve()}\\n\")\n",
    "\n",
    "success_count = 0\n",
    "start_time = time.time()\n",
    "total_downloaded_gb = 0.0\n",
    "for i, url in enumerate(filtered):\n",
    "    decoded = urllib.parse.unquote(url)\n",
    "    filename = Path(decoded).name\n",
    "    dest = out_dir / filename\n",
    "    if dest.exists():\n",
    "        print(f\"[{i+1}/{len(filtered)}] Skipping (already exists): {filename}\")\n",
    "        success_count += 1\n",
    "        continue\n",
    "    print(f\"[{i+1}/{len(filtered)}] Downloading: {filename}\")\n",
    "    file_start_time = time.time()\n",
    "    ok = download_file(url, dest, hf_headers)\n",
    "    if not ok:\n",
    "        print(f\"  Failed to download (skipping): {url}\")\n",
    "        if urls == []:\n",
    "            print(\"  Probe-based download hit missing file — stopping probe downloads.\")\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    # Time tracking and metrics\n",
    "    file_end_time = time.time()\n",
    "    elapsed_time = file_end_time - file_start_time\n",
    "    total_elapsed_time = file_end_time - start_time\n",
    "    file_size_gb = 1.01  # All parquet files will be about 1.01 GB, except the last one which may be smaller\n",
    "    total_downloaded_gb += file_size_gb\n",
    "    download_speed_gbph = file_size_gb / (elapsed_time / 3600)  # GB per hour\n",
    "    download_speed_mbps = (file_size_gb * 1024) / (elapsed_time / 8)  # Mbps\n",
    "    avg_speed_gbph = total_downloaded_gb / (total_elapsed_time / 3600)  # Average GB per hour\n",
    "    avg_speed_mbps = (total_downloaded_gb * 1024) / (total_elapsed_time / 8)  # Average Mbps\n",
    "    remaining_files = len(filtered) - (i + 1)\n",
    "    eta = (total_elapsed_time / (i + 1)) * remaining_files\n",
    "    print(f\"  Download completed in {elapsed_time:.2f} seconds.\")\n",
    "    print(f\"  Current Speed: {download_speed_gbph:.2f} GB/hour ({download_speed_mbps:.2f} Mbps).\")\n",
    "    print(f\"  Average Speed: {avg_speed_gbph:.2f} GB/hour ({avg_speed_mbps:.2f} Mbps).\")\n",
    "    print(f\"  ETA for remaining files: {eta / 60:.2f} minutes.\")\n",
    "    print(f\"  Total elapsed time: {total_elapsed_time / 60:.2f} minutes.\")\n",
    "    success_count += 1\n",
    "    time.sleep(0.5)\n",
    "    # Regular updates every 15 seconds\n",
    "    if total_elapsed_time % 15 < 0.5:\n",
    "        print(f\"[Update] Total downloaded: {total_downloaded_gb:.2f} GB.\")\n",
    "        print(f\"[Update] Average Speed: {avg_speed_gbph:.2f} GB/hour ({avg_speed_mbps:.2f} Mbps).\")\n",
    "        print(f\"[Update] Total elapsed time: {total_elapsed_time / 60:.2f} minutes.\")\n",
    "\n",
    "print(f\"\\nDone. {success_count} file(s) downloaded to: {out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22910d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
