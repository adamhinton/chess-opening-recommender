{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6321962c",
   "metadata": {},
   "source": [
    "# Auto-Download Parquet Files\n",
    "\n",
    "This notebook demonstrates a proof-of-concept for automatically downloading parquet files from a Hugging Face dataset repository. It includes configuration, helper functions, and a main execution flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbfa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG (edit only this block)\n",
    "config = {\n",
    "    \"repo\": \"Lichess/standard-chess-games\",  # Hugging Face repo id\n",
    "    \"year\": \"2025\",                          # 4-digit year (string or int)\n",
    "    \"month\": \"7\",                            # numeric month (e.g., \"7\" or \"07\")\n",
    "    \"max_parquets\": 1,                    # int or None to download all available\n",
    "    \"output_dir\": \"data/raw/auto_download_parquets\",\n",
    "    \"hf_token\": None,                        # set to your HF token string if you need to access gated datasets\n",
    "    \"probe_max_attempts\": 1000,              # for fallback probing\n",
    "    \"probe_patterns\": [                      # tried in order if APIs gave no URLs\n",
    "        # Pattern A: common \"train-00000-of-00066.parquet\" style\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}-of-{total:05d}.parquet\",\n",
    "        # Pattern B: some datasets use plain shard names\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}.parquet\",\n",
    "        # Pattern C: fall back to zero-padded 4-digit name\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/000{idx}.parquet\",\n",
    "    ],\n",
    "}\n",
    "# ---- end config ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beca570",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "The following cells define helper functions for interacting with the Hugging Face API, filtering URLs, and downloading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, math, json, typing, urllib.parse, re\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import requests\n",
    "\n",
    "# Helper functions\n",
    "def flatten_parquet_mapping(mapping: dict) -> List[str]:\n",
    "    \"\"\"Flatten the Hub /api/datasets/.../parquet mapping to a list of URLs.\"\"\"\n",
    "    urls = []\n",
    "    if not isinstance(mapping, dict):\n",
    "        return urls\n",
    "    for subset_val in mapping.values():\n",
    "        if isinstance(subset_val, dict):\n",
    "            for split_val in subset_val.values():\n",
    "                if isinstance(split_val, list):\n",
    "                    urls.extend(split_val)\n",
    "    return urls\n",
    "\n",
    "def get_urls_from_hub_api(repo: str) -> List[str]:\n",
    "    \"\"\"Call https://huggingface.co/api/datasets/{repo}/parquet (Hub API).\"\"\"\n",
    "    try:\n",
    "        api = f\"https://huggingface.co/api/datasets/{repo}/parquet\"\n",
    "        r = requests.get(api, headers=hf_headers, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return []\n",
    "        data = r.json()\n",
    "        urls = flatten_parquet_mapping(data)\n",
    "        return urls\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def get_urls_from_dataset_viewer(repo: str) -> List[str]:\n",
    "    \"\"\"Call dataset-viewer endpoint: https://datasets-server.huggingface.co/parquet?dataset={repo}\"\"\"\n",
    "    try:\n",
    "        api = \"https://datasets-server.huggingface.co/parquet\"\n",
    "        params = {\"dataset\": repo}\n",
    "        r = requests.get(api, headers=hf_headers, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return []\n",
    "        data = r.json()\n",
    "        urls = [entry.get(\"url\") for entry in data.get(\"parquet_files\", []) if entry.get(\"url\")]\n",
    "        return urls\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def filter_urls_for_month(urls: List[str], year: str, month_padded: str) -> List[str]:\n",
    "    \"\"\"Return only URLs that contain the month/year partition (decoded).\"\"\"\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        decoded = urllib.parse.unquote(u)\n",
    "        if f\"year={year}/month={month_padded}\" in decoded or f\"year={year}/month={int(month_padded)}\" in decoded:\n",
    "            out.append(u)\n",
    "    def shard_index(u):\n",
    "        m = re.search(r\"(\\d{1,5})\\.parquet$\", urllib.parse.unquote(u))\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        m2 = re.search(r\"train-(\\d{1,5})-of-(\\d{1,5})\\.parquet\", urllib.parse.unquote(u))\n",
    "        if m2:\n",
    "            return int(m2.group(1))\n",
    "        return 10**9\n",
    "    return sorted(out, key=shard_index)\n",
    "\n",
    "def probe_fallback_urls(repo: str, year: str, month: str, max_attempts: int, patterns: List[str]) -> List[str]:\n",
    "    \"\"\"If APIs fail, try probing plausible URL patterns until 404.\"\"\"\n",
    "    found = []\n",
    "    for pattern in patterns:\n",
    "        if \"{total\" in pattern:\n",
    "            for total_guess in range(1, 201):\n",
    "                consecutive_not_found = 0\n",
    "                for idx in range(max_attempts):\n",
    "                    url = pattern.format(repo=repo, year=year, month=month, idx=idx, total=total_guess)\n",
    "                    if try_head(url):\n",
    "                        found.append(url)\n",
    "                        consecutive_not_found = 0\n",
    "                    else:\n",
    "                        consecutive_not_found += 1\n",
    "                        break\n",
    "                if found:\n",
    "                    return found\n",
    "        else:\n",
    "            for idx in range(max_attempts):\n",
    "                url = pattern.format(repo=repo, year=year, month=month, idx=idx)\n",
    "                if try_head(url):\n",
    "                    found.append(url)\n",
    "                else:\n",
    "                    break\n",
    "            if found:\n",
    "                return found\n",
    "    return found\n",
    "\n",
    "def try_head(url: str, timeout: int = 20) -> bool:\n",
    "    \"\"\"Quick HEAD-ish check (GET with stream and immediate close) to see if URL exists.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=hf_headers, stream=True, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            r.raw.read(1)\n",
    "            r.close()\n",
    "            return True\n",
    "        r.close()\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def download_file(url: str, dest: Path, chunk_size: int = 1024*32) -> bool:\n",
    "    \"\"\"Download url -> dest. Return True on success, False on 404 or error.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=hf_headers, stream=True, timeout=60)\n",
    "        if r.status_code == 404:\n",
    "            return False\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            if r is not None:\n",
    "                r.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"  download error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f808c19",
   "metadata": {},
   "source": [
    "## Main Execution Flow\n",
    "\n",
    "The following cell contains the main logic for querying parquet URLs, filtering them, and downloading the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07368e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main flow\n",
    "print(\"1) Querying Hub API for parquet URLs...\")\n",
    "urls = get_urls_from_hub_api(repo)\n",
    "if urls:\n",
    "    print(f\"  Hub API returned {len(urls)} total parquet URLs (unfiltered).\")\n",
    "else:\n",
    "    print(\"  Hub API returned nothing (or failed).\")\n",
    "\n",
    "filtered = filter_urls_for_month(urls, year, month_padded)\n",
    "if filtered:\n",
    "    print(f\"  Found {len(filtered)} parquet URLs for {year}/{month_padded} via Hub API.\")\n",
    "else:\n",
    "    print(\"2) Trying dataset-viewer endpoint...\")\n",
    "    urls2 = get_urls_from_dataset_viewer(repo)\n",
    "    if urls2:\n",
    "        print(f\"  dataset-viewer returned {len(urls2)} total parquet entries.\")\n",
    "        filtered = filter_urls_for_month(urls2, year, month_padded)\n",
    "        if filtered:\n",
    "            print(f\"  Found {len(filtered)} parquet URLs for {year}/{month_padded} via dataset-viewer.\")\n",
    "if not filtered:\n",
    "    print(\"3) No parquet URLs found via API; falling back to incremental probing (may be slower).\")\n",
    "    patterns = config.get(\"probe_patterns\", [])\n",
    "    found = probe_fallback_urls(repo, year, month_padded, config[\"probe_max_attempts\"], patterns)\n",
    "    filtered = found\n",
    "\n",
    "if not filtered:\n",
    "    print(\"ERROR: no parquet URLs discovered for that month/year by API or fallback probing. Aborting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if max_parquets is not None:\n",
    "    filtered = filtered[:int(max_parquets)]\n",
    "\n",
    "print(f\"\\nWill download {len(filtered)} file(s) into {out_dir.resolve()}\\n\")\n",
    "\n",
    "success_count = 0\n",
    "for i, url in enumerate(filtered):\n",
    "    decoded = urllib.parse.unquote(url)\n",
    "    filename = Path(decoded).name\n",
    "    dest = out_dir / filename\n",
    "    if dest.exists():\n",
    "        print(f\"[{i+1}/{len(filtered)}] Skipping (already exists): {filename}\")\n",
    "        success_count += 1\n",
    "        continue\n",
    "    print(f\"[{i+1}/{len(filtered)}] Downloading: {filename}\")\n",
    "    ok = download_file(url, dest)\n",
    "    if not ok:\n",
    "        print(f\"  Failed to download (skipping): {url}\")\n",
    "        if urls == []:\n",
    "            print(\"  Probe-based download hit missing file — stopping probe downloads.\")\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    success_count += 1\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"\\nDone. {success_count} file(s) downloaded to: {out_dir.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
