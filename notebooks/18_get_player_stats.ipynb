{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3522162b",
   "metadata": {},
   "source": [
    "# Database Performance Analysis\n",
    "\n",
    "### Purpose of this Notebook\n",
    "Our data processing pipeline is experiencing a significant slowdown in write performance. After processing about 20 parquet files, the rate of inserting game statistics has dropped from ~140k games/sec to ~70k games/sec. This notebook aims to diagnose the potential causes of this degradation by thoroughly inspecting the state of our `chess_games.db` DuckDB database.\n",
    "\n",
    "We will investigate:\n",
    "- **Database Size**: How large is the database file?\n",
    "- **Table Counts**: How many players, openings, and player-opening stats entries have we accumulated?\n",
    "- **Partition Health**: How is the data distributed across our partitioned `player_opening_stats` tables? An imbalance could indicate a performance bottleneck.\n",
    "- **Data Skew**: Are a few players or openings responsible for a disproportionate number of records? This could strain the primary key lookups during `UPSERT` operations.\n",
    "\n",
    "By understanding the shape and size of our data, we can better identify whether the slowdown is a temporary issue that will level off or a systemic problem that requires architectural changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ba8c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database path: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "Database exists: True\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure the project root is in the system path to allow for absolute imports\n",
    "project_root = Path.cwd()\n",
    "if \"notebooks\" in str(project_root):\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from notebooks.utils.database.db_utils import get_db_connection\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the DuckDB database file.\n",
    "db_path = project_root / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "\n",
    "# Set pandas display options for better readability\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "print(f\"Database path: {db_path}\")\n",
    "print(f\"Database exists: {db_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6a06e",
   "metadata": {},
   "source": [
    "## 1. High-Level Database Statistics\n",
    "\n",
    "First, let's get a high-level overview of the database. We'll check the file size and the total number of records in our main tables: `player`, `opening`, and the unified `player_opening_stats` view. This will give us a sense of the overall scale of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c868cbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database file size: 2765.01 MB\n",
      "\n",
      "--- Database Record Counts ---\n",
      "                    Metric      Count\n",
      "             Total Players     50,000\n",
      "            Total Openings      3,593\n",
      "Total Player-Opening Stats 28,229,204\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db_size_bytes = os.path.getsize(db_path)\n",
    "    db_size_mb = db_size_bytes / (1024 * 1024)\n",
    "    print(f\"Database file size: {db_size_mb:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Database file not found.\")\n",
    "    db_size_mb = 0\n",
    "\n",
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        num_players = con.execute(\"SELECT COUNT(*) FROM player\").fetchone()[0]\n",
    "        num_openings = con.execute(\"SELECT COUNT(*) FROM opening\").fetchone()[0]\n",
    "        num_player_opening_stats = con.execute(\"SELECT COUNT(*) FROM player_opening_stats\").fetchone()[0]\n",
    "\n",
    "        summary_data = {\n",
    "            \"Metric\": [\"Total Players\", \"Total Openings\", \"Total Player-Opening Stats\"],\n",
    "            \"Count\": [num_players, num_openings, num_player_opening_stats]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df[\"Count\"] = summary_df[\"Count\"].apply('{:,.0f}'.format)\n",
    "\n",
    "        print(\"\\n--- Database Record Counts ---\")\n",
    "        print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddab9c",
   "metadata": {},
   "source": [
    "## 2. Partition Analysis\n",
    "\n",
    "Our `player_opening_stats` table is partitioned by the first letter of the ECO code (A, B, C, D, E, and 'other'). The `UPSERT` operations in our pipeline write directly to these partitioned tables. An uneven distribution of data could cause certain partitions to grow much larger than others, potentially slowing down writes to those specific tables.\n",
    "\n",
    "Let's examine the row counts for each partition to see how the data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b50d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Player-Opening Stats Partition Counts ---\n",
      "                 Partition Row Count Percentage\n",
      "    player_opening_stats_A 6,721,561     23.81%\n",
      "    player_opening_stats_B 7,292,844     25.83%\n",
      "    player_opening_stats_C 9,221,077     32.67%\n",
      "    player_opening_stats_D 3,952,764     14.00%\n",
      "    player_opening_stats_E 1,040,958      3.69%\n",
      "player_opening_stats_other         0      0.00%\n",
      "\n",
      "Total Rows: 28,229,204\n"
     ]
    }
   ],
   "source": [
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        partitions = list(\"ABCDE\") + [\"other\"]\n",
    "        partition_stats = []\n",
    "\n",
    "        for p in partitions:\n",
    "            table_name = f\"player_opening_stats_{p}\"\n",
    "            try:\n",
    "                count = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "                partition_stats.append({\"Partition\": table_name, \"Row Count\": count})\n",
    "            except duckdb.CatalogException:\n",
    "                partition_stats.append({\"Partition\": table_name, \"Row Count\": 0})\n",
    "\n",
    "        partition_df = pd.DataFrame(partition_stats)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_rows = partition_df[\"Row Count\"].sum()\n",
    "        if total_rows > 0:\n",
    "            partition_df[\"Percentage\"] = (partition_df[\"Row Count\"] / total_rows) * 100\n",
    "        else:\n",
    "            partition_df[\"Percentage\"] = 0.0\n",
    "\n",
    "        # Format for display\n",
    "        partition_df[\"Row Count\"] = partition_df[\"Row Count\"].apply('{:,.0f}'.format)\n",
    "        partition_df[\"Percentage\"] = partition_df[\"Percentage\"].apply('{:.2f}%'.format)\n",
    "\n",
    "\n",
    "        print(\"\\n--- Player-Opening Stats Partition Counts ---\")\n",
    "        print(partition_df.to_string(index=False))\n",
    "        if total_rows > 0:\n",
    "            print(f\"\\nTotal Rows: {total_rows:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd6373",
   "metadata": {},
   "source": [
    "## 3. Data Skew Analysis\n",
    "\n",
    "A common cause of `UPSERT` slowdowns is data skew, where a small number of keys are involved in a large number of operations. In our case, this could mean:\n",
    "1.  A few highly active players have played a vast number of different openings.\n",
    "2.  A few very common openings have been played by many different players.\n",
    "\n",
    "When a new batch of games is processed, the database has to check for conflicts on `(player_id, opening_id, color)`. If the same players or openings appear frequently, their corresponding records in the stats tables are updated repeatedly. As the tables grow, finding these records to update takes longer.\n",
    "\n",
    "Let's check for this skew by identifying the top players and openings with the most entries in the `player_opening_stats` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73cfa611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Players by Number of Unique Openings Played ---\n",
      "               name opening_count\n",
      "           sergej-v         2,277\n",
      "              magho         2,198\n",
      "    simplesimpson03         2,178\n",
      "         cdplayer72         2,147\n",
      "               ttch         2,133\n",
      "       Mopsik357357         2,131\n",
      "        DanielFlock         2,092\n",
      "Christ-Ginting-KBPL         2,061\n",
      "          EmirGamis         2,051\n",
      "         alehin2021         1,993\n",
      "\n",
      "--- Top 10 Openings by Number of Unique Players ---\n",
      "                                         name eco player_count\n",
      "                         Van't Kruijs Opening A00       86,429\n",
      "                            Queen's Pawn Game D00       78,891\n",
      "                                 Pirc Defense B00       72,653\n",
      "Scandinavian Defense: Mieses-Kotroc Variation B01       71,853\n",
      "                               Mieses Opening A00       71,683\n",
      "                         Scandinavian Defense B01       70,520\n",
      "                             Philidor Defense C41       69,410\n",
      "            French Defense: Advance Variation C02       68,329\n",
      "                               Modern Defense B06       66,015\n",
      "                                 Bird Opening A02       64,569\n",
      "\n",
      "--- Bottom 10 Openings by Number of Unique Players ---\n",
      "                                                                        name eco player_count\n",
      "                     Grünfeld Defense: Russian Variation, Yugoslav Variation D99            1\n",
      "                                             Hungarian Opening: Asten Gambit A00            1\n",
      "    Sicilian Defense: Dragon Variation, Classical Variation, Bernard Defense B74            1\n",
      "                                             Ruy Lopez: Open, Harksen Gambit C80            1\n",
      "                                         Van Geet Opening: Jendrossek Gambit A00            1\n",
      "King's Gambit Accepted: Kieseritzky Gambit, Brentano Defense, Caro Variation C39            1\n",
      "                                          Ruy Lopez: Closed, Alekhine Gambit C88            1\n",
      "                                             Ruy Lopez: Rabinovich Variation C78            1\n",
      "                  French Defense: McCutcheon Variation, Bogoljubow Variation C12            1\n",
      "                           Ruy Lopez: Open, Bernstein Variation, Luther Line C80            1\n",
      "\n",
      "Total number of openings played by less than 5 players: 49\n",
      "\n",
      "--- Player Game Count Percentiles ---\n",
      "Percentile Game Count\n",
      "       10%      3,504\n",
      "       20%      4,023\n",
      "       30%      4,439\n",
      "       40%      4,849\n",
      "       50%      5,281\n",
      "       60%      5,798\n",
      "       70%      6,472\n",
      "       80%      7,453\n",
      "       90%      9,167\n",
      "      100%     39,603\n",
      "\n",
      "--- Top 10 Players by Total Games ---\n",
      "         name total_games\n",
      "    lemil1960      39,603\n",
      "kasparovgenoa      38,821\n",
      "    newstory1      36,461\n",
      "     Gaby1961      34,185\n",
      "   bucanero57      32,846\n",
      "    Garry1304      32,601\n",
      "    Gagojan56      32,263\n",
      "  king_Richie      31,270\n",
      "   cdplayer72      31,189\n",
      "        Dpone      31,134\n"
     ]
    }
   ],
   "source": [
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        print(\"\\n--- Top 10 Players by Number of Unique Openings Played ---\")\n",
    "        top_players_df = con.execute(\"\"\"\n",
    "            SELECT p.name, COUNT(*) AS opening_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN player p ON pos.player_id = p.id\n",
    "            GROUP BY p.name\n",
    "            ORDER BY opening_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        top_players_df[\"opening_count\"] = top_players_df[\"opening_count\"].apply('{:,.0f}'.format)\n",
    "        print(top_players_df.to_string(index=False))\n",
    "\n",
    "        print(\"\\n--- Top 10 Openings by Number of Unique Players ---\")\n",
    "        top_openings_df = con.execute(\"\"\"\n",
    "            SELECT o.name, o.eco, COUNT(*) AS player_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN opening o ON pos.opening_id = o.id\n",
    "            GROUP BY o.name, o.eco\n",
    "            ORDER BY player_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        top_openings_df[\"player_count\"] = top_openings_df[\"player_count\"].apply('{:,.0f}'.format)\n",
    "        print(top_openings_df.to_string(index=False))\n",
    "\n",
    "        # Same thing but with least common openings\n",
    "        print(\"\\n--- Bottom 10 Openings by Number of Unique Players ---\")\n",
    "        bottom_openings_df = con.execute(\"\"\"\n",
    "            SELECT o.name, o.eco, COUNT(*) AS player_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN opening o ON pos.opening_id = o.id\n",
    "            GROUP BY o.name, o.eco\n",
    "            ORDER BY player_count ASC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        bottom_openings_df[\"player_count\"] = bottom_openings_df[\"player_count\"].apply('{:,.0f}'.format)\n",
    "        print(bottom_openings_df.to_string(index=False))\n",
    "\n",
    "        # All opening which have been played by less than five players; including the number of such openings\n",
    "        rare_openings_count = con.execute(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM (\n",
    "                SELECT o.id\n",
    "                FROM player_opening_stats pos\n",
    "                JOIN opening o ON pos.opening_id = o.id\n",
    "                GROUP BY o.id\n",
    "                HAVING COUNT(DISTINCT pos.player_id) < 5\n",
    "            ) AS rare_openings;\n",
    "        \"\"\").fetchone()[0]\n",
    "        print(f\"\\nTotal number of openings played by less than 5 players: {rare_openings_count:,}\")\n",
    "\n",
    "        # print the number of games by player in percentiles\n",
    "        print(\"\\n--- Player Game Count Percentiles ---\")\n",
    "\n",
    "        percentiles = [\n",
    "            i for i in range(10, 101, 10)\n",
    "        ]  # Percentiles from 10% to 100% in increments of 10\n",
    "        percentile_data = []\n",
    "\n",
    "        for p in percentiles:\n",
    "            percentile_str = \"1.0\" if p == 100 else f\"0.{p:02d}\"\n",
    "            value = con.execute(\n",
    "                f\"\"\"\n",
    "                SELECT PERCENTILE_CONT({percentile_str}) WITHIN GROUP (ORDER BY total_games) AS percentile_value\n",
    "                FROM (\n",
    "                    SELECT \n",
    "                        player_id,\n",
    "                        SUM(num_wins + num_draws) AS total_games\n",
    "                    FROM player_opening_stats\n",
    "                    GROUP BY player_id\n",
    "                ) AS player_game_counts;\n",
    "                \"\"\"\n",
    "            ).fetchone()[0]\n",
    "            percentile_data.append({\"Percentile\": f\"{p}%\", \"Game Count\": value})\n",
    "\n",
    "        # Format and print the results\n",
    "        percentile_df = pd.DataFrame(percentile_data)\n",
    "        percentile_df[\"Game Count\"] = percentile_df[\"Game Count\"].apply(\"{:,.0f}\".format)\n",
    "        print(percentile_df.to_string(index=False))\n",
    "\n",
    "        # Find the players with the most games (outliers)\n",
    "        top_players_by_games = con.execute(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                p.name,\n",
    "                SUM(num_wins + num_draws) AS total_games\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN player p ON pos.player_id = p.id\n",
    "            GROUP BY p.name\n",
    "            ORDER BY total_games DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        top_players_by_games[\"total_games\"] = top_players_by_games[\"total_games\"].apply(\n",
    "            \"{:,.0f}\".format\n",
    "        )\n",
    "        print(\"\\n--- Top 10 Players by Total Games ---\")\n",
    "        print(top_players_by_games.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e07847",
   "metadata": {},
   "source": [
    "## Initial Findings & Next Steps\n",
    "\n",
    "Based on the statistics above, we can draw some preliminary conclusions:\n",
    "\n",
    "- **Scale**: Are the tables growing to a size where DuckDB's `UPSERT` performance is known to degrade? (Typically in the hundreds of millions or billions of rows).\n",
    "- **Balance**: Is the data evenly distributed across partitions, or is one partition taking most of the load? A heavily skewed partition might benefit from further sub-partitioning.\n",
    "- **Skew**: Are a few players or openings dominating the stats table? If so, the constant updates to these \"hot\" records could be the primary source of the slowdown.\n",
    "\n",
    "If significant skew is detected, we might need to reconsider our processing strategy. For example, we could batch updates by player or opening to reduce contention, or explore alternative data structures. If the issue is purely scale, we may need to accept the performance curve or explore more heavy-duty database solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
