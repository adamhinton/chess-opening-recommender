{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3522162b",
   "metadata": {},
   "source": [
    "# Database Performance Analysis\n",
    "\n",
    "### Purpose of this Notebook\n",
    "Our data processing pipeline is experiencing a significant slowdown in write performance. After processing about 20 parquet files, the rate of inserting game statistics has dropped from ~140k games/sec to ~70k games/sec. This notebook aims to diagnose the potential causes of this degradation by thoroughly inspecting the state of our `chess_games.db` DuckDB database.\n",
    "\n",
    "We will investigate:\n",
    "- **Database Size**: How large is the database file?\n",
    "- **Table Counts**: How many players, openings, and player-opening stats entries have we accumulated?\n",
    "- **Partition Health**: How is the data distributed across our partitioned `player_opening_stats` tables? An imbalance could indicate a performance bottleneck.\n",
    "- **Data Skew**: Are a few players or openings responsible for a disproportionate number of records? This could strain the primary key lookups during `UPSERT` operations.\n",
    "\n",
    "By understanding the shape and size of our data, we can better identify whether the slowdown is a temporary issue that will level off or a systemic problem that requires architectural changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ba8c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database path: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "Database exists: True\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure the project root is in the system path to allow for absolute imports\n",
    "project_root = Path.cwd()\n",
    "if \"notebooks\" in str(project_root):\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from notebooks.utils.database.db_utils import get_db_connection\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the DuckDB database file.\n",
    "db_path = project_root / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "\n",
    "# Set pandas display options for better readability\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "print(f\"Database path: {db_path}\")\n",
    "print(f\"Database exists: {db_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6a06e",
   "metadata": {},
   "source": [
    "## 1. High-Level Database Statistics\n",
    "\n",
    "First, let's get a high-level overview of the database. We'll check the file size and the total number of records in our main tables: `player`, `opening`, and the unified `player_opening_stats` view. This will give us a sense of the overall scale of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c868cbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database file size: 1895.76 MB\n",
      "\n",
      "--- Database Record Counts ---\n",
      "                    Metric      Count\n",
      "             Total Players     47,948\n",
      "            Total Openings      3,453\n",
      "Total Player-Opening Stats 19,495,579\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db_size_bytes = os.path.getsize(db_path)\n",
    "    db_size_mb = db_size_bytes / (1024 * 1024)\n",
    "    print(f\"Database file size: {db_size_mb:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Database file not found.\")\n",
    "    db_size_mb = 0\n",
    "\n",
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        num_players = con.execute(\"SELECT COUNT(*) FROM player\").fetchone()[0]\n",
    "        num_openings = con.execute(\"SELECT COUNT(*) FROM opening\").fetchone()[0]\n",
    "        num_player_opening_stats = con.execute(\"SELECT COUNT(*) FROM player_opening_stats\").fetchone()[0]\n",
    "\n",
    "        summary_data = {\n",
    "            \"Metric\": [\"Total Players\", \"Total Openings\", \"Total Player-Opening Stats\"],\n",
    "            \"Count\": [num_players, num_openings, num_player_opening_stats]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df[\"Count\"] = summary_df[\"Count\"].apply('{:,.0f}'.format)\n",
    "\n",
    "        print(\"\\n--- Database Record Counts ---\")\n",
    "        print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddab9c",
   "metadata": {},
   "source": [
    "## 2. Partition Analysis\n",
    "\n",
    "Our `player_opening_stats` table is partitioned by the first letter of the ECO code (A, B, C, D, E, and 'other'). The `UPSERT` operations in our pipeline write directly to these partitioned tables. An uneven distribution of data could cause certain partitions to grow much larger than others, potentially slowing down writes to those specific tables.\n",
    "\n",
    "Let's examine the row counts for each partition to see how the data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b50d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Player-Opening Stats Partition Counts ---\n",
      "                 Partition Row Count Percentage\n",
      "    player_opening_stats_A 4,661,674     23.91%\n",
      "    player_opening_stats_B 5,066,550     25.99%\n",
      "    player_opening_stats_C 6,348,083     32.56%\n",
      "    player_opening_stats_D 2,739,227     14.05%\n",
      "    player_opening_stats_E   680,045      3.49%\n",
      "player_opening_stats_other         0      0.00%\n",
      "\n",
      "Total Rows: 19,495,579\n"
     ]
    }
   ],
   "source": [
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        partitions = list(\"ABCDE\") + [\"other\"]\n",
    "        partition_stats = []\n",
    "\n",
    "        for p in partitions:\n",
    "            table_name = f\"player_opening_stats_{p}\"\n",
    "            try:\n",
    "                count = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "                partition_stats.append({\"Partition\": table_name, \"Row Count\": count})\n",
    "            except duckdb.CatalogException:\n",
    "                partition_stats.append({\"Partition\": table_name, \"Row Count\": 0})\n",
    "\n",
    "        partition_df = pd.DataFrame(partition_stats)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_rows = partition_df[\"Row Count\"].sum()\n",
    "        if total_rows > 0:\n",
    "            partition_df[\"Percentage\"] = (partition_df[\"Row Count\"] / total_rows) * 100\n",
    "        else:\n",
    "            partition_df[\"Percentage\"] = 0.0\n",
    "\n",
    "        # Format for display\n",
    "        partition_df[\"Row Count\"] = partition_df[\"Row Count\"].apply('{:,.0f}'.format)\n",
    "        partition_df[\"Percentage\"] = partition_df[\"Percentage\"].apply('{:.2f}%'.format)\n",
    "\n",
    "\n",
    "        print(\"\\n--- Player-Opening Stats Partition Counts ---\")\n",
    "        print(partition_df.to_string(index=False))\n",
    "        if total_rows > 0:\n",
    "            print(f\"\\nTotal Rows: {total_rows:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd6373",
   "metadata": {},
   "source": [
    "## 3. Data Skew Analysis\n",
    "\n",
    "A common cause of `UPSERT` slowdowns is data skew, where a small number of keys are involved in a large number of operations. In our case, this could mean:\n",
    "1.  A few highly active players have played a vast number of different openings.\n",
    "2.  A few very common openings have been played by many different players.\n",
    "\n",
    "When a new batch of games is processed, the database has to check for conflicts on `(player_id, opening_id, color)`. If the same players or openings appear frequently, their corresponding records in the stats tables are updated repeatedly. As the tables grow, finding these records to update takes longer.\n",
    "\n",
    "Let's check for this skew by identifying the top players and openings with the most entries in the `player_opening_stats` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73cfa611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Players by Number of Unique Openings Played ---\n",
      "               name opening_count\n",
      "              porpo         1,744\n",
      "         cdplayer72         1,700\n",
      "       Mopsik357357         1,691\n",
      "              magho         1,671\n",
      "Christ-Ginting-KBPL         1,667\n",
      "      chasbiLapanjo         1,651\n",
      "           sergej-v         1,647\n",
      "          edwiniski         1,626\n",
      "       Mohamed10sec         1,611\n",
      "    simplesimpson03         1,608\n",
      "\n",
      "--- Top 10 Openings by Number of Unique Players ---\n",
      "                                         name eco player_count\n",
      "                         Van't Kruijs Opening A00       76,533\n",
      "                            Queen's Pawn Game D00       66,323\n",
      "Scandinavian Defense: Mieses-Kotroc Variation B01       62,249\n",
      "                               Mieses Opening A00       60,937\n",
      "                                 Pirc Defense B00       60,853\n",
      "                             Philidor Defense C41       58,945\n",
      "                         Scandinavian Defense B01       58,512\n",
      "                               Modern Defense B06       56,946\n",
      "            French Defense: Advance Variation C02       55,061\n",
      "                            Hungarian Opening A00       54,810\n",
      "\n",
      "--- Bottom 10 Openings by Number of Unique Players ---\n",
      "                                                               name eco player_count\n",
      "                                   Indian Defense: Schnepper Gambit A47            1\n",
      "          Philidor Defense: Lopez Countergambit, Jaenisch Variation C41            1\n",
      "King's Gambit Accepted: Kieseritzky Gambit, Anderssen-Cordel Gambit C39            1\n",
      "                                   Scotch Game: Rosenthal Variation C45            1\n",
      "                          Ruy Lopez: Berlin Defense, Winawer Attack C67            1\n",
      "            Grünfeld Defense: Russian Variation, Yugoslav Variation D99            1\n",
      "          King's Gambit Accepted: Double Muzio Gambit, Young Gambit C37            1\n",
      "                              Sodium Attack: Chenoboskion Variation A00            1\n",
      "                                Zukertort Opening: Regina-Nu Gambit A06            1\n",
      "                    Slav Defense: Quiet Variation, Landau Variation D12            1\n",
      "\n",
      "Total number of openings played by less than 5 players: 63\n",
      "\n",
      "--- Player Game Count Percentiles ---\n",
      "Percentile Game Count\n",
      "       10%      1,155\n",
      "       20%      1,715\n",
      "       30%      2,060\n",
      "       40%      2,349\n",
      "       50%      2,638\n",
      "       60%      2,963\n",
      "       70%      3,378\n",
      "       80%      3,959\n",
      "       90%      4,937\n",
      "      100%     19,863\n",
      "\n",
      "--- Top 10 Players by Total Games ---\n",
      "         name total_games\n",
      "kasparovgenoa      19,863\n",
      "    lemil1960      19,101\n",
      "        Dpone      18,447\n",
      "   timmiklaus      17,861\n",
      "    Alfred108      17,030\n",
      "       saeren      16,963\n",
      "   bucanero57      16,836\n",
      "      Gamer13      16,689\n",
      "     Gaby1961      16,676\n",
      "        a_roz      16,655\n"
     ]
    }
   ],
   "source": [
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        print(\"\\n--- Top 10 Players by Number of Unique Openings Played ---\")\n",
    "        top_players_df = con.execute(\"\"\"\n",
    "            SELECT p.name, COUNT(*) AS opening_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN player p ON pos.player_id = p.id\n",
    "            GROUP BY p.name\n",
    "            ORDER BY opening_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        top_players_df[\"opening_count\"] = top_players_df[\"opening_count\"].apply('{:,.0f}'.format)\n",
    "        print(top_players_df.to_string(index=False))\n",
    "\n",
    "        print(\"\\n--- Top 10 Openings by Number of Unique Players ---\")\n",
    "        top_openings_df = con.execute(\"\"\"\n",
    "            SELECT o.name, o.eco, COUNT(*) AS player_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN opening o ON pos.opening_id = o.id\n",
    "            GROUP BY o.name, o.eco\n",
    "            ORDER BY player_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        top_openings_df[\"player_count\"] = top_openings_df[\"player_count\"].apply('{:,.0f}'.format)\n",
    "        print(top_openings_df.to_string(index=False))\n",
    "\n",
    "        # Same thing but with least common openings\n",
    "        print(\"\\n--- Bottom 10 Openings by Number of Unique Players ---\")\n",
    "        bottom_openings_df = con.execute(\"\"\"\n",
    "            SELECT o.name, o.eco, COUNT(*) AS player_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN opening o ON pos.opening_id = o.id\n",
    "            GROUP BY o.name, o.eco\n",
    "            ORDER BY player_count ASC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        bottom_openings_df[\"player_count\"] = bottom_openings_df[\"player_count\"].apply('{:,.0f}'.format)\n",
    "        print(bottom_openings_df.to_string(index=False))\n",
    "\n",
    "        # All opening which have been played by less than five players; including the number of such openings\n",
    "        rare_openings_count = con.execute(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM (\n",
    "                SELECT o.id\n",
    "                FROM player_opening_stats pos\n",
    "                JOIN opening o ON pos.opening_id = o.id\n",
    "                GROUP BY o.id\n",
    "                HAVING COUNT(DISTINCT pos.player_id) < 5\n",
    "            ) AS rare_openings;\n",
    "        \"\"\").fetchone()[0]\n",
    "        print(f\"\\nTotal number of openings played by less than 5 players: {rare_openings_count:,}\")\n",
    "\n",
    "        # print the number of games by player in percentiles\n",
    "        print(\"\\n--- Player Game Count Percentiles ---\")\n",
    "\n",
    "        percentiles = [\n",
    "            i for i in range(10, 101, 10)\n",
    "        ]  # Percentiles from 10% to 100% in increments of 10\n",
    "        percentile_data = []\n",
    "\n",
    "        for p in percentiles:\n",
    "            percentile_str = \"1.0\" if p == 100 else f\"0.{p:02d}\"\n",
    "            value = con.execute(\n",
    "                f\"\"\"\n",
    "                SELECT PERCENTILE_CONT({percentile_str}) WITHIN GROUP (ORDER BY total_games) AS percentile_value\n",
    "                FROM (\n",
    "                    SELECT \n",
    "                        player_id,\n",
    "                        SUM(num_wins + num_draws) AS total_games\n",
    "                    FROM player_opening_stats\n",
    "                    GROUP BY player_id\n",
    "                ) AS player_game_counts;\n",
    "                \"\"\"\n",
    "            ).fetchone()[0]\n",
    "            percentile_data.append({\"Percentile\": f\"{p}%\", \"Game Count\": value})\n",
    "\n",
    "        # Format and print the results\n",
    "        percentile_df = pd.DataFrame(percentile_data)\n",
    "        percentile_df[\"Game Count\"] = percentile_df[\"Game Count\"].apply(\"{:,.0f}\".format)\n",
    "        print(percentile_df.to_string(index=False))\n",
    "\n",
    "        # Find the players with the most games (outliers)\n",
    "        top_players_by_games = con.execute(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                p.name,\n",
    "                SUM(num_wins + num_draws) AS total_games\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN player p ON pos.player_id = p.id\n",
    "            GROUP BY p.name\n",
    "            ORDER BY total_games DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        top_players_by_games[\"total_games\"] = top_players_by_games[\"total_games\"].apply(\n",
    "            \"{:,.0f}\".format\n",
    "        )\n",
    "        print(\"\\n--- Top 10 Players by Total Games ---\")\n",
    "        print(top_players_by_games.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e07847",
   "metadata": {},
   "source": [
    "## Initial Findings & Next Steps\n",
    "\n",
    "Based on the statistics above, we can draw some preliminary conclusions:\n",
    "\n",
    "- **Scale**: Are the tables growing to a size where DuckDB's `UPSERT` performance is known to degrade? (Typically in the hundreds of millions or billions of rows).\n",
    "- **Balance**: Is the data evenly distributed across partitions, or is one partition taking most of the load? A heavily skewed partition might benefit from further sub-partitioning.\n",
    "- **Skew**: Are a few players or openings dominating the stats table? If so, the constant updates to these \"hot\" records could be the primary source of the slowdown.\n",
    "\n",
    "If significant skew is detected, we might need to reconsider our processing strategy. For example, we could batch updates by player or opening to reduce contention, or explore alternative data structures. If the issue is purely scale, we may need to accept the performance curve or explore more heavy-duty database solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
