{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3522162b",
   "metadata": {},
   "source": [
    "# Database Performance Analysis\n",
    "\n",
    "### Purpose of this Notebook\n",
    "Our data processing pipeline is experiencing a significant slowdown in write performance. After processing about 20 parquet files, the rate of inserting game statistics has dropped from ~140k games/sec to ~70k games/sec. This notebook aims to diagnose the potential causes of this degradation by thoroughly inspecting the state of our `chess_games.db` DuckDB database.\n",
    "\n",
    "We will investigate:\n",
    "- **Database Size**: How large is the database file?\n",
    "- **Table Counts**: How many players, openings, and player-opening stats entries have we accumulated?\n",
    "- **Partition Health**: How is the data distributed across our partitioned `player_opening_stats` tables? An imbalance could indicate a performance bottleneck.\n",
    "- **Data Skew**: Are a few players or openings responsible for a disproportionate number of records? This could strain the primary key lookups during `UPSERT` operations.\n",
    "\n",
    "By understanding the shape and size of our data, we can better identify whether the slowdown is a temporary issue that will level off or a systemic problem that requires architectural changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ba8c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database path: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "Database exists: True\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure the project root is in the system path to allow for absolute imports\n",
    "project_root = Path.cwd()\n",
    "if \"notebooks\" in str(project_root):\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from notebooks.utils.database.db_utils import get_db_connection\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the DuckDB database file.\n",
    "db_path = project_root / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "\n",
    "# Set pandas display options for better readability\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "print(f\"Database path: {db_path}\")\n",
    "print(f\"Database exists: {db_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6a06e",
   "metadata": {},
   "source": [
    "## 1. High-Level Database Statistics\n",
    "\n",
    "First, let's get a high-level overview of the database. We'll check the file size and the total number of records in our main tables: `player`, `opening`, and the unified `player_opening_stats` view. This will give us a sense of the overall scale of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c868cbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database file size: 2043.26 MB\n",
      "\n",
      "--- Database Record Counts ---\n",
      "                    Metric      Count\n",
      "             Total Players     50,000\n",
      "            Total Openings      2,991\n",
      "Total Player-Opening Stats 25,378,100\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db_size_bytes = os.path.getsize(db_path)\n",
    "    db_size_mb = db_size_bytes / (1024 * 1024)\n",
    "    print(f\"Database file size: {db_size_mb:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Database file not found.\")\n",
    "    db_size_mb = 0\n",
    "\n",
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        num_players = con.execute(\"SELECT COUNT(*) FROM player\").fetchone()[0]\n",
    "        num_openings = con.execute(\"SELECT COUNT(*) FROM opening\").fetchone()[0]\n",
    "        num_player_opening_stats = con.execute(\"SELECT COUNT(*) FROM player_opening_stats\").fetchone()[0]\n",
    "\n",
    "        summary_data = {\n",
    "            \"Metric\": [\"Total Players\", \"Total Openings\", \"Total Player-Opening Stats\"],\n",
    "            \"Count\": [num_players, num_openings, num_player_opening_stats]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df[\"Count\"] = summary_df[\"Count\"].apply('{:,.0f}'.format)\n",
    "\n",
    "        print(\"\\n--- Database Record Counts ---\")\n",
    "        print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddab9c",
   "metadata": {},
   "source": [
    "## 2. Partition Analysis\n",
    "\n",
    "Our `player_opening_stats` table is partitioned by the first letter of the ECO code (A, B, C, D, E, and 'other'). The `UPSERT` operations in our pipeline write directly to these partitioned tables. An uneven distribution of data could cause certain partitions to grow much larger than others, potentially slowing down writes to those specific tables.\n",
    "\n",
    "Let's examine the row counts for each partition to see how the data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b50d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Player-Opening Stats Partition Counts ---\n",
      "                 Partition Row Count Percentage\n",
      "    player_opening_stats_A 5,843,574     23.03%\n",
      "    player_opening_stats_B 6,643,720     26.18%\n",
      "    player_opening_stats_C 8,439,229     33.25%\n",
      "    player_opening_stats_D 3,473,275     13.69%\n",
      "    player_opening_stats_E   978,302      3.85%\n",
      "player_opening_stats_other         0      0.00%\n",
      "\n",
      "Total Rows: 25,378,100\n"
     ]
    }
   ],
   "source": [
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        partitions = list(\"ABCDE\") + [\"other\"]\n",
    "        partition_stats = []\n",
    "\n",
    "        for p in partitions:\n",
    "            table_name = f\"player_opening_stats_{p}\"\n",
    "            try:\n",
    "                count = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "                partition_stats.append({\"Partition\": table_name, \"Row Count\": count})\n",
    "            except duckdb.CatalogException:\n",
    "                partition_stats.append({\"Partition\": table_name, \"Row Count\": 0})\n",
    "\n",
    "        partition_df = pd.DataFrame(partition_stats)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_rows = partition_df[\"Row Count\"].sum()\n",
    "        if total_rows > 0:\n",
    "            partition_df[\"Percentage\"] = (partition_df[\"Row Count\"] / total_rows) * 100\n",
    "        else:\n",
    "            partition_df[\"Percentage\"] = 0.0\n",
    "\n",
    "        # Format for display\n",
    "        partition_df[\"Row Count\"] = partition_df[\"Row Count\"].apply('{:,.0f}'.format)\n",
    "        partition_df[\"Percentage\"] = partition_df[\"Percentage\"].apply('{:.2f}%'.format)\n",
    "\n",
    "\n",
    "        print(\"\\n--- Player-Opening Stats Partition Counts ---\")\n",
    "        print(partition_df.to_string(index=False))\n",
    "        if total_rows > 0:\n",
    "            print(f\"\\nTotal Rows: {total_rows:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd6373",
   "metadata": {},
   "source": [
    "## 3. Data Skew Analysis\n",
    "\n",
    "A common cause of `UPSERT` slowdowns is data skew, where a small number of keys are involved in a large number of operations. In our case, this could mean:\n",
    "1.  A few highly active players have played a vast number of different openings.\n",
    "2.  A few very common openings have been played by many different players.\n",
    "\n",
    "When a new batch of games is processed, the database has to check for conflicts on `(player_id, opening_id, color)`. If the same players or openings appear frequently, their corresponding records in the stats tables are updated repeatedly. As the tables grow, finding these records to update takes longer.\n",
    "\n",
    "Let's check for this skew by identifying the top players and openings with the most entries in the `player_opening_stats` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73cfa611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Players by Number of Unique Openings Played ---\n",
      "               name opening_count\n",
      "           sergej-v         2,105\n",
      "              magho         2,013\n",
      "    simplesimpson03         1,992\n",
      "         cdplayer72         1,976\n",
      "               ttch         1,964\n",
      "       Mopsik357357         1,954\n",
      "        DanielFlock         1,920\n",
      "Christ-Ginting-KBPL         1,871\n",
      "          EmirGamis         1,864\n",
      "         alehin2021         1,832\n",
      "\n",
      "--- Top 10 Openings by Number of Unique Players ---\n",
      "                                         name eco player_count\n",
      "                                 Pirc Defense B00       75,425\n",
      "                            Caro-Kann Defense B10       73,344\n",
      "                               Modern Defense B06       72,271\n",
      "Scandinavian Defense: Mieses-Kotroc Variation B01       71,853\n",
      "                               Mieses Opening A00       71,683\n",
      "                         Scandinavian Defense B01       70,520\n",
      "                             Philidor Defense C41       69,410\n",
      "            French Defense: Advance Variation C02       68,329\n",
      "                                 Bird Opening A02       64,569\n",
      "             French Defense: Knight Variation C00       63,765\n",
      "\n",
      "--- Bottom 10 Openings by Number of Unique Players ---\n",
      "                                                                                  name eco player_count\n",
      "                                                    Van Geet Opening: Hulsemann Gambit A00           81\n",
      "                                      Pterodactyl Defense: Fianchetto, Rhamphorhynchus B06           81\n",
      "                                                     Tarrasch Defense: Tarrasch Gambit D32           81\n",
      "                                     Philidor Defense: Lion Variation, Sozin Variation C41           82\n",
      "                            Queen's Gambit Accepted: Classical Defense, Russian Gambit D27           82\n",
      "                                    Caro-Kann Defense: Labahn Attack, Polish Variation B10           82\n",
      "                                          Ruy Lopez: Fianchetto Defense, Kevitz Gambit C60           83\n",
      "                    King's Gambit Accepted: Bishop's Gambit, Boren-Svenonius Variation C33           83\n",
      "Sicilian Defense: Richter-Rauzer Variation, Neo-Modern Variation, Nyezhmetdinov Attack B69           83\n",
      "                                    Russian Game: Classical Attack, Tarrasch Variation C42           84\n",
      "\n",
      "Total number of openings played by less than 5 players: 0\n",
      "\n",
      "--- Player Game Count Percentiles ---\n",
      "Percentile Game Count\n",
      "       10%      2,783\n",
      "       20%      3,277\n",
      "       30%      3,656\n",
      "       40%      4,025\n",
      "       50%      4,419\n",
      "       60%      4,867\n",
      "       70%      5,449\n",
      "       80%      6,277\n",
      "       90%      7,764\n",
      "      100%     31,294\n",
      "\n",
      "--- Top 10 Players by Total Games ---\n",
      "         name total_games\n",
      "    Gagojan56      31,294\n",
      "    lemil1960      31,088\n",
      "kasparovgenoa      30,272\n",
      "jusquamachine      29,895\n",
      "    newstory1      28,314\n",
      " eltvillekuhn      28,134\n",
      "    Garry1304      27,294\n",
      "  king_Richie      27,159\n",
      "     Gaby1961      26,597\n",
      "      sasso48      26,461\n"
     ]
    }
   ],
   "source": [
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        print(\"\\n--- Top 10 Players by Number of Unique Openings Played ---\")\n",
    "        top_players_df = con.execute(\"\"\"\n",
    "            SELECT p.name, COUNT(*) AS opening_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN player p ON pos.player_id = p.id\n",
    "            GROUP BY p.name\n",
    "            ORDER BY opening_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        top_players_df[\"opening_count\"] = top_players_df[\"opening_count\"].apply('{:,.0f}'.format)\n",
    "        print(top_players_df.to_string(index=False))\n",
    "\n",
    "        print(\"\\n--- Top 10 Openings by Number of Unique Players ---\")\n",
    "        top_openings_df = con.execute(\"\"\"\n",
    "            SELECT o.name, o.eco, COUNT(*) AS player_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN opening o ON pos.opening_id = o.id\n",
    "            GROUP BY o.name, o.eco\n",
    "            ORDER BY player_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        top_openings_df[\"player_count\"] = top_openings_df[\"player_count\"].apply('{:,.0f}'.format)\n",
    "        print(top_openings_df.to_string(index=False))\n",
    "\n",
    "        # Same thing but with least common openings\n",
    "        print(\"\\n--- Bottom 10 Openings by Number of Unique Players ---\")\n",
    "        bottom_openings_df = con.execute(\"\"\"\n",
    "            SELECT o.name, o.eco, COUNT(*) AS player_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN opening o ON pos.opening_id = o.id\n",
    "            GROUP BY o.name, o.eco\n",
    "            ORDER BY player_count ASC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        bottom_openings_df[\"player_count\"] = bottom_openings_df[\"player_count\"].apply('{:,.0f}'.format)\n",
    "        print(bottom_openings_df.to_string(index=False))\n",
    "\n",
    "        # All opening which have been played by less than five players; including the number of such openings\n",
    "        rare_openings_count = con.execute(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM (\n",
    "                SELECT o.id\n",
    "                FROM player_opening_stats pos\n",
    "                JOIN opening o ON pos.opening_id = o.id\n",
    "                GROUP BY o.id\n",
    "                HAVING COUNT(DISTINCT pos.player_id) < 5\n",
    "            ) AS rare_openings;\n",
    "        \"\"\").fetchone()[0]\n",
    "        print(f\"\\nTotal number of openings played by less than 5 players: {rare_openings_count:,}\")\n",
    "\n",
    "        # print the number of games by player in percentiles\n",
    "        print(\"\\n--- Player Game Count Percentiles ---\")\n",
    "\n",
    "        percentiles = [\n",
    "            i for i in range(10, 101, 10)\n",
    "        ]  # Percentiles from 10% to 100% in increments of 10\n",
    "        percentile_data = []\n",
    "\n",
    "        for p in percentiles:\n",
    "            percentile_str = \"1.0\" if p == 100 else f\"0.{p:02d}\"\n",
    "            value = con.execute(\n",
    "                f\"\"\"\n",
    "                SELECT PERCENTILE_CONT({percentile_str}) WITHIN GROUP (ORDER BY total_games) AS percentile_value\n",
    "                FROM (\n",
    "                    SELECT \n",
    "                        player_id,\n",
    "                        SUM(num_wins + num_draws) AS total_games\n",
    "                    FROM player_opening_stats\n",
    "                    GROUP BY player_id\n",
    "                ) AS player_game_counts;\n",
    "                \"\"\"\n",
    "            ).fetchone()[0]\n",
    "            percentile_data.append({\"Percentile\": f\"{p}%\", \"Game Count\": value})\n",
    "\n",
    "        # Format and print the results\n",
    "        percentile_df = pd.DataFrame(percentile_data)\n",
    "        percentile_df[\"Game Count\"] = percentile_df[\"Game Count\"].apply(\"{:,.0f}\".format)\n",
    "        print(percentile_df.to_string(index=False))\n",
    "\n",
    "        # Find the players with the most games (outliers)\n",
    "        top_players_by_games = con.execute(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                p.name,\n",
    "                SUM(num_wins + num_draws) AS total_games\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN player p ON pos.player_id = p.id\n",
    "            GROUP BY p.name\n",
    "            ORDER BY total_games DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        top_players_by_games[\"total_games\"] = top_players_by_games[\"total_games\"].apply(\n",
    "            \"{:,.0f}\".format\n",
    "        )\n",
    "        print(\"\\n--- Top 10 Players by Total Games ---\")\n",
    "        print(top_players_by_games.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e07847",
   "metadata": {},
   "source": [
    "## Initial Findings & Next Steps\n",
    "\n",
    "Based on the statistics above, we can draw some preliminary conclusions:\n",
    "\n",
    "- **Scale**: Are the tables growing to a size where DuckDB's `UPSERT` performance is known to degrade? (Typically in the hundreds of millions or billions of rows).\n",
    "- **Balance**: Is the data evenly distributed across partitions, or is one partition taking most of the load? A heavily skewed partition might benefit from further sub-partitioning.\n",
    "- **Skew**: Are a few players or openings dominating the stats table? If so, the constant updates to these \"hot\" records could be the primary source of the slowdown.\n",
    "\n",
    "If significant skew is detected, we might need to reconsider our processing strategy. For example, we could batch updates by player or opening to reduce contention, or explore alternative data structures. If the issue is purely scale, we may need to accept the performance curve or explore more heavy-duty database solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
