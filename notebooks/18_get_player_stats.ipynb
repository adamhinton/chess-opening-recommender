{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3522162b",
   "metadata": {},
   "source": [
    "# Database Performance Analysis\n",
    "\n",
    "### Purpose of this Notebook\n",
    "Our data processing pipeline is experiencing a significant slowdown in write performance. After processing about 20 parquet files, the rate of inserting game statistics has dropped from ~140k games/sec to ~70k games/sec. This notebook aims to diagnose the potential causes of this degradation by thoroughly inspecting the state of our `chess_games.db` DuckDB database.\n",
    "\n",
    "We will investigate:\n",
    "- **Database Size**: How large is the database file?\n",
    "- **Table Counts**: How many players, openings, and player-opening stats entries have we accumulated?\n",
    "- **Partition Health**: How is the data distributed across our partitioned `player_opening_stats` tables? An imbalance could indicate a performance bottleneck.\n",
    "- **Data Skew**: Are a few players or openings responsible for a disproportionate number of records? This could strain the primary key lookups during `UPSERT` operations.\n",
    "\n",
    "By understanding the shape and size of our data, we can better identify whether the slowdown is a temporary issue that will level off or a systemic problem that requires architectural changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ba8c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database path: /Users/a/Documents/personalprojects/chess-opening-recommender/data/processed/chess_games.db\n",
      "Database exists: True\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure the project root is in the system path to allow for absolute imports\n",
    "project_root = Path.cwd()\n",
    "if \"notebooks\" in str(project_root):\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from notebooks.utils.database.db_utils import get_db_connection\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the DuckDB database file.\n",
    "db_path = project_root / \"data\" / \"processed\" / \"chess_games.db\"\n",
    "\n",
    "# Set pandas display options for better readability\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "print(f\"Database path: {db_path}\")\n",
    "print(f\"Database exists: {db_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6a06e",
   "metadata": {},
   "source": [
    "## 1. High-Level Database Statistics\n",
    "\n",
    "First, let's get a high-level overview of the database. We'll check the file size and the total number of records in our main tables: `player`, `opening`, and the unified `player_opening_stats` view. This will give us a sense of the overall scale of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c868cbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database file size: 1682.76 MB\n",
      "\n",
      "--- Database Record Counts ---\n",
      "                    Metric      Count\n",
      "             Total Players     46,928\n",
      "            Total Openings      3,447\n",
      "Total Player-Opening Stats 17,314,254\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db_size_bytes = os.path.getsize(db_path)\n",
    "    db_size_mb = db_size_bytes / (1024 * 1024)\n",
    "    print(f\"Database file size: {db_size_mb:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Database file not found.\")\n",
    "    db_size_mb = 0\n",
    "\n",
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        num_players = con.execute(\"SELECT COUNT(*) FROM player\").fetchone()[0]\n",
    "        num_openings = con.execute(\"SELECT COUNT(*) FROM opening\").fetchone()[0]\n",
    "        num_player_opening_stats = con.execute(\"SELECT COUNT(*) FROM player_opening_stats\").fetchone()[0]\n",
    "\n",
    "        summary_data = {\n",
    "            \"Metric\": [\"Total Players\", \"Total Openings\", \"Total Player-Opening Stats\"],\n",
    "            \"Count\": [num_players, num_openings, num_player_opening_stats]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df[\"Count\"] = summary_df[\"Count\"].apply('{:,.0f}'.format)\n",
    "\n",
    "        print(\"\\n--- Database Record Counts ---\")\n",
    "        print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddab9c",
   "metadata": {},
   "source": [
    "## 2. Partition Analysis\n",
    "\n",
    "Our `player_opening_stats` table is partitioned by the first letter of the ECO code (A, B, C, D, E, and 'other'). The `UPSERT` operations in our pipeline write directly to these partitioned tables. An uneven distribution of data could cause certain partitions to grow much larger than others, potentially slowing down writes to those specific tables.\n",
    "\n",
    "Let's examine the row counts for each partition to see how the data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b50d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Player-Opening Stats Partition Counts ---\n",
      "                 Partition Row Count Percentage\n",
      "    player_opening_stats_A 4,166,723     24.07%\n",
      "    player_opening_stats_B 4,491,446     25.94%\n",
      "    player_opening_stats_C 5,636,023     32.55%\n",
      "    player_opening_stats_D 2,418,099     13.97%\n",
      "    player_opening_stats_E   601,963      3.48%\n",
      "player_opening_stats_other         0      0.00%\n",
      "\n",
      "Total Rows: 17,314,254\n"
     ]
    }
   ],
   "source": [
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        partitions = list(\"ABCDE\") + [\"other\"]\n",
    "        partition_stats = []\n",
    "\n",
    "        for p in partitions:\n",
    "            table_name = f\"player_opening_stats_{p}\"\n",
    "            try:\n",
    "                count = con.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "                partition_stats.append({\"Partition\": table_name, \"Row Count\": count})\n",
    "            except duckdb.CatalogException:\n",
    "                partition_stats.append({\"Partition\": table_name, \"Row Count\": 0})\n",
    "\n",
    "        partition_df = pd.DataFrame(partition_stats)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_rows = partition_df[\"Row Count\"].sum()\n",
    "        if total_rows > 0:\n",
    "            partition_df[\"Percentage\"] = (partition_df[\"Row Count\"] / total_rows) * 100\n",
    "        else:\n",
    "            partition_df[\"Percentage\"] = 0.0\n",
    "\n",
    "        # Format for display\n",
    "        partition_df[\"Row Count\"] = partition_df[\"Row Count\"].apply('{:,.0f}'.format)\n",
    "        partition_df[\"Percentage\"] = partition_df[\"Percentage\"].apply('{:.2f}%'.format)\n",
    "\n",
    "\n",
    "        print(\"\\n--- Player-Opening Stats Partition Counts ---\")\n",
    "        print(partition_df.to_string(index=False))\n",
    "        if total_rows > 0:\n",
    "            print(f\"\\nTotal Rows: {total_rows:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd6373",
   "metadata": {},
   "source": [
    "## 3. Data Skew Analysis\n",
    "\n",
    "A common cause of `UPSERT` slowdowns is data skew, where a small number of keys are involved in a large number of operations. In our case, this could mean:\n",
    "1.  A few highly active players have played a vast number of different openings.\n",
    "2.  A few very common openings have been played by many different players.\n",
    "\n",
    "When a new batch of games is processed, the database has to check for conflicts on `(player_id, opening_id, color)`. If the same players or openings appear frequently, their corresponding records in the stats tables are updated repeatedly. As the tables grow, finding these records to update takes longer.\n",
    "\n",
    "Let's check for this skew by identifying the top players and openings with the most entries in the `player_opening_stats` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73cfa611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Players by Number of Unique Openings Played ---\n",
      "               name opening_count\n",
      "              porpo         1,631\n",
      "         cdplayer72         1,579\n",
      "       Mopsik357357         1,569\n",
      "Christ-Ginting-KBPL         1,550\n",
      "         ashleesh21         1,541\n",
      "              yangb         1,532\n",
      "           sergej-v         1,523\n",
      "           EpikAbur         1,512\n",
      "           Maltbier         1,490\n",
      "      chasbiLapanjo         1,478\n",
      "\n",
      "--- Top 10 Openings by Number of Unique Players ---\n",
      "                                         name eco player_count\n",
      "                         Van't Kruijs Opening A00       73,058\n",
      "                            Queen's Pawn Game D00       62,373\n",
      "Scandinavian Defense: Mieses-Kotroc Variation B01       59,374\n",
      "                               Mieses Opening A00       57,536\n",
      "                                 Pirc Defense B00       57,211\n",
      "                             Philidor Defense C41       55,638\n",
      "                         Scandinavian Defense B01       54,648\n",
      "                               Modern Defense B06       54,024\n",
      "                            Hungarian Opening A00       52,054\n",
      "            French Defense: Advance Variation C02       51,002\n",
      "\n",
      "--- Bottom 10 Openings by Number of Unique Players ---\n",
      "                                                                    name eco player_count\n",
      "                 Queen's Gambit Accepted: Old Variation, Korchnoi Gambit D20            1\n",
      "               Philidor Defense: Lopez Countergambit, Jaenisch Variation C41            1\n",
      "                                  Scotch Game: Paulsen, Gunsberg Defense C45            1\n",
      "                               Ruy Lopez: Berlin Defense, Winawer Attack C67            1\n",
      "                      Grünfeld Defense: Lutikov Variation, Murrey Attack D80            1\n",
      "                                              Vienna Game: Adams' Gambit C27            1\n",
      "         Queen's Gambit Accepted: Alekhine Defense, Alatortsev Variation D22            1\n",
      "                       Ruy Lopez: Open, Bernstein Variation, Luther Line C80            1\n",
      "Sicilian Defense: Dragon Variation, Classical Variation, Bernard Defense B74            1\n",
      "               King's Gambit Accepted: Double Muzio Gambit, Young Gambit C37            1\n",
      "\n",
      "Total number of openings played by less than 5 players: 67\n",
      "\n",
      "--- Player Game Count Percentiles ---\n",
      "Percentile Game Count\n",
      "       10%        864\n",
      "       20%      1,372\n",
      "       30%      1,679\n",
      "       40%      1,924\n",
      "       50%      2,174\n",
      "       60%      2,457\n",
      "       70%      2,805\n",
      "       80%      3,292\n",
      "       90%      4,132\n",
      "      100%     17,851\n",
      "\n",
      "--- Top 10 Players by Total Games ---\n",
      "         name total_games\n",
      "   timmiklaus      17,851\n",
      "    Alfred108      16,718\n",
      "kasparovgenoa      16,291\n",
      "    lemil1960      16,004\n",
      "        Dpone      15,255\n",
      "   mgmrussell      13,988\n",
      "   bucanero57      13,731\n",
      "      Gamer13      13,702\n",
      "       saeren      13,598\n",
      "    Gagojan56      13,407\n"
     ]
    }
   ],
   "source": [
    "if db_size_mb > 0:\n",
    "    with get_db_connection(db_path) as con:\n",
    "        print(\"\\n--- Top 10 Players by Number of Unique Openings Played ---\")\n",
    "        top_players_df = con.execute(\"\"\"\n",
    "            SELECT p.name, COUNT(*) AS opening_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN player p ON pos.player_id = p.id\n",
    "            GROUP BY p.name\n",
    "            ORDER BY opening_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        top_players_df[\"opening_count\"] = top_players_df[\"opening_count\"].apply('{:,.0f}'.format)\n",
    "        print(top_players_df.to_string(index=False))\n",
    "\n",
    "        print(\"\\n--- Top 10 Openings by Number of Unique Players ---\")\n",
    "        top_openings_df = con.execute(\"\"\"\n",
    "            SELECT o.name, o.eco, COUNT(*) AS player_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN opening o ON pos.opening_id = o.id\n",
    "            GROUP BY o.name, o.eco\n",
    "            ORDER BY player_count DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        top_openings_df[\"player_count\"] = top_openings_df[\"player_count\"].apply('{:,.0f}'.format)\n",
    "        print(top_openings_df.to_string(index=False))\n",
    "\n",
    "        # Same thing but with least common openings\n",
    "        print(\"\\n--- Bottom 10 Openings by Number of Unique Players ---\")\n",
    "        bottom_openings_df = con.execute(\"\"\"\n",
    "            SELECT o.name, o.eco, COUNT(*) AS player_count\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN opening o ON pos.opening_id = o.id\n",
    "            GROUP BY o.name, o.eco\n",
    "            ORDER BY player_count ASC\n",
    "            LIMIT 10;\n",
    "        \"\"\").fetchdf()\n",
    "        bottom_openings_df[\"player_count\"] = bottom_openings_df[\"player_count\"].apply('{:,.0f}'.format)\n",
    "        print(bottom_openings_df.to_string(index=False))\n",
    "\n",
    "        # All opening which have been played by less than five players; including the number of such openings\n",
    "        rare_openings_count = con.execute(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM (\n",
    "                SELECT o.id\n",
    "                FROM player_opening_stats pos\n",
    "                JOIN opening o ON pos.opening_id = o.id\n",
    "                GROUP BY o.id\n",
    "                HAVING COUNT(DISTINCT pos.player_id) < 5\n",
    "            ) AS rare_openings;\n",
    "        \"\"\").fetchone()[0]\n",
    "        print(f\"\\nTotal number of openings played by less than 5 players: {rare_openings_count:,}\")\n",
    "\n",
    "        # print the number of games by player in percentiles\n",
    "        print(\"\\n--- Player Game Count Percentiles ---\")\n",
    "\n",
    "        percentiles = [\n",
    "            i for i in range(10, 101, 10)\n",
    "        ]  # Percentiles from 10% to 100% in increments of 10\n",
    "        percentile_data = []\n",
    "\n",
    "        for p in percentiles:\n",
    "            percentile_str = \"1.0\" if p == 100 else f\"0.{p:02d}\"\n",
    "            value = con.execute(\n",
    "                f\"\"\"\n",
    "                SELECT PERCENTILE_CONT({percentile_str}) WITHIN GROUP (ORDER BY total_games) AS percentile_value\n",
    "                FROM (\n",
    "                    SELECT \n",
    "                        player_id,\n",
    "                        SUM(num_wins + num_draws) AS total_games\n",
    "                    FROM player_opening_stats\n",
    "                    GROUP BY player_id\n",
    "                ) AS player_game_counts;\n",
    "                \"\"\"\n",
    "            ).fetchone()[0]\n",
    "            percentile_data.append({\"Percentile\": f\"{p}%\", \"Game Count\": value})\n",
    "\n",
    "        # Format and print the results\n",
    "        percentile_df = pd.DataFrame(percentile_data)\n",
    "        percentile_df[\"Game Count\"] = percentile_df[\"Game Count\"].apply(\"{:,.0f}\".format)\n",
    "        print(percentile_df.to_string(index=False))\n",
    "\n",
    "        # Find the players with the most games (outliers)\n",
    "        top_players_by_games = con.execute(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                p.name,\n",
    "                SUM(num_wins + num_draws) AS total_games\n",
    "            FROM player_opening_stats pos\n",
    "            JOIN player p ON pos.player_id = p.id\n",
    "            GROUP BY p.name\n",
    "            ORDER BY total_games DESC\n",
    "            LIMIT 10;\n",
    "        \"\"\"\n",
    "        ).fetchdf()\n",
    "\n",
    "        top_players_by_games[\"total_games\"] = top_players_by_games[\"total_games\"].apply(\n",
    "            \"{:,.0f}\".format\n",
    "        )\n",
    "        print(\"\\n--- Top 10 Players by Total Games ---\")\n",
    "        print(top_players_by_games.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e07847",
   "metadata": {},
   "source": [
    "## Initial Findings & Next Steps\n",
    "\n",
    "Based on the statistics above, we can draw some preliminary conclusions:\n",
    "\n",
    "- **Scale**: Are the tables growing to a size where DuckDB's `UPSERT` performance is known to degrade? (Typically in the hundreds of millions or billions of rows).\n",
    "- **Balance**: Is the data evenly distributed across partitions, or is one partition taking most of the load? A heavily skewed partition might benefit from further sub-partitioning.\n",
    "- **Skew**: Are a few players or openings dominating the stats table? If so, the constant updates to these \"hot\" records could be the primary source of the slowdown.\n",
    "\n",
    "If significant skew is detected, we might need to reconsider our processing strategy. For example, we could batch updates by player or opening to reduce contention, or explore alternative data structures. If the issue is purely scale, we may need to accept the performance curve or explore more heavy-duty database solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
