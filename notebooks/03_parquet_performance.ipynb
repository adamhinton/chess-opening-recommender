{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41acdd1f",
   "metadata": {},
   "source": [
    "# Parquet Data Performance Testing\n",
    "\n",
    "This notebook measures performance metrics for processing chess game data from parquet files. The main objectives are:\n",
    "\n",
    "1. Efficiently load data from parquet files using DuckDB\n",
    "2. Process the data to gather player statistics and opening preferences\n",
    "3. Log detailed performance metrics (games/sec, processing time, etc.)\n",
    "4. Compare with the performance of processing PGN files directly\n",
    "\n",
    "Update: This notebook seems to do a great job of parsing games quickly. Now we are moving on to better filtering.\n",
    "\n",
    "NOTE If you want to quickly adjust games filtering or processing parameters, ProcessingConfig is probably your best bet. I put as many relevant parameters in there as I could think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f8c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from typing import Dict, TypedDict, Optional, Union, Literal, Set, List\n",
    "import duckdb\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e0e8f",
   "metadata": {},
   "source": [
    "## Type Definitions\n",
    "\n",
    "Let's start by defining our data structures for strong typing support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8442243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define types for game results\n",
    "GameResult = Literal[\"1-0\", \"0-1\", \"1/2-1/2\", \"*\"]\n",
    "\n",
    "# Define types for our data structures\n",
    "class OpeningResults(TypedDict):\n",
    "    \"\"\"Statistics for a player's results with a particular opening.\"\"\"\n",
    "    opening_name: str\n",
    "    results: Dict[str, Union[int, float]]\n",
    "\n",
    "class PlayerStats(TypedDict):\n",
    "    \"\"\"Statistics for an individual player.\"\"\"\n",
    "    rating: int\n",
    "    white_games: Dict[str, OpeningResults]  # ECO code -> results\n",
    "    black_games: Dict[str, OpeningResults]  # ECO code -> results\n",
    "    num_games_total: int\n",
    "\n",
    "\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for the game processing pipeline.\n",
    "    Contains parameters for filtering games, batch processing, and parallelization.\n",
    "    This is designed to ensure that the processing of raw chess game data yields usable results efficiently.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Computer efficiency and organization stuff\n",
    "        parquet_path: str,\n",
    "        batch_size: int = 100_000,\n",
    "        save_interval: int = 1,\n",
    "        save_dir: str = \"../data/processed\",\n",
    "        # Chess game filtering stuff\n",
    "        # Neither the black or white player can be below this rating\n",
    "        min_player_rating: int = 1200,\n",
    "        # Players can't be more than 100 rating points apart\n",
    "        max_elo_difference_between_players: int = 100,\n",
    "        # Exclude bullet and daily games by default\n",
    "        allowed_time_controls: Optional[Set[str]] = None\n",
    "    ):\n",
    "        # Notes on game filters:\n",
    "        # Didn't exclude unrated games because our dataset contains only rated games.\n",
    "        # Also didn't have to filter out bot games, because only games between two humans are rated --- I think so, at least.\n",
    "        # See here to look at the data I used: https://huggingface.co/datasets/Lichess/standard-chess-games\n",
    "\n",
    "        self.parquet_path = parquet_path # Path to the Parquet file containing raw game data\n",
    "        self.batch_size = batch_size\n",
    "        self.save_interval = save_interval\n",
    "        self.save_dir = save_dir # Directory to save intermediate results\n",
    "        self.min_player_rating = min_player_rating\n",
    "        self.max_elo_difference_between_players = max_elo_difference_between_players\n",
    "\n",
    "        # Default to common time controls if none specified\n",
    "        # Exclude bullet and daily games because they're unrepresentative\n",
    "        if allowed_time_controls is None:\n",
    "            self.allowed_time_controls = {\"Blitz\", \"Rapid\", \"Classical\"}\n",
    "        else:\n",
    "            self.allowed_time_controls = allowed_time_controls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f07fce",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "We'll define a class to track performance metrics during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c46796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceTracker:\n",
    "    \"\"\"Track and report performance metrics during processing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.last_log_time = self.start_time\n",
    "        self.total_games = 0\n",
    "        self.batch_times = []\n",
    "        self.batch_sizes = []\n",
    "        self.memory_usage = []\n",
    "        \n",
    "        # Tracking for filtered vs. accepted games\n",
    "        self.accepted_games = 0\n",
    "        self.filtered_games = 0\n",
    "    \n",
    "    def start_batch(self):\n",
    "        \"\"\"Mark the start of a new batch.\"\"\"\n",
    "        self.batch_start_time = time.time()\n",
    "    \n",
    "    def end_batch(self, batch_size: int):\n",
    "        \"\"\"Mark the end of a batch and record metrics.\"\"\"\n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - self.batch_start_time\n",
    "        \n",
    "        self.total_games += batch_size\n",
    "        self.batch_times.append(batch_time)\n",
    "        self.batch_sizes.append(batch_size)\n",
    "        \n",
    "        # Record memory usage\n",
    "        mem = psutil.virtual_memory()\n",
    "        self.memory_usage.append({\n",
    "            \"percent\": mem.percent,\n",
    "            \"used_gb\": mem.used / (1024**3),\n",
    "            \"available_gb\": mem.available / (1024**3)\n",
    "        })\n",
    "        \n",
    "        return batch_time\n",
    "    \n",
    "    def log_progress(self, force: bool = False):\n",
    "        \"\"\"Log progress information if enough time has passed or if forced.\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Log if it's been more than 5 seconds since the last log or if forced\n",
    "        if force or (current_time - self.last_log_time) >= 5:\n",
    "            elapsed_total = current_time - self.start_time\n",
    "            games_per_sec = self.total_games / elapsed_total if elapsed_total > 0 else 0\n",
    "            \n",
    "            # Calculate recent performance (last 5 batches or fewer)\n",
    "            recent_batches = min(5, len(self.batch_times))\n",
    "            if recent_batches > 0:\n",
    "                recent_time = sum(self.batch_times[-recent_batches:])\n",
    "                recent_games = sum(self.batch_sizes[-recent_batches:])\n",
    "                recent_rate = recent_games / recent_time if recent_time > 0 else 0\n",
    "                \n",
    "                # Get the latest memory usage\n",
    "                latest_mem = self.memory_usage[-1] if self.memory_usage else {\"percent\": 0, \"used_gb\": 0, \"available_gb\": 0}\n",
    "                \n",
    "                # Calculate acceptance rate\n",
    "                total_processed = self.accepted_games + self.filtered_games\n",
    "                acceptance_rate = (self.accepted_games / total_processed * 100) if total_processed > 0 else 0\n",
    "                \n",
    "                print(f\"Processed {self.total_games:,} games in {elapsed_total:.2f} seconds\")\n",
    "                print(f\"Accepted: {self.accepted_games:,} games, Filtered: {self.filtered_games:,} games (Acceptance rate: {acceptance_rate:.1f}%)\")\n",
    "                print(f\"Overall rate: {games_per_sec:.1f} games/sec\")\n",
    "                print(f\"Recent rate: {recent_rate:.1f} games/sec\")\n",
    "                print(f\"Memory usage: {latest_mem['percent']}% (Used: {latest_mem['used_gb']:.1f}GB, \"\n",
    "                      f\"Available: {latest_mem['available_gb']:.1f}GB)\")\n",
    "                print(\"-\" * 40)\n",
    "            \n",
    "            self.last_log_time = current_time\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get a summary of all performance metrics.\"\"\"\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - self.start_time\n",
    "        \n",
    "        avg_batch_time = sum(self.batch_times) / len(self.batch_times) if self.batch_times else 0\n",
    "        max_batch_time = max(self.batch_times) if self.batch_times else 0\n",
    "        min_batch_time = min(self.batch_times) if self.batch_times else 0\n",
    "        \n",
    "        avg_batch_size = sum(self.batch_sizes) / len(self.batch_sizes) if self.batch_sizes else 0\n",
    "        \n",
    "        overall_rate = self.total_games / total_time if total_time > 0 else 0\n",
    "        \n",
    "        # Calculate filtering stats\n",
    "        total_processed = self.accepted_games + self.filtered_games\n",
    "        acceptance_rate = (self.accepted_games / total_processed * 100) if total_processed > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_games\": self.total_games,\n",
    "            \"total_time_sec\": total_time,\n",
    "            \"avg_batch_time_sec\": avg_batch_time,\n",
    "            \"min_batch_time_sec\": min_batch_time,\n",
    "            \"max_batch_time_sec\": max_batch_time,\n",
    "            \"avg_batch_size\": avg_batch_size,\n",
    "            \"overall_rate_games_per_sec\": overall_rate,\n",
    "            \"memory_usage\": self.memory_usage,\n",
    "            # Add filtering stats\n",
    "            \"accepted_games\": self.accepted_games,\n",
    "            \"filtered_games\": self.filtered_games,\n",
    "            \"acceptance_rate_percent\": round(acceptance_rate, 1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54f750",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These functions will help us process the data and manage player statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70218eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_player_stats(data1: Dict[str, PlayerStats], data2: Dict[str, PlayerStats]) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Merge two player statistics dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        data1: First player statistics dictionary\n",
    "        data2: Second player statistics dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Merged player statistics\n",
    "    \"\"\"\n",
    "    merged_data: Dict[str, PlayerStats] = data1.copy()\n",
    "    \n",
    "    for player, stats in data2.items():\n",
    "        if player not in merged_data:\n",
    "            merged_data[player] = stats\n",
    "        else:\n",
    "            # Update total game count\n",
    "            merged_data[player][\"num_games_total\"] += stats[\"num_games_total\"]\n",
    "            \n",
    "            # Update white games\n",
    "            for eco, opening_data in stats[\"white_games\"].items():\n",
    "                if eco not in merged_data[player][\"white_games\"]:\n",
    "                    merged_data[player][\"white_games\"][eco] = opening_data\n",
    "                else:\n",
    "                    # Update results for this opening\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_games\"] += opening_data[\"results\"][\"num_games\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_wins\"] += opening_data[\"results\"][\"num_wins\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_losses\"] += opening_data[\"results\"][\"num_losses\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_draws\"] += opening_data[\"results\"][\"num_draws\"]\n",
    "                    \n",
    "                    # Recalculate score percentage\n",
    "                    wins = merged_data[player][\"white_games\"][eco][\"results\"][\"num_wins\"]\n",
    "                    draws = merged_data[player][\"white_games\"][eco][\"results\"][\"num_draws\"]\n",
    "                    total = merged_data[player][\"white_games\"][eco][\"results\"][\"num_games\"]\n",
    "                    score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "            \n",
    "            # Update black games\n",
    "            for eco, opening_data in stats[\"black_games\"].items():\n",
    "                if eco not in merged_data[player][\"black_games\"]:\n",
    "                    merged_data[player][\"black_games\"][eco] = opening_data\n",
    "                else:\n",
    "                    # Update results for this opening\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_games\"] += opening_data[\"results\"][\"num_games\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_wins\"] += opening_data[\"results\"][\"num_wins\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_losses\"] += opening_data[\"results\"][\"num_losses\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_draws\"] += opening_data[\"results\"][\"num_draws\"]\n",
    "                    \n",
    "                    # Recalculate score percentage\n",
    "                    wins = merged_data[player][\"black_games\"][eco][\"results\"][\"num_wins\"]\n",
    "                    draws = merged_data[player][\"black_games\"][eco][\"results\"][\"num_draws\"]\n",
    "                    total = merged_data[player][\"black_games\"][eco][\"results\"][\"num_games\"]\n",
    "                    score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def save_progress(players_data: Dict[str, PlayerStats], \n",
    "                  batch_num: int, \n",
    "                  config: ProcessingConfig,\n",
    "                  perf_tracker: Optional[PerformanceTracker] = None) -> None:\n",
    "    \"\"\"\n",
    "    Save current progress to disk atomically to prevent corruption.\n",
    "    \n",
    "    Args:\n",
    "        players_data: Current player statistics\n",
    "        batch_num: Current batch number\n",
    "        config: Processing configuration\n",
    "        perf_tracker: Performance tracker object\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    save_dir = Path(config.save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save player data\n",
    "    player_data_path = save_dir / \"player_stats_parquet.pkl\"\n",
    "    \n",
    "    # For large datasets, pickle can be more efficient than JSON\n",
    "    with open(player_data_path, 'wb') as f:\n",
    "        pickle.dump(players_data, f)\n",
    "        \n",
    "    # Save progress information\n",
    "    progress_path = save_dir / \"processing_progress_parquet.json\"\n",
    "    \n",
    "    # Create a serializable version of the config (convert set to list)\n",
    "    config_dict = vars(config).copy()\n",
    "    if 'allowed_time_controls' in config_dict and isinstance(config_dict['allowed_time_controls'], set):\n",
    "        config_dict['allowed_time_controls'] = list(config_dict['allowed_time_controls'])\n",
    "        \n",
    "    progress_info = {\n",
    "        \"last_batch_processed\": batch_num,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"num_players\": len(players_data),\n",
    "        \"config\": config_dict\n",
    "    }\n",
    "    \n",
    "    # Add performance metrics if available\n",
    "    if perf_tracker:\n",
    "        progress_info[\"performance\"] = perf_tracker.get_summary()\n",
    "    \n",
    "    # Atomic write: write to a temporary file first, then rename\n",
    "    temp_progress_path = progress_path.with_suffix(\".json.tmp\")\n",
    "    try:\n",
    "        with open(temp_progress_path, 'w') as f:\n",
    "            json.dump(progress_info, f, indent=2)\n",
    "        temp_progress_path.rename(progress_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving progress: {e}\")\n",
    "        if temp_progress_path.exists():\n",
    "            temp_progress_path.unlink()\n",
    "    \n",
    "    print(f\"Saved progress after batch {batch_num}. \" +\n",
    "          f\"Current data includes {len(players_data)} players.\")\n",
    "\n",
    "def load_progress(config: ProcessingConfig) -> tuple[Dict[str, PlayerStats], int]:\n",
    "    \"\"\"\n",
    "    Load previous progress from disk, handling potential file corruption.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (player_data, last_batch_processed)\n",
    "    \"\"\"\n",
    "    player_data_path = Path(config.save_dir) / \"player_stats_parquet.pkl\"\n",
    "    progress_path = Path(config.save_dir) / \"processing_progress_parquet.json\"\n",
    "    \n",
    "    # Default values if no saved progress\n",
    "    players_data: Dict[str, PlayerStats] = {}\n",
    "    last_batch = 0\n",
    "    \n",
    "    # Load player data if it exists\n",
    "    if player_data_path.exists():\n",
    "        try:\n",
    "            with open(player_data_path, 'rb') as f:\n",
    "                players_data = pickle.load(f)\n",
    "            print(f\"Loaded player data with {len(players_data)} players.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading player data: {e}\")\n",
    "            players_data = {}\n",
    "    \n",
    "    # Load progress info if it exists\n",
    "    if progress_path.exists():\n",
    "        try:\n",
    "            with open(progress_path, 'r') as f:\n",
    "                progress_info = json.load(f)\n",
    "                last_batch = progress_info.get(\"last_batch_processed\", 0)\n",
    "            print(f\"Resuming from batch {last_batch}.\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not decode {progress_path}. File may be corrupt. Starting from scratch.\")\n",
    "            last_batch = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading progress info: {e}\")\n",
    "            last_batch = 0\n",
    "            \n",
    "    return players_data, last_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ed179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_game(row: pd.Series, config: ProcessingConfig) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a game meets the filtering criteria. This ensures only relevant, informative games are processed.\n",
    "    \n",
    "    Args:\n",
    "        row: A row from the DataFrame representing a game\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        True if the game passes our filters, False otherwise\n",
    "    \"\"\"\n",
    "    # Check player ratings\n",
    "    if (row['WhiteElo'] < config.min_player_rating or \n",
    "        row['BlackElo'] < config.min_player_rating):\n",
    "        return False\n",
    "\n",
    "    # Check rating difference\n",
    "    if abs(row['WhiteElo'] - row['BlackElo']) > config.max_elo_difference_between_players:\n",
    "        return False\n",
    "\n",
    "    # \"Event\" column on game contains time control, they're titled like \"Rated Blitz Games\"\n",
    "    # Check that the time control is in the allowed time controls (case insensitive)\n",
    "    event_lower = row[\"Event\"].lower()\n",
    "    if not any(tc.lower() in event_lower for tc in config.allowed_time_controls):\n",
    "        return False\n",
    "\n",
    "    # Check for valid result\n",
    "    # If it's something weird that's not a win loss or draw, toss it out\n",
    "    if row['Result'] not in {\"1-0\", \"0-1\", \"1/2-1/2\"}:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d807e9c",
   "metadata": {},
   "source": [
    "## Data Processing Functions\n",
    "\n",
    "Now let's implement the core functions to process the parquet data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e76ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df: pd.DataFrame, config: ProcessingConfig, log_frequency: int = 5000, perf_tracker: Optional[PerformanceTracker] = None, file_context: Optional[Dict] = None) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a batch of games from a DataFrame and return player statistics.\n",
    "    \n",
    "    Args:\n",
    "        batch_df: DataFrame containing a batch of games\n",
    "        config: Processing configuration\n",
    "        log_frequency: Log progress after processing this many games\n",
    "        perf_tracker: Performance tracker object to update with filtering stats\n",
    "        file_context: Dictionary with context about the multi-file processing job\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping player usernames to their statistics\n",
    "    \"\"\"\n",
    "    players_data: Dict[str, PlayerStats] = {}\n",
    "    start_time = time.time()\n",
    "    total_rows = len(batch_df)\n",
    "    \n",
    "    # Tracking for filtered vs. accepted games in this batch\n",
    "    batch_accepted = 0\n",
    "    batch_filtered = 0\n",
    "    \n",
    "    # Process each game in the batch\n",
    "    for i, (_, game) in enumerate(batch_df.iterrows()):\n",
    "        # Log progress periodically within the batch\n",
    "        if (i + 1) % log_frequency == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "            eta = (total_rows - (i + 1)) / rate if rate > 0 else 0\n",
    "            \n",
    "            # Calculate acceptance rate for this batch so far\n",
    "            processed_so_far = batch_accepted + batch_filtered\n",
    "            acceptance_rate = (batch_accepted / processed_so_far * 100) if processed_so_far > 0 else 0\n",
    "            \n",
    "            # Multi-file progress\n",
    "            multi_file_str = \"\"\n",
    "            if file_context:\n",
    "                files_remaining = file_context['total_files'] - file_context['current_file_num']\n",
    "                \n",
    "                # Estimate total progress\n",
    "                rows_done_in_prev_files = (file_context['current_file_num'] - 1) * file_context['avg_rows_per_file']\n",
    "                rows_done_in_current_file = i + 1\n",
    "                total_rows_processed = rows_done_in_prev_files + rows_done_in_current_file\n",
    "                \n",
    "                total_elapsed = time.time() - file_context['total_start_time']\n",
    "                overall_rate = total_rows_processed / total_elapsed if total_elapsed > 0 else 0\n",
    "                \n",
    "                remaining_rows = file_context['total_rows_estimate'] - total_rows_processed\n",
    "                total_eta_seconds = remaining_rows / overall_rate if overall_rate > 0 else 0\n",
    "                \n",
    "                multi_file_str = (\n",
    "                    f\"File {file_context['current_file_num']}/{file_context['total_files']} \"\n",
    "                    f\"({files_remaining} left) - Total ETA: {total_eta_seconds / 60:.1f} min\"\n",
    "                )\n",
    "\n",
    "            print(f\"Progress: {i+1:,}/{total_rows:,} ({(i+1)/total_rows*100:.1f}%) - \"\n",
    "                  f\"Rate: {rate:.1f} games/sec - File ETA: {eta/60:.1f} min - {multi_file_str}\")\n",
    "            print(f\"Batch filtering: Accepted {batch_accepted:,}, Filtered {batch_filtered:,} (Acceptance rate: {acceptance_rate:.1f}%)\")\n",
    "        \n",
    "        # Filter out invalid games\n",
    "        if not is_valid_game(game, config):\n",
    "            batch_filtered += 1\n",
    "            if perf_tracker:\n",
    "                perf_tracker.filtered_games += 1\n",
    "            continue\n",
    "        \n",
    "        # Mark as accepted\n",
    "        batch_accepted += 1\n",
    "        if perf_tracker:\n",
    "            perf_tracker.accepted_games += 1\n",
    "\n",
    "        # Extract relevant fields\n",
    "        white_player = game['White']\n",
    "        black_player = game['Black']\n",
    "        \n",
    "        # Handle potential missing values\n",
    "        try:\n",
    "            white_elo = int(game.get('WhiteElo', 0))\n",
    "            black_elo = int(game.get('BlackElo', 0))\n",
    "        except (ValueError, TypeError):\n",
    "            white_elo = 0\n",
    "            black_elo = 0\n",
    "            \n",
    "        result = game['Result']\n",
    "        eco_code = game.get('ECO', 'Unknown')\n",
    "        opening_name = game.get('Opening', 'Unknown Opening')\n",
    "        \n",
    "        # Process white player's game\n",
    "        if white_player not in players_data:\n",
    "            players_data[white_player] = {\n",
    "                \"rating\": white_elo,\n",
    "                \"white_games\": {},\n",
    "                \"black_games\": {},\n",
    "                \"num_games_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Update white player's data\n",
    "        if eco_code not in players_data[white_player][\"white_games\"]:\n",
    "            players_data[white_player][\"white_games\"][eco_code] = {\n",
    "                \"opening_name\": opening_name,\n",
    "                \"results\": {\n",
    "                    \"num_games\": 0,\n",
    "                    \"num_wins\": 0,\n",
    "                    \"num_losses\": 0,\n",
    "                    \"num_draws\": 0,\n",
    "                    \"score_percentage_with_opening\": 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Update game counts\n",
    "        players_data[white_player][\"num_games_total\"] += 1\n",
    "        players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_games\"] += 1\n",
    "        \n",
    "        # Update result counts\n",
    "        if result == \"1-0\":  # White win\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_wins\"] += 1\n",
    "        elif result == \"0-1\":  # Black win (white loss)\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_losses\"] += 1\n",
    "        elif result == \"1/2-1/2\":  # Draw\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_draws\"] += 1\n",
    "            \n",
    "        # Update score percentage\n",
    "        wins = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_wins\"]\n",
    "        draws = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_draws\"]\n",
    "        total = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_games\"]\n",
    "        score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "        players_data[white_player][\"white_games\"][eco_code][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "        \n",
    "        # Similarly process black player's game\n",
    "        if black_player not in players_data:\n",
    "            players_data[black_player] = {\n",
    "                \"rating\": black_elo,\n",
    "                \"white_games\": {},\n",
    "                \"black_games\": {},\n",
    "                \"num_games_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Update black player's data\n",
    "        if eco_code not in players_data[black_player][\"black_games\"]:\n",
    "            players_data[black_player][\"black_games\"][eco_code] = {\n",
    "                \"opening_name\": opening_name,\n",
    "                \"results\": {\n",
    "                    \"num_games\": 0,\n",
    "                    \"num_wins\": 0,\n",
    "                    \"num_losses\": 0,\n",
    "                    \"num_draws\": 0,\n",
    "                    \"score_percentage_with_opening\": 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Update game counts\n",
    "        players_data[black_player][\"num_games_total\"] += 1\n",
    "        players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_games\"] += 1\n",
    "        \n",
    "        # Update result counts\n",
    "        if result == \"0-1\":  # Black win\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_wins\"] += 1\n",
    "        elif result == \"1-0\":  # White win (black loss)\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_losses\"] += 1\n",
    "        elif result == \"1/2-1/2\":  # Draw\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_draws\"] += 1\n",
    "            \n",
    "        # Update score percentage\n",
    "        wins = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_wins\"]\n",
    "        draws = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_draws\"]\n",
    "        total = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_games\"]\n",
    "        score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "        players_data[black_player][\"black_games\"][eco_code][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "    \n",
    "    # Final progress update\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "    acceptance_rate = (batch_accepted / total_rows * 100) if total_rows > 0 else 0\n",
    "    print(f\"Completed {total_rows:,} games in {elapsed:.1f} seconds - Rate: {rate:.1f} games/sec\")\n",
    "    print(f\"Batch filtering stats: Accepted {batch_accepted:,}, Filtered {batch_filtered:,} (Acceptance rate: {acceptance_rate:.1f}%)\")\n",
    "    \n",
    "    return players_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89be59",
   "metadata": {},
   "source": [
    "## Main Processing Function\n",
    "\n",
    "Now let's implement the main function that processes the parquet file in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686b9ca",
   "metadata": {},
   "source": [
    "## Multi-File Processing with Duplicate Detection\n",
    "\n",
    "Before defining our main processing function, let's implement a system to handle duplicate file detection. This is essential when processing multiple parquet files that might have similar names but come from different months or batches. Our approach uses metadata fingerprinting to uniquely identify each file.\n",
    "\n",
    "This uses the dupe-check utils defined in our utils folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66d3f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported FileRegistry\n"
     ]
    }
   ],
   "source": [
    "# Import our custom file registry utility\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to the Python path to enable imports\n",
    "notebooks_dir = Path.cwd().parent\n",
    "if str(notebooks_dir) not in sys.path:\n",
    "    sys.path.append(str(notebooks_dir))\n",
    "\n",
    "# Try to import our custom file registry utility\n",
    "try:\n",
    "    from notebooks.utils.file_processing.raw_data_file_dupe_checks import FileRegistry\n",
    "    print(\"Successfully imported FileRegistry\")\n",
    "except ImportError:\n",
    "    print(\"Could not import FileRegistry - file duplicate checks will not be available\")\n",
    "    \n",
    "    # Define a simple FileRegistry class if the import fails\n",
    "    class FileRegistry:\n",
    "        \"\"\"Simple FileRegistry implementation for duplicate detection.\"\"\"\n",
    "        def __init__(self):\n",
    "            self.registry_path = Path(notebooks_dir) / \"data/processed/file_registry.json\"\n",
    "            self.processed_files = set()\n",
    "            self._load_registry()\n",
    "            \n",
    "        def _load_registry(self):\n",
    "            import json\n",
    "            if self.registry_path.exists():\n",
    "                try:\n",
    "                    with open(self.registry_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        self.processed_files = set(data.get('processed_files', []))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load registry: {e}\")\n",
    "        \n",
    "        def _save_registry(self):\n",
    "            import json\n",
    "            try:\n",
    "                self.registry_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(self.registry_path, 'w') as f:\n",
    "                    json.dump({'processed_files': list(self.processed_files)}, f)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not save registry: {e}\")\n",
    "                \n",
    "        def is_file_processed(self, file_path: str) -> bool:\n",
    "            return str(file_path) in self.processed_files\n",
    "            \n",
    "        def mark_file_processed(self, file_path: str) -> None:\n",
    "            self.processed_files.add(str(file_path))\n",
    "            self._save_registry()\n",
    "            \n",
    "        def mark_file_skipped(self, file_path: str) -> None:\n",
    "            self.processed_files.add(str(file_path))\n",
    "            self._save_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb402e",
   "metadata": {},
   "source": [
    "## System Information\n",
    "\n",
    "Let's check the system's hardware resources to optimize our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c42a4729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Information:\n",
      "  cpu_count_physical: 6\n",
      "  cpu_count_logical: 12\n",
      "  memory_total_gb: 32.0\n",
      "  memory_available_gb: 23.76\n",
      "\n",
      "Recommended batch size based on memory: 7,470,000\n"
     ]
    }
   ],
   "source": [
    "# Install psutil if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"Installing psutil package...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "    import psutil\n",
    "    print(\"psutil installed successfully\")\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Get information about the system's hardware resources.\"\"\"\n",
    "    info = {\n",
    "        \"cpu_count_physical\": psutil.cpu_count(logical=False),\n",
    "        \"cpu_count_logical\": psutil.cpu_count(logical=True),\n",
    "        \"memory_total_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "        \"memory_available_gb\": round(psutil.virtual_memory().available / (1024**3), 2)\n",
    "    }\n",
    "    return info\n",
    "\n",
    "# Get system information\n",
    "system_info = get_system_info()\n",
    "print(\"System Information:\")\n",
    "for key, value in system_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Calculate optimal batch size based on available memory\n",
    "# Assuming each row needs about 1KB of memory\n",
    "available_memory_gb = system_info[\"memory_available_gb\"]\n",
    "memory_for_batch_gb = available_memory_gb * 0.3  # Use 30% of available memory\n",
    "optimal_batch_size = int(memory_for_batch_gb * 1024**3 / 1024)  # 1KB per row\n",
    "\n",
    "# Round to nearest 10,000\n",
    "optimal_batch_size = max(10_000, round(optimal_batch_size / 10_000) * 10_000)\n",
    "\n",
    "print(f\"\\nRecommended batch size based on memory: {optimal_batch_size:,}\")\n",
    "\n",
    "def process_parquet_file(config: ProcessingConfig, log_frequency: int = 5000, existing_players_data: Optional[Dict[str, PlayerStats]] = None, file_context: Optional[Dict] = None) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a parquet file in batches, with detailed performance tracking.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration\n",
    "        log_frequency: How often to log progress within a batch (in number of games)\n",
    "        existing_players_data: Optional dictionary of existing player data to append to\n",
    "        file_context: Optional dictionary with context for multi-file processing\n",
    "        \n",
    "    Returns:\n",
    "        Player statistics dictionary\n",
    "    \"\"\"\n",
    "    # Check if file has already been processed using FileRegistry\n",
    "    try:\n",
    "        registry = FileRegistry()\n",
    "        if registry.is_file_processed(config.parquet_path):\n",
    "            print(f\"Skipping already processed file: {Path(config.parquet_path).name}\")\n",
    "            return existing_players_data or {}\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not check file registry: {e}\")\n",
    "    \n",
    "    # Initialize DuckDB connection\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    # Load previous progress if any\n",
    "    players_data, start_batch = load_progress(config)\n",
    "\n",
    "    # If we are processing a new file in a multi-file run, we should start from batch 0 for this file\n",
    "    if existing_players_data is not None:\n",
    "        start_batch = 0\n",
    "        players_data = merge_player_stats(players_data, existing_players_data)\n",
    "        print(f\"Started with {len(players_data)} players from previous files. Resetting batch counter for new file.\")\n",
    "    \n",
    "    # Initialize performance tracker\n",
    "    perf_tracker = PerformanceTracker()\n",
    "    \n",
    "    # Get total number of rows\n",
    "    print(\"Counting total rows in parquet file...\")\n",
    "    total_rows = con.execute(\n",
    "        f\"SELECT COUNT(*) FROM '{config.parquet_path}'\"\n",
    "    ).fetchone()[0]\n",
    "    print(f\"Total rows in parquet file: {total_rows:,}\")\n",
    "    \n",
    "    # If resuming, check if we're already done\n",
    "    if start_batch * config.batch_size >= total_rows:\n",
    "        print(f\"Resuming from batch {start_batch}, which is after the end of the file. Skipping.\")\n",
    "        # Mark the file as processed since we are technically done with it\n",
    "        try:\n",
    "            registry.mark_file_processed(config.parquet_path)\n",
    "            print(f\"Marked file {Path(config.parquet_path).name} as processed in the registry\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not update file registry: {e}\")\n",
    "        return players_data\n",
    "\n",
    "    # Calculate number of batches\n",
    "    total_batches = (total_rows + config.batch_size - 1) // config.batch_size\n",
    "    print(f\"Will process in {total_batches} batches of size {config.batch_size:,} (resuming from batch {start_batch})\")\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_num = start_batch\n",
    "    \n",
    "    while True:\n",
    "        # Calculate offset for the current batch\n",
    "        offset = batch_num * config.batch_size\n",
    "        \n",
    "        # Check if we've processed all rows\n",
    "        if offset >= total_rows:\n",
    "            print(\"Processed all rows. Finishing up.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nProcessing batch {batch_num + 1}/{total_batches} (offset {offset:,})\")\n",
    "        perf_tracker.start_batch()\n",
    "        \n",
    "        # Fetch a batch of data\n",
    "        batch_query = f\"\"\"\n",
    "        SELECT \n",
    "            Event, White, Black, Result, \n",
    "            WhiteTitle, BlackTitle, WhiteElo, BlackElo, \n",
    "            WhiteRatingDiff, BlackRatingDiff, ECO, Opening,\n",
    "            Termination, TimeControl\n",
    "        FROM '{config.parquet_path}'\n",
    "        LIMIT {config.batch_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query and convert to DataFrame\n",
    "        batch_df = con.execute(batch_query).df()\n",
    "        \n",
    "        if batch_df.empty:\n",
    "            print(\"Loaded an empty batch. This might mean we are past the end of the file.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Loaded batch with {len(batch_df):,} rows\")\n",
    "        \n",
    "        # Process the batch\n",
    "        batch_data = process_batch(batch_df, config, log_frequency=log_frequency, perf_tracker=perf_tracker, file_context=file_context)\n",
    "        \n",
    "        # Merge with existing data\n",
    "        players_data = merge_player_stats(players_data, batch_data)\n",
    "        \n",
    "        # Record batch completion\n",
    "        batch_time = perf_tracker.end_batch(len(batch_df))\n",
    "        print(f\"Processed batch in {batch_time:.2f} seconds\")\n",
    "        print(f\"Current player count: {len(players_data):,}\")\n",
    "        \n",
    "        # Log performance metrics\n",
    "        perf_tracker.log_progress(force=True)\n",
    "        \n",
    "        # Save progress periodically\n",
    "        batch_num += 1\n",
    "        if batch_num % config.save_interval == 0:\n",
    "            save_progress(players_data, batch_num, config, perf_tracker)\n",
    "    \n",
    "    # Save final progress\n",
    "    save_progress(players_data, batch_num, config, perf_tracker)\n",
    "    \n",
    "    # Mark the file as processed in the registry\n",
    "    try:\n",
    "        registry.mark_file_processed(config.parquet_path)\n",
    "        print(f\"Marked file {Path(config.parquet_path).name} as processed in the registry\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not update file registry: {e}\")\n",
    "    \n",
    "    # Print final performance summary\n",
    "    summary = perf_tracker.get_summary()\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(f\"Total games processed: {summary['total_games']:,}\")\n",
    "    print(f\"Total processing time: {summary['total_time_sec']:.2f} seconds\")\n",
    "    print(f\"Overall processing rate: {summary['overall_rate_games_per_sec']:.2f} games/second\")\n",
    "    print(f\"Average batch processing time: {summary['avg_batch_time_sec']:.2f} seconds\")\n",
    "    \n",
    "    # Add filtering stats to the final summary\n",
    "    print(\"\\nFiltering Statistics:\")\n",
    "    print(f\"Accepted games: {summary['accepted_games']:,}\")\n",
    "    print(f\"Filtered out games: {summary['filtered_games']:,}\")\n",
    "    print(f\"Acceptance rate: {summary['acceptance_rate_percent']}%\")\n",
    "    \n",
    "    return players_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ecc93",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def process_multiple_parquet_files(file_paths: List[str], base_config: ProcessingConfig = None, log_frequency: int = 5000) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process multiple parquet files of raw game data, avoiding duplicates using our registry of previously\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of paths to parquet files to process\n",
    "        base_config: Base configuration to use as a template for each file\n",
    "        log_frequency: How often to log progress within a batch\n",
    "        \n",
    "    Returns:\n",
    "        Combined player statistics from all processed files\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"No files provided for processing\")\n",
    "        return {}\n",
    "    \n",
    "    # Create a default config if none provided\n",
    "    if base_config is None:\n",
    "        base_config = ProcessingConfig(\n",
    "            parquet_path=\"\",  # Will be set for each file\n",
    "            batch_size=100_000,\n",
    "            save_interval=1,\n",
    "            save_dir=\"../data/processed\"\n",
    "        )\n",
    "    \n",
    "    # Initialize file registry\n",
    "    try:\n",
    "        registry = FileRegistry()\n",
    "    except NameError:\n",
    "        print(\"FileRegistry not available, skipping duplicate detection\")\n",
    "        registry = None\n",
    "    \n",
    "    # Filter out already processed files\n",
    "    new_files = []\n",
    "    for file_path in file_paths:\n",
    "        if registry and registry.is_file_processed(file_path):\n",
    "            print(f\"Skipping already processed file: {Path(file_path).name}\")\n",
    "            try:\n",
    "                registry.mark_file_skipped(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not mark file as skipped: {e}\")\n",
    "            continue\n",
    "        new_files.append(file_path)\n",
    "    \n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Found {len(new_files)} new files to process out of {len(file_paths)} total files.\")\n",
    "    \n",
    "    # Estimate total rows for ETA calculation\n",
    "    # For simplicity, we'll get the row count of the first file and multiply\n",
    "    total_rows_estimate = 0\n",
    "    avg_rows_per_file = 0\n",
    "    if new_files:\n",
    "        try:\n",
    "            con = duckdb.connect()\n",
    "            first_file_rows = con.execute(f\"SELECT COUNT(*) FROM '{new_files[0]}'\").fetchone()[0]\n",
    "            avg_rows_per_file = first_file_rows\n",
    "            total_rows_estimate = avg_rows_per_file * len(new_files)\n",
    "            con.close()\n",
    "            print(f\"Estimating total of {total_rows_estimate:,} rows across {len(new_files)} files for ETA.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not estimate total rows for ETA: {e}\")\n",
    "\n",
    "    # Process each file\n",
    "    all_players_data, _ = load_progress(base_config)\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for i, file_path in enumerate(new_files):\n",
    "        print(f\"\\nProcessing file {i+1}/{len(new_files)}: {Path(file_path).name}\")\n",
    "        \n",
    "        # Create a config for this specific file\n",
    "        file_config = ProcessingConfig(\n",
    "            parquet_path=file_path,\n",
    "            batch_size=base_config.batch_size,\n",
    "            save_interval=base_config.save_interval,\n",
    "            save_dir=base_config.save_dir,\n",
    "            min_player_rating=base_config.min_player_rating,\n",
    "            max_elo_difference_between_players=base_config.max_elo_difference_between_players,\n",
    "            allowed_time_controls=base_config.allowed_time_controls\n",
    "        )\n",
    "        \n",
    "        file_context = {\n",
    "            \"current_file_num\": i + 1,\n",
    "            \"total_files\": len(new_files),\n",
    "            \"total_rows_estimate\": total_rows_estimate,\n",
    "            \"avg_rows_per_file\": avg_rows_per_file,\n",
    "            \"total_start_time\": total_start_time,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Process the file, passing in the accumulated data\n",
    "            all_players_data = process_parquet_file(\n",
    "                file_config, \n",
    "                log_frequency=log_frequency,\n",
    "                existing_players_data=all_players_data,\n",
    "                file_context=file_context\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    return all_players_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a7adc",
   "metadata": {},
   "source": [
    "## Run Processing\n",
    "\n",
    "Now let's run the processing with our multi-file processing utility. This will allow us to process multiple parquet files at once while handling duplicate detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported FileRegistry\n",
      "Found 22 parquet files in /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw\n",
      "Loaded registry with 12 processed files\n",
      "Skipping already processed file: train-00005-of-00066.parquet\n",
      "Skipping already processed file: train-00001-of-00066.parquet\n",
      "Skipping already processed file: train-00010-of-00066.parquet\n",
      "Skipping already processed file: train-00004-of-00066.parquet\n",
      "Skipping already processed file: train-00000-of-00066.parquet\n",
      "Skipping already processed file: train-00007-of-00066.parquet\n",
      "Skipping already processed file: train-00008-of-00066.parquet\n",
      "Skipping already processed file: train-00000-of-00072.parquet\n",
      "Skipping already processed file: train-00003-of-00066.parquet\n",
      "Skipping already processed file: train-00006-of-00066.parquet\n",
      "Skipping already processed file: train-00009-of-00066.parquet\n",
      "Skipping already processed file: train-00002-of-00066.parquet\n",
      "Will process 10 new files out of 22 total files.\n",
      "Using automatically determined batch size: 7,540,000\n",
      "\n",
      "Will process 10 files with the following parameters:\n",
      "- Batch size: 7,540,000\n",
      "- Min player rating: 1200\n",
      "- Max rating difference: 100\n",
      "- Allowed time controls: Blitz, Classical, Rapid\n",
      "- Save directory: ../data/processed\n",
      "\n",
      "Processing Configuration:\n",
      "  files_to_process: 10 files\n",
      "    - train-00011-of-00066.parquet\n",
      "    - train-00020-of-00066.parquet\n",
      "    - train-00015-of-00066.parquet\n",
      "    - ... and 7 more files\n",
      "  batch_size: 7540000\n",
      "  min_player_rating: 1200\n",
      "  max_elo_difference: 100\n",
      "  allowed_time_controls: {'Blitz', 'Classical', 'Rapid'}\n",
      "  save_dir: ../data/processed\n",
      "  directory: /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw\n",
      "  files_found: 22\n",
      "  files_skipped: 12\n",
      "Starting processing of 10 files...\n",
      "Loaded registry with 12 processed files\n",
      "Found 10 new files to process out of 10 total files.\n",
      "Estimating total of 14,104,970 rows across 10 files for ETA.\n",
      "Loaded player data with 791955 players.\n",
      "Resuming from batch 1.\n",
      "\n",
      "Processing file 1/10: train-00011-of-00066.parquet\n",
      "Loaded registry with 12 processed files\n",
      "Loaded player data with 791955 players.\n",
      "Resuming from batch 1.\n",
      "Started with 791955 players from previous files. Resetting batch counter for new file.\n",
      "Counting total rows in parquet file...\n",
      "Total rows in parquet file: 1,410,497\n",
      "Will process in 1 batches of size 7,540,000 (resuming from batch 0)\n",
      "\n",
      "Processing batch 1/1 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "# Import our multi-file processing utility\n",
    "from notebooks.utils.file_processing.process_multiple_raw_files import process_multiple_files\n",
    "\n",
    "# Get the processing configuration using the utility\n",
    "# This will show a directory picker dialog and find all parquet files\n",
    "processing_config = process_multiple_files(\n",
    "    # Let the user select a directory via dialog\n",
    "    directory=None,  \n",
    "    # Determine batch size automatically based on memory\n",
    "    batch_size=None,  \n",
    "    # Use the same filtering parameters as before\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference=100,\n",
    "    allowed_time_controls={\"Blitz\", \"Rapid\", \"Classical\"},\n",
    "    save_dir=\"../data/processed\"\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing Configuration:\")\n",
    "for key, value in processing_config.items():\n",
    "    if key != \"files_to_process\":  # Don't print the full file paths\n",
    "        print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {len(value)} files\")\n",
    "        # Print first 3 file names as examples\n",
    "        for i, file_path in enumerate(value[:3]):\n",
    "            print(f\"    - {Path(file_path).name}\")\n",
    "        if len(value) > 3:\n",
    "            print(f\"    - ... and {len(value) - 3} more files\")\n",
    "\n",
    "files_to_process = processing_config.get(\"files_to_process\", [])\n",
    "\n",
    "if not files_to_process:\n",
    "    print(\"No new files to process. Exiting.\")\n",
    "else:\n",
    "    print(f\"Starting processing of {len(files_to_process)} files...\")\n",
    "    \n",
    "    base_config = ProcessingConfig(\n",
    "        parquet_path=\"\",  # Will be set for each file\n",
    "        batch_size=processing_config[\"batch_size\"],\n",
    "        save_interval=1,  # Save after each batch\n",
    "        save_dir=processing_config[\"save_dir\"],\n",
    "        min_player_rating=processing_config[\"min_player_rating\"],\n",
    "        max_elo_difference_between_players=processing_config[\"max_elo_difference\"],\n",
    "        allowed_time_controls=processing_config[\"allowed_time_controls\"]\n",
    "    )\n",
    "    \n",
    "    all_players_data = process_multiple_parquet_files(\n",
    "        files_to_process,\n",
    "        base_config=base_config,\n",
    "        log_frequency=5000\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal combined data statistics:\")\n",
    "    print(f\"Total number of players: {len(all_players_data):,}\")\n",
    "    \n",
    "    # Save the final merged data separately\n",
    "    final_save_path = Path(processing_config[\"save_dir\"]) / \"all_players_stats_combined.pkl\"\n",
    "    with open(final_save_path, 'wb') as f:\n",
    "        pickle.dump(all_players_data, f)\n",
    "    \n",
    "    print(f\"Saved final merged data to: {final_save_path}\")\n",
    "    \n",
    "    # Show an example player from the combined data\n",
    "    if all_players_data:\n",
    "        import random\n",
    "        sample_player = random.choice(list(all_players_data.keys()))\n",
    "        print(f\"\\nSample stats for player from combined data: {sample_player}\")\n",
    "        print(f\"Rating: {all_players_data[sample_player]['rating']}\")\n",
    "        print(f\"Total games: {all_players_data[sample_player]['num_games_total']}\")\n",
    "        \n",
    "        print(\"\\nTop White openings:\")\n",
    "        white_openings = all_players_data[sample_player]['white_games']\n",
    "        if white_openings:\n",
    "            # Sort by number of games\n",
    "            sorted_openings = sorted(\n",
    "                white_openings.items(), \n",
    "                key=lambda x: x[1]['results']['num_games'], \n",
    "                reverse=True\n",
    "            )\n",
    "            for eco, data in sorted_openings[:5]: \n",
    "                print(f\"  {eco} - {data['opening_name']}: \" +\n",
    "                      f\"{data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "            if len(white_openings) > 5:\n",
    "                print(f\"  ... and {len(white_openings) - 5} more openings\")\n",
    "        else:\n",
    "            print(\"  No white openings\")\n",
    "\n",
    "        print(\"\\nTop Black openings:\")\n",
    "        black_openings = all_players_data[sample_player]['black_games']\n",
    "        if black_openings:\n",
    "            # Sort by number of games\n",
    "            sorted_openings = sorted(\n",
    "                black_openings.items(), \n",
    "                key=lambda x: x[1]['results']['num_games'], \n",
    "                reverse=True\n",
    "            )\n",
    "            for eco, data in sorted_openings[:5]: \n",
    "                print(f\"  {eco} - {data['opening_name']}: \" + \n",
    "                      f\"{data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "            if len(black_openings) > 5:\n",
    "                print(f\"  ... and {len(black_openings) - 5} more openings\")\n",
    "        else:\n",
    "            print(\"  No black openings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d4ca7",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "To process multiple parquet files:\n",
    "\n",
    "1. Run the cell above that calls `process_multiple_files()`\n",
    "2. A directory picker dialog will appear - select the folder containing your parquet files\n",
    "3. The utility will identify new files (not previously processed) and process them one by one\n",
    "4. All player statistics will be merged into a combined dataset\n",
    "\n",
    "You can keep adding new parquet files to the same directory, and when you run this notebook again it will only process the new ones. This is perfect for incrementally adding to your dataset over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
