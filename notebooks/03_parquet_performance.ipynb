{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41acdd1f",
   "metadata": {},
   "source": [
    "# Parquet Data Performance Testing\n",
    "\n",
    "This notebook measures performance metrics for processing chess game data from parquet files. The main objectives are:\n",
    "\n",
    "1. Efficiently load data from parquet files using DuckDB\n",
    "2. Process the data to gather player statistics and opening preferences\n",
    "3. Log detailed performance metrics (games/sec, processing time, etc.)\n",
    "4. Compare with the performance of processing PGN files directly\n",
    "\n",
    "Update: This notebook seems to do a great job of parsing games quickly. Now we are moving on to better filtering.\n",
    "\n",
    "NOTE If you want to quickly adjust games filtering or processing parameters, ProcessingConfig is probably your best bet. I put as many relevant parameters in there as I could think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine paths to ensure imports work correctly\n",
    "current_file = Path.cwd()\n",
    "project_root = current_file.parent  # Move up to the project root\n",
    "\n",
    "# Add both to path to ensure imports work regardless of structure\n",
    "if str(current_file) not in sys.path:\n",
    "    sys.path.append(str(current_file))\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Now try the import with the explicit path\n",
    "from notebooks.utils.file_processing.types_and_classes import (  # noqa: E402\n",
    "    PlayerStats,\n",
    "    ProcessingConfig,\n",
    "    PerformanceTracker,\n",
    ")\n",
    "\n",
    "from notebooks.utils.file_processing.save_and_load_progress import (  # noqa: E402\n",
    "    save_progress,\n",
    "    load_progress,\n",
    ")\n",
    "\n",
    "# Import necessary libraries\n",
    "from typing import Dict, Optional, List  # noqa: E402\n",
    "import duckdb  # noqa: E402\n",
    "import time  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "import psutil  # noqa: E402\n",
    "import pickle  # noqa: E402\n",
    "import sys  # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f07fce",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "We'll define a class to track performance metrics during processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54f750",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These functions will help us process the data and manage player statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89ed179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_game(row: pd.Series, config: ProcessingConfig) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a game meets the filtering criteria. This ensures only relevant, informative games are processed.\n",
    "    \n",
    "    Args:\n",
    "        row: A row from the DataFrame representing a game\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        True if the game passes our filters, False otherwise\n",
    "    \"\"\"\n",
    "    # Check player ratings\n",
    "    if (row['WhiteElo'] < config.min_player_rating or \n",
    "        row['BlackElo'] < config.min_player_rating):\n",
    "        return False\n",
    "\n",
    "    # Check rating difference\n",
    "    if abs(row['WhiteElo'] - row['BlackElo']) > config.max_elo_difference_between_players:\n",
    "        return False\n",
    "\n",
    "    # \"Event\" column on game contains time control, they're titled like \"Rated Blitz Games\"\n",
    "    # Check that the time control is in the allowed time controls (case insensitive)\n",
    "    event_lower = row[\"Event\"].lower()\n",
    "    if not any(tc.lower() in event_lower for tc in config.allowed_time_controls):\n",
    "        return False\n",
    "\n",
    "    # Check for valid result\n",
    "    # If it's something weird that's not a win loss or draw, toss it out\n",
    "    if row['Result'] not in {\"1-0\", \"0-1\", \"1/2-1/2\"}:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89be59",
   "metadata": {},
   "source": [
    "## Main Processing Function\n",
    "\n",
    "Now let's implement the main function that processes the parquet file in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686b9ca",
   "metadata": {},
   "source": [
    "## Multi-File Processing with Duplicate Detection\n",
    "\n",
    "Before defining our main processing function, let's implement a system to handle duplicate file detection. This is essential when processing multiple parquet files that might have similar names but come from different months or batches. Our approach uses metadata fingerprinting to uniquely identify each file.\n",
    "\n",
    "This uses the dupe-check utils defined in our utils folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a66d3f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported FileRegistry\n"
     ]
    }
   ],
   "source": [
    "# Add the parent directory to the Python path to enable imports\n",
    "notebooks_dir = Path.cwd().parent\n",
    "if str(notebooks_dir) not in sys.path:\n",
    "    sys.path.append(str(notebooks_dir))\n",
    "\n",
    "# Try to import our custom file registry utility\n",
    "try:\n",
    "    from notebooks.utils.file_processing.raw_data_file_dupe_checks import FileRegistry\n",
    "    print(\"Successfully imported FileRegistry\")\n",
    "except ImportError:\n",
    "    print(\"Could not import FileRegistry - file duplicate checks will not be available\")\n",
    "    \n",
    "    # Define a simple FileRegistry class if the import fails\n",
    "    class FileRegistry:\n",
    "        \"\"\"Simple FileRegistry implementation for duplicate detection.\"\"\"\n",
    "        def __init__(self):\n",
    "            self.registry_path = Path(notebooks_dir) / \"data/processed/file_registry.json\"\n",
    "            self.processed_files = set()\n",
    "            self._load_registry()\n",
    "            \n",
    "        def _load_registry(self):\n",
    "            import json\n",
    "            if self.registry_path.exists():\n",
    "                try:\n",
    "                    with open(self.registry_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        self.processed_files = set(data.get('processed_files', []))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load registry: {e}\")\n",
    "        \n",
    "        def _save_registry(self):\n",
    "            import json\n",
    "            try:\n",
    "                self.registry_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(self.registry_path, 'w') as f:\n",
    "                    json.dump({'processed_files': list(self.processed_files)}, f)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not save registry: {e}\")\n",
    "                \n",
    "        def is_file_processed(self, file_path: str) -> bool:\n",
    "            return str(file_path) in self.processed_files\n",
    "            \n",
    "        def mark_file_processed(self, file_path: str) -> None:\n",
    "            self.processed_files.add(str(file_path))\n",
    "            self._save_registry()\n",
    "            \n",
    "        def mark_file_skipped(self, file_path: str) -> None:\n",
    "            self.processed_files.add(str(file_path))\n",
    "            self._save_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb402e",
   "metadata": {},
   "source": [
    "## System Information\n",
    "\n",
    "Let's check the system's hardware resources to optimize our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c42a4729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Information:\n",
      "  cpu_count_physical: 6\n",
      "  cpu_count_logical: 12\n",
      "  memory_total_gb: 32.0\n",
      "  memory_available_gb: 20.33\n",
      "\n",
      "Recommended batch size based on memory: 6,400,000\n"
     ]
    }
   ],
   "source": [
    "# Install psutil if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"Installing psutil package...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "    import psutil\n",
    "    print(\"psutil installed successfully\")\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Get information about the system's hardware resources.\"\"\"\n",
    "    info = {\n",
    "        \"cpu_count_physical\": psutil.cpu_count(logical=False),\n",
    "        \"cpu_count_logical\": psutil.cpu_count(logical=True),\n",
    "        \"memory_total_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "        \"memory_available_gb\": round(psutil.virtual_memory().available / (1024**3), 2)\n",
    "    }\n",
    "    return info\n",
    "\n",
    "# Get system information\n",
    "system_info = get_system_info()\n",
    "print(\"System Information:\")\n",
    "for key, value in system_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Calculate optimal batch size based on available memory\n",
    "# Assuming each row needs about 1KB of memory\n",
    "available_memory_gb = system_info[\"memory_available_gb\"]\n",
    "memory_for_batch_gb = available_memory_gb * 0.3  # Use 30% of available memory\n",
    "optimal_batch_size = int(memory_for_batch_gb * 1024**3 / 1024)  # 1KB per row\n",
    "\n",
    "# Round to nearest 10,000\n",
    "optimal_batch_size = max(10_000, round(optimal_batch_size / 10_000) * 10_000)\n",
    "\n",
    "print(f\"\\nRecommended batch size based on memory: {optimal_batch_size:,}\")\n",
    "\n",
    "def process_parquet_file(config: ProcessingConfig, \n",
    "                         players_data: Dict[str, PlayerStats],\n",
    "                         log_frequency: int = 5000, \n",
    "                         file_context: Optional[Dict] = None) -> None:\n",
    "    \"\"\"\n",
    "    Process a single parquet file in batches, updating a shared players_data dictionary.\n",
    "    This function orchestrates the processing of one file. It uses a file registry\n",
    "    to skip already processed files and manages batching. For each batch, it calls\n",
    "    process_batch to perform the actual game data processing.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration for this specific file.\n",
    "        players_data: The shared dictionary of player statistics to update.\n",
    "        log_frequency: How often to log progress within a batch.\n",
    "        file_context: Dictionary with context for multi-file processing.\n",
    "    \"\"\"\n",
    "    # Check if file has already been processed using FileRegistry\n",
    "    try:\n",
    "        registry = FileRegistry()\n",
    "        if registry.is_file_processed(config.parquet_path):\n",
    "            print(f\"Skipping already processed file: {Path(config.parquet_path).name}\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not check file registry: {e}\")\n",
    "    \n",
    "    # Initialize DuckDB connection\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    # This prevents the progress from one file affecting the next.\n",
    "    progress_path = Path(config.save_dir) / \"processing_progress_parquet.json\"\n",
    "    if progress_path.exists():\n",
    "        try:\n",
    "            progress_path.unlink()\n",
    "            print(f\"Reset progress file for new run: {progress_path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error removing progress file: {e}\")\n",
    "    \n",
    "    # When processing a new file, we should check if there's partial progress for THIS specific file.\n",
    "    # We load the progress file to get the last batch number, but we will use the `players_data`\n",
    "    # dictionary that was passed in, which contains the combined data from all previous files.\n",
    "    _, start_batch = load_progress(config)\n",
    "\n",
    "    # Initialize performance tracker\n",
    "    perf_tracker = PerformanceTracker()\n",
    "    \n",
    "    # Get total number of rows\n",
    "    print(\"Counting total rows in parquet file...\")\n",
    "    total_rows = con.execute(\n",
    "        f\"SELECT COUNT(*) FROM '{config.parquet_path}'\"\n",
    "    ).fetchone()[0]\n",
    "    print(f\"Total rows in parquet file: {total_rows:,}\")\n",
    "    \n",
    "    # If resuming, check if we're already done\n",
    "    if start_batch * config.batch_size >= total_rows:\n",
    "        print(f\"Resuming from batch {start_batch}, which is after the end of the file. Skipping.\")\n",
    "        # Mark the file as processed since we are technically done with it\n",
    "        try:\n",
    "            registry.mark_file_processed(config.parquet_path)\n",
    "            print(f\"Marked file {Path(config.parquet_path).name} as processed in the registry\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not update file registry: {e}\")\n",
    "        return players_data\n",
    "\n",
    "    # Calculate number of batches\n",
    "    total_batches = (total_rows + config.batch_size - 1) // config.batch_size\n",
    "    print(f\"Will process in {total_batches} batches of size {config.batch_size:,} (resuming from batch {start_batch})\")\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_num = start_batch\n",
    "    \n",
    "    while True:\n",
    "        # Calculate offset for the current batch\n",
    "        offset = batch_num * config.batch_size\n",
    "        \n",
    "        # Check if we've processed all rows\n",
    "        if offset >= total_rows:\n",
    "            print(\"Processed all rows. Finishing up.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nProcessing batch {batch_num + 1}/{total_batches} (offset {offset:,})\")\n",
    "        perf_tracker.start_batch()\n",
    "        \n",
    "        # Fetch a batch of data\n",
    "        batch_query = f\"\"\"\n",
    "        SELECT \n",
    "            Event, White, Black, Result, \n",
    "            WhiteTitle, BlackTitle, WhiteElo, BlackElo, \n",
    "            WhiteRatingDiff, BlackRatingDiff, ECO, Opening,\n",
    "            Termination, TimeControl\n",
    "        FROM '{config.parquet_path}'\n",
    "        LIMIT {config.batch_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query and convert to DataFrame\n",
    "        batch_df = con.execute(batch_query).df()\n",
    "        \n",
    "        if batch_df.empty:\n",
    "            print(\"Loaded an empty batch. This might mean we are past the end of the file.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Loaded batch with {len(batch_df):,} rows\")\n",
    "        \n",
    "        # Process the batch, updating players_data in-place\n",
    "        process_batch(\n",
    "            batch_df, \n",
    "            players_data, \n",
    "            config, \n",
    "            log_frequency=log_frequency, \n",
    "            perf_tracker=perf_tracker, \n",
    "            file_context=file_context\n",
    "        )\n",
    "        \n",
    "        # Record batch completion\n",
    "        batch_time = perf_tracker.end_batch(len(batch_df))\n",
    "        print(f\"Processed batch in {batch_time:.2f} seconds\")\n",
    "        print(f\"Current player count: {len(players_data):,}\")\n",
    "        \n",
    "        # Log performance metrics\n",
    "        perf_tracker.log_progress(force=True)\n",
    "        \n",
    "        # Save progress periodically\n",
    "        batch_num += 1\n",
    "        if batch_num % config.save_interval == 0:\n",
    "            save_progress(players_data, batch_num, config, perf_tracker)\n",
    "    \n",
    "    # Save final progress\n",
    "    save_progress(players_data, batch_num, config, perf_tracker)\n",
    "    \n",
    "    # Mark the file as processed in the registry\n",
    "    try:\n",
    "        registry.mark_file_processed(config.parquet_path)\n",
    "        print(f\"Marked file {Path(config.parquet_path).name} as processed in the registry\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not update file registry: {e}\")\n",
    "    \n",
    "    # Print final performance summary\n",
    "    summary = perf_tracker.get_summary()\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(f\"Total games processed: {summary['total_games']:,}\")\n",
    "    print(f\"Total processing time: {summary['total_time_sec']:.2f} seconds\")\n",
    "    print(f\"Overall processing rate: {summary['overall_rate_games_per_sec']:.2f} games/second\")\n",
    "    print(f\"Average batch processing time: {summary['avg_batch_time_sec']:.2f} seconds\")\n",
    "    \n",
    "    # Add filtering stats to the final summary\n",
    "    print(\"\\nFiltering Statistics:\")\n",
    "    print(f\"Accepted games: {summary['accepted_games']:,}\")\n",
    "    print(f\"Filtered out games: {summary['filtered_games']:,}\")\n",
    "    print(f\"Acceptance rate: {summary['acceptance_rate_percent']}%\")\n",
    "    \n",
    "    return players_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe3ecc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_parquet_files(file_paths: List[str], base_config: ProcessingConfig = None, log_frequency: int = 5000) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process multiple parquet files of raw game data, avoiding duplicates.\n",
    "    This is the main entry point for processing a collection of files. It initializes\n",
    "    a single 'all_players_data' dictionary that is shared and updated across all\n",
    "    file processing jobs. It handles duplicate file detection and orchestrates\n",
    "    the processing of each file by calling 'process_parquet_file'.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of paths to parquet files to process.\n",
    "        base_config: Base configuration to use as a template for each file.\n",
    "        log_frequency: How often to log progress within a batch.\n",
    "        \n",
    "    Returns:\n",
    "        The final, combined player statistics from all processed files.\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"No files provided for processing\")\n",
    "        return {}\n",
    "    \n",
    "    # Create a default config if none provided\n",
    "    if base_config is None:\n",
    "        base_config = ProcessingConfig(\n",
    "            parquet_path=\"\",  # Will be set for each file\n",
    "            batch_size=100_000,\n",
    "            save_interval=1,\n",
    "            save_dir=\"../data/processed\"\n",
    "        )\n",
    "    \n",
    "    # Initialize file registry\n",
    "    try:\n",
    "        registry = FileRegistry()\n",
    "    except NameError:\n",
    "        print(\"FileRegistry not available, skipping duplicate detection\")\n",
    "        registry = None\n",
    "    \n",
    "    # Filter out already processed files\n",
    "    new_files = []\n",
    "    for file_path in file_paths:\n",
    "        if registry and registry.is_file_processed(file_path):\n",
    "            print(f\"Skipping already processed file: {Path(file_path).name}\")\n",
    "            try:\n",
    "                registry.mark_file_skipped(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not mark file as skipped: {e}\")\n",
    "            continue\n",
    "        new_files.append(file_path)\n",
    "    \n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Found {len(new_files)} new files to process out of {len(file_paths)} total files.\")\n",
    "    \n",
    "    # Estimate total rows for ETA calculation\n",
    "    # For simplicity, we'll get the row count of the first file and multiply\n",
    "    total_rows_estimate = 0\n",
    "    avg_rows_per_file = 0\n",
    "    if new_files:\n",
    "        try:\n",
    "            con = duckdb.connect()\n",
    "            first_file_rows = con.execute(f\"SELECT COUNT(*) FROM '{new_files[0]}'\").fetchone()[0]\n",
    "            avg_rows_per_file = first_file_rows\n",
    "            total_rows_estimate = avg_rows_per_file * len(new_files)\n",
    "            con.close()\n",
    "            print(f\"Estimating total of {total_rows_estimate:,} rows across {len(new_files)} files for ETA.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not estimate total rows for ETA: {e}\")\n",
    "\n",
    "    # Load all existing player data and progress once at the beginning.\n",
    "    # This single dictionary will be passed to and updated by all subsequent function calls.\n",
    "    all_players_data, _ = load_progress(base_config)\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for i, file_path in enumerate(new_files):\n",
    "        print(f\"\\nProcessing file {i+1}/{len(new_files)}: {Path(file_path).name}\")\n",
    "        \n",
    "        # Create a config for this specific file\n",
    "        file_config = ProcessingConfig(\n",
    "            parquet_path=file_path,\n",
    "            batch_size=base_config.batch_size,\n",
    "            save_interval=base_config.save_interval,\n",
    "            save_dir=base_config.save_dir,\n",
    "            min_player_rating=base_config.min_player_rating,\n",
    "            max_elo_difference_between_players=base_config.max_elo_difference_between_players,\n",
    "            allowed_time_controls=base_config.allowed_time_controls\n",
    "        )\n",
    "        \n",
    "        file_context = {\n",
    "            \"current_file_num\": i + 1,\n",
    "            \"total_files\": len(new_files),\n",
    "            \"total_rows_estimate\": total_rows_estimate,\n",
    "            \"avg_rows_per_file\": avg_rows_per_file,\n",
    "            \"total_start_time\": total_start_time,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Process the file, passing in the single, shared data dictionary\n",
    "            # to be updated in-place.\n",
    "            process_parquet_file(\n",
    "                file_config, \n",
    "                all_players_data,\n",
    "                log_frequency=log_frequency,\n",
    "                file_context=file_context\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    return all_players_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a7adc",
   "metadata": {},
   "source": [
    "## Run Processing\n",
    "\n",
    "Now let's run the processing with our multi-file processing utility. This will allow us to process multiple parquet files at once while handling duplicate detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our multi-file processing utility\n",
    "from notebooks.utils.file_processing.process_multiple_raw_files import process_multiple_files\n",
    "\n",
    "# Get the processing configuration using the utility\n",
    "# This will show a directory picker dialog and find all parquet files\n",
    "processing_config = process_multiple_files(\n",
    "    # Let the user select a directory via dialog\n",
    "    directory=None,  \n",
    "    # Determine batch size automatically based on memory\n",
    "    batch_size=None,  \n",
    "    # Use the same filtering parameters as before\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference=100,\n",
    "    allowed_time_controls={\"Blitz\", \"Rapid\", \"Classical\"},\n",
    "    save_dir=\"../data/processed\"\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing Configuration:\")\n",
    "for key, value in processing_config.items():\n",
    "    if key != \"files_to_process\":  # Don't print the full file paths\n",
    "        print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {len(value)} files\")\n",
    "        # Print first 3 file names as examples\n",
    "        for i, file_path in enumerate(value[:3]):\n",
    "            print(f\"    - {Path(file_path).name}\")\n",
    "        if len(value) > 3:\n",
    "            print(f\"    - ... and {len(value) - 3} more files\")\n",
    "\n",
    "files_to_process = processing_config.get(\"files_to_process\", [])\n",
    "\n",
    "if not files_to_process:\n",
    "    print(\"No new files to process. Exiting.\")\n",
    "else:\n",
    "    print(f\"Starting processing of {len(files_to_process)} files...\")\n",
    "    \n",
    "    base_config = ProcessingConfig(\n",
    "        parquet_path=\"\",  # Will be set for each file\n",
    "        batch_size=processing_config[\"batch_size\"],\n",
    "        save_interval=1,  # Save after each batch\n",
    "        save_dir=processing_config[\"save_dir\"],\n",
    "        min_player_rating=processing_config[\"min_player_rating\"],\n",
    "        max_elo_difference_between_players=processing_config[\"max_elo_difference\"],\n",
    "        allowed_time_controls=processing_config[\"allowed_time_controls\"]\n",
    "    )\n",
    "    \n",
    "    all_players_data = process_multiple_parquet_files(\n",
    "        files_to_process,\n",
    "        base_config=base_config,\n",
    "        log_frequency=5000\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFinal combined data statistics:\")\n",
    "    print(f\"Total number of players: {len(all_players_data):,}\")\n",
    "    \n",
    "    # Save the final merged data separately\n",
    "    final_save_path = Path(processing_config[\"save_dir\"]) / \"all_players_stats_combined.pkl\"\n",
    "    with open(final_save_path, 'wb') as f:\n",
    "        pickle.dump(all_players_data, f)\n",
    "    \n",
    "    print(f\"Saved final merged data to: {final_save_path}\")\n",
    "    \n",
    "    # Show an example player from the combined data\n",
    "    if all_players_data:\n",
    "        import random\n",
    "        sample_player = random.choice(list(all_players_data.keys()))\n",
    "        print(f\"\\nSample stats for player from combined data: {sample_player}\")\n",
    "        print(f\"Rating: {all_players_data[sample_player]['rating']}\")\n",
    "        print(f\"Total games: {all_players_data[sample_player]['num_games_total']}\")\n",
    "        \n",
    "        print(\"\\nTop White openings:\")\n",
    "        white_openings = all_players_data[sample_player]['white_games']\n",
    "        if white_openings:\n",
    "            # Sort by number of games\n",
    "            sorted_openings = sorted(\n",
    "                white_openings.items(), \n",
    "                key=lambda x: x[1]['results']['num_games'], \n",
    "                reverse=True\n",
    "            )\n",
    "            for eco, data in sorted_openings[:5]: \n",
    "                print(f\"  {eco} - {data['opening_name']}: \" +\n",
    "                      f\"{data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "            if len(white_openings) > 5:\n",
    "                print(f\"  ... and {len(white_openings) - 5} more openings\")\n",
    "        else:\n",
    "            print(\"  No white openings\")\n",
    "\n",
    "        print(\"\\nTop Black openings:\")\n",
    "        black_openings = all_players_data[sample_player]['black_games']\n",
    "        if black_openings:\n",
    "            # Sort by number of games\n",
    "            sorted_openings = sorted(\n",
    "                black_openings.items(), \n",
    "                key=lambda x: x[1]['results']['num_games'], \n",
    "                reverse=True\n",
    "            )\n",
    "            for eco, data in sorted_openings[:5]: \n",
    "                print(f\"  {eco} - {data['opening_name']}: \" + \n",
    "                      f\"{data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "            if len(black_openings) > 5:\n",
    "                print(f\"  ... and {len(black_openings) - 5} more openings\")\n",
    "        else:\n",
    "            print(\"  No black openings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d4ca7",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "To process multiple parquet files:\n",
    "\n",
    "1. Run the cell above that calls `process_multiple_files()`\n",
    "2. A directory picker dialog will appear - select the folder containing your parquet files\n",
    "3. The utility will identify new files (not previously processed) and process them one by one\n",
    "4. All player statistics will be merged into a combined dataset\n",
    "\n",
    "You can keep adding new parquet files to the same directory, and when you run this notebook again it will only process the new ones. This is perfect for incrementally adding to your dataset over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
