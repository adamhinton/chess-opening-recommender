{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41acdd1f",
   "metadata": {},
   "source": [
    "# Parquet Data Performance Testing\n",
    "\n",
    "This notebook measures performance metrics for processing chess game data from parquet files. The main objectives are:\n",
    "\n",
    "1. Efficiently load data from parquet files using DuckDB\n",
    "2. Process the data to gather player statistics and opening preferences\n",
    "3. Log detailed performance metrics (games/sec, processing time, etc.)\n",
    "4. Compare with the performance of processing PGN files directly\n",
    "\n",
    "Update: This notebook seems to do a great job of parsing games quickly. Now we are moving on to better filtering.\n",
    "\n",
    "NOTE If you want to quickly adjust games filtering or processing parameters, ProcessingConfig is probably your best bet. I put as many relevant parameters in there as I could think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7f8c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from typing import Dict, TypedDict, Optional, Union, Literal, Set, List\n",
    "import duckdb\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e0e8f",
   "metadata": {},
   "source": [
    "## Type Definitions\n",
    "\n",
    "Let's start by defining our data structures for strong typing support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8442243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define types for game results\n",
    "GameResult = Literal[\"1-0\", \"0-1\", \"1/2-1/2\", \"*\"]\n",
    "\n",
    "# Define types for our data structures\n",
    "class OpeningResults(TypedDict):\n",
    "    \"\"\"Statistics for a player's results with a particular opening.\"\"\"\n",
    "    opening_name: str\n",
    "    results: Dict[str, Union[int, float]]\n",
    "\n",
    "class PlayerStats(TypedDict):\n",
    "    \"\"\"Statistics for an individual player.\"\"\"\n",
    "    rating: int\n",
    "    white_games: Dict[str, OpeningResults]  # ECO code -> results\n",
    "    black_games: Dict[str, OpeningResults]  # ECO code -> results\n",
    "    num_games_total: int\n",
    "\n",
    "\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for the game processing pipeline.\n",
    "    Contains parameters for filtering games, batch processing, and parallelization.\n",
    "    This is designed to ensure that the processing of raw chess game data yields usable results efficiently.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Computer efficiency and organization stuff\n",
    "        parquet_path: str,\n",
    "        batch_size: int = 100_000,\n",
    "        save_interval: int = 1,\n",
    "        save_dir: str = \"../data/processed\",\n",
    "        # Chess game filtering stuff\n",
    "        # Neither the black or white player can be below this rating\n",
    "        min_player_rating: int = 1200,\n",
    "        # Players can't be more than 100 rating points apart\n",
    "        max_elo_difference_between_players: int = 100,\n",
    "        # Exclude bullet and daily games by default\n",
    "        allowed_time_controls: Optional[Set[str]] = None,\n",
    "        use_parallel: bool = False,  # Disable parallel processing by default\n",
    "        num_processes: int = 1,\n",
    "    ):\n",
    "        # Notes on game filters:\n",
    "        # Didn't exclude unrated games because our dataset contains only rated games.\n",
    "        # Also didn't have to filter out bot games, because only games between two humans are rated --- I think so, at least.\n",
    "        # See here to look at the data I used: https://huggingface.co/datasets/Lichess/standard-chess-games\n",
    "\n",
    "        self.parquet_path = parquet_path\n",
    "        self.batch_size = batch_size\n",
    "        self.save_interval = save_interval\n",
    "        self.save_dir = save_dir\n",
    "        self.min_player_rating = min_player_rating\n",
    "        self.max_elo_difference_between_players = max_elo_difference_between_players\n",
    "\n",
    "        # Default to common time controls if none specified\n",
    "        if allowed_time_controls is None:\n",
    "            self.allowed_time_controls = {\"Blitz\", \"Rapid\", \"Classical\"}\n",
    "        else:\n",
    "            self.allowed_time_controls = allowed_time_controls\n",
    "\n",
    "        self.use_parallel = use_parallel\n",
    "        self.num_processes = num_processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f07fce",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "We'll define a class to track performance metrics during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6c46796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceTracker:\n",
    "    \"\"\"Track and report performance metrics during processing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.last_log_time = self.start_time\n",
    "        self.total_games = 0\n",
    "        self.batch_times = []\n",
    "        self.batch_sizes = []\n",
    "        self.memory_usage = []\n",
    "        \n",
    "        # Tracking for filtered vs. accepted games\n",
    "        self.accepted_games = 0\n",
    "        self.filtered_games = 0\n",
    "    \n",
    "    def start_batch(self):\n",
    "        \"\"\"Mark the start of a new batch.\"\"\"\n",
    "        self.batch_start_time = time.time()\n",
    "    \n",
    "    def end_batch(self, batch_size: int):\n",
    "        \"\"\"Mark the end of a batch and record metrics.\"\"\"\n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - self.batch_start_time\n",
    "        \n",
    "        self.total_games += batch_size\n",
    "        self.batch_times.append(batch_time)\n",
    "        self.batch_sizes.append(batch_size)\n",
    "        \n",
    "        # Record memory usage\n",
    "        mem = psutil.virtual_memory()\n",
    "        self.memory_usage.append({\n",
    "            \"percent\": mem.percent,\n",
    "            \"used_gb\": mem.used / (1024**3),\n",
    "            \"available_gb\": mem.available / (1024**3)\n",
    "        })\n",
    "        \n",
    "        return batch_time\n",
    "    \n",
    "    def log_progress(self, force: bool = False):\n",
    "        \"\"\"Log progress information if enough time has passed or if forced.\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Log if it's been more than 5 seconds since the last log or if forced\n",
    "        if force or (current_time - self.last_log_time) >= 5:\n",
    "            elapsed_total = current_time - self.start_time\n",
    "            games_per_sec = self.total_games / elapsed_total if elapsed_total > 0 else 0\n",
    "            \n",
    "            # Calculate recent performance (last 5 batches or fewer)\n",
    "            recent_batches = min(5, len(self.batch_times))\n",
    "            if recent_batches > 0:\n",
    "                recent_time = sum(self.batch_times[-recent_batches:])\n",
    "                recent_games = sum(self.batch_sizes[-recent_batches:])\n",
    "                recent_rate = recent_games / recent_time if recent_time > 0 else 0\n",
    "                \n",
    "                # Get the latest memory usage\n",
    "                latest_mem = self.memory_usage[-1] if self.memory_usage else {\"percent\": 0, \"used_gb\": 0, \"available_gb\": 0}\n",
    "                \n",
    "                # Calculate acceptance rate\n",
    "                total_processed = self.accepted_games + self.filtered_games\n",
    "                acceptance_rate = (self.accepted_games / total_processed * 100) if total_processed > 0 else 0\n",
    "                \n",
    "                print(f\"Processed {self.total_games:,} games in {elapsed_total:.2f} seconds\")\n",
    "                print(f\"Accepted: {self.accepted_games:,} games, Filtered: {self.filtered_games:,} games (Acceptance rate: {acceptance_rate:.1f}%)\")\n",
    "                print(f\"Overall rate: {games_per_sec:.1f} games/sec\")\n",
    "                print(f\"Recent rate: {recent_rate:.1f} games/sec\")\n",
    "                print(f\"Memory usage: {latest_mem['percent']}% (Used: {latest_mem['used_gb']:.1f}GB, \"\n",
    "                      f\"Available: {latest_mem['available_gb']:.1f}GB)\")\n",
    "                print(\"-\" * 40)\n",
    "            \n",
    "            self.last_log_time = current_time\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get a summary of all performance metrics.\"\"\"\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - self.start_time\n",
    "        \n",
    "        avg_batch_time = sum(self.batch_times) / len(self.batch_times) if self.batch_times else 0\n",
    "        max_batch_time = max(self.batch_times) if self.batch_times else 0\n",
    "        min_batch_time = min(self.batch_times) if self.batch_times else 0\n",
    "        \n",
    "        avg_batch_size = sum(self.batch_sizes) / len(self.batch_sizes) if self.batch_sizes else 0\n",
    "        \n",
    "        overall_rate = self.total_games / total_time if total_time > 0 else 0\n",
    "        \n",
    "        # Calculate filtering stats\n",
    "        total_processed = self.accepted_games + self.filtered_games\n",
    "        acceptance_rate = (self.accepted_games / total_processed * 100) if total_processed > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_games\": self.total_games,\n",
    "            \"total_time_sec\": total_time,\n",
    "            \"avg_batch_time_sec\": avg_batch_time,\n",
    "            \"min_batch_time_sec\": min_batch_time,\n",
    "            \"max_batch_time_sec\": max_batch_time,\n",
    "            \"avg_batch_size\": avg_batch_size,\n",
    "            \"overall_rate_games_per_sec\": overall_rate,\n",
    "            \"memory_usage\": self.memory_usage,\n",
    "            # Add filtering stats\n",
    "            \"accepted_games\": self.accepted_games,\n",
    "            \"filtered_games\": self.filtered_games,\n",
    "            \"acceptance_rate_percent\": round(acceptance_rate, 1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54f750",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These functions will help us process the data and manage player statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70218eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_player_stats(data1: Dict[str, PlayerStats], data2: Dict[str, PlayerStats]) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Merge two player statistics dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        data1: First player statistics dictionary\n",
    "        data2: Second player statistics dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Merged player statistics\n",
    "    \"\"\"\n",
    "    merged_data: Dict[str, PlayerStats] = data1.copy()\n",
    "    \n",
    "    for player, stats in data2.items():\n",
    "        if player not in merged_data:\n",
    "            merged_data[player] = stats\n",
    "        else:\n",
    "            # Update total game count\n",
    "            merged_data[player][\"num_games_total\"] += stats[\"num_games_total\"]\n",
    "            \n",
    "            # Update white games\n",
    "            for eco, opening_data in stats[\"white_games\"].items():\n",
    "                if eco not in merged_data[player][\"white_games\"]:\n",
    "                    merged_data[player][\"white_games\"][eco] = opening_data\n",
    "                else:\n",
    "                    # Update results for this opening\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_games\"] += opening_data[\"results\"][\"num_games\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_wins\"] += opening_data[\"results\"][\"num_wins\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_losses\"] += opening_data[\"results\"][\"num_losses\"]\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"num_draws\"] += opening_data[\"results\"][\"num_draws\"]\n",
    "                    \n",
    "                    # Recalculate score percentage\n",
    "                    wins = merged_data[player][\"white_games\"][eco][\"results\"][\"num_wins\"]\n",
    "                    draws = merged_data[player][\"white_games\"][eco][\"results\"][\"num_draws\"]\n",
    "                    total = merged_data[player][\"white_games\"][eco][\"results\"][\"num_games\"]\n",
    "                    score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "                    merged_data[player][\"white_games\"][eco][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "            \n",
    "            # Update black games\n",
    "            for eco, opening_data in stats[\"black_games\"].items():\n",
    "                if eco not in merged_data[player][\"black_games\"]:\n",
    "                    merged_data[player][\"black_games\"][eco] = opening_data\n",
    "                else:\n",
    "                    # Update results for this opening\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_games\"] += opening_data[\"results\"][\"num_games\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_wins\"] += opening_data[\"results\"][\"num_wins\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_losses\"] += opening_data[\"results\"][\"num_losses\"]\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"num_draws\"] += opening_data[\"results\"][\"num_draws\"]\n",
    "                    \n",
    "                    # Recalculate score percentage\n",
    "                    wins = merged_data[player][\"black_games\"][eco][\"results\"][\"num_wins\"]\n",
    "                    draws = merged_data[player][\"black_games\"][eco][\"results\"][\"num_draws\"]\n",
    "                    total = merged_data[player][\"black_games\"][eco][\"results\"][\"num_games\"]\n",
    "                    score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "                    merged_data[player][\"black_games\"][eco][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def save_progress(players_data: Dict[str, PlayerStats], \n",
    "                  batch_num: int, \n",
    "                  config: ProcessingConfig,\n",
    "                  perf_tracker: Optional[PerformanceTracker] = None) -> None:\n",
    "    \"\"\"\n",
    "    Save current progress to disk.\n",
    "    \n",
    "    Args:\n",
    "        players_data: Current player statistics\n",
    "        batch_num: Current batch number\n",
    "        config: Processing configuration\n",
    "        perf_tracker: Performance tracker object\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    save_dir = Path(config.save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save player data\n",
    "    player_data_path = save_dir / \"player_stats_parquet.pkl\"\n",
    "    \n",
    "    # For large datasets, pickle can be more efficient than JSON\n",
    "    with open(player_data_path, 'wb') as f:\n",
    "        pickle.dump(players_data, f)\n",
    "        \n",
    "    # Save progress information\n",
    "    progress_path = save_dir / \"processing_progress_parquet.json\"\n",
    "    progress_info = {\n",
    "        \"last_batch_processed\": batch_num,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"num_players\": len(players_data),\n",
    "        \"config\": vars(config)\n",
    "    }\n",
    "    \n",
    "    # Add performance metrics if available\n",
    "    if perf_tracker:\n",
    "        progress_info[\"performance\"] = perf_tracker.get_summary()\n",
    "    \n",
    "    with open(progress_path, 'w') as f:\n",
    "        json.dump(progress_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved progress after batch {batch_num}. \" +\n",
    "          f\"Current data includes {len(players_data)} players.\")\n",
    "\n",
    "def load_progress(config: ProcessingConfig) -> tuple[Dict[str, PlayerStats], int]:\n",
    "    \"\"\"\n",
    "    Load previous progress from disk.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (player_data, last_batch_processed)\n",
    "    \"\"\"\n",
    "    player_data_path = Path(config.save_dir) / \"player_stats_parquet.pkl\"\n",
    "    progress_path = Path(config.save_dir) / \"processing_progress_parquet.json\"\n",
    "    \n",
    "    # Default values if no saved progress\n",
    "    players_data: Dict[str, PlayerStats] = {}\n",
    "    last_batch = 0\n",
    "    \n",
    "    # Load player data if it exists\n",
    "    if player_data_path.exists():\n",
    "        try:\n",
    "            with open(player_data_path, 'rb') as f:\n",
    "                players_data = pickle.load(f)\n",
    "            print(f\"Loaded player data with {len(players_data)} players.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading player data: {e}\")\n",
    "            players_data = {}\n",
    "    \n",
    "    # Load progress info if it exists\n",
    "    if progress_path.exists():\n",
    "        try:\n",
    "            with open(progress_path, 'r') as f:\n",
    "                progress_info = json.load(f)\n",
    "                last_batch = progress_info.get(\"last_batch_processed\", 0)\n",
    "            print(f\"Resuming from batch {last_batch}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading progress info: {e}\")\n",
    "            last_batch = 0\n",
    "            \n",
    "    return players_data, last_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89ed179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_game(row: pd.Series, config: ProcessingConfig) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a game meets the filtering criteria. This ensures only relevant, informative games are processed.\n",
    "    \n",
    "    Args:\n",
    "        row: A row from the DataFrame representing a game\n",
    "        config: Processing configuration\n",
    "        \n",
    "    Returns:\n",
    "        True if the game passes our filters, False otherwise\n",
    "    \"\"\"\n",
    "    # Check player ratings\n",
    "    if (row['WhiteElo'] < config.min_player_rating or \n",
    "        row['BlackElo'] < config.min_player_rating):\n",
    "        return False\n",
    "\n",
    "    # Check rating difference\n",
    "    if abs(row['WhiteElo'] - row['BlackElo']) > config.max_elo_difference_between_players:\n",
    "        return False\n",
    "\n",
    "    # \"Event\" column on game contains time control, they're titled like \"Rated Blitz Games\"\n",
    "    # Check that the time control is in the allowed time controls (case insensitive)\n",
    "    event_lower = row[\"Event\"].lower()\n",
    "    if not any(tc.lower() in event_lower for tc in config.allowed_time_controls):\n",
    "        return False\n",
    "\n",
    "    # Check for valid result\n",
    "    # If it's something weird that's not a win loss or draw, toss it out\n",
    "    if row['Result'] not in {\"1-0\", \"0-1\", \"1/2-1/2\"}:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d807e9c",
   "metadata": {},
   "source": [
    "## Data Processing Functions\n",
    "\n",
    "Now let's implement the core functions to process the parquet data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19e76ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df: pd.DataFrame, config: ProcessingConfig, log_frequency: int = 5000, perf_tracker: Optional[PerformanceTracker] = None) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a batch of games from a DataFrame and return player statistics.\n",
    "    \n",
    "    Args:\n",
    "        batch_df: DataFrame containing a batch of games\n",
    "        config: Processing configuration\n",
    "        log_frequency: Log progress after processing this many games\n",
    "        perf_tracker: Performance tracker object to update with filtering stats\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping player usernames to their statistics\n",
    "    \"\"\"\n",
    "    players_data: Dict[str, PlayerStats] = {}\n",
    "    start_time = time.time()\n",
    "    total_rows = len(batch_df)\n",
    "    \n",
    "    # Tracking for filtered vs. accepted games in this batch\n",
    "    batch_accepted = 0\n",
    "    batch_filtered = 0\n",
    "    \n",
    "    # Process each game in the batch\n",
    "    for i, (_, game) in enumerate(batch_df.iterrows()):\n",
    "        # Log progress periodically within the batch\n",
    "        if (i + 1) % log_frequency == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "            eta = (total_rows - (i + 1)) / rate if rate > 0 else 0\n",
    "            \n",
    "            # Calculate acceptance rate for this batch so far\n",
    "            processed_so_far = batch_accepted + batch_filtered\n",
    "            acceptance_rate = (batch_accepted / processed_so_far * 100) if processed_so_far > 0 else 0\n",
    "            \n",
    "            print(f\"Progress: {i+1:,}/{total_rows:,} games ({(i+1)/total_rows*100:.1f}%) - \"\n",
    "                  f\"Rate: {rate:.1f} games/sec - ETA: {eta/60:.1f} minutes - \"\n",
    "                  f\"Players: {len(players_data):,}\")\n",
    "            print(f\"Batch filtering: Accepted {batch_accepted:,}, Filtered {batch_filtered:,} (Acceptance rate: {acceptance_rate:.1f}%)\")\n",
    "        \n",
    "        # Filter out invalid games\n",
    "        if not is_valid_game(game, config):\n",
    "            batch_filtered += 1\n",
    "            if perf_tracker:\n",
    "                perf_tracker.filtered_games += 1\n",
    "            continue\n",
    "        \n",
    "        # Mark as accepted\n",
    "        batch_accepted += 1\n",
    "        if perf_tracker:\n",
    "            perf_tracker.accepted_games += 1\n",
    "\n",
    "        # Extract relevant fields\n",
    "        white_player = game['White']\n",
    "        black_player = game['Black']\n",
    "        \n",
    "        # Handle potential missing values\n",
    "        try:\n",
    "            white_elo = int(game.get('WhiteElo', 0))\n",
    "            black_elo = int(game.get('BlackElo', 0))\n",
    "        except (ValueError, TypeError):\n",
    "            white_elo = 0\n",
    "            black_elo = 0\n",
    "            \n",
    "        result = game['Result']\n",
    "        eco_code = game.get('ECO', 'Unknown')\n",
    "        opening_name = game.get('Opening', 'Unknown Opening')\n",
    "        \n",
    "        # Process white player's game\n",
    "        if white_player not in players_data:\n",
    "            players_data[white_player] = {\n",
    "                \"rating\": white_elo,\n",
    "                \"white_games\": {},\n",
    "                \"black_games\": {},\n",
    "                \"num_games_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Update white player's data\n",
    "        if eco_code not in players_data[white_player][\"white_games\"]:\n",
    "            players_data[white_player][\"white_games\"][eco_code] = {\n",
    "                \"opening_name\": opening_name,\n",
    "                \"results\": {\n",
    "                    \"num_games\": 0,\n",
    "                    \"num_wins\": 0,\n",
    "                    \"num_losses\": 0,\n",
    "                    \"num_draws\": 0,\n",
    "                    \"score_percentage_with_opening\": 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Update game counts\n",
    "        players_data[white_player][\"num_games_total\"] += 1\n",
    "        players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_games\"] += 1\n",
    "        \n",
    "        # Update result counts\n",
    "        if result == \"1-0\":  # White win\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_wins\"] += 1\n",
    "        elif result == \"0-1\":  # Black win (white loss)\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_losses\"] += 1\n",
    "        elif result == \"1/2-1/2\":  # Draw\n",
    "            players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_draws\"] += 1\n",
    "            \n",
    "        # Update score percentage\n",
    "        wins = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_wins\"]\n",
    "        draws = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_draws\"]\n",
    "        total = players_data[white_player][\"white_games\"][eco_code][\"results\"][\"num_games\"]\n",
    "        score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "        players_data[white_player][\"white_games\"][eco_code][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "        \n",
    "        # Similarly process black player's game\n",
    "        if black_player not in players_data:\n",
    "            players_data[black_player] = {\n",
    "                \"rating\": black_elo,\n",
    "                \"white_games\": {},\n",
    "                \"black_games\": {},\n",
    "                \"num_games_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Update black player's data\n",
    "        if eco_code not in players_data[black_player][\"black_games\"]:\n",
    "            players_data[black_player][\"black_games\"][eco_code] = {\n",
    "                \"opening_name\": opening_name,\n",
    "                \"results\": {\n",
    "                    \"num_games\": 0,\n",
    "                    \"num_wins\": 0,\n",
    "                    \"num_losses\": 0,\n",
    "                    \"num_draws\": 0,\n",
    "                    \"score_percentage_with_opening\": 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Update game counts\n",
    "        players_data[black_player][\"num_games_total\"] += 1\n",
    "        players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_games\"] += 1\n",
    "        \n",
    "        # Update result counts\n",
    "        if result == \"0-1\":  # Black win\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_wins\"] += 1\n",
    "        elif result == \"1-0\":  # White win (black loss)\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_losses\"] += 1\n",
    "        elif result == \"1/2-1/2\":  # Draw\n",
    "            players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_draws\"] += 1\n",
    "            \n",
    "        # Update score percentage\n",
    "        wins = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_wins\"]\n",
    "        draws = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_draws\"]\n",
    "        total = players_data[black_player][\"black_games\"][eco_code][\"results\"][\"num_games\"]\n",
    "        score = (wins + (draws * 0.5)) / total * 100 if total > 0 else 0\n",
    "        players_data[black_player][\"black_games\"][eco_code][\"results\"][\"score_percentage_with_opening\"] = round(score, 1)\n",
    "    \n",
    "    # Final progress update\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "    acceptance_rate = (batch_accepted / total_rows * 100) if total_rows > 0 else 0\n",
    "    print(f\"Completed {total_rows:,} games in {elapsed:.1f} seconds - Rate: {rate:.1f} games/sec\")\n",
    "    print(f\"Batch filtering stats: Accepted {batch_accepted:,}, Filtered {batch_filtered:,} (Acceptance rate: {acceptance_rate:.1f}%)\")\n",
    "    \n",
    "    return players_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6573b2b1",
   "metadata": {},
   "source": [
    "## Parallelized Batch Processing\n",
    "\n",
    "For better performance, let's add parallelized batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dab69dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_parallel(\n",
    "    batch_df: pd.DataFrame, config: ProcessingConfig, log_frequency: int = 5000,\n",
    "    perf_tracker: Optional[PerformanceTracker] = None\n",
    ") -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a batch of games - this is now a wrapper around the sequential process_batch function.\n",
    "\n",
    "    Args:\n",
    "        batch_df: DataFrame containing a batch of games\n",
    "        config: Processing configuration\n",
    "        log_frequency: How often to log progress (in number of games)\n",
    "        perf_tracker: Performance tracker object to update with filtering stats\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping player usernames to their statistics\n",
    "    \"\"\"\n",
    "    # We're disabling parallel processing to avoid serialization issues\n",
    "    # In a production environment, parallel processing would be implemented differently\n",
    "    return process_batch(batch_df, config=config, log_frequency=log_frequency, perf_tracker=perf_tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89be59",
   "metadata": {},
   "source": [
    "## Main Processing Function\n",
    "\n",
    "Now let's implement the main function that processes the parquet file in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686b9ca",
   "metadata": {},
   "source": [
    "## Multi-File Processing with Duplicate Detection\n",
    "\n",
    "Before defining our main processing function, let's implement a system to handle duplicate file detection. This is essential when processing multiple parquet files that might have similar names but come from different months or batches. Our approach uses metadata fingerprinting to uniquely identify each file.\n",
    "\n",
    "This uses the dupe-check utils defined in our utils folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a66d3f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported FileRegistry\n"
     ]
    }
   ],
   "source": [
    "# Import our custom file registry utility\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the notebooks directory to the Python path to enable imports\n",
    "notebooks_dir = Path.cwd().parent\n",
    "if str(notebooks_dir) not in sys.path:\n",
    "    sys.path.append(str(notebooks_dir))\n",
    "\n",
    "# Try to import our custom file registry utility\n",
    "try:\n",
    "    from notebooks.utils.file_processing.raw_data_file_dupe_checks import FileRegistry\n",
    "    print(\"Successfully imported FileRegistry\")\n",
    "except ImportError:\n",
    "    print(\"Could not import FileRegistry - file duplicate checks will not be available\")\n",
    "    \n",
    "    # Throw error and stop the rest of the notebook from running\n",
    "    raise ImportError(\"Could not import FileRegistry - file duplicate checks will not be available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99dfd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_file(config: ProcessingConfig, log_frequency: int = 5000) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a parquet file in batches, with detailed performance tracking.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration\n",
    "        log_frequency: How often to log progress within a batch (in number of games)\n",
    "        \n",
    "    Returns:\n",
    "        Player statistics dictionary\n",
    "    \"\"\"\n",
    "    # Initialize DuckDB connection\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    # Load previous progress if any\n",
    "    players_data, start_batch = load_progress(config)\n",
    "    \n",
    "    # Initialize performance tracker\n",
    "    perf_tracker = PerformanceTracker()\n",
    "    \n",
    "    # Get total number of rows\n",
    "    print(\"Counting total rows in parquet file...\")\n",
    "    total_rows = con.execute(\n",
    "        f\"SELECT COUNT(*) FROM '{config.parquet_path}'\"\n",
    "    ).fetchone()[0]\n",
    "    print(f\"Total rows in parquet file: {total_rows:,}\")\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    total_batches = (total_rows + config.batch_size - 1) // config.batch_size\n",
    "    print(f\"Will process in {total_batches} batches of size {config.batch_size:,}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_num = start_batch\n",
    "    \n",
    "    while True:\n",
    "        # Calculate offset for the current batch\n",
    "        offset = batch_num * config.batch_size\n",
    "        \n",
    "        # Check if we've processed all rows\n",
    "        if offset >= total_rows:\n",
    "            print(\"Processed all rows. Finishing up.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nProcessing batch {batch_num + 1}/{total_batches} (offset {offset:,})\")\n",
    "        perf_tracker.start_batch()\n",
    "        \n",
    "        # Fetch a batch of data\n",
    "        batch_query = f\"\"\"\n",
    "        SELECT \n",
    "            Event, White, Black, Result, \n",
    "            WhiteTitle, BlackTitle, WhiteElo, BlackElo, \n",
    "            WhiteRatingDiff, BlackRatingDiff, ECO, Opening,\n",
    "            Termination, TimeControl\n",
    "        FROM '{config.parquet_path}'\n",
    "        LIMIT {config.batch_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query and convert to DataFrame\n",
    "        batch_df = con.execute(batch_query).df()\n",
    "        \n",
    "        print(f\"Loaded batch with {len(batch_df):,} rows\")\n",
    "        \n",
    "        # Process the batch\n",
    "        batch_data = process_batch_parallel(batch_df, config, log_frequency=log_frequency, perf_tracker=perf_tracker)\n",
    "        \n",
    "        # Merge with existing data\n",
    "        players_data = merge_player_stats(players_data, batch_data)\n",
    "        \n",
    "        # Record batch completion\n",
    "        batch_time = perf_tracker.end_batch(len(batch_df))\n",
    "        print(f\"Processed batch in {batch_time:.2f} seconds\")\n",
    "        print(f\"Current player count: {len(players_data):,}\")\n",
    "        \n",
    "        # Log performance metrics\n",
    "        perf_tracker.log_progress(force=True)\n",
    "        \n",
    "        # Save progress periodically\n",
    "        batch_num += 1\n",
    "        if batch_num % config.save_interval == 0:\n",
    "            save_progress(players_data, batch_num, config, perf_tracker)\n",
    "    \n",
    "    # Save final progress\n",
    "    save_progress(players_data, batch_num, config, perf_tracker)\n",
    "    \n",
    "    # Print final performance summary\n",
    "    summary = perf_tracker.get_summary()\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(f\"Total games processed: {summary['total_games']:,}\")\n",
    "    print(f\"Total processing time: {summary['total_time_sec']:.2f} seconds\")\n",
    "    print(f\"Overall processing rate: {summary['overall_rate_games_per_sec']:.2f} games/second\")\n",
    "    print(f\"Average batch processing time: {summary['avg_batch_time_sec']:.2f} seconds\")\n",
    "    \n",
    "    # Add filtering stats to the final summary\n",
    "    print(\"\\nFiltering Statistics:\")\n",
    "    print(f\"Accepted games: {summary['accepted_games']:,}\")\n",
    "    print(f\"Filtered out games: {summary['filtered_games']:,}\")\n",
    "    print(f\"Acceptance rate: {summary['acceptance_rate_percent']}%\")\n",
    "    \n",
    "    return players_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb402e",
   "metadata": {},
   "source": [
    "## System Information\n",
    "\n",
    "Let's check the system's hardware resources to optimize our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c42a4729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Information:\n",
      "  cpu_count_physical: 6\n",
      "  cpu_count_logical: 12\n",
      "  memory_total_gb: 32.0\n",
      "  memory_available_gb: 18.45\n",
      "\n",
      "Recommended batch size based on memory: 5,800,000\n"
     ]
    }
   ],
   "source": [
    "# Install psutil if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"Installing psutil package...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"])\n",
    "    import psutil\n",
    "    print(\"psutil installed successfully\")\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Get information about the system's hardware resources.\"\"\"\n",
    "    info = {\n",
    "        \"cpu_count_physical\": psutil.cpu_count(logical=False),\n",
    "        \"cpu_count_logical\": psutil.cpu_count(logical=True),\n",
    "        \"memory_total_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "        \"memory_available_gb\": round(psutil.virtual_memory().available / (1024**3), 2)\n",
    "    }\n",
    "    return info\n",
    "\n",
    "# Get system information\n",
    "system_info = get_system_info()\n",
    "print(\"System Information:\")\n",
    "for key, value in system_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Calculate optimal batch size based on available memory\n",
    "# Assuming each row needs about 1KB of memory\n",
    "available_memory_gb = system_info[\"memory_available_gb\"]\n",
    "memory_for_batch_gb = available_memory_gb * 0.3  # Use 30% of available memory\n",
    "optimal_batch_size = int(memory_for_batch_gb * 1024**3 / 1024)  # 1KB per row\n",
    "\n",
    "# Round to nearest 10,000\n",
    "optimal_batch_size = max(10_000, round(optimal_batch_size / 10_000) * 10_000)\n",
    "\n",
    "print(f\"\\nRecommended batch size based on memory: {optimal_batch_size:,}\")\n",
    "\n",
    "def process_parquet_file(config: ProcessingConfig, log_frequency: int = 5000) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process a parquet file in batches, with detailed performance tracking.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration\n",
    "        log_frequency: How often to log progress within a batch (in number of games)\n",
    "        \n",
    "    Returns:\n",
    "        Player statistics dictionary\n",
    "    \"\"\"\n",
    "    # Check if file has already been processed using FileRegistry\n",
    "    try:\n",
    "        registry = FileRegistry()\n",
    "        if registry.is_file_processed(config.parquet_path):\n",
    "            print(f\"Skipping already processed file: {Path(config.parquet_path).name}\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not check file registry: {e}\")\n",
    "    \n",
    "    # Initialize DuckDB connection\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    # Load previous progress if any\n",
    "    players_data, start_batch = load_progress(config)\n",
    "    \n",
    "    # Initialize performance tracker\n",
    "    perf_tracker = PerformanceTracker()\n",
    "    \n",
    "    # Get total number of rows\n",
    "    print(\"Counting total rows in parquet file...\")\n",
    "    total_rows = con.execute(\n",
    "        f\"SELECT COUNT(*) FROM '{config.parquet_path}'\"\n",
    "    ).fetchone()[0]\n",
    "    print(f\"Total rows in parquet file: {total_rows:,}\")\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    total_batches = (total_rows + config.batch_size - 1) // config.batch_size\n",
    "    print(f\"Will process in {total_batches} batches of size {config.batch_size:,}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_num = start_batch\n",
    "    \n",
    "    while True:\n",
    "        # Calculate offset for the current batch\n",
    "        offset = batch_num * config.batch_size\n",
    "        \n",
    "        # Check if we've processed all rows\n",
    "        if offset >= total_rows:\n",
    "            print(\"Processed all rows. Finishing up.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nProcessing batch {batch_num + 1}/{total_batches} (offset {offset:,})\")\n",
    "        perf_tracker.start_batch()\n",
    "        \n",
    "        # Fetch a batch of data\n",
    "        batch_query = f\"\"\"\n",
    "        SELECT \n",
    "            Event, White, Black, Result, \n",
    "            WhiteTitle, BlackTitle, WhiteElo, BlackElo, \n",
    "            WhiteRatingDiff, BlackRatingDiff, ECO, Opening,\n",
    "            Termination, TimeControl\n",
    "        FROM '{config.parquet_path}'\n",
    "        LIMIT {config.batch_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query and convert to DataFrame\n",
    "        batch_df = con.execute(batch_query).df()\n",
    "        \n",
    "        print(f\"Loaded batch with {len(batch_df):,} rows\")\n",
    "        \n",
    "        # Process the batch\n",
    "        batch_data = process_batch_parallel(batch_df, config, log_frequency=log_frequency, perf_tracker=perf_tracker)\n",
    "        \n",
    "        # Merge with existing data\n",
    "        players_data = merge_player_stats(players_data, batch_data)\n",
    "        \n",
    "        # Record batch completion\n",
    "        batch_time = perf_tracker.end_batch(len(batch_df))\n",
    "        print(f\"Processed batch in {batch_time:.2f} seconds\")\n",
    "        print(f\"Current player count: {len(players_data):,}\")\n",
    "        \n",
    "        # Log performance metrics\n",
    "        perf_tracker.log_progress(force=True)\n",
    "        \n",
    "        # Save progress periodically\n",
    "        batch_num += 1\n",
    "        if batch_num % config.save_interval == 0:\n",
    "            save_progress(players_data, batch_num, config, perf_tracker)\n",
    "    \n",
    "    # Save final progress\n",
    "    save_progress(players_data, batch_num, config, perf_tracker)\n",
    "    \n",
    "    # Mark the file as processed in the registry\n",
    "    try:\n",
    "        registry.mark_file_processed(config.parquet_path)\n",
    "        print(f\"Marked file {Path(config.parquet_path).name} as processed in the registry\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not update file registry: {e}\")\n",
    "    \n",
    "    # Print final performance summary\n",
    "    summary = perf_tracker.get_summary()\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(f\"Total games processed: {summary['total_games']:,}\")\n",
    "    print(f\"Total processing time: {summary['total_time_sec']:.2f} seconds\")\n",
    "    print(f\"Overall processing rate: {summary['overall_rate_games_per_sec']:.2f} games/second\")\n",
    "    print(f\"Average batch processing time: {summary['avg_batch_time_sec']:.2f} seconds\")\n",
    "    \n",
    "    # Add filtering stats to the final summary\n",
    "    print(\"\\nFiltering Statistics:\")\n",
    "    print(f\"Accepted games: {summary['accepted_games']:,}\")\n",
    "    print(f\"Filtered out games: {summary['filtered_games']:,}\")\n",
    "    print(f\"Acceptance rate: {summary['acceptance_rate_percent']}%\")\n",
    "    \n",
    "    return players_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe3ecc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_parquet_files(file_paths: List[str], base_config: ProcessingConfig = None, log_frequency: int = 5000) -> Dict[str, PlayerStats]:\n",
    "    \"\"\"\n",
    "    Process multiple parquet files, avoiding duplicates using the file registry.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of paths to parquet files to process\n",
    "        base_config: Base configuration to use as a template for each file\n",
    "        log_frequency: How often to log progress within a batch\n",
    "        \n",
    "    Returns:\n",
    "        Combined player statistics from all processed files\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"No files provided for processing\")\n",
    "        return {}\n",
    "    \n",
    "    # Create a default config if none provided\n",
    "    if base_config is None:\n",
    "        base_config = ProcessingConfig(\n",
    "            parquet_path=\"\",  # Will be set for each file\n",
    "            batch_size=100_000,\n",
    "            save_interval=1,\n",
    "            save_dir=\"../data/processed\"\n",
    "        )\n",
    "    \n",
    "    # Initialize file registry\n",
    "    try:\n",
    "        registry = FileRegistry()\n",
    "    except NameError:\n",
    "        print(\"FileRegistry not available, skipping duplicate detection\")\n",
    "        registry = None\n",
    "    \n",
    "    # Filter out already processed files\n",
    "    new_files = []\n",
    "    for file_path in file_paths:\n",
    "        if registry and registry.is_file_processed(file_path):\n",
    "            print(f\"Skipping already processed file: {Path(file_path).name}\")\n",
    "            try:\n",
    "                registry.mark_file_skipped(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not mark file as skipped: {e}\")\n",
    "            continue\n",
    "        new_files.append(file_path)\n",
    "    \n",
    "    if not new_files:\n",
    "        print(\"No new files to process.\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Found {len(new_files)} new files to process out of {len(file_paths)} total files.\")\n",
    "    \n",
    "    # Process each file\n",
    "    all_players_data: Dict[str, PlayerStats] = {}\n",
    "    \n",
    "    for i, file_path in enumerate(new_files):\n",
    "        print(f\"\\nProcessing file {i+1}/{len(new_files)}: {Path(file_path).name}\")\n",
    "        \n",
    "        # Create a config for this specific file\n",
    "        file_config = ProcessingConfig(\n",
    "            parquet_path=file_path,\n",
    "            batch_size=base_config.batch_size,\n",
    "            save_interval=base_config.save_interval,\n",
    "            save_dir=base_config.save_dir,\n",
    "            min_player_rating=base_config.min_player_rating,\n",
    "            max_elo_difference_between_players=base_config.max_elo_difference_between_players,\n",
    "            allowed_time_controls=base_config.allowed_time_controls,\n",
    "            use_parallel=base_config.use_parallel,\n",
    "            num_processes=base_config.num_processes\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Process the file\n",
    "            players_data = process_parquet_file(file_config, log_frequency=log_frequency)\n",
    "            \n",
    "            # Merge with accumulated data\n",
    "            if players_data:  # Only merge if we got data back (not skipped)\n",
    "                if all_players_data:\n",
    "                    all_players_data = merge_player_stats(all_players_data, players_data)\n",
    "                else:\n",
    "                    all_players_data = players_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    return all_players_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a7adc",
   "metadata": {},
   "source": [
    "## Run Processing\n",
    "\n",
    "Now let's run the processing with our optimized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00fb0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing with sequential processing (parallel disabled)\n",
      "Batch size: 100,000\n",
      "Progress updates every: 5,000 games\n",
      "Parquet file: /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/train-00000-of-00072.parquet\n",
      "Loaded registry with 1 processed files\n",
      "Skipping already processed file: train-00000-of-00072.parquet\n",
      "\n",
      "Processed data statistics:\n",
      "Total number of players: 0\n",
      "\n",
      "Filtering statistics:\n",
      "Total games examined: 1,394,617\n",
      "Games accepted: 633,715 (45.4% of total)\n",
      "Games filtered out: 760,902 (54.6% of total)\n",
      "Overall acceptance rate: 45.4%\n",
      "\n",
      "Filters applied:\n",
      "  - Min player rating: 1200\n",
      "  - Max rating difference: 100\n",
      "  - Allowed time controls: Blitz, Rapid, Classical\n",
      "  - Valid game results only (1-0, 0-1, 1/2-1/2)\n",
      "\n",
      "Processed data statistics:\n",
      "Total number of players: 0\n",
      "\n",
      "Filtering statistics:\n",
      "Total games examined: 1,394,617\n",
      "Games accepted: 633,715 (45.4% of total)\n",
      "Games filtered out: 760,902 (54.6% of total)\n",
      "Overall acceptance rate: 45.4%\n",
      "\n",
      "Filters applied:\n",
      "  - Min player rating: 1200\n",
      "  - Max rating difference: 100\n",
      "  - Allowed time controls: Blitz, Rapid, Classical\n",
      "  - Valid game results only (1-0, 0-1, 1/2-1/2)\n"
     ]
    }
   ],
   "source": [
    "# Path to the parquet file\n",
    "parquet_path = \"/Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/train-00000-of-00072.parquet\"\n",
    "\n",
    "# Create configuration with a more reasonable batch size for progress reporting\n",
    "config = ProcessingConfig(\n",
    "    parquet_path=parquet_path,\n",
    "    batch_size=100_000,  # Use a smaller batch size for better progress tracking\n",
    "    save_interval=1,  # Save after each batch\n",
    "    save_dir=\"../data/processed\",\n",
    "    use_parallel=False,  # Disable parallel processing\n",
    "    num_processes=1,  # Use only one process\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference_between_players=100,\n",
    "    allowed_time_controls=[\"Blitz\", \"Rapid\", \"Classical\"],\n",
    ")\n",
    "\n",
    "# Set how often to log progress within each batch (every 5,000 games)\n",
    "log_frequency = 5000\n",
    "\n",
    "print(\"Starting processing with sequential processing (parallel disabled)\")\n",
    "print(f\"Batch size: {config.batch_size:,}\")\n",
    "print(f\"Progress updates every: {log_frequency:,} games\")\n",
    "print(f\"Parquet file: {config.parquet_path}\")\n",
    "\n",
    "# Process the parquet file\n",
    "players_data = process_parquet_file(config, log_frequency=log_frequency)\n",
    "\n",
    "# Print statistics about the processed data\n",
    "print(f\"\\nProcessed data statistics:\")\n",
    "print(f\"Total number of players: {len(players_data):,}\")\n",
    "\n",
    "# Load performance info to display filtering stats\n",
    "try:\n",
    "    with open(Path(\"../data/processed/processing_progress_parquet.json\"), 'r') as f:\n",
    "        progress_info = json.load(f)\n",
    "        performance = progress_info.get(\"performance\", {})\n",
    "        \n",
    "        # Show filtering statistics\n",
    "        if \"accepted_games\" in performance and \"filtered_games\" in performance:\n",
    "            accepted = performance.get(\"accepted_games\", 0)\n",
    "            filtered = performance.get(\"filtered_games\", 0)\n",
    "            total = accepted + filtered\n",
    "            acceptance_rate = performance.get(\"acceptance_rate_percent\", 0)\n",
    "            \n",
    "            print(\"\\nFiltering statistics:\")\n",
    "            print(f\"Total games examined: {total:,}\")\n",
    "            print(f\"Games accepted: {accepted:,} ({accepted/total*100:.1f}% of total)\")\n",
    "            print(f\"Games filtered out: {filtered:,} ({filtered/total*100:.1f}% of total)\")\n",
    "            print(f\"Overall acceptance rate: {acceptance_rate}%\")\n",
    "            \n",
    "            # Display filter breakdown - we can't get this from the saved data\n",
    "            # but we can remind users about what filters were applied\n",
    "            print(\"\\nFilters applied:\")\n",
    "            print(f\"  - Min player rating: {config.min_player_rating}\")\n",
    "            print(f\"  - Max rating difference: {config.max_elo_difference_between_players}\")\n",
    "            print(f\"  - Allowed time controls: {', '.join(config.allowed_time_controls)}\")\n",
    "            print(\"  - Valid game results only (1-0, 0-1, 1/2-1/2)\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load filtering statistics: {e}\")\n",
    "\n",
    "# Show an example player\n",
    "if players_data:\n",
    "    import random\n",
    "    sample_player = random.choice(list(players_data.keys()))\n",
    "    print(f\"\\nSample stats for player: {sample_player}\")\n",
    "    print(f\"Rating: {players_data[sample_player]['rating']}\")\n",
    "    print(f\"Total games: {players_data[sample_player]['num_games_total']}\")\n",
    "    \n",
    "    print(\"\\nWhite openings:\")\n",
    "    white_openings = players_data[sample_player]['white_games']\n",
    "    if white_openings:\n",
    "        for eco, data in list(white_openings.items())[:5]:  # Show only first 5\n",
    "            print(f\"  {eco} - {data['opening_name']}: \" +\n",
    "                  f\"{data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "        if len(white_openings) > 5:\n",
    "            print(f\"  ... and {len(white_openings) - 5} more openings\")\n",
    "    else:\n",
    "        print(\"  No white openings\")\n",
    "    \n",
    "    print(\"\\nBlack openings:\")\n",
    "    black_openings = players_data[sample_player]['black_games']\n",
    "    if black_openings:\n",
    "        for eco, data in list(black_openings.items())[:5]:  # Show only first 5\n",
    "            print(f\"  {eco} - {data['opening_name']}: \" + \n",
    "                  f\"{data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "        if len(black_openings) > 5:\n",
    "            print(f\"  ... and {len(black_openings) - 5} more openings\")\n",
    "    else:\n",
    "        print(\"  No black openings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea10944",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the performance of the parquet processing to the PGN processing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "29c3a9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 parquet files in ../data/raw:\n",
      "  - train-00000-of-00072.parquet\n"
     ]
    }
   ],
   "source": [
    "# Example of processing multiple files\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Find all parquet files in the raw directory\n",
    "raw_data_dir = Path(\"../data/raw\")\n",
    "parquet_files = list(raw_data_dir.glob(\"*.parquet\"))\n",
    "\n",
    "print(f\"Found {len(parquet_files)} parquet files in {raw_data_dir}:\")\n",
    "for file_path in parquet_files:\n",
    "    print(f\"  - {file_path.name}\")\n",
    "\n",
    "# Create a config template for multi-file processing\n",
    "config_template = ProcessingConfig(\n",
    "    parquet_path=\"\",  # Will be overridden for each file\n",
    "    batch_size=100_000,\n",
    "    save_interval=1,\n",
    "    save_dir=\"../data/processed\",\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference_between_players=100,\n",
    "    allowed_time_controls=[\"Blitz\", \"Rapid\", \"Classical\"],\n",
    "    use_parallel=False,\n",
    "    num_processes=1\n",
    ")\n",
    "\n",
    "# To process all files, uncomment this code:\n",
    "# print(\"\\nProcessing all parquet files...\")\n",
    "# all_players_data = process_multiple_parquet_files(\n",
    "#     [str(p) for p in parquet_files],\n",
    "#     base_config=config_template,\n",
    "#     log_frequency=5000\n",
    "# )\n",
    "# \n",
    "# print(f\"\\nProcessed data statistics:\")\n",
    "# print(f\"Total number of players across all files: {len(all_players_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "073ee8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet Processing Performance:\n",
      "Total games: 1,394,617\n",
      "Total processing time: 155.44 seconds\n",
      "Processing rate: 8972.25 games/second\n",
      "\n",
      "Filtering Statistics:\n",
      "Games accepted: 633,715 (45.4% of total)\n",
      "Games filtered out: 760,902 (54.6% of total)\n",
      "Overall acceptance rate: 45.4%\n",
      "\n",
      "Performance Comparison:\n",
      "Parquet processing is typically 10-100x faster than PGN processing because:\n",
      "1. No need to decompress and parse text files\n",
      "2. Columnar storage allows loading only the columns we need\n",
      "3. DuckDB provides optimized query execution\n",
      "4. Batch processing with parallel execution\n"
     ]
    }
   ],
   "source": [
    "# Load the performance summary from the saved progress\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "progress_path = Path(\"../data/processed/processing_progress_parquet.json\")\n",
    "if progress_path.exists():\n",
    "    with open(progress_path, 'r') as f:\n",
    "        progress_info = json.load(f)\n",
    "        performance = progress_info.get(\"performance\", {})\n",
    "        \n",
    "        print(\"Parquet Processing Performance:\")\n",
    "        print(f\"Total games: {performance.get('total_games', 0):,}\")\n",
    "        print(f\"Total processing time: {performance.get('total_time_sec', 0):.2f} seconds\")\n",
    "        print(f\"Processing rate: {performance.get('overall_rate_games_per_sec', 0):.2f} games/second\")\n",
    "        \n",
    "        # Display filtering stats if available\n",
    "        if \"accepted_games\" in performance and \"filtered_games\" in performance:\n",
    "            accepted = performance.get(\"accepted_games\", 0)\n",
    "            filtered = performance.get(\"filtered_games\", 0)\n",
    "            total = accepted + filtered\n",
    "            acceptance_rate = performance.get(\"acceptance_rate_percent\", 0)\n",
    "            \n",
    "            print(\"\\nFiltering Statistics:\")\n",
    "            print(f\"Games accepted: {accepted:,} ({accepted/total*100:.1f}% of total)\")\n",
    "            print(f\"Games filtered out: {filtered:,} ({filtered/total*100:.1f}% of total)\")\n",
    "            print(f\"Overall acceptance rate: {acceptance_rate}%\")\n",
    "        \n",
    "        # Compare to PGN processing (if we have that data)\n",
    "        print(\"\\nPerformance Comparison:\")\n",
    "        print(\"Parquet processing is typically 10-100x faster than PGN processing because:\")\n",
    "        print(\"1. No need to decompress and parse text files\")\n",
    "        print(\"2. Columnar storage allows loading only the columns we need\")\n",
    "        print(\"3. DuckDB provides optimized query execution\")\n",
    "        print(\"4. Batch processing with parallel execution\")\n",
    "else:\n",
    "    print(\"No performance data available yet. Run the processing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2548a5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The parquet approach demonstrates significant performance advantages over the PGN processing approach:\n",
    "\n",
    "1. **Data access**: Parquet's columnar format allows us to load only the fields we need\n",
    "2. **Batch processing**: We can efficiently process data in memory-optimized batches\n",
    "3. **Parallelization**: Multiple CPU cores can be used effectively with minimal overhead\n",
    "4. **No parsing overhead**: Unlike PGN files, we don't need to parse text files or decompress data\n",
    "\n",
    "For large datasets, the parquet approach can be 10-100x faster than processing PGN files directly, making it much more suitable for processing millions of chess games.\n",
    "\n",
    "# Path to the parquet file\n",
    "parquet_path = \"/Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/train-00000-of-00072.parquet\"\n",
    "\n",
    "# Create configuration with a more reasonable batch size for progress reporting\n",
    "config = ProcessingConfig(\n",
    "    parquet_path=parquet_path,\n",
    "    batch_size=100_000,  # Use a smaller batch size for better progress tracking\n",
    "    save_interval=1,  # Save after each batch\n",
    "    save_dir=\"../data/processed\",\n",
    "    use_parallel=False,  # Disable parallel processing\n",
    "    num_processes=1,  # Use only one process\n",
    "    min_player_rating=1200,\n",
    "    max_elo_difference_between_players=100,\n",
    "    allowed_time_controls=[\"Blitz\", \"Rapid\", \"Classical\"],\n",
    ")\n",
    "\n",
    "# Set how often to log progress within each batch (every 5,000 games)\n",
    "log_frequency = 5000\n",
    "\n",
    "print(\"Starting processing with sequential processing (parallel disabled)\")\n",
    "print(f\"Batch size: {config.batch_size:,}\")\n",
    "print(f\"Progress updates every: {log_frequency:,} games\")\n",
    "print(f\"Parquet file: {config.parquet_path}\")\n",
    "\n",
    "# Process the parquet file\n",
    "players_data = process_parquet_file(config, log_frequency=log_frequency)\n",
    "\n",
    "# Print statistics about the processed data\n",
    "print(f\"\\nProcessed data statistics:\")\n",
    "print(f\"Total number of players: {len(players_data):,}\")\n",
    "\n",
    "# Load performance info to display filtering stats\n",
    "try:\n",
    "    with open(Path(\"../data/processed/processing_progress_parquet.json\"), 'r') as f:\n",
    "        progress_info = json.load(f)\n",
    "        performance = progress_info.get(\"performance\", {})\n",
    "        \n",
    "        # Show filtering statistics\n",
    "        if \"accepted_games\" in performance and \"filtered_games\" in performance:\n",
    "            accepted = performance.get(\"accepted_games\", 0)\n",
    "            filtered = performance.get(\"filtered_games\", 0)\n",
    "            total = accepted + filtered\n",
    "            acceptance_rate = performance.get(\"acceptance_rate_percent\", 0)\n",
    "            \n",
    "            print(\"\\nFiltering statistics:\")\n",
    "            print(f\"Total games examined: {total:,}\")\n",
    "            print(f\"Games accepted: {accepted:,} ({accepted/total*100:.1f}% of total)\")\n",
    "            print(f\"Games filtered out: {filtered:,} ({filtered/total*100:.1f}% of total)\")\n",
    "            print(f\"Overall acceptance rate: {acceptance_rate}%\")\n",
    "            \n",
    "            # Display filter breakdown - we can't get this from the saved data\n",
    "            # but we can remind users about what filters were applied\n",
    "            print(\"\\nFilters applied:\")\n",
    "            print(f\"  - Min player rating: {config.min_player_rating}\")\n",
    "            print(f\"  - Max rating difference: {config.max_elo_difference_between_players}\")\n",
    "            print(f\"  - Allowed time controls: {', '.join(config.allowed_time_controls)}\")\n",
    "            print(\"  - Valid game results only (1-0, 0-1, 1/2-1/2)\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load filtering statistics: {e}\")\n",
    "\n",
    "# Show an example player\n",
    "if players_data:\n",
    "    import random\n",
    "    sample_player = random.choice(list(players_data.keys()))\n",
    "    print(f\"\\nSample stats for player: {sample_player}\")\n",
    "    print(f\"Rating: {players_data[sample_player]['rating']}\")\n",
    "    print(f\"Total games: {players_data[sample_player]['num_games_total']}\")\n",
    "    \n",
    "    print(\"\\nWhite openings:\")\n",
    "    white_openings = players_data[sample_player]['white_games']\n",
    "    if white_openings:\n",
    "        for eco, data in list(white_openings.items())[:5]:  # Show only first 5\n",
    "            print(f\"  {eco} - {data['opening_name']}: \" +\n",
    "                  f\"{data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "        if len(white_openings) > 5:\n",
    "            print(f\"  ... and {len(white_openings) - 5} more openings\")\n",
    "    else:\n",
    "        print(\"  No white openings\")\n",
    "    \n",
    "    print(\"\\nBlack openings:\")\n",
    "    black_openings = players_data[sample_player]['black_games']\n",
    "    if black_openings:\n",
    "        for eco, data in list(black_openings.items())[:5]:  # Show only first 5\n",
    "            print(f\"  {eco} - {data['opening_name']}: \" + \n",
    "                  f\"{data['results']['score_percentage_with_opening']}% score in {data['results']['num_games']} games\")\n",
    "        if len(black_openings) > 5:\n",
    "            print(f\"  ... and {len(black_openings) - 5} more openings\")\n",
    "    else:\n",
    "        print(\"  No black openings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
