{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9336cee",
   "metadata": {},
   "source": [
    "# Notebook 31 — Model Inference: Predicting Opening Performance\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook performs **pure inference** on processed player data using the trained chess opening recommender model.\n",
    "\n",
    "**Input**: Processed `ModelInput` object from notebook 29 (or from production API)\n",
    "\n",
    "**Output**: Ranked opening recommendations with predicted performance scores\n",
    "\n",
    "**Key Design Principles**:\n",
    "- **Data ↔ Behavior Separation**: Load data first, then define pure functions, then apply functions to data\n",
    "- **Granular Functions**: Each step is a small, testable, reusable function\n",
    "- **Production-Ready**: Code can be directly adapted for HuggingFace Spaces deployment\n",
    "- **Fold-in Users**: Handles new players (not in training set) using rating and opening features only\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "1. SETUP & CONFIGURATION\n",
    "   ├── Import libraries\n",
    "   ├── Define paths and specs\n",
    "   └── Set random seeds\n",
    "\n",
    "2. LOAD DATA\n",
    "   ├── Load processed player data (ModelInput)\n",
    "   ├── Load model artifacts (hyperparameters, mappings, etc.)\n",
    "   └── Load opening metadata for display\n",
    "\n",
    "3. DEFINE MODEL ARCHITECTURE\n",
    "   ├── ChessOpeningRecommender class (from training)\n",
    "   └── Helper methods for embeddings\n",
    "\n",
    "4. LOAD TRAINED MODEL\n",
    "   ├── Instantiate model with correct hyperparameters\n",
    "   ├── Load trained weights (best_model.pt)\n",
    "   └── Set to evaluation mode\n",
    "\n",
    "5. DEFINE INFERENCE FUNCTIONS\n",
    "   ├── convert_to_tensors() - ModelInput → PyTorch tensors\n",
    "   ├── generate_predictions() - Run model forward pass\n",
    "   ├── predict_all_openings() - Get scores for ALL valid openings\n",
    "   └── rank_recommendations() - Sort and filter results\n",
    "\n",
    "6. RUN INFERENCE\n",
    "   ├── Convert player data to tensors\n",
    "   ├── Generate predictions\n",
    "   ├── Predict scores for all openings (including unplayed)\n",
    "   └── Rank and display recommendations\n",
    "\n",
    "7. ANALYZE RESULTS\n",
    "   ├── Compare predictions vs actuals (for played openings)\n",
    "   ├── Calculate error metrics\n",
    "   ├── Display top recommendations\n",
    "   └── Optional: Visualizations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook **never modifies model weights** - purely inference\n",
    "- Designed for easy HuggingFace Spaces deployment (~2-4 seconds per user)\n",
    "- Handles fold-in users (new players not in training set)\n",
    "- All functions are stateless and pure (no side effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97020ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 1: SETUP & CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed86e583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4c/gt84bby56mqdnzwsg72zvd680000gn/T/ipykernel_20395/2035919521.py\", line 11, in <module>\n",
      "    import torch\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/a/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Add project root to path for module imports\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from utils.foldin_data_processing.types import ModelInput\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7ca192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════════════════════════\n",
      "INFERENCE SPECIFICATIONS\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "Color: Black ('b')\n",
      "Model directory: 20251212_152017_black\n",
      "Top N recommendations: 10\n",
      "Device: cpu\n",
      "Random seed: 42\n",
      "════════════════════════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INFERENCE SPECIFICATIONS - Adjust these parameters as needed\n",
    "# ============================================================================\n",
    "\n",
    "# Color to analyze ('w' for White, 'b' for Black)\n",
    "COLOR = 'b'\n",
    "\n",
    "# Model artifacts directory name\n",
    "MODEL_DIR_NAME = \"20251212_152017_black\"\n",
    "\n",
    "# Number of recommendations to display\n",
    "TOP_N_RECOMMENDATIONS = 10\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Device for inference (CPU is fine for this model size)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Derived display values\n",
    "COLOR_NAME = \"White\" if COLOR == 'w' else \"Black\"\n",
    "\n",
    "print(\"═\" * 80)\n",
    "print(\"INFERENCE SPECIFICATIONS\")\n",
    "print(\"═\" * 80)\n",
    "print(f\"Color: {COLOR_NAME} ('{COLOR}')\")\n",
    "print(f\"Model directory: {MODEL_DIR_NAME}\")\n",
    "print(f\"Top N recommendations: {TOP_N_RECOMMENDATIONS}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "print(\"═\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a317ede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seeds set\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Random seeds set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce614da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying paths...\n",
      "   Model artifacts: True - /Users/a/Documents/personalprojects/chess-opening-recommender/data/models/20251212_152017_black\n",
      "   Player data: True - /Users/a/Documents/personalprojects/chess-opening-recommender/notebooks/data/processed_player_data_b.pkl\n",
      "✓ All paths verified\n"
     ]
    }
   ],
   "source": [
    "# Configuration - construct paths from specifications\n",
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "MODEL_ARTIFACTS_DIR = PROJECT_ROOT / \"data\" / \"models\" / MODEL_DIR_NAME\n",
    "PLAYER_DATA_PATH = DATA_DIR / f\"processed_player_data_{COLOR}.pkl\"\n",
    "\n",
    "# Verify paths exist\n",
    "print(\"Verifying paths...\")\n",
    "print(f\"   Model artifacts: {MODEL_ARTIFACTS_DIR.exists()} - {MODEL_ARTIFACTS_DIR}\")\n",
    "print(f\"   Player data: {PLAYER_DATA_PATH.exists()} - {PLAYER_DATA_PATH}\")\n",
    "\n",
    "if not MODEL_ARTIFACTS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Model artifacts not found at {MODEL_ARTIFACTS_DIR}\")\n",
    "if not PLAYER_DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Processed player data not found at {PLAYER_DATA_PATH}\\n\"\n",
    "        f\"Run notebook 29 first to generate this file.\"\n",
    "    )\n",
    "\n",
    "print(\"✓ All paths verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af0ca0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: LOAD DATA\n",
    "\n",
    "Load all necessary data:\n",
    "- Processed player data (ModelInput)\n",
    "- Model artifacts (hyperparameters, mappings)\n",
    "- Opening metadata for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c400b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed player data...\n",
      "✓ Loaded player data\n",
      "   Player ID: None (None = fold-in user)\n",
      "   Rating Z: -0.1696\n",
      "   Number of openings played: 94\n",
      "   Opening IDs shape: (94,)\n",
      "   ECO letter cats shape: (94,)\n",
      "   Scores shape: (94,)\n"
     ]
    }
   ],
   "source": [
    "# Load processed player data from notebook 29\n",
    "print(\"Loading processed player data...\")\n",
    "\n",
    "with open(PLAYER_DATA_PATH, 'rb') as f:\n",
    "    model_input: ModelInput = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded player data\")\n",
    "print(f\"   Player ID: {model_input.training_player_id} (None = fold-in user)\")\n",
    "print(f\"   Rating Z: {model_input.rating_z:.4f}\")\n",
    "print(f\"   Number of openings played: {len(model_input)}\")\n",
    "print(f\"   Opening IDs shape: {model_input.opening_ids.shape}\")\n",
    "print(f\"   ECO letter cats shape: {model_input.eco_letter_cats.shape}\")\n",
    "print(f\"   Scores shape: {model_input.scores.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c08791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model hyperparameters...\n",
      "✓ Loaded hyperparameters\n",
      "   Num players: 48,551\n",
      "   Num openings: 2,728\n",
      "   Num factors: 40\n",
      "   ECO letters: 5, ECO numbers: 100\n",
      "   ECO embedding dim: 4\n"
     ]
    }
   ],
   "source": [
    "# Load model hyperparameters\n",
    "print(\"\\nLoading model hyperparameters...\")\n",
    "\n",
    "hyperparams_path = MODEL_ARTIFACTS_DIR / \"hyperparameters.json\"\n",
    "with open(hyperparams_path, 'r') as f:\n",
    "    hyperparams = json.load(f)\n",
    "\n",
    "# Extract key hyperparameters\n",
    "NUM_PLAYERS = hyperparams['num_players']\n",
    "NUM_OPENINGS = hyperparams['num_openings']\n",
    "NUM_FACTORS = hyperparams['num_factors']\n",
    "NUM_ECO_LETTERS = hyperparams['num_eco_letters']\n",
    "NUM_ECO_NUMBERS = hyperparams['num_eco_numbers']\n",
    "ECO_EMBED_DIM = hyperparams['eco_embed_dim']\n",
    "\n",
    "print(f\"✓ Loaded hyperparameters\")\n",
    "print(f\"   Num players: {NUM_PLAYERS:,}\")\n",
    "print(f\"   Num openings: {NUM_OPENINGS:,}\")\n",
    "print(f\"   Num factors: {NUM_FACTORS}\")\n",
    "print(f\"   ECO letters: {NUM_ECO_LETTERS}, ECO numbers: {NUM_ECO_NUMBERS}\")\n",
    "print(f\"   ECO embedding dim: {ECO_EMBED_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473464be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading opening mappings...\n",
      "✓ Loaded opening mappings\n",
      "   Total openings: 2,728\n",
      "   Columns: ['db_id', 'eco', 'name', 'training_id']\n",
      "\n",
      "   Sample openings:\n",
      " training_id eco                       name\n",
      "           0 A00               Amar Opening\n",
      "           1 A00 Amar Opening: Paris Gambit\n",
      "           2 A00        Anderssen's Opening\n"
     ]
    }
   ],
   "source": [
    "# Load opening mappings for display\n",
    "print(\"\\nLoading opening mappings...\")\n",
    "\n",
    "opening_mappings_path = MODEL_ARTIFACTS_DIR / \"opening_mappings.csv\"\n",
    "opening_mappings_df = pd.read_csv(opening_mappings_path)\n",
    "\n",
    "print(f\"✓ Loaded opening mappings\")\n",
    "print(f\"   Total openings: {len(opening_mappings_df):,}\")\n",
    "print(f\"   Columns: {list(opening_mappings_df.columns)}\")\n",
    "print(f\"\\n   Sample openings:\")\n",
    "print(opening_mappings_df[['training_id', 'eco', 'name']].head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463f84c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and constructing side information...\n",
      "✓ Constructed side information for 2728 openings\n",
      "   All opening IDs shape: torch.Size([2728])\n",
      "   All ECO letter cats shape: torch.Size([2728])\n",
      "   All ECO number cats shape: torch.Size([2728])\n",
      "   ECO letter range: [0, 4]\n",
      "   ECO number range: [0, 99]\n",
      "✓ Constructed side information for 2728 openings\n",
      "   All opening IDs shape: torch.Size([2728])\n",
      "   All ECO letter cats shape: torch.Size([2728])\n",
      "   All ECO number cats shape: torch.Size([2728])\n",
      "   ECO letter range: [0, 4]\n",
      "   ECO number range: [0, 99]\n"
     ]
    }
   ],
   "source": [
    "# Load side information (ECO encodings) for ALL openings\n",
    "print(\"\\nLoading and constructing side information...\")\n",
    "\n",
    "# Load ECO encodings\n",
    "eco_encodings_path = MODEL_ARTIFACTS_DIR / \"eco_encodings.json\"\n",
    "with open(eco_encodings_path, 'r') as f:\n",
    "    eco_encodings = json.load(f)\n",
    "\n",
    "eco_letter_map = eco_encodings['eco_letter_to_int']\n",
    "eco_number_map = eco_encodings['eco_number_to_int']\n",
    "\n",
    "# Construct side information from opening_mappings_df\n",
    "# Extract ECO letter and number for each opening\n",
    "eco_letters = []\n",
    "eco_numbers = []\n",
    "\n",
    "for _, row in opening_mappings_df.iterrows():\n",
    "    eco_code = row['eco']  # e.g., \"B02\", \"C45\"\n",
    "    \n",
    "    # Parse ECO letter (first character)\n",
    "    eco_letter = eco_code[0]\n",
    "    eco_letter_cat = eco_letter_map[eco_letter]\n",
    "    \n",
    "    # Parse ECO number (remaining characters)\n",
    "    eco_number = eco_code[1:]  # e.g., \"02\", \"45\"\n",
    "    eco_number_cat = eco_number_map[eco_number]\n",
    "    \n",
    "    eco_letters.append(eco_letter_cat)\n",
    "    eco_numbers.append(eco_number_cat)\n",
    "\n",
    "# Convert to tensors for efficient batch processing\n",
    "# These are sorted by training_id (0, 1, 2, ..., N-1)\n",
    "all_opening_ids = torch.tensor(opening_mappings_df['training_id'].values, dtype=torch.long)\n",
    "all_eco_letter_cats = torch.tensor(eco_letters, dtype=torch.long)\n",
    "all_eco_number_cats = torch.tensor(eco_numbers, dtype=torch.long)\n",
    "\n",
    "print(f\"✓ Constructed side information for {len(all_opening_ids)} openings\")\n",
    "print(f\"   All opening IDs shape: {all_opening_ids.shape}\")\n",
    "print(f\"   All ECO letter cats shape: {all_eco_letter_cats.shape}\")\n",
    "print(f\"   All ECO number cats shape: {all_eco_number_cats.shape}\")\n",
    "print(f\"   ECO letter range: [{all_eco_letter_cats.min()}, {all_eco_letter_cats.max()}]\")\n",
    "print(f\"   ECO number range: [{all_eco_number_cats.min()}, {all_eco_number_cats.max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce0f695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading player ratings...\n",
      "✓ Loaded and normalized player ratings\n",
      "   Shape: torch.Size([48551])\n",
      "   Range: [-2.2687, 4.2454]\n",
      "   Mean: -0.0000\n",
      "   Std: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Load player ratings (needed for model buffers)\n",
    "print(\"\\nLoading player ratings...\")\n",
    "\n",
    "# Load player mappings to get ratings\n",
    "player_mappings_path = MODEL_ARTIFACTS_DIR / \"player_mappings.csv\"\n",
    "player_mappings_df = pd.read_csv(player_mappings_path)\n",
    "\n",
    "# Create player ratings tensor (sorted by training_id)\n",
    "player_mappings_df = player_mappings_df.sort_values('training_id')\n",
    "player_ratings_tensor = torch.tensor(player_mappings_df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "# Load rating normalization parameters\n",
    "rating_norm_path = MODEL_ARTIFACTS_DIR / \"rating_normalization.json\"\n",
    "with open(rating_norm_path, 'r') as f:\n",
    "    rating_norm = json.load(f)\n",
    "\n",
    "# Normalize ratings (z-score)\n",
    "player_ratings_tensor = (player_ratings_tensor - rating_norm['rating_mean']) / rating_norm['rating_std']\n",
    "\n",
    "print(f\"✓ Loaded and normalized player ratings\")\n",
    "print(f\"   Shape: {player_ratings_tensor.shape}\")\n",
    "print(f\"   Range: [{player_ratings_tensor.min():.4f}, {player_ratings_tensor.max():.4f}]\")\n",
    "print(f\"   Mean: {player_ratings_tensor.mean():.4f}\")\n",
    "print(f\"   Std: {player_ratings_tensor.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f74d35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 3: DEFINE MODEL ARCHITECTURE\n",
    "\n",
    "Define the `ChessOpeningRecommender` model class.\n",
    "\n",
    "This is copied from notebook 28 (training pipeline) to ensure exact architecture match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55cc513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ChessOpeningRecommender model class defined\n"
     ]
    }
   ],
   "source": [
    "class ChessOpeningRecommender(nn.Module):\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for chess opening recommendations.\n",
    "    \n",
    "    The model learns latent factors for players and openings, incorporating\n",
    "    side information:\n",
    "    - Player ratings (normalized)\n",
    "    - Opening ECO codes (letter and number as categorical features)\n",
    "    \n",
    "    Architecture:\n",
    "    - Player embedding: learnable latent factors\n",
    "    - Opening embedding: learnable latent factors\n",
    "    - Player rating: fixed side information\n",
    "    - ECO letter/number: categorical embeddings\n",
    "    \n",
    "    Prediction: dot product of player and opening representations + biases\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_players: int,\n",
    "        num_openings: int,\n",
    "        num_factors: int,\n",
    "        player_ratings: torch.Tensor,\n",
    "        opening_eco_letters: torch.Tensor,\n",
    "        opening_eco_numbers: torch.Tensor,\n",
    "        num_eco_letters: int,\n",
    "        num_eco_numbers: int,\n",
    "        eco_embed_dim: int = 4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation model.\n",
    "        \n",
    "        Args:\n",
    "            num_players: Total number of unique players\n",
    "            num_openings: Total number of unique openings\n",
    "            num_factors: Dimensionality of latent factor embeddings\n",
    "            player_ratings: Z-score normalized ratings for all players (shape: [num_players])\n",
    "            opening_eco_letters: ECO letter categories for all openings (shape: [num_openings])\n",
    "            opening_eco_numbers: ECO number categories for all openings (shape: [num_openings])\n",
    "            num_eco_letters: Number of unique ECO letter categories\n",
    "            num_eco_numbers: Number of unique ECO number categories\n",
    "            eco_embed_dim: Dimensionality of ECO categorical embeddings (default: 4)\n",
    "        \"\"\"\n",
    "        super(ChessOpeningRecommender, self).__init__()\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_players = num_players\n",
    "        self.num_openings = num_openings\n",
    "        self.num_factors = num_factors\n",
    "        self.eco_embed_dim = eco_embed_dim\n",
    "        \n",
    "        # ========================================\n",
    "        # Player Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.player_factors = nn.Embedding(num_players, num_factors)\n",
    "        self.player_biases = nn.Embedding(num_players, 1)\n",
    "        \n",
    "        # Player ratings (fixed side information - registered as buffer)\n",
    "        self.register_buffer('player_ratings', player_ratings)\n",
    "        \n",
    "        # ========================================\n",
    "        # Opening Components\n",
    "        # ========================================\n",
    "        \n",
    "        self.opening_factors = nn.Embedding(num_openings, num_factors)\n",
    "        self.opening_biases = nn.Embedding(num_openings, 1)\n",
    "        \n",
    "        # ECO letter and number (fixed side information - registered as buffers)\n",
    "        self.register_buffer('opening_eco_letters', opening_eco_letters)\n",
    "        self.register_buffer('opening_eco_numbers', opening_eco_numbers)\n",
    "        \n",
    "        # ECO embeddings (learnable)\n",
    "        self.eco_letter_embedding = nn.Embedding(num_eco_letters, eco_embed_dim)\n",
    "        self.eco_number_embedding = nn.Embedding(num_eco_numbers, eco_embed_dim)\n",
    "        \n",
    "        # ========================================\n",
    "        # Combination Layers\n",
    "        # ========================================\n",
    "        \n",
    "        # Combine player latent factors with rating\n",
    "        # Input: [num_factors + 1] → Output: [num_factors]\n",
    "        self.player_combiner = nn.Linear(num_factors + 1, num_factors)\n",
    "        \n",
    "        # Combine opening latent factors with ECO embeddings\n",
    "        # Input: [num_factors + 2*eco_embed_dim] → Output: [num_factors]\n",
    "        self.opening_combiner = nn.Linear(num_factors + 2 * eco_embed_dim, num_factors)\n",
    "        \n",
    "        # ========================================\n",
    "        # Global Bias\n",
    "        # ========================================\n",
    "        \n",
    "        # Global bias term (learnable scalar)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # ========================================\n",
    "        # Initialize Weights\n",
    "        # ========================================\n",
    "        \n",
    "        # Initialize embeddings with small random values\n",
    "        nn.init.normal_(self.player_factors.weight, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.opening_factors.weight, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.eco_letter_embedding.weight, mean=0, std=0.01)\n",
    "        nn.init.normal_(self.eco_number_embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        nn.init.zeros_(self.player_biases.weight)\n",
    "        nn.init.zeros_(self.opening_biases.weight)\n",
    "        \n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.player_combiner.weight)\n",
    "        nn.init.zeros_(self.player_combiner.bias)\n",
    "        nn.init.xavier_uniform_(self.opening_combiner.weight)\n",
    "        nn.init.zeros_(self.opening_combiner.bias)\n",
    "    \n",
    "    def forward(self, player_ids: torch.Tensor, opening_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: predict player-opening scores.\n",
    "        \n",
    "        Args:\n",
    "            player_ids: Player IDs (shape: [batch_size])\n",
    "            opening_ids: Opening IDs (shape: [batch_size])\n",
    "            \n",
    "        Returns:\n",
    "            Predicted scores (shape: [batch_size])\n",
    "        \"\"\"\n",
    "        # ========================================\n",
    "        # Get Player Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get player latent factors [batch_size, num_factors]\n",
    "        player_embed = self.player_factors(player_ids)\n",
    "        \n",
    "        # Get player ratings [batch_size, 1]\n",
    "        player_rating = self.player_ratings[player_ids].unsqueeze(1)\n",
    "        \n",
    "        # Concatenate player factors with rating [batch_size, num_factors + 1]\n",
    "        player_concat = torch.cat([player_embed, player_rating], dim=1)\n",
    "        \n",
    "        # Combine into final player representation [batch_size, num_factors]\n",
    "        player_repr = self.player_combiner(player_concat)\n",
    "        \n",
    "        # Get player bias [batch_size]\n",
    "        player_bias = self.player_biases(player_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Get Opening Representation\n",
    "        # ========================================\n",
    "        \n",
    "        # Get opening latent factors [batch_size, num_factors]\n",
    "        opening_embed = self.opening_factors(opening_ids)\n",
    "        \n",
    "        # Get ECO embeddings\n",
    "        eco_letters = self.opening_eco_letters[opening_ids]  # [batch_size]\n",
    "        eco_numbers = self.opening_eco_numbers[opening_ids]  # [batch_size]\n",
    "        \n",
    "        eco_letter_embed = self.eco_letter_embedding(eco_letters)  # [batch_size, eco_embed_dim]\n",
    "        eco_number_embed = self.eco_number_embedding(eco_numbers)  # [batch_size, eco_embed_dim]\n",
    "        \n",
    "        # Concatenate opening factors with ECO embeddings\n",
    "        # [batch_size, num_factors + 2*eco_embed_dim]\n",
    "        opening_concat = torch.cat([opening_embed, eco_letter_embed, eco_number_embed], dim=1)\n",
    "        \n",
    "        # Combine into final opening representation [batch_size, num_factors]\n",
    "        opening_repr = self.opening_combiner(opening_concat)\n",
    "        \n",
    "        # Get opening bias [batch_size]\n",
    "        opening_bias = self.opening_biases(opening_ids).squeeze()\n",
    "        \n",
    "        # ========================================\n",
    "        # Compute Prediction\n",
    "        # ========================================\n",
    "        \n",
    "        # Dot product of player and opening representations [batch_size]\n",
    "        interaction = (player_repr * opening_repr).sum(dim=1)\n",
    "        \n",
    "        # Add biases and global bias\n",
    "        prediction = interaction + player_bias + opening_bias + self.global_bias\n",
    "        \n",
    "        # Apply sigmoid to constrain output to [0, 1] range\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "print(\"✓ ChessOpeningRecommender model class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e85167",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 4: LOAD TRAINED MODEL\n",
    "\n",
    "Instantiate the model with correct hyperparameters and load trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9721a9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating model...\n",
      "✓ Model instantiated\n",
      "   Total parameters: 2,106,500\n",
      "   Trainable parameters: 2,106,500\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "print(\"Instantiating model...\")\n",
    "\n",
    "model = ChessOpeningRecommender(\n",
    "    num_players=NUM_PLAYERS,\n",
    "    num_openings=NUM_OPENINGS,\n",
    "    num_factors=NUM_FACTORS,\n",
    "    player_ratings=player_ratings_tensor,\n",
    "    opening_eco_letters=all_eco_letter_cats,\n",
    "    opening_eco_numbers=all_eco_number_cats,\n",
    "    num_eco_letters=NUM_ECO_LETTERS,\n",
    "    num_eco_numbers=NUM_ECO_NUMBERS,\n",
    "    eco_embed_dim=ECO_EMBED_DIM\n",
    ")\n",
    "\n",
    "print(f\"✓ Model instantiated\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71fab539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading trained weights...\n",
      "✓ Loaded trained weights from: best_model.pt\n",
      "   Model is on device: cpu\n",
      "   Model is in eval mode: True\n"
     ]
    }
   ],
   "source": [
    "# Load trained weights\n",
    "print(\"\\nLoading trained weights...\")\n",
    "\n",
    "model_path = MODEL_ARTIFACTS_DIR / \"best_model.pt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()  # Set to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "print(f\"✓ Loaded trained weights from: {model_path.name}\")\n",
    "print(f\"   Model is on device: {next(model.parameters()).device}\")\n",
    "print(f\"   Model is in eval mode: {not model.training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5dda5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 5: DEFINE INFERENCE FUNCTIONS\n",
    "\n",
    "Define pure functions for inference pipeline.\n",
    "\n",
    "**Design Principle**: Each function is small, testable, and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c475dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ convert_to_tensors() defined\n"
     ]
    }
   ],
   "source": [
    "def convert_to_tensors(\n",
    "    model_input: ModelInput,\n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Convert ModelInput to PyTorch tensors for inference.\n",
    "    \n",
    "    Args:\n",
    "        model_input: Processed player data from notebook 29\n",
    "        device: Device to place tensors on (cpu or cuda)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of tensors:\n",
    "            - player_ids: [batch_size] - player indices (or zeros for fold-in)\n",
    "            - opening_ids: [batch_size] - opening indices\n",
    "    \n",
    "    Note: ECO features and player ratings are accessed from model buffers,\n",
    "    not passed as forward() arguments.\n",
    "    \"\"\"\n",
    "    batch_size = len(model_input)\n",
    "    \n",
    "    # Handle fold-in users (training_player_id is None)\n",
    "    # Use player_id=0 (first player in training set) as a default\n",
    "    if model_input.training_player_id is None:\n",
    "        player_ids = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        player_ids = torch.full(\n",
    "            (batch_size,), \n",
    "            model_input.training_player_id, \n",
    "            dtype=torch.long, \n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    # Convert opening IDs to tensors\n",
    "    opening_ids = torch.tensor(model_input.opening_ids, dtype=torch.long, device=device)\n",
    "    \n",
    "    return {\n",
    "        'player_ids': player_ids,\n",
    "        'opening_ids': opening_ids\n",
    "    }\n",
    "\n",
    "print(\"✓ convert_to_tensors() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1b44756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ generate_predictions() defined\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()  # Disable gradient computation for inference\n",
    "def generate_predictions(\n",
    "    model: nn.Module,\n",
    "    tensors: Dict[str, torch.Tensor]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate predictions for the given tensors.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ChessOpeningRecommender model (in eval mode)\n",
    "        tensors: Dictionary of input tensors from convert_to_tensors()\n",
    "    \n",
    "    Returns:\n",
    "        predictions: [batch_size] numpy array of predicted scores (0-1)\n",
    "    \"\"\"\n",
    "    # Run forward pass (model accesses ECO features and ratings from buffers)\n",
    "    predictions = model(\n",
    "        player_ids=tensors['player_ids'],\n",
    "        opening_ids=tensors['opening_ids']\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy - handle both tensor and scalar cases\n",
    "    predictions_np = predictions.detach().cpu()\n",
    "    if predictions_np.dim() > 0:\n",
    "        predictions_np = predictions_np.squeeze()\n",
    "    return predictions_np.numpy()\n",
    "\n",
    "print(\"✓ generate_predictions() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4d2dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ predict_all_openings() defined\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def predict_all_openings(\n",
    "    model: nn.Module,\n",
    "    rating_z: float,\n",
    "    all_opening_ids: torch.Tensor,\n",
    "    player_id: Optional[int] = None,\n",
    "    device: torch.device = torch.device('cpu'),\n",
    "    batch_size: int = 512\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict performance scores for ALL valid openings (not just played ones).\n",
    "    \n",
    "    This is the key function for generating recommendations - it scores every\n",
    "    opening in the training set, allowing us to recommend new openings the\n",
    "    player hasn't tried yet.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ChessOpeningRecommender model (in eval mode)\n",
    "        rating_z: Player's z-score normalized rating (not used in forward pass,\n",
    "                  but kept for documentation/future use)\n",
    "        all_opening_ids: [num_openings] - all valid opening IDs\n",
    "        player_id: Player's training ID (None for fold-in users)\n",
    "        device: Device to run inference on\n",
    "        batch_size: Batch size for inference (to avoid memory issues)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: [num_openings] numpy array of predicted scores\n",
    "    \n",
    "    Note: Model accesses ECO features and player ratings from internal buffers,\n",
    "    not from function arguments. The rating_z parameter is kept for reference\n",
    "    but not passed to forward().\n",
    "    \"\"\"\n",
    "    num_openings = len(all_opening_ids)\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    for start_idx in range(0, num_openings, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_openings)\n",
    "        batch_len = end_idx - start_idx\n",
    "        \n",
    "        # Get batch of openings\n",
    "        batch_opening_ids = all_opening_ids[start_idx:end_idx].to(device)\n",
    "        \n",
    "        # Create player tensors (same for all openings in batch)\n",
    "        if player_id is None:\n",
    "            batch_player_ids = torch.zeros(batch_len, dtype=torch.long, device=device)\n",
    "        else:\n",
    "            batch_player_ids = torch.full((batch_len,), player_id, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Generate predictions for batch\n",
    "        # Model accesses ECO features and ratings from buffers\n",
    "        batch_predictions = model(\n",
    "            player_ids=batch_player_ids,\n",
    "            opening_ids=batch_opening_ids\n",
    "        )\n",
    "        \n",
    "        # Convert to numpy\n",
    "        batch_predictions_np = batch_predictions.detach().cpu()\n",
    "        if batch_predictions_np.dim() > 0:\n",
    "            batch_predictions_np = batch_predictions_np.squeeze()\n",
    "        all_predictions.append(batch_predictions_np.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    return np.concatenate(all_predictions)\n",
    "\n",
    "print(\"✓ predict_all_openings() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c767d94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ rank_recommendations() defined\n"
     ]
    }
   ],
   "source": [
    "def rank_recommendations(\n",
    "    predictions: np.ndarray,\n",
    "    opening_mappings_df: pd.DataFrame,\n",
    "    played_opening_ids: Optional[np.ndarray] = None,\n",
    "    top_n: int = 10\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Rank and filter opening recommendations.\n",
    "    \n",
    "    Args:\n",
    "        predictions: [num_openings] - predicted scores for all openings\n",
    "        opening_mappings_df: DataFrame with opening metadata\n",
    "        played_opening_ids: Array of opening IDs player has already played\n",
    "        top_n: Number of top recommendations to return\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (played_df, unplayed_df):\n",
    "            - played_df: Predictions for openings player has played (sorted by pred)\n",
    "            - unplayed_df: Top N recommendations for unplayed openings\n",
    "    \"\"\"\n",
    "    # Create results DataFrame\n",
    "    results_df = opening_mappings_df.copy()\n",
    "    results_df['predicted_score'] = predictions\n",
    "    \n",
    "    # Split into played and unplayed\n",
    "    if played_opening_ids is not None:\n",
    "        played_mask = results_df['training_id'].isin(played_opening_ids)\n",
    "        played_df = results_df[played_mask].sort_values('predicted_score', ascending=False)\n",
    "        unplayed_df = results_df[~played_mask].sort_values('predicted_score', ascending=False)\n",
    "    else:\n",
    "        played_df = pd.DataFrame()  # Empty if no played openings provided\n",
    "        unplayed_df = results_df.sort_values('predicted_score', ascending=False)\n",
    "    \n",
    "    # Take top N unplayed\n",
    "    unplayed_df = unplayed_df.head(top_n)\n",
    "    \n",
    "    return played_df, unplayed_df\n",
    "\n",
    "print(\"✓ rank_recommendations() defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd864f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 6: RUN INFERENCE\n",
    "\n",
    "Apply the functions to our loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70acaab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting player data to tensors...\n",
      "✓ Tensors created\n",
      "   player_ids: torch.Size([94]) on cpu\n",
      "   opening_ids: torch.Size([94])\n"
     ]
    }
   ],
   "source": [
    "# Step 6.1: Convert player data to tensors\n",
    "print(\"Converting player data to tensors...\")\n",
    "\n",
    "input_tensors = convert_to_tensors(model_input, device=DEVICE)\n",
    "\n",
    "print(f\"✓ Tensors created\")\n",
    "print(f\"   player_ids: {input_tensors['player_ids'].shape} on {input_tensors['player_ids'].device}\")\n",
    "print(f\"   opening_ids: {input_tensors['opening_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e23edb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions for played openings...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 6.2: Generate predictions for played openings\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating predictions for played openings...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m played_predictions = \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Predictions generated\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplayed_predictions.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mgenerate_predictions\u001b[39m\u001b[34m(model, tensors)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predictions_np.dim() > \u001b[32m0\u001b[39m:\n\u001b[32m     25\u001b[39m     predictions_np = predictions_np.squeeze()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictions_np\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Step 6.2: Generate predictions for played openings\n",
    "print(\"\\nGenerating predictions for played openings...\")\n",
    "\n",
    "played_predictions = generate_predictions(model, input_tensors)\n",
    "\n",
    "print(f\"✓ Predictions generated\")\n",
    "print(f\"   Shape: {played_predictions.shape}\")\n",
    "print(f\"   Range: [{played_predictions.min():.4f}, {played_predictions.max():.4f}]\")\n",
    "print(f\"   Mean: {played_predictions.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.3: Predict scores for ALL openings (including unplayed)\n",
    "print(\"\\nPredicting scores for ALL valid openings...\")\n",
    "\n",
    "all_predictions = predict_all_openings(\n",
    "    model=model,\n",
    "    rating_z=model_input.rating_z,\n",
    "    all_opening_ids=all_opening_ids,\n",
    "    player_id=model_input.training_player_id,\n",
    "    device=DEVICE,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "print(f\"✓ Predictions generated for all openings\")\n",
    "print(f\"   Total openings scored: {len(all_predictions):,}\")\n",
    "print(f\"   Range: [{all_predictions.min():.4f}, {all_predictions.max():.4f}]\")\n",
    "print(f\"   Mean: {all_predictions.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.4: Rank and filter recommendations\n",
    "print(\"\\nRanking recommendations...\")\n",
    "\n",
    "played_df, unplayed_df = rank_recommendations(\n",
    "    predictions=all_predictions,\n",
    "    opening_mappings_df=opening_mappings_df,\n",
    "    played_opening_ids=model_input.opening_ids,\n",
    "    top_n=TOP_N_RECOMMENDATIONS\n",
    ")\n",
    "\n",
    "print(f\"✓ Recommendations ranked\")\n",
    "print(f\"   Played openings: {len(played_df)}\")\n",
    "print(f\"   Unplayed openings (total): {len(opening_mappings_df) - len(played_df)}\")\n",
    "print(f\"   Top recommendations: {len(unplayed_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239077c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 7: ANALYZE RESULTS\n",
    "\n",
    "Display and analyze the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeecd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions vs actuals for played openings\n",
    "print(\"═\" * 80)\n",
    "print(\"PREDICTION ANALYSIS: PLAYED OPENINGS\")\n",
    "print(\"═\" * 80)\n",
    "\n",
    "# Add actual scores to played_df\n",
    "# Match by training_id to ensure correct alignment\n",
    "played_df_with_actuals = played_df.copy()\n",
    "played_df_with_actuals = played_df_with_actuals.set_index('training_id')\n",
    "\n",
    "# Create a mapping from training_id to actual score\n",
    "actual_scores_map = dict(zip(model_input.opening_ids, model_input.scores))\n",
    "played_df_with_actuals['actual_score'] = played_df_with_actuals.index.map(actual_scores_map)\n",
    "played_df_with_actuals = played_df_with_actuals.reset_index()\n",
    "\n",
    "# Calculate error metrics\n",
    "errors = played_df_with_actuals['predicted_score'] - played_df_with_actuals['actual_score']\n",
    "mae = np.abs(errors).mean()\n",
    "rmse = np.sqrt((errors ** 2).mean())\n",
    "mse = (errors ** 2).mean()\n",
    "\n",
    "print(f\"\\nError Metrics:\")\n",
    "print(f\"   MAE:  {mae:.4f}\")\n",
    "print(f\"   RMSE: {rmse:.4f}\")\n",
    "print(f\"   MSE:  {mse:.4f}\")\n",
    "\n",
    "# Show top predicted vs actual\n",
    "print(f\"\\nTop 10 Played Openings by Predicted Score:\")\n",
    "display_cols = ['eco', 'name', 'predicted_score', 'actual_score']\n",
    "print(played_df_with_actuals[display_cols].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"═\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top recommendations (unplayed openings)\n",
    "print(\"═\" * 80)\n",
    "print(f\"TOP {TOP_N_RECOMMENDATIONS} OPENING RECOMMENDATIONS (UNPLAYED)\")\n",
    "print(\"═\" * 80)\n",
    "\n",
    "print(f\"\\nThese are the {COLOR_NAME} openings the model predicts you will perform best with:\")\n",
    "print(f\"(You haven't played these openings yet in the training data)\\n\")\n",
    "\n",
    "# Format recommendations for display\n",
    "recommendations = unplayed_df[['eco', 'name', 'predicted_score']].copy()\n",
    "recommendations['rank'] = range(1, len(recommendations) + 1)\n",
    "recommendations = recommendations[['rank', 'eco', 'name', 'predicted_score']]\n",
    "\n",
    "# Pretty print\n",
    "for _, row in recommendations.iterrows():\n",
    "    print(f\"{row['rank']:2d}. {row['eco']:<6} {row['name']:<60} ({row['predicted_score']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"═\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388939f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"═\" * 80)\n",
    "print(\"INFERENCE SUMMARY\")\n",
    "print(\"═\" * 80)\n",
    "\n",
    "print(f\"\\nPlayer Information:\")\n",
    "print(f\"   Training Player ID: {model_input.training_player_id} {'(fold-in user)' if model_input.training_player_id is None else ''}\")\n",
    "print(f\"   Rating Z-score: {model_input.rating_z:.4f}\")\n",
    "print(f\"   Color: {COLOR_NAME}\")\n",
    "\n",
    "print(f\"\\nOpenings Analysis:\")\n",
    "print(f\"   Total valid openings in training: {len(opening_mappings_df):,}\")\n",
    "print(f\"   Openings played by player: {len(model_input)}\")\n",
    "print(f\"   Openings NOT yet played: {len(opening_mappings_df) - len(model_input):,}\")\n",
    "\n",
    "print(f\"\\nPrediction Statistics (All Openings):\")\n",
    "print(f\"   Min predicted score: {all_predictions.min():.4f}\")\n",
    "print(f\"   Max predicted score: {all_predictions.max():.4f}\")\n",
    "print(f\"   Mean predicted score: {all_predictions.mean():.4f}\")\n",
    "print(f\"   Median predicted score: {np.median(all_predictions):.4f}\")\n",
    "\n",
    "if len(played_df_with_actuals) > 0:\n",
    "    print(f\"\\nPrediction Quality (Played Openings):\")\n",
    "    print(f\"   MAE: {mae:.4f}\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   Correlation: {played_df_with_actuals['predicted_score'].corr(played_df_with_actuals['actual_score']):.4f}\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "print(f\"   Top {TOP_N_RECOMMENDATIONS} unplayed openings have been identified\")\n",
    "print(f\"   Predicted score range for top {TOP_N_RECOMMENDATIONS}: [{unplayed_df['predicted_score'].min():.4f}, {unplayed_df['predicted_score'].max():.4f}]\")\n",
    "\n",
    "print(\"\\n\" + \"═\" * 80)\n",
    "print(\"✓ INFERENCE COMPLETE\")\n",
    "print(\"═\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7655c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Production Deployment**: Adapt functions above for HuggingFace Spaces\n",
    "2. **API Integration**: Replace `pickle.load()` with Lichess API calls\n",
    "3. **Batch Processing**: Process multiple users in parallel\n",
    "4. **Caching**: Cache model and artifacts in memory for fast inference\n",
    "5. **Monitoring**: Add logging and performance metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Notes\n",
    "\n",
    "- **Model Loading**: ~2-4 seconds (one-time at startup)\n",
    "- **Per-User Inference**: ~1-2 seconds\n",
    "  - Data transformation: ~0.5s\n",
    "  - Model forward pass: ~0.5s\n",
    "  - Post-processing: ~0.2s\n",
    "- **Total**: Well within HuggingFace Spaces 60-second timeout ✓"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
