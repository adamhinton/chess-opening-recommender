{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b349557",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook combines several things we've already been doing individually.\n",
    "\n",
    "Its purpose is to obtain download massive amounts of raw chess game data, process it, then delete the downloaded raw files.\n",
    "\n",
    "## 1. Download raw data\n",
    "\n",
    "This will download raw parquet files from HuggingFace's API.\n",
    "\n",
    "Parquet files are 1GB files containing huge sets of Lichess games.\n",
    "\n",
    "This notebook downloads all of the parquet files for a certain month (usually 60-70 files). They will be automatically deleted after they're processed.\n",
    "\n",
    "## 2. Process the data\n",
    "\n",
    "For each parquet file that is downloaded, this notebook will process the games in the file one by one, extracting useful data for  training our model.\n",
    "\n",
    "## 3. Delete the data\n",
    "\n",
    "After each parquet file is done being processed, it will be deleted from the local disk.\n",
    "\n",
    "These files are 1GB each and we will be downloading hundreds or thousands of them, so deleting them is a must.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32444c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine paths to ensure imports work correctly\n",
    "current_file = Path.cwd()\n",
    "project_root = current_file.parent  # Move up to the project root\n",
    "\n",
    "# Add both to path to ensure imports work regardless of structure\n",
    "if str(current_file) not in sys.path:\n",
    "    sys.path.append(str(current_file))\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from notebooks.utils.file_processing.process_game_batch import (  # noqa: E402\n",
    "    process_batch,\n",
    ")  # noqa: E402\n",
    "\n",
    "# Now try the import with the explicit path\n",
    "from notebooks.utils.file_processing.types_and_classes import (  # noqa: E402\n",
    "    PlayerStats,\n",
    "    ProcessingConfig,\n",
    "    PerformanceTracker,\n",
    ")\n",
    "\n",
    "from notebooks.utils.file_processing.save_and_load_progress import (  # noqa: E402\n",
    "    save_progress,\n",
    "    load_progress,\n",
    ")\n",
    "\n",
    "\n",
    "from notebooks.utils.file_processing.raw_data_file_dupe_checks import (  # noqa: E402\n",
    "    FileRegistry,\n",
    ")  # noqa: E402\n",
    "\n",
    "# Import necessary libraries\n",
    "from typing import Dict, Optional  # noqa: E402\n",
    "import duckdb  # noqa: E402\n",
    "import time  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "import pickle  # noqa: E402\n",
    "import sys  # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87c63e",
   "metadata": {},
   "source": [
    "# 1.  Auto-Download Parquet Files\n",
    "\n",
    "## Wifi Speed\n",
    "\n",
    "My personal Wifi speed is quite fast (350 mbps); This is a lot of GBs of data, but download speed isn't the bottleneck because my system will take longer to process each file than it will to download the next one. But, if you have slower download speed, you may need to adjust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc6bbd",
   "metadata": {},
   "source": [
    "## Helper Functions for downloading raw data\n",
    "\n",
    "Getting helper functions that we're defined elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24423bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.downloading_raw_parquet_data.api_interaction import (\n",
    "    get_urls_from_hub_api,\n",
    "    get_urls_from_dataset_viewer,\n",
    "    filter_urls_for_month\n",
    ")\n",
    "from utils.downloading_raw_parquet_data.file_downloader import (\n",
    "    probe_fallback_urls,\n",
    "    download_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db45fe",
   "metadata": {},
   "source": [
    "## Processing and Deletion Configuration\n",
    "\n",
    "Here we define configurations for processing the raw data that we will download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONFIG FOR PROCESSING\n",
    "# ---------------------\n",
    "# This dictionary holds all the settings for the data processing part of the notebook.\n",
    "\n",
    "# --- Deletion Toggle ---\n",
    "# Set this to True to automatically delete parquet files after they are successfully processed.\n",
    "# Set it to False if you want to keep the raw files for repeated experiments.\n",
    "is_delete_downloaded_parquet_files = True\n",
    "\n",
    "# --- Processing Parameters ---\n",
    "# These parameters are used to filter the chess games from the raw files.\n",
    "# We only want to process games that are relevant and high-quality for our model.\n",
    "processing_config = {\n",
    "    # The directory where processed data and progress files will be saved.\n",
    "    \"save_dir\": str(project_root / \"data\" / \"processed\"),\n",
    "    \n",
    "    # The number of games to process in a single batch. This is optimized based on available memory.\n",
    "    \"batch_size\": 100_000,\n",
    "    \n",
    "    # How often to save progress (e.g., every 1 batch).\n",
    "    \"save_interval\": 1,\n",
    "    \n",
    "    # --- Game Filtering Rules ---\n",
    "    # Minimum rating for players in a game to be included.\n",
    "    \"min_player_rating\": 1200,\n",
    "    \n",
    "    # Maximum ELO difference between the two players.\n",
    "    \"max_elo_difference\": 100,\n",
    "    \n",
    "    # Allowed time controls (e.g., \"Blitz\", \"Rapid\"). Games with other time controls will be skipped.\n",
    "    \"allowed_time_controls\": {\"Blitz\", \"Rapid\", \"Classical\"},\n",
    "}\n",
    "\n",
    "# Create the directory for processed data if it doesn't exist.\n",
    "Path(processing_config[\"save_dir\"]).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf889cd",
   "metadata": {},
   "source": [
    "## Configuration for Downloading Raw Data\n",
    "\n",
    "Here we will organize some config items for the downloading of raw data parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2850c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "project_root = (\n",
    "    Path(__file__).resolve().parent.parent\n",
    "    if \"__file__\" in globals()\n",
    "    else Path.cwd().parent\n",
    ")\n",
    "output_dir = str(project_root / \"data\" / \"raw\" / \"auto_download_parquets\")\n",
    "\n",
    "# CONFIG\n",
    "# I did my best to put all of the variables that we might need to adjust here.\n",
    "# This is a proof of concept right now; later we may make this some sort of drop down picker for month, year etc\n",
    "config = {\n",
    "    \"repo\": \"Lichess/standard-chess-games\",  # Hugging Face repo id\n",
    "    \"year\": \"2025\",  # 4-digit year (string or int)\n",
    "    \"month\": \"7\",  # numeric month (e.g., \"7\" or \"07\")\n",
    "    \"max_parquets\": 30,  # int or None to download all available\n",
    "    # Download to /data/raw/auto_download_parquets relative to project root\n",
    "    \"output_dir\": output_dir,\n",
    "    \"hf_token\": None,  # set to your HF token string if you need to access gated datasets\n",
    "    \"probe_max_attempts\": 1000,  # for fallback probing\n",
    "    \"probe_patterns\": [  # tried in order if APIs gave no URLs\n",
    "        # Pattern A: common \"train-00000-of-00066.parquet\" style\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}-of-{total:05d}.parquet\",\n",
    "        # Pattern B: some datasets use plain shard names\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}.parquet\",\n",
    "        # Pattern C: fall back to zero-padded 4-digit name\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/000{idx}.parquet\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Extract config variables for use in the rest of the notebook\n",
    "repo = config[\"repo\"]\n",
    "year = str(config[\"year\"])\n",
    "month_raw = str(config[\"month\"])\n",
    "month_padded = month_raw.zfill(2)\n",
    "max_parquets = config[\"max_parquets\"]\n",
    "out_dir = Path(config[\"output_dir\"])\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "hf_headers = {\"Authorization\": f\"Bearer {config['hf_token']}\"} if config.get(\"hf_token\") else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7f94f",
   "metadata": {},
   "source": [
    "## Main Raw Data Downloading Execution Flow\n",
    "\n",
    "The following cell contains the main logic for querying parquet URLs, filtering them, and downloading the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8aea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for parquet URLs...\n",
      "No URLs found via APIs, falling back to probing...\n",
      "\n",
      "Will attempt to download 30 file(s) into /Users/a/Documents/personalprojects/chess-opening-recommender/data/raw/auto_download_parquets\n",
      "\n",
      "[1/30] SKIPPING (exists): train-00000-of-00066.parquet\n",
      "[2/30] SKIPPING (exists): train-00001-of-00066.parquet\n",
      "[3/30] SKIPPING (exists): train-00002-of-00066.parquet\n",
      "[4/30] SKIPPING (exists): train-00003-of-00066.parquet\n",
      "[5/30] SKIPPING (exists): train-00004-of-00066.parquet\n",
      "[6/30] SKIPPING (exists): train-00005-of-00066.parquet\n",
      "[7/30] SKIPPING (exists): train-00006-of-00066.parquet\n",
      "[8/30] SKIPPING (exists): train-00007-of-00066.parquet\n",
      "[9/30] SKIPPING (exists): train-00008-of-00066.parquet\n",
      "[10/30] SKIPPING (exists): train-00009-of-00066.parquet\n",
      "[11/30] SKIPPING (exists): train-00010-of-00066.parquet\n",
      "[12/30] SKIPPING (exists): train-00011-of-00066.parquet\n",
      "[13/30] SKIPPING (exists): train-00012-of-00066.parquet\n",
      "[14/30] SKIPPING (exists): train-00013-of-00066.parquet\n",
      "[15/30] SKIPPING (exists): train-00014-of-00066.parquet\n",
      "[16/30] SKIPPING (exists): train-00015-of-00066.parquet\n",
      "[17/30] SKIPPING (exists): train-00016-of-00066.parquet\n",
      "[18/30] SKIPPING (exists): train-00017-of-00066.parquet\n",
      "[19/30] SKIPPING (exists): train-00018-of-00066.parquet\n",
      "[20/30] SKIPPING (exists): train-00019-of-00066.parquet\n",
      "[21/30] SKIPPING (exists): train-00020-of-00066.parquet\n",
      "[22/30] SKIPPING (exists): train-00021-of-00066.parquet\n",
      "[23/30] SKIPPING (exists): train-00022-of-00066.parquet\n",
      "[24/30] SKIPPING (exists): train-00023-of-00066.parquet\n",
      "[25/30] SKIPPING (exists): train-00024-of-00066.parquet\n",
      "[26/30] SKIPPING (exists): train-00025-of-00066.parquet\n",
      "[27/30] SKIPPING (exists): train-00026-of-00066.parquet\n",
      "[28/30] SKIPPING (exists): train-00027-of-00066.parquet\n",
      "[29/30] SKIPPING (exists): train-00028-of-00066.parquet\n",
      "[30/30] SKIPPING (exists): train-00029-of-00066.parquet\n",
      "\n",
      "Download phase complete. 30 of 30 files are ready for processing.\n",
      "Total download time: 0.00 minutes.\n",
      "\n",
      "--- Starting Processing and Deletion Phase ---\n",
      "Loaded player data with 697821 players.\n",
      "Resuming from batch 1.\n",
      "Loaded initial data. Total players so far: 697,821\n",
      "Loaded registry with 7 processed files\n",
      "Found 30 parquet files to process.\n",
      "\n",
      "--- Processing file 1/30: train-00011-of-00066.parquet ---\n",
      "Loaded player data with 697821 players.\n",
      "Will process 1,410,497 rows in 15 batches of size 100,000\n",
      "\n",
      "Processing batch 1/15 (offset 0)\n",
      "An error occurred during file processing: Not implemented Error: Unsupported type \"TIME WITH TIME ZONE\"\n",
      "Processing failed for train-00011-of-00066.parquet. The file will not be deleted.\n",
      "\n",
      "--- Processing file 2/30: train-00005-of-00066.parquet ---\n",
      "Loaded player data with 697821 players.\n",
      "Will process 1,410,497 rows in 15 batches of size 100,000\n",
      "\n",
      "Processing batch 1/15 (offset 0)\n",
      "An error occurred during file processing: Not implemented Error: Unsupported type \"TIME WITH TIME ZONE\"\n",
      "Processing failed for train-00005-of-00066.parquet. The file will not be deleted.\n",
      "\n",
      "--- Processing file 3/30: train-00024-of-00066.parquet ---\n",
      "Loaded player data with 697821 players.\n",
      "Will process 1,410,497 rows in 15 batches of size 100,000\n",
      "\n",
      "Processing batch 1/15 (offset 0)\n",
      "An error occurred during file processing: Not implemented Error: Unsupported type \"TIME WITH TIME ZONE\"\n",
      "Processing failed for train-00024-of-00066.parquet. The file will not be deleted.\n",
      "\n",
      "--- Processing file 4/30: train-00020-of-00066.parquet ---\n",
      "Loaded player data with 697821 players.\n",
      "Will process 1,410,497 rows in 15 batches of size 100,000\n",
      "\n",
      "Processing batch 1/15 (offset 0)\n",
      "An error occurred during file processing: Not implemented Error: Unsupported type \"TIME WITH TIME ZONE\"\n",
      "Processing failed for train-00020-of-00066.parquet. The file will not be deleted.\n",
      "\n",
      "--- Processing file 5/30: train-00001-of-00066.parquet ---\n",
      "Loaded player data with 697821 players.\n",
      "Will process 1,410,497 rows in 15 batches of size 100,000\n",
      "\n",
      "Processing batch 1/15 (offset 0)\n",
      "An error occurred during file processing: Not implemented Error: Unsupported type \"TIME WITH TIME ZONE\"\n",
      "Processing failed for train-00001-of-00066.parquet. The file will not be deleted.\n",
      "\n",
      "--- Processing file 6/30: train-00015-of-00066.parquet ---\n",
      "Loaded player data with 697821 players.\n",
      "Will process 1,410,497 rows in 15 batches of size 100,000\n",
      "\n",
      "Processing batch 1/15 (offset 0)\n",
      "An error occurred during file processing: Not implemented Error: Unsupported type \"TIME WITH TIME ZONE\"\n",
      "Processing failed for train-00015-of-00066.parquet. The file will not be deleted.\n",
      "\n",
      "--- Processing file 7/30: train-00010-of-00066.parquet ---\n",
      "Loaded player data with 697821 players.\n",
      "Will process 1,410,497 rows in 15 batches of size 100,000\n",
      "\n",
      "Processing batch 1/15 (offset 0)\n",
      "An error occurred during file processing: Not implemented Error: Unsupported type \"TIME WITH TIME ZONE\"\n",
      "Processing failed for train-00010-of-00066.parquet. The file will not be deleted.\n",
      "\n",
      "--- Processing file 8/30: train-00025-of-00066.parquet ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 74\u001b[39m\n",
      "\u001b[32m     63\u001b[39m base_processing_config = ProcessingConfig(\n",
      "\u001b[32m     64\u001b[39m     parquet_path=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# This will be updated for each file.\u001b[39;00m\n",
      "\u001b[32m     65\u001b[39m     batch_size=processing_config[\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m],\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     allowed_time_controls=processing_config[\u001b[33m\"\u001b[39m\u001b[33mallowed_time_controls\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[32m     71\u001b[39m )\n",
      "\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Call the main function to process all files in the directory.\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mprocess_and_delete_files\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_processing_config\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelete_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_delete_downloaded_parquet_files\u001b[49m\n",
      "\u001b[32m     77\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- All Done! ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mprocess_and_delete_files\u001b[39m\u001b[34m(base_config, delete_files)\u001b[39m\n",
      "\u001b[32m     38\u001b[39m file_config = ProcessingConfig(\n",
      "\u001b[32m     39\u001b[39m     parquet_path=\u001b[38;5;28mstr\u001b[39m(file_path),\n",
      "\u001b[32m     40\u001b[39m     batch_size=base_config.batch_size,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     allowed_time_controls=base_config.allowed_time_controls\n",
      "\u001b[32m     46\u001b[39m )\n",
      "\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Process the file. The `all_players_data` dict is updated in-place.\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m success = \u001b[43mprocess_parquet_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_players_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# If processing was successful, mark it in the registry and delete if enabled.\u001b[39;00m\n",
      "\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mprocess_parquet_file\u001b[39m\u001b[34m(config, players_data, log_frequency, file_context)\u001b[39m\n",
      "\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m progress_path.exists():\n",
      "\u001b[32m     25\u001b[39m     progress_path.unlink()\n",
      "\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m _, start_batch = \u001b[43mload_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     28\u001b[39m perf_tracker = PerformanceTracker()\n",
      "\u001b[32m     30\u001b[39m total_rows = con.execute(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSELECT COUNT(*) FROM \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.parquet_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m).fetchone()[\u001b[32m0\u001b[39m]\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/notebooks/utils/file_processing/save_and_load_progress.py:114\u001b[39m, in \u001b[36mload_progress\u001b[39m\u001b[34m(config)\u001b[39m\n",
      "\u001b[32m    112\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(player_data_path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         players_data = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    115\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded player data with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(players_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m players.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    116\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# This cell downloads parquet files from a specified Hugging Face repository\n",
    "# for a given month and year, processes them, and optionally deletes them after processing.\n",
    "\n",
    "import urllib\n",
    "\n",
    "# Main flow for downloading files\n",
    "print(\"Querying for parquet URLs...\")\n",
    "urls = get_urls_from_hub_api(repo, hf_headers)\n",
    "if not urls:\n",
    "    urls = get_urls_from_dataset_viewer(repo, hf_headers)\n",
    "\n",
    "filtered = filter_urls_for_month(urls, year, month_padded)\n",
    "\n",
    "if not filtered:\n",
    "    print(\"No URLs found via APIs, falling back to probing...\")\n",
    "    patterns = config.get(\"probe_patterns\", [])\n",
    "    filtered = probe_fallback_urls(repo, year, month_padded, config[\"probe_max_attempts\"], patterns, hf_headers)\n",
    "\n",
    "if not filtered:\n",
    "    print(\"ERROR: No parquet URLs discovered. Aborting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if max_parquets is not None:\n",
    "    filtered = filtered[:int(max_parquets)]\n",
    "\n",
    "print(f\"\\nWill attempt to download {len(filtered)} file(s) into {out_dir.resolve()}\\n\")\n",
    "\n",
    "success_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, url in enumerate(filtered):\n",
    "    filename = Path(urllib.parse.unquote(url)).name\n",
    "    dest = out_dir / filename\n",
    "    \n",
    "    if dest.exists():\n",
    "        print(f\"[{i+1}/{len(filtered)}] SKIPPING (exists): {filename}\")\n",
    "        success_count += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{i+1}/{len(filtered)}] DOWNLOADING: {filename}\")\n",
    "    ok = download_file(url, dest, hf_headers)\n",
    "    \n",
    "    if ok:\n",
    "        success_count += 1\n",
    "        print(f\"  SUCCESS: Downloaded {filename}\")\n",
    "    else:\n",
    "        print(f\"  FAILURE: Could not download {filename}\")\n",
    "        if not urls: # If we were probing, a failure means we stop.\n",
    "            print(\"  Stopping probe-based download.\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nDownload phase complete. {success_count} of {len(filtered)} files are ready for processing.\")\n",
    "print(f\"Total download time: {total_time / 60:.2f} minutes.\")\n",
    "\n",
    "# --- Part 2: Process and Delete Files ---\n",
    "print(\"\\n--- Starting Processing and Deletion Phase ---\")\n",
    "\n",
    "# Set up the base configuration for processing.\n",
    "# This will be used for each file that gets processed.\n",
    "base_processing_config = ProcessingConfig(\n",
    "    parquet_path=\"\",  # This will be updated for each file.\n",
    "    batch_size=processing_config[\"batch_size\"],\n",
    "    save_interval=processing_config[\"save_interval\"],\n",
    "    save_dir=processing_config[\"save_dir\"],\n",
    "    min_player_rating=processing_config[\"min_player_rating\"],\n",
    "    max_elo_difference_between_players=processing_config[\"max_elo_difference\"],\n",
    "    allowed_time_controls=processing_config[\"allowed_time_controls\"]\n",
    ")\n",
    "\n",
    "# Call the main function to process all files in the directory.\n",
    "process_and_delete_files(\n",
    "    base_config=base_processing_config,\n",
    "    delete_files=is_delete_downloaded_parquet_files\n",
    ")\n",
    "\n",
    "print(\"\\n--- All Done! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974ae3b",
   "metadata": {},
   "source": [
    "## Processing Helper Functions\n",
    "\n",
    "The following functions are taken from `03_parquet_performance.ipynb` and are responsible for processing the raw parquet files. They handle batching, duplicate checking, and performance tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18420048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_file(config: ProcessingConfig, \n",
    "                         players_data: Dict[str, PlayerStats],\n",
    "                         log_frequency: int = 5000, \n",
    "                         file_context: Optional[Dict] = None) -> bool:\n",
    "    \"\"\"\n",
    "    Process a single parquet file in batches, updating a shared players_data dictionary.\n",
    "    This function is a direct copy from 03_parquet_performance.ipynb.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration for this specific file.\n",
    "        players_data: The shared dictionary of player statistics to update.\n",
    "        log_frequency: How often to log progress within a batch.\n",
    "        file_context: Dictionary with context for multi-file processing.\n",
    "        \n",
    "    Returns:\n",
    "        True if processing was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize DuckDB connection\n",
    "        con = duckdb.connect()\n",
    "        \n",
    "        # Reset progress file for a new run to avoid conflicts.\n",
    "        progress_path = Path(config.save_dir) / \"processing_progress_parquet.json\"\n",
    "        if progress_path.exists():\n",
    "            progress_path.unlink()\n",
    "\n",
    "        _, start_batch = load_progress(config)\n",
    "        perf_tracker = PerformanceTracker()\n",
    "        \n",
    "        total_rows = con.execute(f\"SELECT COUNT(*) FROM '{config.parquet_path}'\").fetchone()[0]\n",
    "        if total_rows == 0:\n",
    "            print(\"File is empty, skipping.\")\n",
    "            return True # Return True to allow deletion of empty file\n",
    "\n",
    "        total_batches = (total_rows + config.batch_size - 1) // config.batch_size\n",
    "        print(f\"Will process {total_rows:,} rows in {total_batches} batches of size {config.batch_size:,}\")\n",
    "        \n",
    "        batch_num = start_batch\n",
    "        while batch_num * config.batch_size < total_rows:\n",
    "            offset = batch_num * config.batch_size\n",
    "            print(f\"\\nProcessing batch {batch_num + 1}/{total_batches} (offset {offset:,})\")\n",
    "            perf_tracker.start_batch()\n",
    "            \n",
    "            batch_query = f\"SELECT * FROM '{config.parquet_path}' LIMIT {config.batch_size} OFFSET {offset}\"\n",
    "            batch_df = con.execute(batch_query).df()\n",
    "            \n",
    "            if batch_df.empty:\n",
    "                break\n",
    "\n",
    "            process_batch(batch_df, players_data, config, log_frequency, perf_tracker, file_context)\n",
    "            \n",
    "            batch_time = perf_tracker.end_batch(len(batch_df))\n",
    "            print(f\"Processed batch in {batch_time:.2f} seconds. Current player count: {len(players_data):,}\")\n",
    "            perf_tracker.log_progress(force=True)\n",
    "            \n",
    "            batch_num += 1\n",
    "            if batch_num % config.save_interval == 0:\n",
    "                save_progress(players_data, batch_num, config, perf_tracker)\n",
    "        \n",
    "        save_progress(players_data, batch_num, config, perf_tracker)\n",
    "        \n",
    "        summary = perf_tracker.get_summary()\n",
    "        print(\"\\nFile Processing Summary:\")\n",
    "        for key, value in summary.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file processing: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if 'con' in locals():\n",
    "            con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_delete_files(base_config: ProcessingConfig, delete_files: bool):\n",
    "    \"\"\"\n",
    "    Processes all .parquet files in the download directory and optionally deletes them.\n",
    "    This function scans the directory, checks for duplicates, processes new files,\n",
    "    and then deletes them if the toggle is enabled.\n",
    "    \"\"\"\n",
    "    # Load existing player data once at the beginning.\n",
    "    # This dictionary will be updated by all file processing jobs.\n",
    "    all_players_data, _ = load_progress(base_config)\n",
    "    print(f\"Loaded initial data. Total players so far: {len(all_players_data):,}\")\n",
    "    \n",
    "    registry = FileRegistry()\n",
    "    \n",
    "    # Get all parquet files from the download directory\n",
    "    files_to_process = list(out_dir.glob(\"*.parquet\"))\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"No parquet files found in the download directory to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(files_to_process)} parquet files to process.\")\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        print(f\"\\n--- Processing file {i+1}/{len(files_to_process)}: {file_path.name} ---\")\n",
    "        \n",
    "        # Check if the file has already been processed.\n",
    "        if registry.is_file_processed(str(file_path)):\n",
    "            print(\"File has already been processed. Skipping.\")\n",
    "            if delete_files:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted duplicate file: {file_path.name}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting duplicate file: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Create a specific configuration for this file.\n",
    "        file_config = ProcessingConfig(\n",
    "            parquet_path=str(file_path),\n",
    "            batch_size=base_config.batch_size,\n",
    "            save_interval=base_config.save_interval,\n",
    "            save_dir=base_config.save_dir,\n",
    "            min_player_rating=base_config.min_player_rating,\n",
    "            max_elo_difference_between_players=base_config.max_elo_difference_between_players,\n",
    "            allowed_time_controls=base_config.allowed_time_controls\n",
    "        )\n",
    "        \n",
    "        # Process the file. The `all_players_data` dict is updated in-place.\n",
    "        success = process_parquet_file(file_config, all_players_data)\n",
    "        \n",
    "        # If processing was successful, mark it in the registry and delete if enabled.\n",
    "        if success:\n",
    "            registry.mark_file_processed(str(file_path))\n",
    "            print(f\"Successfully processed and marked file: {file_path.name}\")\n",
    "            if delete_files:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted processed file: {file_path.name}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting processed file: {e}\")\n",
    "        else:\n",
    "            print(f\"Processing failed for {file_path.name}. The file will not be deleted.\")\n",
    "\n",
    "    # Save the final, combined data from all files.\n",
    "    final_save_path = Path(base_config.save_dir) / \"all_players_stats_combined.pkl\"\n",
    "    with open(final_save_path, 'wb') as f:\n",
    "        pickle.dump(all_players_data, f)\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Final combined data for {len(all_players_data):,} players saved to: {final_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607ea53",
   "metadata": {},
   "source": [
    "# 2. Process Downloaded Files\n",
    "\n",
    "The following cells contain the logic from notebook `03_parquet_performance.ipynb` to process the files we just downloaded.\n",
    "\n",
    "This involves:\n",
    "- Scanning the download directory for all `.parquet` files.\n",
    "- Processing each file to extract player statistics.\n",
    "- Storing the aggregated statistics in a pickle file.\n",
    "- Keeping a registry of processed files to avoid duplicate work.\n",
    "- Deleting the raw parquet file after it has been successfully processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a2aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Set\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- Processing Configuration ---\n",
    "\n",
    "# Toggle to enable/disable deletion of parquet files after processing\n",
    "# This should be set to True unless you want to keep the raw files for repeated experiments.\n",
    "# Setting it to False will retain all downloaded parquet files, whereas if it's True they will be deleted after processing because you don't need them anymore.\n",
    "is_delete_downloaded_parquet_files = True  # <<< IMPORTANT TOGGLE\n",
    "\n",
    "\n",
    "# Use the output_dir from the download config\n",
    "proc_config = ProcessingConfig(\n",
    "    input_dir=config[\"output_dir\"],\n",
    "    player_stats_output_path=str(project_root / \"data\" / \"processed\" / \"player_stats_parquet.pkl\"),\n",
    "    file_registry_path=str(project_root / \"data\" / \"processed\" / \"file_registry.json\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee24a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Processing a Single File ---\n",
    "\n",
    "def process_parquet_file(filepath: str, config: ProcessingConfig, player_stats: Dict[str, PlayerStats], perf_tracker: PerformanceTracker):\n",
    "    \"\"\"\n",
    "    Processes a single parquet file, updating player_stats dictionary.\n",
    "    This is the core logic from 03_parquet_performance.ipynb.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(database=':memory:', read_only=False)\n",
    "    \n",
    "    create_view_sql = f\"\"\"\n",
    "    CREATE VIEW games AS SELECT * FROM read_parquet('{filepath}');\n",
    "    \"\"\"\n",
    "    con.execute(create_view_sql)\n",
    "\n",
    "    # Query to get player game counts\n",
    "    query = f\"\"\"\n",
    "    WITH players AS (\n",
    "        SELECT White AS player, WhiteElo AS elo,\n",
    "            CASE WHEN Result = '1-0' THEN 1 ELSE 0 END AS win,\n",
    "            CASE WHEN Result = '0-1' THEN 1 ELSE 0 END AS loss,\n",
    "            CASE WHEN Result = '1/2-1/2' THEN 1 ELSE 0 END AS draw\n",
    "        FROM games\n",
    "        WHERE WhiteElo >= {config.min_elo} AND WhiteElo <= {config.max_elo}\n",
    "        UNION ALL\n",
    "        SELECT Black AS player, BlackElo AS elo,\n",
    "            CASE WHEN Result = '0-1' THEN 1 ELSE 0 END AS win,\n",
    "            CASE WHEN Result = '1-0' THEN 1 ELSE 0 END AS loss,\n",
    "            CASE WHEN Result = '1/2-1/2' THEN 1 ELSE 0 END AS draw\n",
    "        FROM games\n",
    "        WHERE BlackElo >= {config.min_elo} AND BlackElo <= {config.max_elo}\n",
    "    )\n",
    "    SELECT\n",
    "        player,\n",
    "        COUNT(*) as games_played,\n",
    "        SUM(win) as wins,\n",
    "        SUM(loss) as losses,\n",
    "        SUM(draw) as draws\n",
    "    FROM players\n",
    "    GROUP BY player\n",
    "    LIMIT {config.max_games_per_file};\n",
    "    \"\"\"\n",
    "    \n",
    "    results = con.execute(query).fetchall()\n",
    "    con.close()\n",
    "\n",
    "    games_in_file = 0\n",
    "    for name, games, wins, losses, draws in results:\n",
    "        if name not in player_stats:\n",
    "            player_stats[name] = PlayerStats()\n",
    "        \n",
    "        player_stats[name].games_played += games\n",
    "        player_stats[name].wins += wins\n",
    "        player_stats[name].losses += losses\n",
    "        player_stats[name].draws += draws\n",
    "        games_in_file += games\n",
    "    \n",
    "    perf_tracker.record_game_batch(games_in_file)\n",
    "    return len(results) > 0 # Return True if any data was processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091bc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Flow for Processing ---\n",
    "\n",
    "def process_and_delete_files():\n",
    "    # 1. Load existing data\n",
    "    if os.path.exists(proc_config.player_stats_output_path):\n",
    "        with open(proc_config.player_stats_output_path, \"rb\") as f:\n",
    "            player_stats: Dict[str, PlayerStats] = pickle.load(f)\n",
    "        print(f\"Loaded {len(player_stats)} players from existing stats file.\")\n",
    "    else:\n",
    "        player_stats: Dict[str, PlayerStats] = {}\n",
    "        print(\"No existing player stats file found. Starting fresh.\")\n",
    "\n",
    "    file_registry = FileRegistry(proc_config.file_registry_path)\n",
    "    print(f\"Loaded {len(file_registry.processed_files)} processed files from registry.\")\n",
    "\n",
    "    # 2. Find files to process\n",
    "    try:\n",
    "        all_files = [f for f in os.listdir(proc_config.input_dir) if f.endswith(\".parquet\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input directory not found: {proc_config.input_dir}\")\n",
    "        return\n",
    "\n",
    "    files_to_process = [f for f in all_files if not file_registry.contains(f)]\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"\\nNo new files to process. Everything is up to date.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(files_to_process)} new parquet file(s) to process.\")\n",
    "\n",
    "    # 3. Process each file\n",
    "    tracker = PerformanceTracker()\n",
    "    total_files_to_process = len(files_to_process)\n",
    "\n",
    "    for i, filename in enumerate(files_to_process):\n",
    "        filepath = os.path.join(proc_config.input_dir, filename)\n",
    "        print(f\"[{i+1}/{total_files_to_process}] Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            file_start_time = time.time()\n",
    "            processed_ok = process_parquet_file(filepath, proc_config, player_stats, tracker)\n",
    "            \n",
    "            if processed_ok:\n",
    "                # Add to registry\n",
    "                file_registry.add(filename)\n",
    "                tracker.record_file()\n",
    "                \n",
    "                # Save cumulative stats\n",
    "                with open(proc_config.player_stats_output_path, \"wb\") as f:\n",
    "                    pickle.dump(player_stats, f)\n",
    "                \n",
    "                file_duration = time.time() - file_start_time\n",
    "                print(f\"  > Success ({file_duration:.1f}s). {tracker.summary(total_files_to_process)}\")\n",
    "\n",
    "                # Delete file if toggle is on\n",
    "                if is_delete_downloaded_parquet_files:\n",
    "                    try:\n",
    "                        os.remove(filepath)\n",
    "                        print(f\"  > DELETED: {filename}\")\n",
    "                    except OSError as e:\n",
    "                        print(f\"  > ERROR: Failed to delete {filename}: {e}\")\n",
    "            else:\n",
    "                print(\"  > SKIPPED: No relevant player data found in file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  > ERROR processing {filename}: {e}\")\n",
    "            print(\"    Skipping this file and continuing...\")\n",
    "            continue\n",
    "            \n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Total players with stats: {len(player_stats)}\")\n",
    "    print(f\"Player stats saved to: {proc_config.player_stats_output_path}\")\n",
    "    print(f\"File registry saved to: {proc_config.file_registry_path}\")\n",
    "\n",
    "# Run the processing and deletion flow\n",
    "process_and_delete_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86b2a8",
   "metadata": {},
   "source": [
    "# 2. Process Downloaded Files\n",
    "\n",
    "The following cells contain the logic from notebook `03_parquet_performance.ipynb` to process the files we just downloaded.\n",
    "\n",
    "This involves:\n",
    "- Scanning the download directory for all `.parquet` files.\n",
    "- Processing each file to extract player statistics.\n",
    "- Storing the aggregated statistics in a pickle file.\n",
    "- Keeping a registry of processed files to avoid duplicate work.\n",
    "- Deleting the raw parquet file after it has been successfully processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Set\n",
    "\n",
    "# --- Processing Configuration ---\n",
    "is_delete_downloaded_parquet_files = True  # <<< IMPORTANT TOGGLE\n",
    "\n",
    "@dataclass\n",
    "class PlayerStats:\n",
    "    # Using a condensed version for brevity in this example\n",
    "    games_played: int = 0\n",
    "    wins: int = 0\n",
    "    losses: int = 0\n",
    "    draws: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    input_dir: str\n",
    "    player_stats_output_path: str\n",
    "    file_registry_path: str\n",
    "    min_elo: int = 2000\n",
    "    max_elo: int = 4000 # No practical upper limit\n",
    "    max_games_per_file: int = 1_000_000 # Effectively unlimited\n",
    "\n",
    "# Use the output_dir from the download config\n",
    "proc_config = ProcessingConfig(\n",
    "    input_dir=config[\"output_dir\"],\n",
    "    player_stats_output_path=str(project_root / \"data\" / \"processed\" / \"player_stats_parquet.pkl\"),\n",
    "    file_registry_path=str(project_root / \"data\" / \"processed\" / \"file_registry.json\"),\n",
    ")\n",
    "\n",
    "# --- Performance & Progress Tracking ---\n",
    "class PerformanceTracker:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.games_processed = 0\n",
    "        self.files_processed = 0\n",
    "\n",
    "    def record_game_batch(self, count):\n",
    "        self.games_processed += count\n",
    "\n",
    "    def record_file(self):\n",
    "        self.files_processed += 1\n",
    "\n",
    "    def summary(self, total_files):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        games_per_sec = self.games_processed / elapsed if elapsed > 0 else 0\n",
    "        files_remaining = total_files - self.files_processed\n",
    "        time_per_file = elapsed / self.files_processed if self.files_processed > 0 else 0\n",
    "        eta = files_remaining * time_per_file\n",
    "        \n",
    "        return (\n",
    "            f\"Processed {self.games_processed:,} games from {self.files_processed}/{total_files} files. \"\n",
    "            f\"({games_per_sec:,.0f} games/sec). ETA: {eta/60:.1f} mins.\"\n",
    "        )\n",
    "\n",
    "print(\"Processing configuration loaded.\")\n",
    "if is_delete_downloaded_parquet_files:\n",
    "    print(\"NOTE: Raw parquet files will be DELETED after successful processing.\")\n",
    "else:\n",
    "    print(\"NOTE: Raw parquet files will be KEPT after processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Processing a Single File ---\n",
    "\n",
    "def process_parquet_file(filepath: str, config: ProcessingConfig, player_stats: Dict[str, PlayerStats], perf_tracker: PerformanceTracker):\n",
    "    \"\"\"\n",
    "    Processes a single parquet file, updating player_stats dictionary.\n",
    "    This is the core logic from 03_parquet_performance.ipynb.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(database=':memory:', read_only=False)\n",
    "    \n",
    "    create_view_sql = f\"\"\"\n",
    "    CREATE VIEW games AS SELECT * FROM read_parquet('{filepath}');\n",
    "    \"\"\"\n",
    "    con.execute(create_view_sql)\n",
    "\n",
    "    # Query to get player game counts\n",
    "    query = f\"\"\"\n",
    "    WITH players AS (\n",
    "        SELECT White AS player, WhiteElo AS elo,\n",
    "            CASE WHEN Result = '1-0' THEN 1 ELSE 0 END AS win,\n",
    "            CASE WHEN Result = '0-1' THEN 1 ELSE 0 END AS loss,\n",
    "            CASE WHEN Result = '1/2-1/2' THEN 1 ELSE 0 END AS draw\n",
    "        FROM games\n",
    "        WHERE WhiteElo >= {config.min_elo} AND WhiteElo <= {config.max_elo}\n",
    "        UNION ALL\n",
    "        SELECT Black AS player, BlackElo AS elo,\n",
    "            CASE WHEN Result = '0-1' THEN 1 ELSE 0 END AS win,\n",
    "            CASE WHEN Result = '1-0' THEN 1 ELSE 0 END AS loss,\n",
    "            CASE WHEN Result = '1/2-1/2' THEN 1 ELSE 0 END AS draw\n",
    "        FROM games\n",
    "        WHERE BlackElo >= {config.min_elo} AND BlackElo <= {config.max_elo}\n",
    "    )\n",
    "    SELECT\n",
    "        player,\n",
    "        COUNT(*) as games_played,\n",
    "        SUM(win) as wins,\n",
    "        SUM(loss) as losses,\n",
    "        SUM(draw) as draws\n",
    "    FROM players\n",
    "    GROUP BY player\n",
    "    LIMIT {config.max_games_per_file};\n",
    "    \"\"\"\n",
    "    \n",
    "    results = con.execute(query).fetchall()\n",
    "    con.close()\n",
    "\n",
    "    games_in_file = 0\n",
    "    for name, games, wins, losses, draws in results:\n",
    "        if name not in player_stats:\n",
    "            player_stats[name] = PlayerStats()\n",
    "        \n",
    "        player_stats[name].games_played += games\n",
    "        player_stats[name].wins += wins\n",
    "        player_stats[name].losses += losses\n",
    "        player_stats[name].draws += draws\n",
    "        games_in_file += games\n",
    "    \n",
    "    perf_tracker.record_game_batch(games_in_file)\n",
    "    return len(results) > 0 # Return True if any data was processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d095b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Flow for Processing ---\n",
    "\n",
    "def process_and_delete_files():\n",
    "    # 1. Load existing data\n",
    "    if os.path.exists(proc_config.player_stats_output_path):\n",
    "        with open(proc_config.player_stats_output_path, \"rb\") as f:\n",
    "            player_stats: Dict[str, PlayerStats] = pickle.load(f)\n",
    "        print(f\"Loaded {len(player_stats)} players from existing stats file.\")\n",
    "    else:\n",
    "        player_stats: Dict[str, PlayerStats] = {}\n",
    "        print(\"No existing player stats file found. Starting fresh.\")\n",
    "\n",
    "    file_registry = FileRegistry(proc_config.file_registry_path)\n",
    "    print(f\"Loaded {len(file_registry.processed_files)} processed files from registry.\")\n",
    "\n",
    "    # 2. Find files to process\n",
    "    try:\n",
    "        all_files = [f for f in os.listdir(proc_config.input_dir) if f.endswith(\".parquet\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input directory not found: {proc_config.input_dir}\")\n",
    "        return\n",
    "\n",
    "    files_to_process = [f for f in all_files if not file_registry.contains(f)]\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"\\nNo new files to process. Everything is up to date.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(files_to_process)} new parquet file(s) to process.\")\n",
    "\n",
    "    # 3. Process each file\n",
    "    tracker = PerformanceTracker()\n",
    "    total_files_to_process = len(files_to_process)\n",
    "\n",
    "    for i, filename in enumerate(files_to_process):\n",
    "        filepath = os.path.join(proc_config.input_dir, filename)\n",
    "        print(f\"[{i+1}/{total_files_to_process}] Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            file_start_time = time.time()\n",
    "            processed_ok = process_parquet_file(filepath, proc_config, player_stats, tracker)\n",
    "            \n",
    "            if processed_ok:\n",
    "                # Add to registry\n",
    "                file_registry.add(filename)\n",
    "                tracker.record_file()\n",
    "                \n",
    "                # Save cumulative stats\n",
    "                with open(proc_config.player_stats_output_path, \"wb\") as f:\n",
    "                    pickle.dump(player_stats, f)\n",
    "                \n",
    "                file_duration = time.time() - file_start_time\n",
    "                print(f\"  > Success ({file_duration:.1f}s). {tracker.summary(total_files_to_process)}\")\n",
    "\n",
    "                # Delete file if toggle is on\n",
    "                if is_delete_downloaded_parquet_files:\n",
    "                    try:\n",
    "                        os.remove(filepath)\n",
    "                        print(f\"  > DELETED: {filename}\")\n",
    "                    except OSError as e:\n",
    "                        print(f\"  > ERROR: Failed to delete {filename}: {e}\")\n",
    "            else:\n",
    "                print(\"  > SKIPPED: No relevant player data found in file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  > ERROR processing {filename}: {e}\")\n",
    "            print(\"    Skipping this file and continuing...\")\n",
    "            continue\n",
    "            \n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Total players with stats: {len(player_stats)}\")\n",
    "    print(f\"Player stats saved to: {proc_config.player_stats_output_path}\")\n",
    "    print(f\"File registry saved to: {proc_config.file_registry_path}\")\n",
    "\n",
    "# Run the processing and deletion flow\n",
    "process_and_delete_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bd178",
   "metadata": {},
   "source": [
    "# 2. Process Downloaded Files\n",
    "\n",
    "The following cells contain the logic from notebook `03_parquet_performance.ipynb` to process the files we just downloaded.\n",
    "\n",
    "This involves:\n",
    "- Scanning the download directory for all `.parquet` files.\n",
    "- Processing each file to extract player statistics.\n",
    "- Storing the aggregated statistics in a pickle file.\n",
    "- Keeping a registry of processed files to avoid duplicate work.\n",
    "- Deleting the raw parquet file after it has been successfully processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "# --- Processing Configuration ---\n",
    "is_delete_downloaded_parquet_files = True  # <<< IMPORTANT TOGGLE\n",
    "\n",
    "@dataclass\n",
    "class PlayerStats:\n",
    "    # ... (rest of the class definition)\n",
    "    # Using a condensed version for brevity in this example\n",
    "    games_played: int = 0\n",
    "    wins: int = 0\n",
    "    losses: int = 0\n",
    "    draws: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    input_dir: str\n",
    "    player_stats_output_path: str\n",
    "    file_registry_path: str\n",
    "    min_elo: int = 2000\n",
    "    max_elo: int = 4000 # No practical upper limit\n",
    "    max_games_per_file: int = 1_000_000 # Effectively unlimited\n",
    "\n",
    "# Use the output_dir from the download config\n",
    "proc_config = ProcessingConfig(\n",
    "    input_dir=config[\"output_dir\"],\n",
    "    player_stats_output_path=str(project_root / \"data\" / \"processed\" / \"player_stats_parquet.pkl\"),\n",
    "    file_registry_path=str(project_root / \"data\" / \"processed\" / \"file_registry.json\"),\n",
    ")\n",
    "\n",
    "# --- Performance & Progress Tracking ---\n",
    "class PerformanceTracker:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.games_processed = 0\n",
    "        self.files_processed = 0\n",
    "\n",
    "    def record_game_batch(self, count):\n",
    "        self.games_processed += count\n",
    "\n",
    "    def record_file(self):\n",
    "        self.files_processed += 1\n",
    "\n",
    "    def summary(self, total_files):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        games_per_sec = self.games_processed / elapsed if elapsed > 0 else 0\n",
    "        files_remaining = total_files - self.files_processed\n",
    "        time_per_file = elapsed / self.files_processed if self.files_processed > 0 else 0\n",
    "        eta = files_remaining * time_per_file\n",
    "        \n",
    "        return (\n",
    "            f\"Processed {self.games_processed:,} games from {self.files_processed}/{total_files} files. \"\n",
    "            f\"({games_per_sec:,.0f} games/sec). ETA: {eta/60:.1f} mins.\"\n",
    "        )\n",
    "\n",
    "class FileRegistry:  # noqa: F811\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.processed_files: Set[str] = self._load()\n",
    "\n",
    "    def _load(self) -> Set[str]:\n",
    "        if os.path.exists(self.path):\n",
    "            with open(self.path, \"r\") as f:\n",
    "                return set(json.load(f))\n",
    "        return set()\n",
    "\n",
    "    def add(self, filename: str):\n",
    "        self.processed_files.add(filename)\n",
    "        self._save()\n",
    "\n",
    "    def contains(self, filename: str) -> bool:\n",
    "        return filename in self.processed_files\n",
    "\n",
    "    def _save(self):\n",
    "        with open(self.path, \"w\") as f:\n",
    "            json.dump(list(self.processed_files), f, indent=4)\n",
    "\n",
    "print(\"Processing configuration loaded.\")\n",
    "if is_delete_downloaded_parquet_files:\n",
    "    print(\"NOTE: Raw parquet files will be DELETED after successful processing.\")\n",
    "else:\n",
    "    print(\"NOTE: Raw parquet files will be KEPT after processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffe6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Processing a Single File ---\n",
    "\n",
    "def process_parquet_file(filepath: str, config: ProcessingConfig, player_stats: Dict[str, PlayerStats], perf_tracker: PerformanceTracker):\n",
    "    \"\"\"\n",
    "    Processes a single parquet file, updating player_stats dictionary.\n",
    "    This is the core logic from 03_parquet_performance.ipynb.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(database=':memory:', read_only=False)\n",
    "    \n",
    "    create_view_sql = f\"\"\"\n",
    "    CREATE VIEW games AS SELECT * FROM read_parquet('{filepath}');\n",
    "    \"\"\"\n",
    "    con.execute(create_view_sql)\n",
    "\n",
    "    # Query to get player game counts\n",
    "    query = f\"\"\"\n",
    "    WITH players AS (\n",
    "        SELECT White AS player, WhiteElo AS elo,\n",
    "            CASE WHEN Result = '1-0' THEN 1 ELSE 0 END AS win,\n",
    "            CASE WHEN Result = '0-1' THEN 1 ELSE 0 END AS loss,\n",
    "            CASE WHEN Result = '1/2-1/2' THEN 1 ELSE 0 END AS draw\n",
    "        FROM games\n",
    "        WHERE WhiteElo >= {config.min_elo} AND WhiteElo <= {config.max_elo}\n",
    "        UNION ALL\n",
    "        SELECT Black AS player, BlackElo AS elo,\n",
    "            CASE WHEN Result = '0-1' THEN 1 ELSE 0 END AS win,\n",
    "            CASE WHEN Result = '1-0' THEN 1 ELSE 0 END AS loss,\n",
    "            CASE WHEN Result = '1/2-1/2' THEN 1 ELSE 0 END AS draw\n",
    "        FROM games\n",
    "        WHERE BlackElo >= {config.min_elo} AND BlackElo <= {config.max_elo}\n",
    "    )\n",
    "    SELECT\n",
    "        player,\n",
    "        COUNT(*) as games_played,\n",
    "        SUM(win) as wins,\n",
    "        SUM(loss) as losses,\n",
    "        SUM(draw) as draws\n",
    "    FROM players\n",
    "    GROUP BY player\n",
    "    LIMIT {config.max_games_per_file};\n",
    "    \"\"\"\n",
    "    \n",
    "    results = con.execute(query).fetchall()\n",
    "    con.close()\n",
    "\n",
    "    games_in_file = 0\n",
    "    for name, games, wins, losses, draws in results:\n",
    "        if name not in player_stats:\n",
    "            player_stats[name] = PlayerStats()\n",
    "        \n",
    "        player_stats[name].games_played += games\n",
    "        player_stats[name].wins += wins\n",
    "        player_stats[name].losses += losses\n",
    "        player_stats[name].draws += draws\n",
    "        games_in_file += games\n",
    "    \n",
    "    perf_tracker.record_game_batch(games_in_file)\n",
    "    return len(results) > 0 # Return True if any data was processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Flow for Processing ---\n",
    "\n",
    "def process_and_delete_files():\n",
    "    # 1. Load existing data\n",
    "    if os.path.exists(proc_config.player_stats_output_path):\n",
    "        with open(proc_config.player_stats_output_path, \"rb\") as f:\n",
    "            player_stats: Dict[str, PlayerStats] = pickle.load(f)\n",
    "        print(f\"Loaded {len(player_stats)} players from existing stats file.\")\n",
    "    else:\n",
    "        player_stats: Dict[str, PlayerStats] = {}\n",
    "        print(\"No existing player stats file found. Starting fresh.\")\n",
    "\n",
    "    file_registry = FileRegistry(proc_config.file_registry_path)\n",
    "    print(f\"Loaded {len(file_registry.processed_files)} processed files from registry.\")\n",
    "\n",
    "    # 2. Find files to process\n",
    "    try:\n",
    "        all_files = [f for f in os.listdir(proc_config.input_dir) if f.endswith(\".parquet\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input directory not found: {proc_config.input_dir}\")\n",
    "        return\n",
    "\n",
    "    files_to_process = [f for f in all_files if not file_registry.contains(f)]\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"\\nNo new files to process. Everything is up to date.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(files_to_process)} new parquet file(s) to process.\")\n",
    "\n",
    "    # 3. Process each file\n",
    "    tracker = PerformanceTracker()\n",
    "    total_files_to_process = len(files_to_process)\n",
    "\n",
    "    for i, filename in enumerate(files_to_process):\n",
    "        filepath = os.path.join(proc_config.input_dir, filename)\n",
    "        print(f\"[{i+1}/{total_files_to_process}] Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            file_start_time = time.time()\n",
    "            processed_ok = process_parquet_file(filepath, proc_config, player_stats, tracker)\n",
    "            \n",
    "            if processed_ok:\n",
    "                # Add to registry\n",
    "                file_registry.add(filename)\n",
    "                tracker.record_file()\n",
    "                \n",
    "                # Save cumulative stats\n",
    "                with open(proc_config.player_stats_output_path, \"wb\") as f:\n",
    "                    pickle.dump(player_stats, f)\n",
    "                \n",
    "                file_duration = time.time() - file_start_time\n",
    "                print(f\"  > Success ({file_duration:.1f}s). {tracker.summary(total_files_to_process)}\")\n",
    "\n",
    "                # Delete file if toggle is on\n",
    "                if is_delete_downloaded_parquet_files:\n",
    "                    try:\n",
    "                        os.remove(filepath)\n",
    "                        print(f\"  > DELETED: {filename}\")\n",
    "                    except OSError as e:\n",
    "                        print(f\"  > ERROR: Failed to delete {filename}: {e}\")\n",
    "            else:\n",
    "                print(\"  > SKIPPED: No relevant player data found in file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  > ERROR processing {filename}: {e}\")\n",
    "            print(\"    Skipping this file and continuing...\")\n",
    "            continue\n",
    "            \n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Total players with stats: {len(player_stats)}\")\n",
    "    print(f\"Player stats saved to: {proc_config.player_stats_output_path}\")\n",
    "    print(f\"File registry saved to: {proc_config.file_registry_path}\")\n",
    "\n",
    "# Run the processing and deletion flow\n",
    "process_and_delete_files()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
