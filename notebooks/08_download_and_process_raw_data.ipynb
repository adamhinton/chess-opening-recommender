{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b349557",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook combines several things we've already been doing individually.\n",
    "\n",
    "Its purpose is to obtain download massive amounts of raw chess game data, process it, then delete the downloaded raw files.\n",
    "\n",
    "## 1. Download raw data\n",
    "\n",
    "This will download raw parquet files from HuggingFace's API.\n",
    "\n",
    "Parquet files are 1GB files containing huge sets of Lichess games.\n",
    "\n",
    "This notebook downloads all of the parquet files for a certain month (usually 60-70 files). They will be automatically deleted after they're processed.\n",
    "\n",
    "## 2. Process the data\n",
    "\n",
    "For each parquet file that is downloaded, this notebook will process the games in the file one by one, extracting useful data for  training our model.\n",
    "\n",
    "## 3. Delete the data\n",
    "\n",
    "After each parquet file is done being processed, it will be deleted from the local disk.\n",
    "\n",
    "These files are 1GB each and we will be downloading hundreds or thousands of them, so deleting them is a must.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32444c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine paths to ensure imports work correctly\n",
    "current_file = Path.cwd()\n",
    "project_root = current_file.parent  # Move up to the project root\n",
    "\n",
    "# Add both to path to ensure imports work regardless of structure\n",
    "if str(current_file) not in sys.path:\n",
    "    sys.path.append(str(current_file))\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from notebooks.utils.file_processing.process_game_batch import (  # noqa: E402\n",
    "    process_batch,\n",
    ")  # noqa: E402\n",
    "\n",
    "# Now try the import with the explicit path\n",
    "from notebooks.utils.file_processing.types_and_classes import (  # noqa: E402\n",
    "    PlayerStats,\n",
    "    ProcessingConfig,\n",
    "    PerformanceTracker,\n",
    ")\n",
    "\n",
    "from notebooks.utils.file_processing.save_and_load_progress import (  # noqa: E402\n",
    "    save_progress,\n",
    "    load_progress,\n",
    ")\n",
    "\n",
    "\n",
    "from notebooks.utils.file_processing.raw_data_file_dupe_checks import (  # noqa: E402\n",
    "    FileRegistry,\n",
    ")  # noqa: E402\n",
    "\n",
    "# Import necessary libraries\n",
    "from typing import Dict, Optional  # noqa: E402\n",
    "import duckdb  # noqa: E402\n",
    "import time  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "import pickle  # noqa: E402\n",
    "import sys  # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87c63e",
   "metadata": {},
   "source": [
    "# 1.  Auto-Download Parquet Files\n",
    "\n",
    "## Wifi Speed\n",
    "\n",
    "My personal Wifi speed is quite fast (350 mbps); This is a lot of GBs of data, but download speed isn't the bottleneck because my system will take longer to process each file than it will to download the next one. But, if you have slower download speed, you may need to adjust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc6bbd",
   "metadata": {},
   "source": [
    "## Helper Functions for downloading raw data\n",
    "\n",
    "Getting helper functions that we're defined elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24423bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.downloading_raw_parquet_data.api_interaction import (\n",
    "    get_urls_from_hub_api,\n",
    "    get_urls_from_dataset_viewer,\n",
    "    filter_urls_for_month\n",
    ")\n",
    "from utils.downloading_raw_parquet_data.file_downloader import (\n",
    "    probe_fallback_urls,\n",
    "    download_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db45fe",
   "metadata": {},
   "source": [
    "## Processing and Deletion Configuration\n",
    "\n",
    "Here we define configurations for processing the raw data that we will download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d63f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONFIG FOR PROCESSING\n",
    "# ---------------------\n",
    "# This dictionary holds all the settings for the data processing part of the notebook.\n",
    "\n",
    "# --- Deletion Toggle ---\n",
    "# Set this to True to automatically delete parquet files after they are successfully processed.\n",
    "# Set it to False if you want to keep the raw files for repeated experiments.\n",
    "is_delete_downloaded_parquet_files = True\n",
    "\n",
    "# --- Processing Parameters ---\n",
    "# These parameters are used to filter the chess games from the raw files.\n",
    "# We only want to process games that are relevant and high-quality for our model.\n",
    "processing_config = {\n",
    "    # The directory where processed data and progress files will be saved.\n",
    "    \"save_dir\": str(project_root / \"data\" / \"processed\"),\n",
    "    \n",
    "    # The number of games to process in a single batch. This is optimized based on available memory.\n",
    "    \"batch_size\": 100_000,\n",
    "    \n",
    "    # How often to save progress (e.g., every 1 batch).\n",
    "    \"save_interval\": 1,\n",
    "    \n",
    "    # --- Game Filtering Rules ---\n",
    "    # Minimum rating for players in a game to be included.\n",
    "    \"min_player_rating\": 1200,\n",
    "    \n",
    "    # Maximum ELO difference between the two players.\n",
    "    \"max_elo_difference\": 100,\n",
    "    \n",
    "    # Allowed time controls (e.g., \"Blitz\", \"Rapid\"). Games with other time controls will be skipped.\n",
    "    \"allowed_time_controls\": {\"Blitz\", \"Rapid\", \"Classical\"},\n",
    "}\n",
    "\n",
    "# Create the directory for processed data if it doesn't exist.\n",
    "Path(processing_config[\"save_dir\"]).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf889cd",
   "metadata": {},
   "source": [
    "## Configuration for Downloading Raw Data\n",
    "\n",
    "Here we will organize some config items for the downloading of raw data parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2850c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "project_root = (\n",
    "    Path(__file__).resolve().parent.parent\n",
    "    if \"__file__\" in globals()\n",
    "    else Path.cwd().parent\n",
    ")\n",
    "output_dir = str(project_root / \"data\" / \"raw\" / \"auto_download_parquets\")\n",
    "\n",
    "# CONFIG\n",
    "# I did my best to put all of the variables that we might need to adjust here.\n",
    "# This is a proof of concept right now; later we may make this some sort of drop down picker for month, year etc\n",
    "config = {\n",
    "    \"repo\": \"Lichess/standard-chess-games\",  # Hugging Face repo id\n",
    "    \"year\": \"2025\",  # 4-digit year (string or int)\n",
    "    \"month\": \"7\",  # numeric month (e.g., \"7\" or \"07\")\n",
    "    \"max_parquets\": 30,  # int or None to download all available\n",
    "    # Download to /data/raw/auto_download_parquets relative to project root\n",
    "    \"output_dir\": output_dir,\n",
    "    \"hf_token\": None,  # set to your HF token string if you need to access gated datasets\n",
    "    \"probe_max_attempts\": 1000,  # for fallback probing\n",
    "    \"probe_patterns\": [  # tried in order if APIs gave no URLs\n",
    "        # Pattern A: common \"train-00000-of-00066.parquet\" style\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}-of-{total:05d}.parquet\",\n",
    "        # Pattern B: some datasets use plain shard names\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/train-{idx:05d}.parquet\",\n",
    "        # Pattern C: fall back to zero-padded 4-digit name\n",
    "        \"https://huggingface.co/datasets/{repo}/resolve/main/data/year={year}/month={month}/000{idx}.parquet\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Extract config variables for use in the rest of the notebook\n",
    "repo = config[\"repo\"]\n",
    "year = str(config[\"year\"])\n",
    "month_raw = str(config[\"month\"])\n",
    "month_padded = month_raw.zfill(2)\n",
    "max_parquets = config[\"max_parquets\"]\n",
    "out_dir = Path(config[\"output_dir\"])\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "hf_headers = {\"Authorization\": f\"Bearer {config['hf_token']}\"} if config.get(\"hf_token\") else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7f94f",
   "metadata": {},
   "source": [
    "## Main Raw Data Downloading Execution Flow\n",
    "\n",
    "The following cell contains the main logic for querying parquet URLs, filtering them, and downloading the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8aea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for parquet URLs...\n",
      "No URLs found via APIs, falling back to probing...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo URLs found via APIs, falling back to probing...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     patterns = config.get(\u001b[33m\"\u001b[39m\u001b[33mprobe_patterns\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     filtered = \u001b[43mprobe_fallback_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprobe_max_attempts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mERROR: No parquet URLs discovered. Aborting.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/notebooks/utils/downloading_raw_parquet_data/file_downloader.py:44\u001b[39m, in \u001b[36mprobe_fallback_urls\u001b[39m\u001b[34m(repo, year, month, max_attempts, patterns, hf_headers)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_attempts):\n\u001b[32m     41\u001b[39m     url = pattern.format(\n\u001b[32m     42\u001b[39m         repo=repo, year=year, month=month, idx=idx, total=total_guess\n\u001b[32m     43\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtry_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     45\u001b[39m         found.append(url)\n\u001b[32m     46\u001b[39m         consecutive_not_found = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/notebooks/utils/downloading_raw_parquet_data/file_downloader.py:15\u001b[39m, in \u001b[36mtry_head\u001b[39m\u001b[34m(url, hf_headers, timeout)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Quick HEAD-ish check (GET with stream and immediate close) to see if URL exists.\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m r.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     17\u001b[39m         r.raw.read(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/requests/sessions.py:724\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/requests/sessions.py:265\u001b[39m, in \u001b[36mSessionRedirectMixin.resolve_redirects\u001b[39m\u001b[34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m.cookies, prepared_request, resp.raw)\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/personalprojects/chess-opening-recommender/.venv/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# This cell downloads parquet files from a specified Hugging Face repository\n",
    "# for a given month and year, processes them, and optionally deletes them after processing.\n",
    "\n",
    "import urllib\n",
    "\n",
    "# Main flow for downloading files\n",
    "print(\"Querying for parquet URLs...\")\n",
    "urls = get_urls_from_hub_api(repo, hf_headers)\n",
    "if not urls:\n",
    "    urls = get_urls_from_dataset_viewer(repo, hf_headers)\n",
    "\n",
    "filtered = filter_urls_for_month(urls, year, month_padded)\n",
    "\n",
    "if not filtered:\n",
    "    print(\"No URLs found via APIs, falling back to probing...\")\n",
    "    patterns = config.get(\"probe_patterns\", [])\n",
    "    filtered = probe_fallback_urls(repo, year, month_padded, config[\"probe_max_attempts\"], patterns, hf_headers)\n",
    "\n",
    "if not filtered:\n",
    "    print(\"ERROR: No parquet URLs discovered. Aborting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if max_parquets is not None:\n",
    "    filtered = filtered[:int(max_parquets)]\n",
    "\n",
    "print(f\"\\nWill attempt to download {len(filtered)} file(s) into {out_dir.resolve()}\\n\")\n",
    "\n",
    "success_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, url in enumerate(filtered):\n",
    "    filename = Path(urllib.parse.unquote(url)).name\n",
    "    dest = out_dir / filename\n",
    "    \n",
    "    if dest.exists():\n",
    "        print(f\"[{i+1}/{len(filtered)}] SKIPPING (exists): {filename}\")\n",
    "        success_count += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{i+1}/{len(filtered)}] DOWNLOADING: {filename}\")\n",
    "    ok = download_file(url, dest, hf_headers)\n",
    "    \n",
    "    if ok:\n",
    "        success_count += 1\n",
    "        print(f\"  SUCCESS: Downloaded {filename}\")\n",
    "    else:\n",
    "        print(f\"  FAILURE: Could not download {filename}\")\n",
    "        if not urls: # If we were probing, a failure means we stop.\n",
    "            print(\"  Stopping probe-based download.\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nDownload phase complete. {success_count} of {len(filtered)} files are ready for processing.\")\n",
    "print(f\"Total download time: {total_time / 60:.2f} minutes.\")\n",
    "\n",
    "# --- Part 2: Process and Delete Files ---\n",
    "print(\"\\n--- Starting Processing and Deletion Phase ---\")\n",
    "\n",
    "# Set up the base configuration for processing.\n",
    "# This will be used for each file that gets processed.\n",
    "base_processing_config = ProcessingConfig(\n",
    "    parquet_path=\"\",  # This will be updated for each file.\n",
    "    batch_size=processing_config[\"batch_size\"],\n",
    "    save_interval=processing_config[\"save_interval\"],\n",
    "    save_dir=processing_config[\"save_dir\"],\n",
    "    min_player_rating=processing_config[\"min_player_rating\"],\n",
    "    max_elo_difference_between_players=processing_config[\"max_elo_difference\"],\n",
    "    allowed_time_controls=processing_config[\"allowed_time_controls\"]\n",
    ")\n",
    "\n",
    "# Call the main function to process all files in the directory.\n",
    "process_and_delete_files(\n",
    "    base_config=base_processing_config,\n",
    "    delete_files=is_delete_downloaded_parquet_files\n",
    ")\n",
    "\n",
    "print(\"\\n--- All Done! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974ae3b",
   "metadata": {},
   "source": [
    "## Processing Helper Functions\n",
    "\n",
    "The following functions are taken from `03_parquet_performance.ipynb` and are responsible for processing the raw parquet files. They handle batching, duplicate checking, and performance tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18420048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_file(config: ProcessingConfig, \n",
    "                         players_data: Dict[str, PlayerStats],\n",
    "                         log_frequency: int = 5000, \n",
    "                         file_context: Optional[Dict] = None) -> bool:\n",
    "    \"\"\"\n",
    "    Process a single parquet file in batches, updating a shared players_data dictionary.\n",
    "    This function is a direct copy from 03_parquet_performance.ipynb.\n",
    "    \n",
    "    Args:\n",
    "        config: Processing configuration for this specific file.\n",
    "        players_data: The shared dictionary of player statistics to update.\n",
    "        log_frequency: How often to log progress within a batch.\n",
    "        file_context: Dictionary with context for multi-file processing.\n",
    "        \n",
    "    Returns:\n",
    "        True if processing was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize DuckDB connection\n",
    "        con = duckdb.connect()\n",
    "        \n",
    "        # Reset progress file for a new run to avoid conflicts.\n",
    "        progress_path = Path(config.save_dir) / \"processing_progress_parquet.json\"\n",
    "        if progress_path.exists():\n",
    "            progress_path.unlink()\n",
    "\n",
    "        _, start_batch = load_progress(config)\n",
    "        perf_tracker = PerformanceTracker()\n",
    "        \n",
    "        total_rows = con.execute(f\"SELECT COUNT(*) FROM '{config.parquet_path}'\").fetchone()[0]\n",
    "        if total_rows == 0:\n",
    "            print(\"File is empty, skipping.\")\n",
    "            return True # Return True to allow deletion of empty file\n",
    "\n",
    "        total_batches = (total_rows + config.batch_size - 1) // config.batch_size\n",
    "        print(f\"Will process {total_rows:,} rows in {total_batches} batches of size {config.batch_size:,}\")\n",
    "        \n",
    "        batch_num = start_batch\n",
    "        while batch_num * config.batch_size < total_rows:\n",
    "            offset = batch_num * config.batch_size\n",
    "            print(f\"\\nProcessing batch {batch_num + 1}/{total_batches} (offset {offset:,})\")\n",
    "            perf_tracker.start_batch()\n",
    "            \n",
    "            batch_query = f\"SELECT * FROM '{config.parquet_path}' LIMIT {config.batch_size} OFFSET {offset}\"\n",
    "            batch_df = con.execute(batch_query).df()\n",
    "            \n",
    "            if batch_df.empty:\n",
    "                break\n",
    "\n",
    "            process_batch(batch_df, players_data, config, log_frequency, perf_tracker, file_context)\n",
    "            \n",
    "            batch_time = perf_tracker.end_batch(len(batch_df))\n",
    "            print(f\"Processed batch in {batch_time:.2f} seconds. Current player count: {len(players_data):,}\")\n",
    "            perf_tracker.log_progress(force=True)\n",
    "            \n",
    "            batch_num += 1\n",
    "            if batch_num % config.save_interval == 0:\n",
    "                save_progress(players_data, batch_num, config, perf_tracker)\n",
    "        \n",
    "        save_progress(players_data, batch_num, config, perf_tracker)\n",
    "        \n",
    "        summary = perf_tracker.get_summary()\n",
    "        print(\"\\nFile Processing Summary:\")\n",
    "        for key, value in summary.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file processing: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if 'con' in locals():\n",
    "            con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_delete_files(base_config: ProcessingConfig, delete_files: bool):\n",
    "    \"\"\"\n",
    "    Processes all .parquet files in the download directory and optionally deletes them.\n",
    "    This function scans the directory, checks for duplicates, processes new files,\n",
    "    and then deletes them if the toggle is enabled.\n",
    "    \"\"\"\n",
    "    # Load existing player data once at the beginning.\n",
    "    # This dictionary will be updated by all file processing jobs.\n",
    "    all_players_data, _ = load_progress(base_config)\n",
    "    print(f\"Loaded initial data. Total players so far: {len(all_players_data):,}\")\n",
    "    \n",
    "    registry = FileRegistry()\n",
    "    \n",
    "    # Get all parquet files from the download directory\n",
    "    files_to_process = list(out_dir.glob(\"*.parquet\"))\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"No parquet files found in the download directory to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(files_to_process)} parquet files to process.\")\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process):\n",
    "        print(f\"\\n--- Processing file {i+1}/{len(files_to_process)}: {file_path.name} ---\")\n",
    "        \n",
    "        # Check if the file has already been processed.\n",
    "        if registry.is_file_processed(str(file_path)):\n",
    "            print(\"File has already been processed. Skipping.\")\n",
    "            if delete_files:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted duplicate file: {file_path.name}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting duplicate file: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Create a specific configuration for this file.\n",
    "        file_config = ProcessingConfig(\n",
    "            parquet_path=str(file_path),\n",
    "            batch_size=base_config.batch_size,\n",
    "            save_interval=base_config.save_interval,\n",
    "            save_dir=base_config.save_dir,\n",
    "            min_player_rating=base_config.min_player_rating,\n",
    "            max_elo_difference_between_players=base_config.max_elo_difference_between_players,\n",
    "            allowed_time_controls=base_config.allowed_time_controls\n",
    "        )\n",
    "        \n",
    "        # Process the file. The `all_players_data` dict is updated in-place.\n",
    "        success = process_parquet_file(file_config, all_players_data)\n",
    "        \n",
    "        # If processing was successful, mark it in the registry and delete if enabled.\n",
    "        if success:\n",
    "            registry.mark_file_processed(str(file_path))\n",
    "            print(f\"Successfully processed and marked file: {file_path.name}\")\n",
    "            if delete_files:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted processed file: {file_path.name}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting processed file: {e}\")\n",
    "        else:\n",
    "            print(f\"Processing failed for {file_path.name}. The file will not be deleted.\")\n",
    "\n",
    "    # Save the final, combined data from all files.\n",
    "    final_save_path = Path(base_config.save_dir) / \"all_players_stats_combined.pkl\"\n",
    "    with open(final_save_path, 'wb') as f:\n",
    "        pickle.dump(all_players_data, f)\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Final combined data for {len(all_players_data):,} players saved to: {final_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607ea53",
   "metadata": {},
   "source": [
    "# 2. Process Downloaded Files\n",
    "\n",
    "The following cells contain the logic from notebook `03_parquet_performance.ipynb` to process the files we just downloaded.\n",
    "\n",
    "This involves:\n",
    "- Scanning the download directory for all `.parquet` files.\n",
    "- Processing each file to extract player statistics.\n",
    "- Storing the aggregated statistics in a pickle file.\n",
    "- Keeping a registry of processed files to avoid duplicate work.\n",
    "- Deleting the raw parquet file after it has been successfully processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a2aeed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProcessingConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m is_delete_downloaded_parquet_files = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# <<< IMPORTANT TOGGLE\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Use the output_dir from the download config\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m proc_config = \u001b[43mProcessingConfig\u001b[49m(\n\u001b[32m     17\u001b[39m     input_dir=config[\u001b[33m\"\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     18\u001b[39m     player_stats_output_path=\u001b[38;5;28mstr\u001b[39m(project_root / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mplayer_stats_parquet.pkl\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     19\u001b[39m     file_registry_path=\u001b[38;5;28mstr\u001b[39m(project_root / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mfile_registry.json\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'ProcessingConfig' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Set\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- Processing Configuration ---\n",
    "\n",
    "# Toggle to enable/disable deletion of parquet files after processing\n",
    "# This should be set to True unless you want to keep the raw files for repeated experiments.\n",
    "# Setting it to False will retain all downloaded parquet files, whereas if it's True they will be deleted after processing because you don't need them anymore.\n",
    "is_delete_downloaded_parquet_files = True  # <<< IMPORTANT TOGGLE\n",
    "\n",
    "\n",
    "# Use the output_dir from the download config\n",
    "proc_config = ProcessingConfig(\n",
    "    input_dir=config[\"output_dir\"],\n",
    "    player_stats_output_path=str(project_root / \"data\" / \"processed\" / \"player_stats_parquet.pkl\"),\n",
    "    file_registry_path=str(project_root / \"data\" / \"processed\" / \"file_registry.json\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee24a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Processing a Single File ---\n",
    "\n",
    "def process_parquet_file(filepath: str, config: ProcessingConfig, player_stats: Dict[str, PlayerStats], perf_tracker: PerformanceTracker):\n",
    "    \"\"\"\n",
    "    Processes a single parquet file, updating player_stats dictionary.\n",
    "    This is the core logic from 03_parquet_performance.ipynb.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(database=':memory:', read_only=False)\n",
    "    \n",
    "    create_view_sql = f\"\"\"\n",
    "    CREATE VIEW games AS SELECT * FROM read_parquet('{filepath}');\n",
    "    \"\"\"\n",
    "    con.execute(create_view_sql)\n",
    "\n",
    "    # Query to get player game counts\n",
    "    query = f\"\"\"\n",
    "    WITH players AS (\n",
    "        SELECT White AS player, WhiteElo AS elo,\n",
    "            CASE WHEN Result = '1-0' THEN 1 ELSE 0 END AS win,\n",
    "            CASE WHEN Result = '0-1' THEN 1 ELSE 0 END AS loss,\n",
    "            CASE WHEN Result = '1/2-1/2' THEN 1 ELSE 0 END AS draw\n",
    "        FROM games\n",
    "        WHERE WhiteElo >= {config.min_elo} AND WhiteElo <= {config.max_elo}\n",
    "        UNION ALL\n",
    "        SELECT Black AS player, BlackElo AS elo,\n",
    "            CASE WHEN Result = '0-1' THEN 1 ELSE 0 END AS win,\n",
    "            CASE WHEN Result = '1-0' THEN 1 ELSE 0 END AS loss,\n",
    "            CASE WHEN Result = '1/2-1/2' THEN 1 ELSE 0 END AS draw\n",
    "        FROM games\n",
    "        WHERE BlackElo >= {config.min_elo} AND BlackElo <= {config.max_elo}\n",
    "    )\n",
    "    SELECT\n",
    "        player,\n",
    "        COUNT(*) as games_played,\n",
    "        SUM(win) as wins,\n",
    "        SUM(loss) as losses,\n",
    "        SUM(draw) as draws\n",
    "    FROM players\n",
    "    GROUP BY player\n",
    "    LIMIT {config.max_games_per_file};\n",
    "    \"\"\"\n",
    "    \n",
    "    results = con.execute(query).fetchall()\n",
    "    con.close()\n",
    "\n",
    "    games_in_file = 0\n",
    "    for name, games, wins, losses, draws in results:\n",
    "        if name not in player_stats:\n",
    "            player_stats[name] = PlayerStats()\n",
    "        \n",
    "        player_stats[name].games_played += games\n",
    "        player_stats[name].wins += wins\n",
    "        player_stats[name].losses += losses\n",
    "        player_stats[name].draws += draws\n",
    "        games_in_file += games\n",
    "    \n",
    "    perf_tracker.record_game_batch(games_in_file)\n",
    "    return len(results) > 0 # Return True if any data was processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091bc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Flow for Processing ---\n",
    "\n",
    "def process_and_delete_files():\n",
    "    # 1. Load existing data\n",
    "    if os.path.exists(proc_config.player_stats_output_path):\n",
    "        with open(proc_config.player_stats_output_path, \"rb\") as f:\n",
    "            player_stats: Dict[str, PlayerStats] = pickle.load(f)\n",
    "        print(f\"Loaded {len(player_stats)} players from existing stats file.\")\n",
    "    else:\n",
    "        player_stats: Dict[str, PlayerStats] = {}\n",
    "        print(\"No existing player stats file found. Starting fresh.\")\n",
    "\n",
    "    file_registry = FileRegistry(proc_config.file_registry_path)\n",
    "    print(f\"Loaded {len(file_registry.processed_files)} processed files from registry.\")\n",
    "\n",
    "    # 2. Find files to process\n",
    "    try:\n",
    "        all_files = [f for f in os.listdir(proc_config.input_dir) if f.endswith(\".parquet\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input directory not found: {proc_config.input_dir}\")\n",
    "        return\n",
    "\n",
    "    files_to_process = [f for f in all_files if not file_registry.contains(f)]\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"\\nNo new files to process. Everything is up to date.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(files_to_process)} new parquet file(s) to process.\")\n",
    "\n",
    "    # 3. Process each file\n",
    "    tracker = PerformanceTracker()\n",
    "    total_files_to_process = len(files_to_process)\n",
    "\n",
    "    for i, filename in enumerate(files_to_process):\n",
    "        filepath = os.path.join(proc_config.input_dir, filename)\n",
    "        print(f\"[{i+1}/{total_files_to_process}] Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            file_start_time = time.time()\n",
    "            processed_ok = process_parquet_file(filepath, proc_config, player_stats, tracker)\n",
    "            \n",
    "            if processed_ok:\n",
    "                # Add to registry\n",
    "                file_registry.add(filename)\n",
    "                tracker.record_file()\n",
    "                \n",
    "                # Save cumulative stats\n",
    "                with open(proc_config.player_stats_output_path, \"wb\") as f:\n",
    "                    pickle.dump(player_stats, f)\n",
    "                \n",
    "                file_duration = time.time() - file_start_time\n",
    "                print(f\"  > Success ({file_duration:.1f}s). {tracker.summary(total_files_to_process)}\")\n",
    "\n",
    "                # Delete file if toggle is on\n",
    "                if is_delete_downloaded_parquet_files:\n",
    "                    try:\n",
    "                        os.remove(filepath)\n",
    "                        print(f\"  > DELETED: {filename}\")\n",
    "                    except OSError as e:\n",
    "                        print(f\"  > ERROR: Failed to delete {filename}: {e}\")\n",
    "            else:\n",
    "                print(\"  > SKIPPED: No relevant player data found in file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  > ERROR processing {filename}: {e}\")\n",
    "            print(\"    Skipping this file and continuing...\")\n",
    "            continue\n",
    "            \n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Total players with stats: {len(player_stats)}\")\n",
    "    print(f\"Player stats saved to: {proc_config.player_stats_output_path}\")\n",
    "    print(f\"File registry saved to: {proc_config.file_registry_path}\")\n",
    "\n",
    "# Run the processing and deletion flow\n",
    "process_and_delete_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86b2a8",
   "metadata": {},
   "source": [
    "# 2. Process Downloaded Files\n",
    "\n",
    "The following cells contain the logic from notebook `03_parquet_performance.ipynb` to process the files we just downloaded.\n",
    "\n",
    "This involves:\n",
    "- Scanning the download directory for all `.parquet` files.\n",
    "- Processing each file to extract player statistics.\n",
    "- Storing the aggregated statistics in a pickle file.\n",
    "- Keeping a registry of processed files to avoid duplicate work.\n",
    "- Deleting the raw parquet file after it has been successfully processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# --- Processing Configuration ---\n",
    "is_delete_downloaded_parquet_files = True  # <<< IMPORTANT TOGGLE\n",
    "\n",
    "@dataclass\n",
    "class PlayerStats:\n",
    "    # Using a condensed version for brevity in this example\n",
    "    games_played: int = 0\n",
    "    wins: int = 0\n",
    "    losses: int = 0\n",
    "    draws: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    input_dir: str\n",
    "    player_stats_output_path: str\n",
    "    file_registry_path: str\n",
    "    min_elo: int = 2000\n",
    "    max_elo: int = 4000 # No practical upper limit\n",
    "    max_games_per_file: int = 1_000_000 # Effectively unlimited\n",
    "\n",
    "# Use the output_dir from the download config\n",
    "proc_config = ProcessingConfig(\n",
    "    input_dir=config[\"output_dir\"],\n",
    "    player_stats_output_path=str(project_root / \"data\" / \"processed\" / \"player_stats_parquet.pkl\"),\n",
    "    file_registry_path=str(project_root / \"data\" / \"processed\" / \"file_registry.json\"),\n",
    ")\n",
    "\n",
    "# --- Performance & Progress Tracking ---\n",
    "class PerformanceTracker:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.games_processed = 0\n",
    "        self.files_processed = 0\n",
    "\n",
    "    def record_game_batch(self, count):\n",
    "        self.games_processed += count\n",
    "\n",
    "    def record_file(self):\n",
    "        self.files_processed += 1\n",
    "\n",
    "    def summary(self, total_files):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        games_per_sec = self.games_processed / elapsed if elapsed > 0 else 0\n",
    "        files_remaining = total_files - self.files_processed\n",
    "        time_per_file = elapsed / self.files_processed if self.files_processed > 0 else 0\n",
    "        eta = files_remaining * time_per_file\n",
    "        \n",
    "        return (\n",
    "            f\"Processed {self.games_processed:,} games from {self.files_processed}/{total_files} files. \"\n",
    "            f\"({games_per_sec:,.0f} games/sec). ETA: {eta/60:.1f} mins.\"\n",
    "        )\n",
    "\n",
    "print(\"Processing configuration loaded.\")\n",
    "if is_delete_downloaded_parquet_files:\n",
    "    print(\"NOTE: Raw parquet files will be DELETED after successful processing.\")\n",
    "else:\n",
    "    print(\"NOTE: Raw parquet files will be KEPT after processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d095b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Flow for Processing ---\n",
    "\n",
    "def process_and_delete_files():\n",
    "    # 1. Load existing data\n",
    "    if os.path.exists(proc_config.player_stats_output_path):\n",
    "        with open(proc_config.player_stats_output_path, \"rb\") as f:\n",
    "            player_stats: Dict[str, PlayerStats] = pickle.load(f)\n",
    "        print(f\"Loaded {len(player_stats)} players from existing stats file.\")\n",
    "    else:\n",
    "        player_stats: Dict[str, PlayerStats] = {}\n",
    "        print(\"No existing player stats file found. Starting fresh.\")\n",
    "\n",
    "    file_registry = FileRegistry(proc_config.file_registry_path)\n",
    "    print(f\"Loaded {len(file_registry.processed_files)} processed files from registry.\")\n",
    "\n",
    "    # 2. Find files to process\n",
    "    try:\n",
    "        all_files = [f for f in os.listdir(proc_config.input_dir) if f.endswith(\".parquet\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input directory not found: {proc_config.input_dir}\")\n",
    "        return\n",
    "\n",
    "    files_to_process = [f for f in all_files if not file_registry.contains(f)]\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"\\nNo new files to process. Everything is up to date.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(files_to_process)} new parquet file(s) to process.\")\n",
    "\n",
    "    # 3. Process each file\n",
    "    tracker = PerformanceTracker()\n",
    "    total_files_to_process = len(files_to_process)\n",
    "\n",
    "    for i, filename in enumerate(files_to_process):\n",
    "        filepath = os.path.join(proc_config.input_dir, filename)\n",
    "        print(f\"[{i+1}/{total_files_to_process}] Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            file_start_time = time.time()\n",
    "            processed_ok = process_parquet_file(filepath, proc_config, player_stats, tracker)\n",
    "            \n",
    "            if processed_ok:\n",
    "                # Add to registry\n",
    "                file_registry.add(filename)\n",
    "                tracker.record_file()\n",
    "                \n",
    "                # Save cumulative stats\n",
    "                with open(proc_config.player_stats_output_path, \"wb\") as f:\n",
    "                    pickle.dump(player_stats, f)\n",
    "                \n",
    "                file_duration = time.time() - file_start_time\n",
    "                print(f\"  > Success ({file_duration:.1f}s). {tracker.summary(total_files_to_process)}\")\n",
    "\n",
    "                # Delete file if toggle is on\n",
    "                if is_delete_downloaded_parquet_files:\n",
    "                    try:\n",
    "                        os.remove(filepath)\n",
    "                        print(f\"  > DELETED: {filename}\")\n",
    "                    except OSError as e:\n",
    "                        print(f\"  > ERROR: Failed to delete {filename}: {e}\")\n",
    "            else:\n",
    "                print(\"  > SKIPPED: No relevant player data found in file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  > ERROR processing {filename}: {e}\")\n",
    "            print(\"    Skipping this file and continuing...\")\n",
    "            continue\n",
    "            \n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Total players with stats: {len(player_stats)}\")\n",
    "    print(f\"Player stats saved to: {proc_config.player_stats_output_path}\")\n",
    "    print(f\"File registry saved to: {proc_config.file_registry_path}\")\n",
    "\n",
    "# Run the processing and deletion flow\n",
    "process_and_delete_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bd178",
   "metadata": {},
   "source": [
    "# 2. Process Downloaded Files\n",
    "\n",
    "The following cells contain the logic from notebook `03_parquet_performance.ipynb` to process the files we just downloaded.\n",
    "\n",
    "This involves:\n",
    "- Scanning the download directory for all `.parquet` files.\n",
    "- Processing each file to extract player statistics.\n",
    "- Storing the aggregated statistics in a pickle file.\n",
    "- Keeping a registry of processed files to avoid duplicate work.\n",
    "- Deleting the raw parquet file after it has been successfully processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "# --- Processing Configuration ---\n",
    "is_delete_downloaded_parquet_files = True  # <<< IMPORTANT TOGGLE\n",
    "\n",
    "@dataclass\n",
    "class PlayerStats:\n",
    "    # ... (rest of the class definition)\n",
    "    # Using a condensed version for brevity in this example\n",
    "    games_played: int = 0\n",
    "    wins: int = 0\n",
    "    losses: int = 0\n",
    "    draws: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    input_dir: str\n",
    "    player_stats_output_path: str\n",
    "    file_registry_path: str\n",
    "    min_elo: int = 2000\n",
    "    max_elo: int = 4000 # No practical upper limit\n",
    "    max_games_per_file: int = 1_000_000 # Effectively unlimited\n",
    "\n",
    "# Use the output_dir from the download config\n",
    "proc_config = ProcessingConfig(\n",
    "    input_dir=config[\"output_dir\"],\n",
    "    player_stats_output_path=str(project_root / \"data\" / \"processed\" / \"player_stats_parquet.pkl\"),\n",
    "    file_registry_path=str(project_root / \"data\" / \"processed\" / \"file_registry.json\"),\n",
    ")\n",
    "\n",
    "# --- Performance & Progress Tracking ---\n",
    "class PerformanceTracker:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.games_processed = 0\n",
    "        self.files_processed = 0\n",
    "\n",
    "    def record_game_batch(self, count):\n",
    "        self.games_processed += count\n",
    "\n",
    "    def record_file(self):\n",
    "        self.files_processed += 1\n",
    "\n",
    "    def summary(self, total_files):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        games_per_sec = self.games_processed / elapsed if elapsed > 0 else 0\n",
    "        files_remaining = total_files - self.files_processed\n",
    "        time_per_file = elapsed / self.files_processed if self.files_processed > 0 else 0\n",
    "        eta = files_remaining * time_per_file\n",
    "        \n",
    "        return (\n",
    "            f\"Processed {self.games_processed:,} games from {self.files_processed}/{total_files} files. \"\n",
    "            f\"({games_per_sec:,.0f} games/sec). ETA: {eta/60:.1f} mins.\"\n",
    "        )\n",
    "\n",
    "class FileRegistry:  # noqa: F811\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.processed_files: Set[str] = self._load()\n",
    "\n",
    "    def _load(self) -> Set[str]:\n",
    "        if os.path.exists(self.path):\n",
    "            with open(self.path, \"r\") as f:\n",
    "                return set(json.load(f))\n",
    "        return set()\n",
    "\n",
    "    def add(self, filename: str):\n",
    "        self.processed_files.add(filename)\n",
    "        self._save()\n",
    "\n",
    "    def contains(self, filename: str) -> bool:\n",
    "        return filename in self.processed_files\n",
    "\n",
    "    def _save(self):\n",
    "        with open(self.path, \"w\") as f:\n",
    "            json.dump(list(self.processed_files), f, indent=4)\n",
    "\n",
    "print(\"Processing configuration loaded.\")\n",
    "if is_delete_downloaded_parquet_files:\n",
    "    print(\"NOTE: Raw parquet files will be DELETED after successful processing.\")\n",
    "else:\n",
    "    print(\"NOTE: Raw parquet files will be KEPT after processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution Flow for Processing ---\n",
    "\n",
    "def process_and_delete_files():\n",
    "    # 1. Load existing data\n",
    "    if os.path.exists(proc_config.player_stats_output_path):\n",
    "        with open(proc_config.player_stats_output_path, \"rb\") as f:\n",
    "            player_stats: Dict[str, PlayerStats] = pickle.load(f)\n",
    "        print(f\"Loaded {len(player_stats)} players from existing stats file.\")\n",
    "    else:\n",
    "        player_stats: Dict[str, PlayerStats] = {}\n",
    "        print(\"No existing player stats file found. Starting fresh.\")\n",
    "\n",
    "    file_registry = FileRegistry(proc_config.file_registry_path)\n",
    "    print(f\"Loaded {len(file_registry.processed_files)} processed files from registry.\")\n",
    "\n",
    "    # 2. Find files to process\n",
    "    try:\n",
    "        all_files = [f for f in os.listdir(proc_config.input_dir) if f.endswith(\".parquet\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input directory not found: {proc_config.input_dir}\")\n",
    "        return\n",
    "\n",
    "    files_to_process = [f for f in all_files if not file_registry.contains(f)]\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"\\nNo new files to process. Everything is up to date.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(files_to_process)} new parquet file(s) to process.\")\n",
    "\n",
    "    # 3. Process each file\n",
    "    tracker = PerformanceTracker()\n",
    "    total_files_to_process = len(files_to_process)\n",
    "\n",
    "    for i, filename in enumerate(files_to_process):\n",
    "        filepath = os.path.join(proc_config.input_dir, filename)\n",
    "        print(f\"[{i+1}/{total_files_to_process}] Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            file_start_time = time.time()\n",
    "            processed_ok = process_parquet_file(filepath, proc_config, player_stats, tracker)\n",
    "            \n",
    "            if processed_ok:\n",
    "                # Add to registry\n",
    "                file_registry.add(filename)\n",
    "                tracker.record_file()\n",
    "                \n",
    "                # Save cumulative stats\n",
    "                with open(proc_config.player_stats_output_path, \"wb\") as f:\n",
    "                    pickle.dump(player_stats, f)\n",
    "                \n",
    "                file_duration = time.time() - file_start_time\n",
    "                print(f\"  > Success ({file_duration:.1f}s). {tracker.summary(total_files_to_process)}\")\n",
    "\n",
    "                # Delete file if toggle is on\n",
    "                if is_delete_downloaded_parquet_files:\n",
    "                    try:\n",
    "                        os.remove(filepath)\n",
    "                        print(f\"  > DELETED: {filename}\")\n",
    "                    except OSError as e:\n",
    "                        print(f\"  > ERROR: Failed to delete {filename}: {e}\")\n",
    "            else:\n",
    "                print(\"  > SKIPPED: No relevant player data found in file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  > ERROR processing {filename}: {e}\")\n",
    "            print(\"    Skipping this file and continuing...\")\n",
    "            continue\n",
    "            \n",
    "    print(\"\\nProcessing complete.\")\n",
    "    print(f\"Total players with stats: {len(player_stats)}\")\n",
    "    print(f\"Player stats saved to: {proc_config.player_stats_output_path}\")\n",
    "    print(f\"File registry saved to: {proc_config.file_registry_path}\")\n",
    "\n",
    "# Run the processing and deletion flow\n",
    "process_and_delete_files()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
